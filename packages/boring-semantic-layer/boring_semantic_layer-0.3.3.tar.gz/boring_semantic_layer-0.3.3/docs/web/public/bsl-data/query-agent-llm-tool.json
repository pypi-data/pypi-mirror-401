{
  "markdown": "# Query Agent: LLM Tool\n\nLLM tools are Python functions that a language model can call during a conversation. When the model needs data, it invokes a tool, receives the result, and continues reasoning.\n\nThe advantage of this approach is that the LLM can directly execute Ibis-style chained queries\u2014unlike MCP, which requires passing JSON payloads through a separate server.\n\n**Benefits:**\n- No additional server to run\n- Full access to native BSL features without an intermediate DSL\n\n## BSLTools: Framework-Agnostic Tool Layer\n\n`BSLTools` provides tool definitions in OpenAI JSON Schema format (the de facto standard), making it compatible with any LLM provider:\n\n- **OpenAI**: `client.chat.completions.create(tools=bsl.tools)`\n- **LangChain**: `llm.bind_tools(bsl.tools)`\n- **Anthropic**, **PydanticAI**, **AI SDK**, etc.\n\n### Installation\n\n```bash\npip install boring-semantic-layer[agent]\n```\n\n### Usage\n\n```python\nimport json\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom boring_semantic_layer.agents.tools import BSLTools\n\n# Initialize BSLTools with your semantic model\nbsl = BSLTools(\n    model_path=Path(\"flights.yml\"),\n    profile=\"dev\",                        # Profile name (optional)\n    profile_file=Path(\"profiles.yml\"),    # Profile file path (optional)\n    chart_backend=\"plotext\",              # plotext, altair, or plotly\n)\n\n# Use with OpenAI SDK\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": bsl.system_prompt},\n        {\"role\": \"user\", \"content\": \"Show top 5 carriers by flight count\"},\n    ],\n    tools=bsl.tools,\n)\n\n# Execute tool calls\nfor tool_call in response.choices[0].message.tool_calls or []:\n    result = bsl.execute(\n        tool_call.function.name,\n        json.loads(tool_call.function.arguments)\n    )\n    print(result)\n```\n\nSee [YAML Config](/building/yaml) for the semantic model format and [Backend Profiles](/building/profile) for connection setup.\n\n### What BSLTools Provides\n\n| Attribute | Description |\n|-----------|-------------|\n| `bsl.tools` | Tool definitions in OpenAI JSON Schema format |\n| `bsl.system_prompt` | System prompt teaching the LLM how to write BSL queries |\n| `bsl.execute(name, args)` | Execute a tool and return the result |\n\n### Available Tools\n\nThe LLM has access to three tools:\n\n#### `list_models`\n\nLists all available semantic models with their dimensions and measures. Useful when multiple models are loaded and the LLM needs to pick the right one.\n\n#### `query_model`\n\nExecutes a BSL query and returns results. The LLM passes an Ibis-style query string:\n\n```python\nsm.group_by(\"origin\").aggregate(\"flight_count\")\n```\n\n**Parameters:**\n- `query` \u2014 The BSL query string to execute\n- `chart_spec` \u2014 Chart specification (backend, format, and visualization options)\n\n#### `get_documentation`\n\nReturns BSL documentation split into topics (query syntax, methods, charting, etc.). The LLM can explore relevant topics on demand to learn how to construct valid queries and charts.\n\n## LangChain Reference Implementation\n\nFor multi-turn conversations with history management, we provide a LangChain-based agent:\n\n\ud83d\udc49 [`langchain.py`](https://github.com/boringdata/boring-semantic-layer/blob/main/src/boring_semantic_layer/agents/backends/langchain.py)\n\nThis implementation powers the [BSL CLI demo chat](/agents/chat).\n\n### Installation\n\nInstall the agent dependencies plus your LLM provider:\n\n```bash\npip install boring-semantic-layer[agent]\n\n# OpenAI\npip install langchain-openai\n\n# Anthropic\npip install langchain-anthropic\n\n# Google\npip install langchain-google-genai\n```\n\n### Usage\n\n```python\nfrom pathlib import Path\nfrom boring_semantic_layer.agents.backends.langchain import LangChainAgent\n\nagent = LangChainAgent(\n    model_path=Path(\"flights.yaml\"),      # Semantic model YAML\n    llm_model=\"gpt-4o\",                   # LLM to use\n    chart_backend=\"plotext\",              # plotext, altair, or plotly\n    profile=\"dev\",                        # Profile name (optional)\n    profile_file=Path(\"profiles.yml\"),    # Profile file path (optional)\n)\n\ntool_output, response = agent.query(\"What are the top 10 origins by flight count?\")\n```\n",
  "queries": {},
  "files": {}
}
