# UnifyLLM

**ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹æ¥å£è°ƒç”¨æ¡†æ¶**

UnifyLLM æ˜¯ä¸€ä¸ª Python æ¡†æ¶ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹ APIã€‚é€šè¿‡ UnifyLLMï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä»£ç è°ƒç”¨ OpenAIã€Anthropicã€Google Geminiã€Ollama ç­‰ä¸åŒçš„ LLM æä¾›å•†ã€‚

## æ ¸å¿ƒç‰¹æ€§

### LLM è°ƒç”¨
- âœ… **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨ç›¸åŒçš„è°ƒç”¨æ–¹å¼
- âœ… **å¤šæä¾›å•†æ”¯æŒ**: OpenAI, Anthropic (Claude), Google Gemini, Ollama, Databricks
- âœ… **æµå¼/éæµå¼**: æ”¯æŒä¸¤ç§å“åº”æ¨¡å¼
- âœ… **åŒæ­¥/å¼‚æ­¥**: å®Œæ•´æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥è°ƒç”¨
- âœ… **é”™è¯¯å¤„ç†**: ç»Ÿä¸€çš„å¼‚å¸¸ç±»å‹å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶
- âœ… **ç±»å‹æç¤º**: å®Œæ•´çš„ Python type hints

### AI Agent æ¡†æ¶ (ğŸ†• New!)
- ğŸ¤– **æ™ºèƒ½ä»£ç†**: æ”¯æŒå·¥å…·è°ƒç”¨çš„è‡ªä¸» AI ä»£ç†
- ğŸ”§ **å·¥å…·ç³»ç»Ÿ**: çµæ´»çš„å·¥å…·æ³¨å†Œå’Œæ‰§è¡Œæœºåˆ¶
- ğŸ§  **è®°å¿†ç®¡ç†**: å¯¹è¯å†å²å’Œå…±äº«å†…å­˜æ”¯æŒ
- ğŸ”„ **å·¥ä½œæµç¼–æ’**: å¤šä»£ç†åä½œå’Œå·¥ä½œæµè‡ªåŠ¨åŒ–
- ğŸ“Š **å¤šç§æ¨¡å¼**: æ”¯æŒé¡ºåºã€å¹¶è¡Œã€æ¡ä»¶åˆ†æ”¯ç­‰æ‰§è¡Œæ¨¡å¼
- ğŸ¯ **ç±»ä¼¼ n8n**: å€Ÿé‰´ n8n çš„è®¾è®¡ç†å¿µï¼Œæä¾›å¼ºå¤§çš„è‡ªåŠ¨åŒ–èƒ½åŠ›

### MCP & A2A åè®® (ğŸ”¥ Latest!)
- ğŸŒ **MCP (Model Context Protocol)**: Anthropic çš„å¼€æ”¾åè®®ï¼Œæš´éœ²å·¥å…·ã€èµ„æºå’Œæç¤º
- ğŸ¤ **A2A (Agent-to-Agent)**: å¤šä»£ç†é€šä¿¡å’Œåä½œåè®®
- ğŸ” **ä»£ç†å‘ç°**: è‡ªåŠ¨å‘ç°å’Œè¿æ¥å…·æœ‰ç‰¹å®šèƒ½åŠ›çš„ä»£ç†
- ğŸ“¡ **ä»»åŠ¡å§”æ‰˜**: æ™ºèƒ½ä»»åŠ¡åˆ†é…å’Œæ‰§è¡Œ
- ğŸ—³ï¸ **å…±è¯†æœºåˆ¶**: å¤šä»£ç†æŠ•ç¥¨å’Œå†³ç­–
- ğŸš€ **Databricks æ”¯æŒ**: å®Œæ•´æ”¯æŒ Claude Opus 4.5 æµ‹è¯•

## å®‰è£…

```bash
pip install unify-llm
```

æˆ–ä»æºç å®‰è£…ï¼š

```bash
git clone https://github.com/yourusername/unify-llm.git
cd unify-llm
pip install -e .
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
from unify_llm import UnifyLLM

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = UnifyLLM(
    provider="openai",
    api_key="your-api-key"
)

# å‘é€æ¶ˆæ¯
response = client.chat(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Hello, how are you?"}
    ]
)

print(response.content)
```

### æµå¼å“åº”

```python
for chunk in client.chat_stream(
    model="gpt-4",
    messages=[{"role": "user", "content": "Tell me a story"}]
):
    print(chunk.content, end="", flush=True)
```

### å¼‚æ­¥è°ƒç”¨

```python
import asyncio

async def main():
    response = await client.achat(
        model="gpt-4",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    print(response.content)

asyncio.run(main())
```

## æ”¯æŒçš„æä¾›å•†

### OpenAI

```python
client = UnifyLLM(
    provider="openai",
    api_key="sk-..."
)

response = client.chat(
    model="gpt-4",  # æˆ– "gpt-3.5-turbo"
    messages=[{"role": "user", "content": "Hello"}]
)
```

**æ”¯æŒçš„æ¨¡å‹**: `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`, ç­‰

### Anthropic (Claude)

```python
client = UnifyLLM(
    provider="anthropic",
    api_key="sk-ant-..."
)

response = client.chat(
    model="claude-3-opus-20240229",
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=1000  # Anthropic è¦æ±‚è®¾ç½® max_tokens
)
```

**æ”¯æŒçš„æ¨¡å‹**: `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307`, ç­‰

### Google Gemini

```python
client = UnifyLLM(
    provider="gemini",
    api_key="your-gemini-api-key"
)

response = client.chat(
    model="gemini-pro",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**æ”¯æŒçš„æ¨¡å‹**: `gemini-pro`, `gemini-pro-vision`, ç­‰

### Ollama (æœ¬åœ°æ¨¡å‹)

```python
client = UnifyLLM(
    provider="ollama",
    base_url="http://localhost:11434"  # é»˜è®¤å€¼
)

response = client.chat(
    model="llama2",  # æˆ–å…¶ä»–å·²å®‰è£…çš„æ¨¡å‹
    messages=[{"role": "user", "content": "Hello"}]
)
```

**æ”¯æŒçš„æ¨¡å‹**: ä»»ä½•é€šè¿‡ Ollama å®‰è£…çš„æ¨¡å‹ (llama2, mistral, phi, ç­‰)

### Databricks

Databricks æä¾› OpenAI å…¼å®¹çš„ API ç«¯ç‚¹ï¼Œç”¨äºéƒ¨ç½²å’Œè°ƒç”¨æ¨¡å‹ã€‚

```python
# æ–¹å¼1: è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å– (æ¨è)
client = UnifyLLM(provider="databricks")

# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®š
client = UnifyLLM(
    provider="databricks",
    api_key="dapi...",
    base_url="https://your-workspace.cloud.databricks.com/serving-endpoints"
)

response = client.chat(
    model="your-endpoint-name",  # Databricks serving endpoint åç§°
    messages=[{"role": "user", "content": "Hello"}]
)
```

**ç¯å¢ƒå˜é‡é…ç½®**:
- `DATABRICKS_API_KEY`: Databricks ä¸ªäººè®¿é—®ä»¤ç‰Œ
- `DATABRICKS_BASE_URL`: Databricks serving endpoint çš„åŸºç¡€ URL

**æ”¯æŒçš„æ¨¡å‹**: ä»»ä½•åœ¨ Databricks ä¸Šéƒ¨ç½²çš„æ¨¡å‹ (DBRX, Llama, Mixtral, ç­‰)

## API æ–‡æ¡£

### UnifyLLM ç±»

#### åˆå§‹åŒ–å‚æ•°

- `provider` (str): æä¾›å•†åç§° ("openai", "anthropic", "gemini", "ollama", "databricks")
- `api_key` (str, optional): API å¯†é’¥
- `base_url` (str, optional): è‡ªå®šä¹‰ API ç«¯ç‚¹
- `timeout` (float, optional): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 60
- `max_retries` (int, optional): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 3
- `organization` (str, optional): ç»„ç»‡ IDï¼ˆä»…éƒ¨åˆ†æä¾›å•†æ”¯æŒï¼‰
- `extra_headers` (dict, optional): é¢å¤–çš„ HTTP è¯·æ±‚å¤´

#### chat() æ–¹æ³•

åŒæ­¥èŠå¤©è¯·æ±‚ã€‚

```python
response = client.chat(
    model: str,                          # æ¨¡å‹åç§°
    messages: List[Dict],                # æ¶ˆæ¯åˆ—è¡¨
    temperature: float = None,           # æ¸©åº¦å‚æ•° (0.0-2.0)
    max_tokens: int = None,              # æœ€å¤§ç”Ÿæˆ token æ•°
    top_p: float = None,                 # Top-p é‡‡æ ·
    frequency_penalty: float = None,     # é¢‘ç‡æƒ©ç½š
    presence_penalty: float = None,      # å­˜åœ¨æƒ©ç½š
    stop: Union[str, List[str]] = None,  # åœæ­¢åºåˆ—
    **extra_params                       # æä¾›å•†ç‰¹å®šå‚æ•°
)
```

**è¿”å›**: `ChatResponse` å¯¹è±¡

#### chat_stream() æ–¹æ³•

åŒæ­¥æµå¼èŠå¤©è¯·æ±‚ã€‚

```python
for chunk in client.chat_stream(
    model: str,
    messages: List[Dict],
    **params  # åŒ chat() æ–¹æ³•
):
    print(chunk.content, end="")
```

**è¿”å›**: `Iterator[StreamChunk]`

#### achat() æ–¹æ³•

å¼‚æ­¥èŠå¤©è¯·æ±‚ï¼ˆå‚æ•°åŒ `chat()`ï¼‰ã€‚

```python
response = await client.achat(...)
```

**è¿”å›**: `ChatResponse` å¯¹è±¡

#### achat_stream() æ–¹æ³•

å¼‚æ­¥æµå¼èŠå¤©è¯·æ±‚ï¼ˆå‚æ•°åŒ `chat()`ï¼‰ã€‚

```python
async for chunk in client.achat_stream(...):
    print(chunk.content, end="")
```

**è¿”å›**: `AsyncIterator[StreamChunk]`

### æ•°æ®æ¨¡å‹

#### Message

```python
class Message:
    role: str                           # "system", "user", "assistant"
    content: str                        # æ¶ˆæ¯å†…å®¹
    name: Optional[str] = None          # å‘é€è€…åç§°
```

#### ChatResponse

```python
class ChatResponse:
    id: str                             # å“åº” ID
    model: str                          # ä½¿ç”¨çš„æ¨¡å‹
    choices: List[ChatResponseChoice]   # ç”Ÿæˆçš„é€‰é¡¹
    usage: Usage                        # Token ä½¿ç”¨æƒ…å†µ
    created: int                        # åˆ›å»ºæ—¶é—´æˆ³
    provider: str                       # æä¾›å•†åç§°

    # ä¾¿æ·å±æ€§
    @property
    def content(self) -> str:           # ç¬¬ä¸€ä¸ªé€‰é¡¹çš„å†…å®¹
        ...

    @property
    def finish_reason(self) -> str:     # å®ŒæˆåŸå› 
        ...
```

#### StreamChunk

```python
class StreamChunk:
    id: str                             # æµ ID
    model: str                          # ä½¿ç”¨çš„æ¨¡å‹
    choices: List[StreamChoiceDelta]    # å¢é‡æ›´æ–°
    created: int                        # åˆ›å»ºæ—¶é—´æˆ³
    provider: str                       # æä¾›å•†åç§°

    # ä¾¿æ·å±æ€§
    @property
    def content(self) -> str:           # å†…å®¹å¢é‡
        ...

    @property
    def finish_reason(self) -> str:     # å®ŒæˆåŸå› 
        ...
```

#### Usage

```python
class Usage:
    prompt_tokens: int                  # æç¤ºè¯ token æ•°
    completion_tokens: int              # ç”Ÿæˆ token æ•°
    total_tokens: int                   # æ€» token æ•°
```

### å¼‚å¸¸ç±»å‹

æ‰€æœ‰å¼‚å¸¸éƒ½ç»§æ‰¿è‡ª `UnifyLLMError`ã€‚

- `AuthenticationError`: è®¤è¯å¤±è´¥
- `RateLimitError`: é€Ÿç‡é™åˆ¶
- `InvalidRequestError`: æ— æ•ˆè¯·æ±‚
- `APIError`: API é”™è¯¯
- `TimeoutError`: è¯·æ±‚è¶…æ—¶
- `ModelNotFoundError`: æ¨¡å‹æœªæ‰¾åˆ°
- `ContentFilterError`: å†…å®¹è¢«è¿‡æ»¤

```python
from unify_llm import UnifyLLM, AuthenticationError

try:
    response = client.chat(...)
except AuthenticationError as e:
    print(f"è®¤è¯å¤±è´¥: {e}")
```

## é«˜çº§ç”¨æ³•

### ç¯å¢ƒå˜é‡é…ç½®

UnifyLLM æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥ï¼š

```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
```

```python
# API å¯†é’¥ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
client = UnifyLLM(provider="openai")
```

### è‡ªå®šä¹‰æä¾›å•†

æ‚¨å¯ä»¥æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†ï¼š

```python
from unify_llm import UnifyLLM
from unify_llm.providers import BaseProvider


class MyCustomProvider(BaseProvider):
    # å®ç°å¿…è¦çš„æŠ½è±¡æ–¹æ³•
    def _get_headers(self):
        ...

    def _convert_request(self, request):
        ...

    # ... å…¶ä»–æ–¹æ³• ...


# æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
UnifyLLM.register_provider("custom", MyCustomProvider)

# ä½¿ç”¨è‡ªå®šä¹‰æä¾›å•†
client = UnifyLLM(provider="custom", api_key="...")
```

### å¹¶å‘è¯·æ±‚

```python
import asyncio

async def concurrent_requests():
    client = UnifyLLM(provider="openai", api_key="...")

    tasks = [
        client.achat(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": f"Tell me about {topic}"}]
        )
        for topic in ["Python", "JavaScript", "Rust"]
    ]

    responses = await asyncio.gather(*tasks)

    for response in responses:
        print(response.content)

asyncio.run(concurrent_requests())
```

### è‡ªå®šä¹‰è¶…æ—¶å’Œé‡è¯•

```python
client = UnifyLLM(
    provider="openai",
    api_key="...",
    timeout=120.0,      # 120 ç§’è¶…æ—¶
    max_retries=5       # æœ€å¤šé‡è¯• 5 æ¬¡
)
```

## ç¤ºä¾‹é¡¹ç›®

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹ï¼š

- `basic_usage.py`: åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹
- `streaming.py`: æµå¼å“åº”ç¤ºä¾‹
- `async_usage.py`: å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹
- `multi_provider.py`: å¤šæä¾›å•†å¯¹æ¯”ç¤ºä¾‹

## å¼€å‘

### å®‰è£…å¼€å‘ä¾èµ–

```bash
pip install -e ".[dev]"
```

### è¿è¡Œæµ‹è¯•

```bash
pytest
```

### ä»£ç æ ¼å¼åŒ–

```bash
black unify_llm tests
ruff check unify_llm tests
```

### ç±»å‹æ£€æŸ¥

```bash
mypy unify_llm
```

## AI Agent åŠŸèƒ½ (ğŸ†• New!)

UnifyLLM ç°åœ¨æ”¯æŒå¼ºå¤§çš„ AI Agent åŠŸèƒ½ï¼Œçµæ„Ÿæ¥è‡ª n8n çš„å·¥ä½œæµè‡ªåŠ¨åŒ–è®¾è®¡ï¼

### å¿«é€Ÿå¼€å§‹ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„ Agent

```python
from unify_llm import UnifyLLM
from unify_llm.agent import Agent, AgentConfig, AgentExecutor, ToolRegistry
from unify_llm.agent.builtin_tools import create_calculator_tool

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = UnifyLLM(provider="openai", api_key="sk-...")

# åˆ›å»ºå·¥å…·æ³¨å†Œè¡¨
registry = ToolRegistry()
registry.register(create_calculator_tool())

# é…ç½® Agent
config = AgentConfig(
    name="math_assistant",
    model="gpt-4",
    provider="openai",
    system_prompt="You are a helpful math assistant.",
    tools=["calculator"]
)

# åˆ›å»ºå¹¶è¿è¡Œ Agent
agent = Agent(config=config, client=client)
executor = AgentExecutor(agent=agent, tool_registry=registry)

result = executor.run("What is 15 * 23 + 100?")
print(result.output)
```

### æ ¸å¿ƒç»„ä»¶

#### 1. Agent (æ™ºèƒ½ä»£ç†)
- æ”¯æŒå·¥å…·é€‰æ‹©å’Œè°ƒç”¨
- å¯¹è¯è®°å¿†ç®¡ç†
- å¤šç§ä»£ç†ç±»å‹ï¼ˆå·¥å…·å‹ã€å¯¹è¯å‹ã€è·¯ç”±å‹ã€åˆ†å±‚å‹ï¼‰

#### 2. Tools (å·¥å…·ç³»ç»Ÿ)
- çµæ´»çš„å·¥å…·å®šä¹‰
- è‡ªåŠ¨å‚æ•°æ£€æµ‹
- æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥æ‰§è¡Œ
- å†…ç½®å·¥å…·ï¼šè®¡ç®—å™¨ã€å­—ç¬¦ä¸²å¤„ç†ã€æ•°æ®æ ¼å¼åŒ–ç­‰

```python
from unify_llm.agent import Tool, ToolParameter, ToolParameterType, ToolResult


def my_custom_tool(param1: str, param2: int) -> ToolResult:
    # è‡ªå®šä¹‰é€»è¾‘
    result = f"Processing {param1} with {param2}"
    return ToolResult(success=True, output=result)


# æ³¨å†Œå·¥å…·
registry.register_function(
    name="my_tool",
    description="My custom tool",
    function=my_custom_tool
)
```

#### 3. Memory (è®°å¿†ç³»ç»Ÿ)
- **ConversationMemory**: å¯¹è¯å†å²ç®¡ç†
- **SharedMemory**: å¤šä»£ç†å…±äº«å†…å­˜

```python
from unify_llm.agent import ConversationMemory

memory = ConversationMemory(window_size=10)
memory.add_user_message("Hello!")
memory.add_assistant_message("Hi! How can I help?")
```

#### 4. Workflow (å·¥ä½œæµç¼–æ’)
æ”¯æŒå¤šä»£ç†åä½œçš„å¤æ‚å·¥ä½œæµï¼š

```python
from unify_llm.agent import Workflow, WorkflowConfig, WorkflowNode, NodeType

# å®šä¹‰å·¥ä½œæµï¼šç ”ç©¶ -> åˆ†æ -> æ’°å†™
workflow_config = WorkflowConfig(
    name="research_workflow",
    description="Research, analyze, and write",
    start_node="research",
    nodes=[
        WorkflowNode(
            id="research",
            type=NodeType.AGENT,
            name="Research",
            agent_name="researcher",
            next_nodes=["analyze"]
        ),
        WorkflowNode(
            id="analyze",
            type=NodeType.AGENT,
            name="Analyze",
            agent_name="analyst",
            next_nodes=["write"]
        ),
        WorkflowNode(
            id="write",
            type=NodeType.AGENT,
            name="Write",
            agent_name="writer",
            next_nodes=[]
        )
    ]
)

# åˆ›å»ºå¹¶è¿è¡Œå·¥ä½œæµ
workflow = Workflow(
    config=workflow_config,
    agents={"researcher": researcher, "analyst": analyst, "writer": writer}
)

result = workflow.run("Explain quantum computing")
```

### Agent æ¶æ„æ¨¡å¼

#### 1. Tools-Based Agent (åŸºäºå·¥å…·çš„ä»£ç†)
ä»£ç†è‡ªä¸»é€‰æ‹©å’Œä½¿ç”¨å·¥å…·ï¼š
```python
config = AgentConfig(
    name="assistant",
    agent_type=AgentType.TOOLS,
    tools=["search", "calculator", "email"]
)
```

#### 2. Router Agent (è·¯ç”±ä»£ç†)
æ ¹æ®æ¡ä»¶åˆ†å‘ä»»åŠ¡ï¼š
```python
WorkflowNode(
    id="router",
    type=NodeType.CONDITION,
    condition=lambda results, mem: check_condition(results)
)
```

#### 3. Hierarchical Agent (åˆ†å±‚ä»£ç†)
ä¸»ä»£ç†ç®¡ç†å¤šä¸ªå­ä»£ç†ï¼š
```python
# ä¸»ä»£ç†åè°ƒå¤šä¸ªä¸“ä¸šä»£ç†
agents = {
    "manager": manager_agent,
    "researcher": research_agent,
    "writer": writer_agent,
    "reviewer": review_agent
}
```

#### 4. Human-in-the-Loop (äººæœºåä½œ)
åœ¨å…³é”®èŠ‚ç‚¹éœ€è¦äººå·¥å¹²é¢„ï¼š
```python
WorkflowNode(
    id="approval",
    type=NodeType.HUMAN_IN_LOOP,
    name="Human Approval"
)
```

### å†…ç½®å·¥å…·

UnifyLLM æä¾›äº†å¤šç§å¼€ç®±å³ç”¨çš„å·¥å…·ï¼š

**åŸºç¡€å·¥å…·**:
- **calculator**: æ•°å­¦è®¡ç®—ï¼ˆæ”¯æŒåŸºæœ¬è¿ç®—å’Œå‡½æ•°ï¼‰
- **to_uppercase/to_lowercase**: å­—ç¬¦ä¸²å¤§å°å†™è½¬æ¢
- **reverse_string**: å­—ç¬¦ä¸²åè½¬
- **count_words**: è¯æ•°ç»Ÿè®¡
- **format_data**: æ•°æ®æ ¼å¼åŒ–ï¼ˆJSONã€YAMLã€è¡¨æ ¼ï¼‰

**æ‰©å±•å·¥å…·**:
- **æ—¥æœŸæ—¶é—´å·¥å…·**: get_current_datetime, calculate_date_diff, add_time_to_date
- **æ–‡æœ¬åˆ†æå·¥å…·**: extract_emails, extract_urls, extract_numbers, analyze_text_stats
- **æ–‡ä»¶æ“ä½œå·¥å…·**: read_text_file, write_text_file, list_directory
- **JSON å·¥å…·**: parse_json, stringify_json, extract_json_field

### Agent æ¨¡æ¿

ä½¿ç”¨é¢„é…ç½®çš„ Agent æ¨¡æ¿å¿«é€Ÿå¼€å§‹ï¼š

```python
from unify_llm.agent import AgentTemplates, Agent

# ç ”ç©¶åŠ©æ‰‹
config = AgentTemplates.research_assistant()

# ä»£ç åŠ©æ‰‹
config = AgentTemplates.code_assistant()

# æ•°æ®åˆ†æå¸ˆ
config = AgentTemplates.data_analyst()

# å†…å®¹ä½œå®¶
config = AgentTemplates.content_writer()

# å®¢æœä»£è¡¨
config = AgentTemplates.customer_support()

# ä»»åŠ¡è§„åˆ’å¸ˆ
config = AgentTemplates.task_planner()
```

### é«˜çº§åŠŸèƒ½

#### 1. å¹¶è¡Œæ‰§è¡Œ

```python
from unify_llm.agent import ParallelExecutor

parallel = ParallelExecutor(max_workers=3)
results = parallel.execute_parallel(
    agents=[agent1, agent2, agent3],
    executors=[exec1, exec2, exec3],
    inputs=["task 1", "task 2", "task 3"]
)
```

#### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from unify_llm.agent import ErrorHandler

handler = ErrorHandler(max_retries=3, backoff_factor=2.0)
result = handler.execute_with_retry(
    executor=executor,
    user_input="task",
    on_error=lambda e: print(f"Error: {e}")
)
```

#### 3. Agent é“¾å¼è°ƒç”¨

```python
from unify_llm.agent import AgentChain

chain = AgentChain()
chain.add_agent(researcher, researcher_exec)
chain.add_agent(analyst, analyst_exec, transform=lambda x: f"Analyze: {x}")
chain.add_agent(writer, writer_exec, transform=lambda x: f"Write about: {x}")

result = chain.execute("Research AI trends")
```

#### 4. å·¥ä½œæµå¯è§†åŒ–

```python
from unify_llm.agent import WorkflowVisualizer

viz = WorkflowVisualizer(workflow)

# ASCII å›¾
print(viz.to_ascii())

# Mermaid å›¾ï¼ˆå¯åœ¨ GitHub/Markdown ä¸­æ¸²æŸ“ï¼‰
print(viz.to_mermaid())

# JSON å¯¼å‡º
print(viz.to_json())
```

#### 5. æ€§èƒ½ç›‘æ§

```python
from unify_llm.agent import PerformanceMonitor

monitor = PerformanceMonitor()

# è·Ÿè¸ªæ‰§è¡Œ
with monitor.track("my_agent"):
    result = executor.run("task")

# è®°å½•ç»“æœ
monitor.record_result("my_agent", result)

# æŸ¥çœ‹æŒ‡æ ‡
monitor.print_summary()
```

### å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹ï¼š

- `agent_basic.py`: åŸºç¡€ Agent ä½¿ç”¨
- `agent_workflow.py`: å¤šä»£ç†å·¥ä½œæµ
- `agent_custom_tools.py`: è‡ªå®šä¹‰å·¥å…·
- `agent_advanced.py`: é«˜çº§åŠŸèƒ½ï¼ˆå¹¶è¡Œã€é”™è¯¯å¤„ç†ã€é“¾å¼è°ƒç”¨ï¼‰

## MCP & A2A åè®®åŠŸèƒ½ (ğŸ”¥ Latest!)

UnifyLLM ç°åœ¨æ”¯æŒ MCP (Model Context Protocol) å’Œ A2A (Agent-to-Agent) åè®®ï¼

### MCP Protocol - æš´éœ²ä»£ç†èƒ½åŠ›

ä½¿ç”¨ MCP å°†ä»£ç†çš„å·¥å…·ã€èµ„æºå’Œæç¤ºæš´éœ²ä¸ºæ ‡å‡†åŒ–æœåŠ¡ï¼š

```python
from unify_llm.mcp import MCPServer, MCPServerConfig

# åˆ›å»º MCP æœåŠ¡å™¨
server = MCPServer(MCPServerConfig(server_name="my-agent"))


# æ³¨å†Œå·¥å…·
@server.tool("calculator", "æ‰§è¡Œæ•°å­¦è®¡ç®—")
async def calculator(expression: str) -> dict:
    return {"result": eval(expression)}


# æ³¨å†Œèµ„æº
@server.resource("file://data.json", "application/json", "æ•°æ®èµ„æº")
async def get_data() -> str:
    return '{"data": "value"}'


# æ³¨å†Œæç¤ºæ¨¡æ¿
@server.prompt("greeting", "ç”Ÿæˆé—®å€™è¯­")
async def greeting_prompt(name: str) -> dict:
    return {
        "messages": [
            {"role": "user", "content": f"Say hello to {name}"}
        ]
    }
```

### A2A Protocol - ä»£ç†é—´é€šä¿¡

ä½¿ç”¨ A2A åè®®è®©å¤šä¸ªä»£ç†ç›¸äº’é€šä¿¡å’Œåä½œï¼š

```python
from unify_llm import UnifyLLM
from unify_llm.agent import Agent, AgentConfig
from unify_llm.a2a import A2AAgent, A2AAgentConfig, AgentCapability, AgentRegistry

# åˆ›å»ºå…±äº«æ³¨å†Œè¡¨
registry = AgentRegistry()

# åˆ›å»º A2A ä»£ç†
client = UnifyLLM(provider="databricks", api_key="...", base_url="...")
base_agent = Agent(
    config=AgentConfig(name="math_expert", model="claude-opus-4-5", provider="databricks"),
    client=client
)

a2a_agent = A2AAgent(
    base_agent=base_agent,
    config=A2AAgentConfig(
        agent_name="math_expert",
        capabilities=[
            AgentCapability(
                name="solve_math",
                description="è§£å†³æ•°å­¦é—®é¢˜",
                input_schema={"type": "object", "properties": {"problem": {"type": "string"}}},
                output_schema={"type": "object", "properties": {"solution": {"type": "string"}}}
            )
        ]
    ),
    registry=registry
)

await a2a_agent.start()

# å‘ç°å…¶ä»–ä»£ç†
from unify_llm.a2a import AgentDiscovery

discovery = AgentDiscovery(registry)
agents = await discovery.discover(capabilities=["solve_math"])

# å§”æ‰˜ä»»åŠ¡
result = await a2a_agent.delegate_task(
    target_agent_id=other_agent.agent_id,
    capability="solve_math",
    input_data={"problem": "What is 15 * 23?"}
)
```

### å¤šä»£ç†åä½œ

ä½¿ç”¨ä¸åŒç­–ç•¥è¿›è¡Œå¤šä»£ç†åä½œï¼š

```python
from unify_llm.a2a import AgentCollaboration, CollaborationStrategy

# é¡ºåºåä½œ (sequential)
collab = AgentCollaboration(strategy=CollaborationStrategy.SEQUENTIAL)
collab.add_agent(researcher)
collab.add_agent(analyst)
collab.add_agent(writer)
result = await collab.execute({"task": "create_report", "data": {...}})

# å¹¶è¡Œåä½œ (parallel)
collab = AgentCollaboration(strategy=CollaborationStrategy.PARALLEL)
collab.add_agent(agent1)
collab.add_agent(agent2)
result = await collab.execute({"task": "parallel_analysis", "data": {...}})

# å…±è¯†åä½œ (consensus)
collab = AgentCollaboration(strategy=CollaborationStrategy.CONSENSUS)
collab.add_agent(expert1)
collab.add_agent(expert2)
collab.add_agent(expert3)
result = await collab.execute({
    "task": "make_decision",
    "data": {"question": "Should we proceed?"},
    "voting_method": "majority"
})
```

### ä½¿ç”¨ Databricks Claude Opus 4.5 æµ‹è¯•

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export DATABRICKS_API_KEY="dapi..."
export DATABRICKS_BASE_URL="https://your-workspace.cloud.databricks.com"

# è¿è¡Œæµ‹è¯•
python tests/test_mcp_a2a_databricks.py
```

æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ï¼š
- [MCP & A2A å®Œæ•´æŒ‡å—](docs/MCP_A2A_GUIDE.md)
- [Databricks ä»£ç†æµ‹è¯•](docs/DATABRICKS_AGENT_GUIDE.md)

## é¡¹ç›®å®šä½

UnifyLLM ç°åœ¨æä¾›ï¼š

âœ… **ç»Ÿä¸€çš„ LLM API è°ƒç”¨æ¥å£**
âœ… **AI Agent æ¡†æ¶**ï¼ˆå·¥å…·è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€å·¥ä½œæµç¼–æ’ï¼‰
âœ… **å¤šä»£ç†åä½œ**
âœ… **MCP Protocol** (Model Context Protocol) - æ ‡å‡†åŒ–å·¥å…·æš´éœ²
âœ… **A2A Protocol** (Agent-to-Agent) - ä»£ç†é—´é€šä¿¡å’Œåä½œ
âœ… **Databricks æ”¯æŒ** - Claude Opus 4.5 å®Œæ•´é›†æˆ

**ä¸æ¶‰åŠ**ï¼š
- âŒ å‘é‡æ•°æ®åº“
- âŒ æ–‡æ¡£åŠ è½½å™¨
- âŒ RAG ç‰¹å®šå®ç°ï¼ˆä½†æä¾›æ„å»º RAG çš„åŸºç¡€ï¼‰

å¦‚æœæ‚¨éœ€è¦æ›´é«˜çº§çš„ RAG åŠŸèƒ½æˆ–å‘é‡æ•°æ®åº“é›†æˆï¼Œå¯ä»¥è€ƒè™‘ä¸ LangChain æˆ– LlamaIndex ç»“åˆä½¿ç”¨ã€‚

## è·¯çº¿å›¾

- [x] æ”¯æŒ MCP (Model Context Protocol)
- [x] æ”¯æŒ A2A (Agent-to-Agent Protocol)
- [x] Databricks Claude Opus 4.5 é›†æˆ
- [ ] æ”¯æŒæ›´å¤šå›½å†…æ¨¡å‹ï¼ˆæ™ºè°±ã€æ–‡å¿ƒã€é€šä¹‰åƒé—®ç­‰ï¼‰
- [ ] æ”¯æŒ vLLM
- [ ] å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰æ”¯æŒä¼˜åŒ–
- [ ] æ‰¹é‡è¯·æ±‚ä¼˜åŒ–
- [ ] æ›´è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] MCP æœåŠ¡å™¨æ³¨å†Œä¸­å¿ƒ
- [ ] A2A æ¶ˆæ¯æ€»çº¿å®ç°

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## è‡´è°¢

UnifyLLM å—åˆ°ä»¥ä¸‹é¡¹ç›®çš„å¯å‘ï¼š

- [OpenAI Python SDK](https://github.com/openai/openai-python)
- [LiteLLM](https://github.com/BerriAI/litellm)
- [LangChain](https://github.com/langchain-ai/langchain)

---

**Star è¿™ä¸ªé¡¹ç›®** å¦‚æœæ‚¨è§‰å¾—å®ƒæœ‰ç”¨ï¼
