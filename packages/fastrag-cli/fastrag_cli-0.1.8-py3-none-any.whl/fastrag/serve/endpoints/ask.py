import json
import time

from fastapi import APIRouter, Depends, Path, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from fastrag.serve.db import get_chat_repository, init_db
from fastrag.serve.geolocalization import get_country_from_ip
from fastrag.serve.monitoring.metrics import (
    http_request_errors_total,
    http_requests_in_flight,
    http_requests_total,
    llm_answer_length,
    llm_question_length,
    llm_time_to_first_token,
    llm_time_to_last_token,
    requests_per_ip_total,
)
from fastrag.serve.rate_limiter import limiter

from ..dependencies import (
    get_config,
    get_embedding_model,
    get_llm,
    get_vector_store,
)

router = APIRouter()
init_db()  # Ensure DB is initialized on import
chat_repo = get_chat_repository()


class QuestionRequest(BaseModel):
    question: str


def build_prompt(context: str, question: str) -> str:
    return f"""
    You are a helpful assistant and expert in data spaces.

    Always use inline references in the form [<NUMBER OF DOCUMENT>](ref:<NUMBER OF DOCUMENT>)
    ONLY if you use information from a document. For example, if you use the information from
    Document[3], you should write [3](ref:3) at the end of the sentence where you used that
    information.
    Give a precise, accurate and structured answer without repeating the question.
    
    These are the documents:
    {context}

    Question:
    {question}

    Answer:
    """.strip()


@router.post("/ask/{chat_id}")
@limiter.limit("5/minute")
async def ask_question(
    chat_id: str = Path(..., description="Chat session id generated by the client"),
    request: Request = None,
    req: QuestionRequest = None,
    vector_store=Depends(get_vector_store),
    llm=Depends(get_llm),
    config=Depends(get_config),
    embedding_model=Depends(get_embedding_model),
) -> StreamingResponse:
    path = "/ask/{chat_id}"
    method = "POST"
    client_ip = request.client.host

    http_requests_in_flight.labels(path).inc()
    requests_per_ip_total.labels(client_ip).inc()
    llm_question_length.observe(len(req.question))

    # Get country from IP
    country = get_country_from_ip(client_ip)

    # Save user message (this will create the chat with IP and country if it doesn't exist)
    chat_repo.save_message(
        chat_id=chat_id, content=req.question, role="user", ip=client_ip, country=country
    )

    query_embedding = await embedding_model.embed_query(req.question)

    results = await vector_store.similarity_search(
        query=req.question, query_embedding=query_embedding, k=5
    )
    context_parts = [f"Document[{i}]: {doc.page_content}" for i, doc in enumerate(results)]
    context = "\n\n".join(context_parts)
    sources_metadata = [doc.metadata.get("source") for doc in results]

    start_time = time.monotonic()
    first_token_time = None

    answer = ""

    try:

        async def generate():
            yield f"data: {json.dumps({'type': 'sources', 'data': sources_metadata})}\n\n"
            nonlocal first_token_time, answer
            prompt = build_prompt(context, req.question)

            async for token in llm.stream(prompt):
                if first_token_time is None:
                    first_token_time = time.monotonic()
                    llm_time_to_first_token.observe(first_token_time - start_time)
                answer += token
                yield f"data: {json.dumps({'type': 'token', 'data': token})}\n\n"
            chat_repo.save_message(
                chat_id=chat_id, content=answer, role="assistant", sources=sources_metadata
            )

        return StreamingResponse(
            content=generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )
    except Exception as e:
        http_request_errors_total.labels(path, type(e).__name__).inc()
        raise
    finally:
        http_requests_in_flight.labels(path).dec()

        if first_token_time:
            llm_time_to_last_token.observe(time.monotonic() - start_time)
            llm_answer_length.observe(len(answer))

        http_requests_total.labels(method, path, "200").inc()
