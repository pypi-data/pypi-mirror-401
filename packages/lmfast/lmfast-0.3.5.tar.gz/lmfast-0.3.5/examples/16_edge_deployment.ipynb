{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcf1 LMFast: Edge Deployment\n",
    "\n",
    "**Run SLMs on Raspberry Pi, Android, and Edge Devices!**\n",
    "\n",
    "## What You'll Learn\n",
    "- Export models to GGUF format\n",
    "- Run models with `llama.cpp`\n",
    "- Optimize for low-RAM devices (RPi 4/5)\n",
    "- Build a simple terminal chat app for edge\n",
    "\n",
    "## Supported Hardware\n",
    "- **Raspberry Pi 4/5** (4GB+ RAM)\n",
    "- **Android Phones** (via UserLAnd or Termux)\n",
    "- **NVIDIA Jetson Nano**\n",
    "- **Laptops** (Mac M1/M2/M3, Windows, Linux)\n",
    "\n",
    "**Time to complete:** ~15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install standard tools\n",
    "!pip install -q lmfast[all]\n",
    "\n",
    "# Install python bindings for llama.cpp\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python  # Use CUBLAS if on Colab GPU\n",
    "# On Raspberry Pi: pip install llama-cpp-python (no CMAKE_ARGS needed usually)\n",
    "\n",
    "import lmfast\n",
    "lmfast.setup_colab_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Export to GGUF\n",
    "\n",
    "We need the GGUF format for efficient CPU/Edge inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfast.inference import export_gguf\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "# Export to 4-bit quantized GGUF (LMFast handles llama.cpp setup!)\n",
    "# This reduces size from ~270MB to ~100MB\n",
    "export_gguf(\n",
    "    model_path=model_id,\n",
    "    output_path=\"./smollm-135m-q4.gguf\",\n",
    "    quantization=\"q4_k_m\"\n",
    ")\n",
    "\n",
    "print(\"\u2705 Exported: ./smollm-135m-q4.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Run on \"Edge\" (Simulated)\n",
    "\n",
    "We'll use `Llama` class to load the GGUF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load model (set n_gpu_layers=0 to simulate pure CPU edge device)\n",
    "llm = Llama(\n",
    "    model_path=\"./smollm-135m-q4.gguf\",\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=0,  # Run purely on CPU\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\ud83e\udd16 Model Loaded on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Edge Inference Loop\n",
    "\n",
    "A simple chat loop optimized for low latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_edge_model(prompt):\n",
    "    # Format prompt for SmolLM/ChatML\n",
    "    # <|im_start|>user\\n...<|im_end|>\\n<|im_start|>assistant\\n\n",
    "    full_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    stream = llm(\n",
    "        full_prompt,\n",
    "        max_tokens=100,\n",
    "        stop=[\"<|im_end|>\"],\n",
    "        echo=False,\n",
    "        stream=True  # Stream for perceived speed!\n",
    "    )\n",
    "    \n",
    "    print(\"EdgeAI: \", end=\"\", flush=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        text = chunk['choices'][0]['text']\n",
    "        print(text, end=\"\", flush=True)\n",
    "        response += text\n",
    "    print(\"\\n\")\n",
    "    return response\n",
    "\n",
    "chat_with_edge_model(\"What is the best way to save energy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Building a Standalone Edge App\n",
    "\n",
    "Save this script as `app.py` and run it on your Pi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_code = \"\"\"\n",
    "from llama_cpp import Llama\n",
    "import sys\n",
    "\n",
    "print(\"Loading model...\")\n",
    "llm = Llama(model_path=\"./smollm-135m-q4.gguf\", verbose=False)\n",
    "\n",
    "print(\"Ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    prompt = f\"<|im_start|>user\\\\n{user_input}<|im_end|>\\\\n<|im_start|>assistant\\\\n\"\n",
    "    output = llm(prompt, max_tokens=128, stop=[\"<|im_end|>\"])\n",
    "    print(f\"AI: {output['choices'][0]['text']}\")\n",
    "\"\"\"\n",
    "\n",
    "with open(\"edge_app.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"\u2705 Created edge_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Summary\n",
    "\n",
    "You've learned how to:\n",
    "- \u2705 Convert models for edge usage\n",
    "- \u2705 Run inference on CPU\n",
    "- \u2705 Create a standalone script for Raspberry Pi\n",
    "\n",
    "### Tips for Raspberry Pi\n",
    "- Use 64-bit OS (Raspberry Pi OS 64-bit).\n",
    "- Overclock slightly for 10-20% speedup.\n",
    "- Use a cooling fan!\n",
    "\n",
    "### Next Steps\n",
    "- Copy `edge_app.py` and `smollm-135m-q4.gguf` to your device and run!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}