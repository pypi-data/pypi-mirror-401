{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üíø LMFast: Custom Datasets\n",
                "\n",
                "**Train SLMs on your own data!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Load data from various formats (JSON, CSV, JSONL)\n",
                "- Format data for instruction tuning\n",
                "- Handle conversation/chat datasets\n",
                "- Push prepared datasets to HuggingFace Hub\n",
                "\n",
                "## Supported Formats\n",
                "- **Alpaca Style**: `{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}`\n",
                "- **ShareGPT / Chat**: `{\"conversations\": [{\"from\": \"human\", \"value\": \"...\"}, ...]}`\n",
                "- **Text Completion**: `\"Just raw text for pretraining...\"`\n",
                "\n",
                "**Time to complete:** ~10 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all]\n",
                "\n",
                "import lmfast\n",
                "import pandas as pd\n",
                "from datasets import Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Loading from Local Files\n",
                "\n",
                "Let's create some dummy files to demonstrate loading."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dummy JSONL file (common dataset format)\n",
                "data = [\n",
                "    {\"instruction\": \"What is the capital of France?\", \"output\": \"Paris is the capital of France.\"},\n",
                "    {\"instruction\": \"Add 2+2\", \"output\": \"The answer is 4.\"},\n",
                "    {\"instruction\": \"Write a function to print hello\", \"output\": \"print('Hello')\"}\n",
                "]\n",
                "\n",
                "import json\n",
                "with open(\"my_data.jsonl\", \"w\") as f:\n",
                "    for entry in data:\n",
                "        f.write(json.dumps(entry) + \"\\n\")\n",
                "        \n",
                "print(\"Created my_data.jsonl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load with LMFast utility (wraps 'datasets' library)\n",
                "from lmfast.training.data import load_dataset\n",
                "\n",
                "dataset = load_dataset(\"json\", data_files=\"my_data.jsonl\")\n",
                "print(f\"Loaded {len(dataset)} examples\")\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Formatting for Instruction Tuning\n",
                "\n",
                "Models like SmolLM or Llama-3 require specific prompt templates. LMFast handles this via `DataCollator` or explicit mapping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_alpaca(example):\n",
                "    input_text = example.get('input', '')\n",
                "    if input_text:\n",
                "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{example['output']}\"\n",
                "    else:\n",
                "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
                "    return {\"text\": text}\n",
                "\n",
                "formatted_ds = dataset.map(format_alpaca)\n",
                "print(\"Formatted Example:\")\n",
                "print(formatted_ds[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Handling Chat Data (ShareGPT)\n",
                "\n",
                "Chat data is more complex but standard for fine-tuning assistants."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chat_data = [\n",
                "    {\n",
                "        \"conversations\": [\n",
                "            {\"from\": \"human\", \"value\": \"Hi there!\"},\n",
                "            {\"from\": \"gpt\", \"value\": \"Hello! How can I help you today?\"},\n",
                "            {\"from\": \"human\", \"value\": \"Tell me a joke.\"},\n",
                "            {\"from\": \"gpt\", \"value\": \"Why did the chicken cross the road? To get to the other side!\"}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "# Create dataset\n",
                "chat_ds = Dataset.from_list(chat_data)\n",
                "\n",
                "def apply_chat_template(example, tokenizer):\n",
                "    # Uses the model's chat template (e.g. ChatML)\n",
                "    messages = []\n",
                "    for msg in example['conversations']:\n",
                "        role = \"user\" if msg['from'] == \"human\" else \"assistant\"\n",
                "        messages.append({\"role\": role, \"content\": msg['value']})\n",
                "    \n",
                "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
                "\n",
                "# Must load a tokenizer to apply template\n",
                "from transformers import AutoTokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
                "\n",
                "formatted_chat = chat_ds.map(lambda x: apply_chat_template(x, tokenizer))\n",
                "\n",
                "print(\"ChatML Formatted Example:\")\n",
                "print(formatted_chat[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Using CSV / Excel Files\n",
                "\n",
                "Common in enterprise settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dummy CSV\n",
                "df = pd.DataFrame({\n",
                "    'question': ['How do I reset password?', 'Where is the login button?'],\n",
                "    'answer': ['Go to settings > reset.', 'Top right corner.']\n",
                "})\n",
                "df.to_csv(\"support.csv\", index=False)\n",
                "\n",
                "# Load csv\n",
                "csv_ds = load_dataset(\"csv\", data_files=\"support.csv\")\n",
                "\n",
                "# Map columns to instruction format\n",
                "def map_csv(example):\n",
                "    return {\n",
                "        \"instruction\": example['question'],\n",
                "        \"output\": example['answer']\n",
                "    }\n",
                "\n",
                "final_ds = csv_ds.map(map_csv)\n",
                "final_ds = final_ds.map(format_alpaca)\n",
                "\n",
                "print(\"CSV Example:\")\n",
                "print(final_ds[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Preparing for Training\n",
                "\n",
                "Once formatted, you can pass this directly to `SLMTrainer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into train/test\n",
                "split_ds = final_ds.train_test_split(test_size=0.1)\n",
                "\n",
                "print(f\"Train size: {len(split_ds['train'])}\")\n",
                "print(f\"Test size: {len(split_ds['test'])}\")\n",
                "\n",
                "# Ready to train!\n",
                "# lmfast.train(\"HuggingFaceTB/SmolLM-135M\", dataset=split_ds['train'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Load JSONL, CSV, and Python lists\n",
                "- ‚úÖ Format for Alpaca and ChatML styles\n",
                "- ‚úÖ Split datasets for training/evaluation\n",
                "\n",
                "### Best Practices\n",
                "- **Clean your data**: Remove duplicates and low-quality entries.\n",
                "- **Use Chat Templates**: Always use `apply_chat_template` for chat models.\n",
                "- **Balance your data**: Ensure coverage of all tasks you want the model to do.\n",
                "\n",
                "### Next Steps\n",
                "- `01_quickstart_training.ipynb`: Use your custom dataset to train!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}