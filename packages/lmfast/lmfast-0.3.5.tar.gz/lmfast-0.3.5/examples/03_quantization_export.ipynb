{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udce6 LMFast: Quantization & Export\n",
    "\n",
    "**Shrink your models for deployment without losing intelligence!**\n",
    "\n",
    "## What You'll Learn\n",
    "- 4-bit (QLoRA) vs 8-bit quantization\n",
    "- Export to GGUF (for llama.cpp / Ollama)\n",
    "- Export to ONNX (for standard runtimes)\n",
    "- Understand AWQ vs GPTQ\n",
    "\n",
    "## Quick Guide\n",
    "| Format | Best For | Speed | Size |\n",
    "|--------|----------|-------|------|\n",
    "| **GGUF** | CPU / Mac / Edge | \u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50 |\n",
    "| **Int4** | GPU Serving | \u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50 |\n",
    "| **ONNX** | Browser / Web | \u2b50\u2b50 | \u2b50\u2b50 |\n",
    "\n",
    "**Time to complete:** ~10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lmfast[all]\n",
    "\n",
    "import lmfast\n",
    "lmfast.setup_colab_env()\n",
    "\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Load a Model\n",
    "\n",
    "We'll use a small model for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the base model for export demos\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "# You can also point to your locally trained model:\n",
    "# MODEL_ID = \"./my_first_slm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Export to GGUF (llama.cpp)\n",
    "\n",
    "GGUF is the gold standard for running LLMs on consumer hardware. \n",
    "\n",
    "**Note:** LMFast automatically clones and sets up `llama.cpp` for you if it's not found in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfast.inference import export_gguf\n",
    "\n",
    "print(\"\ud83d\udce6 Exporting to GGUF (q4_k_m)...\")\n",
    "\n",
    "try:\n",
    "    export_gguf(\n",
    "        model_path=MODEL_ID,\n",
    "        output_path=\"./smollm-135m-q4.gguf\",\n",
    "        quantization=\"q4_k_m\"  # Balanced 4-bit quantization\n",
    "    )\n",
    "    print(\"\u2705 GGUF Export Successful!\")\n",
    "    \n",
    "    # Check size\n",
    "    import os\n",
    "    size_mb = os.path.getsize(\"./smollm-135m-q4.gguf\") / 1024 / 1024\n",
    "    print(f\"File Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f GGUF Export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 In-Place Quantization (Int4 / Int8)\n",
    "\n",
    "If you want to serve the model using Python (transformers/bitsandbytes), you can save a quantized version locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfast.inference import quantize_model\n",
    "\n",
    "print(\"\u2696\ufe0f Quantizing to 4-bit (NF4)...\")\n",
    "\n",
    "quantize_model(\n",
    "    model_path=MODEL_ID,\n",
    "    output_path=\"./smollm-int4\",\n",
    "    method=\"int4\"  # Uses bitsandbytes NF4\n",
    ")\n",
    "\n",
    "print(\"\u2705 Int4 Model Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Export to ONNX\n",
    "\n",
    "Great for running in the browser or cross-platform apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfast.deployment import export_for_browser\n",
    "\n",
    "print(\"\ud83c\udf10 Exporting for Browser (ONNX)...\")\n",
    "\n",
    "# The browser exporter handles ONNX conversion, optimization, and demo generation\n",
    "artifacts = export_for_browser(\n",
    "    model_path=MODEL_ID,\n",
    "    output_dir=\"./onnx_model\",\n",
    "    target=\"onnx\",\n",
    "    quantization=\"int8\",\n",
    "    create_demo=False\n",
    ")\n",
    "\n",
    "print(f\"\u2705 ONNX Export complete!\")\n",
    "print(f\"Artifacts: {list(artifacts.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Verify the Quantized Model\n",
    "\n",
    "LMFast's `SLMServer` automatically detects quantization and optimizes inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfast.inference import SLMServer\n",
    "\n",
    "print(\"\ud83d\ude80 Loading Quantized Model...\")\n",
    "\n",
    "# Load the int4 model we saved earlier\n",
    "server = SLMServer(\"./smollm-int4\")\n",
    "\n",
    "prompt = \"What is the speed of light? Answer briefly.\"\n",
    "response = server.generate(prompt)\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Summary\n",
    "\n",
    "You've learned how to:\n",
    "- \u2705 Create **GGUF** files for edge devices\n",
    "- \u2705 Save **Int4/Int8** models for high-performance Python serving\n",
    "- \u2705 Export **ONNX** models for browser deployment\n",
    "\n",
    "### Next Steps\n",
    "- `15_browser_deployment.ipynb`: Use the ONNX model in a web app!\n",
    "- `16_edge_deployment.ipynb`: Run the GGUF model on a Raspberry Pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}