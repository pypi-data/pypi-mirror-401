{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåê LMFast: Browser Deployment\n",
                "\n",
                "**Run your SLM entirely in the user's browser! No server costs!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Export models for WebLLM (WebGPU accelerated)\n",
                "- Export models for ONNX Runtime Web\n",
                "- Generate a complete HTML/JS demo app automatically\n",
                "- Host your AI app for free (GitHub Pages / Netlify)\n",
                "\n",
                "## Technologies\n",
                "- **WebGPU**: Near-native GPU performance in Chrome/Edge\n",
                "- **WebAssembly (Wasm)**: Efficient execution\n",
                "\n",
                "**Time to complete:** ~15 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all] optimum[onnxruntime]\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Select a Model\n",
                "\n",
                "We need a small model that fits in browser memory (RAM + VRAM). \n",
                "SmolLM-135M is perfect (only ~100MB compressed!)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_ID = \"HuggingFaceTB/SmolLM-135M-Instruct\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Export for Web (ONNX)\n",
                "\n",
                "We'll export to ONNX format first, which works on almost all devices (even without WebGPU)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.deployment import export_for_browser\n",
                "\n",
                "print(\"üåç Exporting to ONNX for Browser...\")\n",
                "export_for_browser(\n",
                "    model_path=MODEL_ID,\n",
                "    output_dir=\"./my_web_ai\",\n",
                "    target=\"onnx\",\n",
                "    quantization=\"int8\", # 8-bit quantization is a good balance\n",
                "    create_demo=True\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Export Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Explore the Generated App\n",
                "\n",
                "LMFast automatically created a `demo/` folder with a working chat interface."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "print(\"üìÇ Generated Files:\")\n",
                "for root, dirs, files in os.walk(\"./my_web_ai\"):\n",
                "    level = root.replace(\"./my_web_ai\", \"\").count(os.sep)\n",
                "    indent = \" \" * 4 * (level)\n",
                "    print(f\"{indent}{os.path.basename(root)}/\")\n",
                "    subindent = \" \" * 4 * (level + 1)\n",
                "    for f in files:\n",
                "        print(f\"{subindent}{f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Preview the App (Colab Trick)\n",
                "\n",
                "We can't run a web server easily in Colab, but we can inspect the HTML code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import HTML\n",
                "\n",
                "# Display the structure, not the running app (limitations of iframe context)\n",
                "with open(\"./my_web_ai/demo/index.html\", \"r\") as f:\n",
                "    html_content = f.read()\n",
                "\n",
                "print(\"üìÑ HTML Source Preview (First 500 chars):\")\n",
                "print(html_content[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Advanced: WebLLM (WebGPU)\n",
                "\n",
                "For maximum performance, target WebLevel (MLC).\n",
                "*Note: This requires the `mlc-llm` CLI tool which is a heavy install. We generate the config for you.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Generating WebLLM Config...\")\n",
                "export_for_browser(\n",
                "    model_path=MODEL_ID,\n",
                "    output_dir=\"./my_web_gpu\",\n",
                "    target=\"webllm\",\n",
                "    create_demo=True\n",
                ")\n",
                "\n",
                "print(\"‚úÖ WebLLM Config Ready!\")\n",
                "print(\"Check ./my_web_gpu/mlc/README.md for conversion steps.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Deployment Instructions\n",
                "\n",
                "To make your AI live:\n",
                "\n",
                "1. **Download** the `./my_web_ai` folder.\n",
                "2. **Upload** to GitHub.\n",
                "3. **Enable GitHub Pages** in settings.\n",
                "\n",
                "Or simply run locally:\n",
                "```bash\n",
                "cd my_web_ai/demo\n",
                "python -m http.server 8000\n",
                "```\n",
                "Then visit `http://localhost:8000`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You just built a **serverless AI application**!\n",
                "- The model runs inside the user's device.\n",
                "- Zero cloud costs.\n",
                "- Complete privacy.\n",
                "\n",
                "### Next Steps\n",
                "- Customize the HTML/CSS in `demo/style.css`\n",
                "- Add RAG support manually in JS\n",
                "- Deploy to Netlify/Vercel"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}