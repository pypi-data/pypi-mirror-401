{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä LMFast: Knowledge Distillation\n",
                "\n",
                "**Transfer knowledge from a powerful teacher model to a tiny student model!**\n",
                "\n",
                "## What You'll Learn\n",
                "- How knowledge distillation works\n",
                "- Distill Qwen2.5-1.5B ‚Üí SmolLM-135M\n",
                "- Compare student performance before/after\n",
                "- Offline logit generation for memory efficiency\n",
                "\n",
                "## Why Distillation?\n",
                "- Get GPT-quality from a tiny model\n",
                "- Run on edge devices\n",
                "- 10x faster inference\n",
                "- 100x cheaper serving\n",
                "\n",
                "**Time to complete:** ~20 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all]\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Prepare Distillation Data\n",
                "\n",
                "For distillation, we need prompts for the teacher to generate responses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load instruction dataset\n",
                "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n",
                "\n",
                "# Format prompts\n",
                "def format_prompt(example):\n",
                "    if example[\"input\"]:\n",
                "        prompt = f\"\"\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\"\"\n",
                "    else:\n",
                "        prompt = f\"\"\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\"\"\n",
                "    return {\"prompt\": prompt, \"response\": example[\"output\"]}\n",
                "\n",
                "dataset = dataset.map(format_prompt)\n",
                "print(f\"Dataset size: {len(dataset)} examples\")\n",
                "print(f\"\\nExample prompt:\\n{dataset[0]['prompt'][:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Baseline: Test Student Before Distillation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "# Load untrained student\n",
                "student_baseline = SLMServer(\"HuggingFaceTB/SmolLM-135M\")\n",
                "\n",
                "test_prompts = [\n",
                "    \"Explain machine learning in simple terms.\",\n",
                "    \"Write a Python function to calculate factorial.\",\n",
                "    \"What are the benefits of small language models?\"\n",
                "]\n",
                "\n",
                "print(\"üìä BASELINE (Before Distillation)\")\n",
                "print(\"=\" * 50)\n",
                "for prompt in test_prompts:\n",
                "    response = student_baseline.generate(prompt, max_new_tokens=100)\n",
                "    print(f\"\\nQ: {prompt}\")\n",
                "    print(f\"A: {response[:200]}...\")\n",
                "\n",
                "# Free memory\n",
                "del student_baseline\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Configure Distillation\n",
                "\n",
                "LMFast supports multiple distillation strategies:\n",
                "\n",
                "| Method | Description | Best For |\n",
                "|--------|-------------|----------|\n",
                "| **Standard KD** | KL divergence on logits | General tasks |\n",
                "| **CoT Distillation** | Transfer reasoning traces | Math, logic |\n",
                "| **Offline Distillation** | Pre-compute teacher outputs | Memory-constrained |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.core.config import DistillationConfig\n",
                "\n",
                "distill_config = DistillationConfig(\n",
                "    teacher_model=\"Qwen/Qwen2.5-1.5B-Instruct\",  # Powerful teacher\n",
                "    temperature=2.0,  # Softer distributions for better transfer\n",
                "    alpha=0.5,  # 50% KD loss, 50% CE loss\n",
                "    max_seq_length=512,\n",
                ")\n",
                "\n",
                "print(f\"Teacher: {distill_config.teacher_model}\")\n",
                "print(f\"Temperature: {distill_config.temperature}\")\n",
                "print(f\"Alpha (KD weight): {distill_config.alpha}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Run Distillation\n",
                "\n",
                "This will:\n",
                "1. Load teacher model (Qwen-1.5B)\n",
                "2. Load student model (SmolLM-135M) \n",
                "3. Generate teacher logits\n",
                "4. Train student to match teacher distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.distillation import DistillationTrainer\n",
                "\n",
                "# Create distillation trainer\n",
                "trainer = DistillationTrainer(\n",
                "    student_model=\"HuggingFaceTB/SmolLM-135M\",\n",
                "    distillation_config=distill_config,\n",
                ")\n",
                "\n",
                "# Run distillation\n",
                "print(\"üéì Starting distillation...\")\n",
                "print(\"This may take 10-15 minutes on T4\")\n",
                "\n",
                "trainer.distill(\n",
                "    dataset,\n",
                "    output_dir=\"./distilled_model\",\n",
                "    max_steps=200,\n",
                "    batch_size=2,\n",
                "    gradient_accumulation_steps=8,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Distillation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Test Distilled Student"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load distilled model\n",
                "student_distilled = SLMServer(\"./distilled_model\")\n",
                "\n",
                "print(\"üéì AFTER DISTILLATION\")\n",
                "print(\"=\" * 50)\n",
                "for prompt in test_prompts:\n",
                "    response = student_distilled.generate(prompt, max_new_tokens=100)\n",
                "    print(f\"\\nQ: {prompt}\")\n",
                "    print(f\"A: {response[:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Alternative: Offline Distillation (Memory Efficient)\n",
                "\n",
                "If you can't fit teacher + student in memory simultaneously, use offline distillation:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.distillation import generate_teacher_labels\n",
                "\n",
                "# Step 1: Generate and save teacher outputs (can be done with a larger GPU)\n",
                "# generate_teacher_labels(\n",
                "#     teacher_model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
                "#     dataset=dataset,\n",
                "#     output_path=\"./teacher_logits.pt\",\n",
                "#     batch_size=4\n",
                "# )\n",
                "\n",
                "# Step 2: Train student with saved logits\n",
                "# trainer = DistillationTrainer(\n",
                "#     student_model=\"HuggingFaceTB/SmolLM-135M\",\n",
                "#     distillation_config=distill_config,\n",
                "# )\n",
                "# trainer.distill_from_logits(\n",
                "#     logits_path=\"./teacher_logits.pt\",\n",
                "#     output_dir=\"./distilled_offline\"\n",
                "# )\n",
                "\n",
                "print(\"üí° Offline distillation is useful when:\")\n",
                "print(\"   - Teacher model is too large (7B+)\")\n",
                "print(\"   - You want to reuse teacher outputs\")\n",
                "print(\"   - Running on Colab free tier\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Export Distilled Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference.quantization import quantize_model\n",
                "\n",
                "# Quantize for deployment\n",
                "quantize_model(\n",
                "    \"./distilled_model\",\n",
                "    \"./distilled_model_int4\",\n",
                "    method=\"int4\"\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model quantized and ready for deployment!\")\n",
                "\n",
                "# Compare sizes\n",
                "import os\n",
                "def get_dir_size(path):\n",
                "    total = 0\n",
                "    for f in os.listdir(path):\n",
                "        fp = os.path.join(path, f)\n",
                "        if os.path.isfile(fp):\n",
                "            total += os.path.getsize(fp)\n",
                "    return total / 1e6\n",
                "\n",
                "print(f\"\\nOriginal size: {get_dir_size('./distilled_model'):.1f} MB\")\n",
                "print(f\"Quantized size: {get_dir_size('./distilled_model_int4'):.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Transfer knowledge from a 1.5B model to a 135M model\n",
                "- ‚úÖ Use temperature scaling for better transfer\n",
                "- ‚úÖ Apply offline distillation for memory efficiency\n",
                "- ‚úÖ Quantize the distilled model for deployment\n",
                "\n",
                "### Distillation Tips\n",
                "\n",
                "| Tip | Why |\n",
                "|-----|-----|\n",
                "| Higher temperature (2-4) | Softer distributions transfer better |\n",
                "| Balanced Œ± (0.3-0.7) | Don't ignore ground truth labels |\n",
                "| More data | Distillation benefits from scale |\n",
                "| Matching architectures | Similar tokenizers help |\n",
                "\n",
                "### Next Steps\n",
                "- `06_preference_alignment.ipynb` - ORPO/DPO alignment\n",
                "- `09_basic_agents.ipynb` - Build agents with your model"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}