{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† LMFast: Advanced Reasoning Agents\n",
                "\n",
                "**Unlock \"System 2\" thinking in Small Language Models!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Chain-of-Thought (CoT) prompting\n",
                "- Test-Time Compute Scaling (Best-of-N)\n",
                "- Self-Verification loops \n",
                "- Solver math problems with 135M parameters\n",
                "\n",
                "## The Concept\n",
                "By giving the model more time to \"think\" (generate multiple reasoning paths) and verify its own answers, we can significantly boost performance on logic and math tasks without retraining.\n",
                "\n",
                "**Time to complete:** ~15 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all]\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load a Reasoner Model\n",
                "\n",
                "For reasoning, instructional tuning is key. We'll use SmolLM-135M-Instruct."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "model = SLMServer(\"HuggingFaceTB/SmolLM-360M-Instruct\")\n",
                "\n",
                "# Wrapper for simple string-in/string-out\n",
                "def generate_fn(prompt):\n",
                "    return model.generate(prompt, max_new_tokens=256, temperature=0.7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Chain-of-Thought (CoT)\n",
                "\n",
                "Standard prompting vs. CoT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "problem = \"If I have 3 apples, eat 1, and buy 5 more, how many do I have?\"\n",
                "\n",
                "print(\"üî¥ Standard Generation:\")\n",
                "print(generate_fn(f\"Question: {problem}\\nAnswer:\"))\n",
                "\n",
                "print(\"\\nüü¢ Chain of Thought:\")\n",
                "prompt_cot = f\"Question: {problem}\\nLet's think step by step.\\nAnswer:\"\n",
                "print(generate_fn(prompt_cot))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Using ThinkingAgent\n",
                "\n",
                "LMFast automates improved reasoning strategies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.reasoning import ThinkingAgent\n",
                "\n",
                "# Create agent\n",
                "agent = ThinkingAgent(generate_fn, n=5)  # n=5 candidates\n",
                "\n",
                "hard_problem = \"\"\"\n",
                "A train leaves New York at 60 mph. Another leaves Boston at 50 mph. \n",
                "The distance is 220 miles. When they meet, how far is the NY train from New York?\n",
                "\"\"\"\n",
                "\n",
                "print(\"üß† Best-of-N Reasoning (Sampling 5 paths)...\")\n",
                "answer = agent.reason(hard_problem, method=\"best_of_n\")\n",
                "print(f\"\\nAnswer: {answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Self-Verification\n",
                "\n",
                "The agent generates an answer, then checks its work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîç Self-Verification Mode...\")\n",
                "# Note: 135M models struggle with verification but 1B+ excel at it\n",
                "\n",
                "verified_answer = agent.reason(hard_problem, method=\"self_verify\")\n",
                "print(f\"\\nVerified Answer: {verified_answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ One-Line Reasoning API\n",
                "\n",
                "You don't need to instantiate classes if you want a quick fix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast import reason\n",
                "\n",
                "quick_ans = reason(\n",
                "    model_fn=generate_fn,\n",
                "    problem=\"What is 15% of 80?\",\n",
                "    method=\"adaptive\"  # Automatically chooses strategy based on difficulty\n",
                ")\n",
                "\n",
                "print(f\"Correct Answer (12): {quick_ans}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Benchmarking Scale\n",
                "\n",
                "Let's see if adding computing (N) improves accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "math_questions = [\n",
                "    (\"23 + 45\", \"68\"),\n",
                "    (\"12 * 8\", \"96\"),\n",
                "    (\"100 / 4\", \"25\")\n",
                "]\n",
                "\n",
                "def evaluate(n_samples):\n",
                "    correct = 0\n",
                "    print(f\"\\nTesting with N={n_samples}...\")\n",
                "    for q, a in math_questions:\n",
                "        resp = reason(generate_fn, q, n=n_samples, method=\"best_of_n\")\n",
                "        if a in resp:\n",
                "            correct += 1\n",
                "    return correct / len(math_questions)\n",
                "\n",
                "print(f\"Accuracy (N=1): {evaluate(1):.1%}\")\n",
                "print(f\"Accuracy (N=5): {evaluate(5):.1%}\")\n",
                "print(\"Notice the improvement! (Results may vary with randomness)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Implement \"System 2\" thinking with SLMs\n",
                "- ‚úÖ Use `ThinkingAgent` for difficult tasks\n",
                "- ‚úÖ Improve accuracy by scaling test-time compute\n",
                "\n",
                "### Tip\n",
                "- For best results, use models trained on math/code (e.g. Qwen2.5-Math).\n",
                "\n",
                "### Next Steps\n",
                "- `12_rag_agents.ipynb`: Combine reasoning with external knowledge."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}