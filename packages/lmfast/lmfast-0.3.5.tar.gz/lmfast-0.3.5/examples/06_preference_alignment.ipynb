{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚öñÔ∏è LMFast: Preference Alignment (ORPO/DPO)\n",
                "\n",
                "**Align your SLM with human preferences efficiently!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Align models using ORPO (Odds Ratio Preference Optimization)\n",
                "- Prepare preference datasets (Chosen vs Rejected)\n",
                "- Fine-tune a chat model to be more helpful/harmless\n",
                "- Evaluate alignment quality\n",
                "\n",
                "## Why Preference Alignment?\n",
                "- **Safety**: Reduce toxic outputs\n",
                "- **Style**: Make the model speak like a pirate, or a professional\n",
                "- **Accuracy**: penalize hallucinations\n",
                "\n",
                "**Time to complete:** ~15 minutes (Colab T4 optimized)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all]\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Prepare Preference Data\n",
                "\n",
                "Alignment requires \"triplets\": `(Prompt, Chosen Response, Rejected Response)`.\n",
                "The model learns to prefer the *Chosen* one and avoid the *Rejected* one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "# Example: Teaching the model to be polite\n",
                "data = [\n",
                "    {\n",
                "        \"prompt\": \"Give me the report now.\",\n",
                "        \"chosen\": \"Here is the report you requested. Let me know if you need anything else.\",\n",
                "        \"rejected\": \"Here it is. take it.\"\n",
                "    },\n",
                "    {\n",
                "        \"prompt\": \"This code is broken.\",\n",
                "        \"chosen\": \"I'm sorry to hear that. Could you share the error message so I can help fix it?\",\n",
                "        \"rejected\": \"You probably wrote it wrong. Check your syntax.\"\n",
                "    },\n",
                "    {\n",
                "        \"prompt\": \"I hate you.\",\n",
                "        \"chosen\": \"I understand you're frustrated, but I'm just an AI trying to help.\",\n",
                "        \"rejected\": \"That is not very nice. I don't like you either.\"\n",
                "    }\n",
                "]\n",
                "\n",
                "# In real scenarios, use \"HuggingFaceH4/ultrafeedback_binarized\" or similar\n",
                "dataset = Dataset.from_list(data)\n",
                "print(f\"Dataset Example:\\n{dataset[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Configure ORPO\n",
                "\n",
                "We use **ORPO** (Odds Ratio Preference Optimization) because it aligns *during* SFT (Supervised Fine-Tuning), saving memory and time compared to RLHF or DPO."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.alignment import align\n",
                "\n",
                "# One-line alignment training\n",
                "# This automatically configures ORPOConfig for T4\n",
                "\n",
                "print(\"üöÄ Starting ORPO Alignment...\")\n",
                "print(\"This optimizes the model to favor 'chosen' responses.\")\n",
                "\n",
                "# Note: In a notebook, this returns the trainer object\n",
                "trainer = align(\n",
                "    model_name=\"HuggingFaceTB/SmolLM-135M-Instruct\",\n",
                "    dataset=dataset,\n",
                "    output_dir=\"./aligned_model\",\n",
                "    method=\"orpo\",\n",
                "    max_steps=50,  # Short demo run\n",
                "    learning_rate=5e-6, # Lower LR for alignment usually\n",
                "    beta=0.1,  # Weight of the preference penalty\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Training initiated...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Evaluate Results\n",
                "\n",
                "Let's compare the base model vs. the aligned model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "# Load aligned model\n",
                "aligned_server = SLMServer(\"./aligned_model\")\n",
                "\n",
                "test_prompts = [\n",
                "    \"This product is terrible! Fix it!\",\n",
                "    \"You are stupid.\"\n",
                "]\n",
                "\n",
                "print(\"üß™ Testing Aligned Model Responses:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for p in test_prompts:\n",
                "    response = aligned_server.generate(p, max_new_tokens=60)\n",
                "    print(f\"\\nUser: {p}\")\n",
                "    print(f\"AI: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Advanced: GRPO (Group Relative Policy Optimization)\n",
                "\n",
                "For reasoning tasks (like math), GRPO is better. It takes a group of samples and reinforces the best ones relative to the group average.\n",
                "\n",
                "*Note: GRPO requires a reward function or ground-truth verifier.*\n",
                "\n",
                "```python\n",
                "# Conceptual Example for GRPO\n",
                "from lmfast.alignment import align\n",
                "\n",
                "def reward_func(completions, **kwargs):\n",
                "    # Return 1.0 if answer is correct, 0.0 otherwise\n",
                "    return [1.0 if \"42\" in c else 0.0 for c in completions]\n",
                "\n",
                "trainer = align(\n",
                "    model_name=\"HuggingFaceTB/SmolLM-135M\",\n",
                "    dataset=math_dataset,\n",
                "    method=\"grpo\",\n",
                "    reward_function=reward_func\n",
                ")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Structure preference datasets\n",
                "- ‚úÖ Run ORPO alignment with one line of code\n",
                "- ‚úÖ Understand the difference between ORPO and GRPO\n",
                "\n",
                "### Tips\n",
                "- **Data Quality**: Alignment is very sensitive to data quality. Ensure 'chosen' is genuinely better.\n",
                "- **Beta Parameter**: Controls how much preference guides the training. 0.1 is a good default.\n",
                "\n",
                "### Next Steps\n",
                "- `13_guardrails.ipynb`: Add hard constraints to your aligned model."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}