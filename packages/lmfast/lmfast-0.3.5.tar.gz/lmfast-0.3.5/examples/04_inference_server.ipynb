{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ LMFast: High-Performance Inference Server\n",
                "\n",
                "**Deploy your SLM with an OpenAI-compatible API!**\n",
                "\n",
                "## What You'll Learn\n",
                "- Start a high-throughput inference server\n",
                "- Use vLLM acceleration (2-5x faster)\n",
                "- Query the API using the standard OpenAI Python client\n",
                "- Benchmark inference speed (Tokens per Second)\n",
                "\n",
                "## Why Use `SLMServer`?\n",
                "- **Easy**: 1 line to start.\n",
                "- **Compatible**: Works with LangChain, AutoGen, CrewAI.\n",
                "- **Fast**: Optimized for T4 GPUs with batching.\n",
                "\n",
                "**Time to complete:** ~10 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q lmfast[all] vllm openai\n",
                "\n",
                "import lmfast\n",
                "lmfast.setup_colab_env()\n",
                "\n",
                "import torch\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Initialize Server\n",
                "\n",
                "We can load a HuggingFace model or a local path."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lmfast.inference import SLMServer\n",
                "\n",
                "# Initialize server with efficient serving backend\n",
                "server = SLMServer(\n",
                "    \"HuggingFaceTB/SmolLM-360M-Instruct\",\n",
                "    use_vllm=True  # Acceleration (if available)\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Server Initialized (in-process)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Direct Generation (Python API)\n",
                "\n",
                "Great for scripts running on the same machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"Write a haiku about speed.\"\n",
                "\n",
                "print(f\"üìù Prompt: {prompt}\")\n",
                "output = server.generate(prompt, max_new_tokens=50, temperature=0.7)\n",
                "print(f\"ü§ñ Output: {output}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Run as HTTP Server (OpenAI API)\n",
                "\n",
                "This allows external tools to connect. \n",
                "*Note: In Colab, this blocks the cell. We run it in background for demo.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import threading\n",
                "import time\n",
                "import requests\n",
                "\n",
                "# Start server in a background thread\n",
                "def start_server():\n",
                "    # This blocks, so we run it in thread\n",
                "    server.serve(host=\"127.0.0.1\", port=8000)\n",
                "\n",
                "thread = threading.Thread(target=start_server, daemon=True)\n",
                "thread.start()\n",
                "\n",
                "# Wait for server to start\n",
                "print(\"‚è≥ Waiting for server to start...\")\n",
                "time.sleep(10)  # Give it a few seconds\n",
                "print(\"‚úÖ Server should be running on http://127.0.0.1:8000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Connect with OpenAI Client\n",
                "\n",
                "Now we can use the standard OpenAI library!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "\n",
                "# Point client to local server\n",
                "client = OpenAI(\n",
                "    base_url=\"http://127.0.0.1:8000/v1\",\n",
                "    api_key=\"lmfast-key\"  # Dummy key\n",
                ")\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"smollm\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n",
                "    ],\n",
                "    max_tokens=100\n",
                ")\n",
                "\n",
                "print(f\"ü§ñ OpenAI Client Response:\\n{response.choices[0].message.content}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Benchmark Performance\n",
                "\n",
                "Let's see how fast it is."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_time = time.time()\n",
                "tokens = 0\n",
                "N = 5\n",
                "\n",
                "print(f\"üèéÔ∏è Benchmarking {N} requests...\")\n",
                "\n",
                "for _ in range(N):\n",
                "    resp = client.chat.completions.create(\n",
                "        model=\"smollm\",\n",
                "        messages=[{\"role\": \"user\", \"content\": \"Count to 20.\"}],\n",
                "        max_tokens=50\n",
                "    )\n",
                "    tokens += resp.usage.completion_tokens\n",
                "\n",
                "duration = time.time() - start_time\n",
                "tps = tokens / duration\n",
                "\n",
                "print(f\"‚ö° Speed: {tps:.2f} tokens/sec\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Summary\n",
                "\n",
                "You've learned how to:\n",
                "- ‚úÖ Serve SLMs with `SLMServer`\n",
                "- ‚úÖ Enable vLLM speedups\n",
                "- ‚úÖ Drop-in replace OpenAI API in your apps\n",
                "\n",
                "### Compatibility\n",
                "Because it's OpenAI compatible, you can use this server with:\n",
                "- **LangChain / LlamaIndex**\n",
                "- **AutoGen / CrewAI**\n",
                "- **Cursor / VS Code extensions**\n",
                "\n",
                "### Next Steps\n",
                "- `15_browser_deployment.ipynb`: No server needed!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
