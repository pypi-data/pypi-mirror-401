#!/usr/bin/env python3
"""
arifOS Governance Telemetry Analyzer v36.3Œ©

Parses cooling ledger JSONL events and generates governance audit reports.
Produces both CSV (structured data) and Markdown (narrative analysis).

Usage:
    python analyze_governance.py [--ledger PATH] [--output DIR]

Example:
    python analyze_governance.py --ledger cooling_ledger/L1_cooling_ledger.jsonl --output analysis/

Output Files:
    - arifos_governance_telemetry_summary.csv (data table)
    - ARIFOS_GOVERNANCE_TELEMETRY_vXX.md (narrative report)

Author: arifOS Project
License: Apache-2.0
"""

import json
import csv
import sys
import argparse
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Optional, Tuple


# ============================================================================
# CONSTANTS
# ============================================================================

DEFAULT_LEDGER_PATH = "cooling_ledger/L1_cooling_ledger.jsonl"
DEFAULT_OUTPUT_DIR = "analysis"

FLOOR_DEFS = {
    "F1_Truth": {"id": 1, "threshold": 0.99, "type": "hard"},
    "F2_DeltaS": {"id": 2, "threshold": 0.0, "type": "hard"},
    "F3_PeaceSquared": {"id": 3, "threshold": 1.0, "type": "soft"},
    "F4_KappaR": {"id": 4, "threshold": 0.95, "type": "soft"},
    "F5_Omega0": {"id": 5, "threshold_min": 0.03, "threshold_max": 0.05, "type": "hard"},
    "F6_Amanah": {"id": 6, "threshold": True, "type": "hard"},
    "F7_RASA": {"id": 7, "threshold": True, "type": "hard"},
    "F8_TriWitness": {"id": 8, "threshold": 0.95, "type": "soft"},
    "F9_AntiHantu": {"id": 9, "threshold": True, "type": "meta"},
}

VERDICT_TYPES = ["SEAL", "PARTIAL", "VOID", "SABAR", "888_HOLD"]


# ============================================================================
# DATA PARSING
# ============================================================================

def load_ledger(ledger_path: str) -> List[Dict[str, Any]]:
    """Load JSONL cooling ledger events."""
    events = []
    try:
        with open(ledger_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        event = json.loads(line)
                        events.append(event)
                    except json.JSONDecodeError as e:
                        print(f"‚ö†Ô∏è  Parse error on line: {e}", file=sys.stderr)
                        continue
    except FileNotFoundError:
        print(f"‚ùå Ledger not found: {ledger_path}", file=sys.stderr)
        return []
    
    return events


def extract_receipt_events(events: List[Dict]) -> List[Dict]:
    """Filter for zkPC receipt events (actual governance decisions)."""
    return [e for e in events if e.get('type') == 'zkpc_receipt']


def parse_event(event: Dict) -> Dict[str, Any]:
    """Extract telemetry data from a zkPC receipt."""
    receipt = event.get('receipt', {})
    metrics = receipt.get('metrics', {})
    tri_witness = receipt.get('tri_witness', {})
    eye_report = receipt.get('eye_report', {})
    cce_audits = receipt.get('cce_audits', {})
    phases = receipt.get('phases', {})
    context = receipt.get('context_meta', {})
    
    record = {
        'receipt_id': event.get('receipt_id', event.get('id', 'UNKNOWN')),
        'timestamp': event.get('timestamp', ''),
        'verdict': receipt.get('verdict', 'UNKNOWN'),
        'high_stakes': context.get('high_stakes', False),
        'sabar_triggered': receipt.get('sabar_triggered', False),
        'source': event.get('source', ''),
        
        # Metrics
        'truth': metrics.get('truth'),
        'delta_s': metrics.get('delta_s'),
        'peace_squared': metrics.get('peace_squared'),
        'kappa_r': metrics.get('kappa_r'),
        'omega_0': metrics.get('omega_0'),
        'amanah': metrics.get('amanah'),
        'rasa': metrics.get('rasa'),
        'tri_witness': metrics.get('tri_witness'),
        'anti_hantu': metrics.get('anti_hantu'),
        'psi': metrics.get('psi'),
        'shadow': metrics.get('shadow'),
        
        # Eye report
        'drift_detected': eye_report.get('drift_detected', False),
        'hantu_scan': eye_report.get('hantu_scan', 'UNKNOWN'),
        'shadow_level': eye_report.get('shadow_level', ''),
        
        # CCE Audits
        'delta_p_audit': cce_audits.get('delta_p', 'UNKNOWN'),
        'omega_p_audit': cce_audits.get('omega_p', 'UNKNOWN'),
        'phi_p_audit': cce_audits.get('phi_p', 'UNKNOWN'),
        'psi_p_audit': cce_audits.get('psi_p', 'UNKNOWN'),
        
        # Phases
        'seal_phase': phases.get('seal', 'UNKNOWN'),
        'cool_phase': phases.get('cool', 'UNKNOWN'),
        
        # Tri-Witness breakdown
        'tri_witness_ai': tri_witness.get('ai'),
        'tri_witness_consensus': tri_witness.get('consensus'),
        'tri_witness_earth': tri_witness.get('earth'),
        'tri_witness_human': tri_witness.get('human'),
    }
    return record


# ============================================================================
# CSV GENERATION
# ============================================================================

def generate_csv(records: List[Dict], output_path: str) -> None:
    """Generate aggregated statistics CSV."""
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header metadata
        writer.writerow(['arifOS Governance Telemetry Summary', f'v36.3Œ©'])
        writer.writerow(['Analysis Date', datetime.now().isoformat()])
        writer.writerow(['Data Source', 'cooling_ledger/L1_cooling_ledger.jsonl'])
        writer.writerow([])
        
        # Overall stats
        writer.writerow(['OVERALL STATISTICS'])
        writer.writerow(['Metric', 'Value', 'Unit'])
        writer.writerow(['Total Governed Events', len(records), 'events'])
        writer.writerow(['Analysis Coverage', f'{len(records)} zkPC receipt(s)', 'type'])
        writer.writerow(['Sample Size', 'MINIMAL' if len(records) < 30 else 'ADEQUATE', 'note'])
        writer.writerow([])
        
        # Verdict distribution
        writer.writerow(['VERDICT DISTRIBUTION'])
        writer.writerow(['Verdict', 'Count', 'Percentage'])
        verdict_counts = defaultdict(int)
        for r in records:
            verdict_counts[r['verdict']] += 1
        for verdict in VERDICT_TYPES:
            count = verdict_counts[verdict]
            pct = (count / len(records) * 100) if records else 0
            if count > 0 or verdict == 'SEAL':
                writer.writerow([verdict, count, f'{pct:.1f}%'])
        writer.writerow(['TOTAL', len(records), '100.0%'])
        writer.writerow([])
        
        # Context
        writer.writerow(['CONTEXT & RISK ASSESSMENT'])
        writer.writerow(['Category', 'Value', 'Percentage'])
        high_stakes = sum(1 for r in records if r['high_stakes'])
        sabar_triggers = sum(1 for r in records if r['sabar_triggered'])
        drift_detected = sum(1 for r in records if r['drift_detected'])
        writer.writerow(['High-Stakes Events', high_stakes, f'{100*high_stakes/len(records) if records else 0:.1f}%'])
        writer.writerow(['SABAR Triggers', sabar_triggers, f'{100*sabar_triggers/len(records) if records else 0:.1f}%'])
        writer.writerow(['Drift Detected', drift_detected, f'{100*drift_detected/len(records) if records else 0:.1f}%'])
        writer.writerow([])
        
        # Floor metrics (numeric)
        writer.writerow(['CONSTITUTIONAL FLOOR METRICS (Numeric)'])
        writer.writerow(['Floor', 'Type', 'Value/Mean', 'Min', 'Max', 'Status'])
        numeric_floors = {
            'F1_Truth': ('hard', 'truth'),
            'F2_DeltaS': ('hard', 'delta_s'),
            'F3_PeaceSquared': ('soft', 'peace_squared'),
            'F4_KappaR': ('soft', 'kappa_r'),
            'F5_Omega0': ('hard', 'omega_0'),
            'F7_RASA': ('hard', 'rasa'),
            'F8_TriWitness': ('soft', 'tri_witness'),
        }
        for floor_name, (ftype, metric_key) in numeric_floors.items():
            values = [r[metric_key] for r in records if r[metric_key] is not None]
            if values:
                try:
                    numeric_values = [float(v) for v in values if isinstance(v, (int, float))]
                    if numeric_values:
                        mean = sum(numeric_values) / len(numeric_values)
                        writer.writerow([
                            floor_name,
                            ftype,
                            f'{mean:.4f}',
                            f'{min(numeric_values):.4f}',
                            f'{max(numeric_values):.4f}',
                            'PASS' if all(v >= FLOOR_DEFS[floor_name].get('threshold', 0) for v in numeric_values) else 'CHECK'
                        ])
                except (ValueError, TypeError):
                    pass
        writer.writerow([])
        
        # Floor metrics (categorical)
        writer.writerow(['CONSTITUTIONAL FLOOR METRICS (Categorical)'])
        writer.writerow(['Floor', 'Type', 'Value', 'Status'])
        writer.writerow(['F6_Amanah', 'hard', 'LOCK', 'ENFORCED'])
        hantu_statuses = [r['anti_hantu'] for r in records if r['anti_hantu']]
        if hantu_statuses:
            writer.writerow(['F9_AntiHantu', 'meta', hantu_statuses[0], 'PASS' if hantu_statuses[0] == 'PASS' else 'CHECK'])
        writer.writerow([])
        
        # System health
        writer.writerow(['SYSTEM HEALTH METRICS'])
        writer.writerow(['Metric', 'Value', 'Interpretation'])
        psi_vals = [r['psi'] for r in records if r['psi'] is not None]
        if psi_vals:
            psi_mean = sum(psi_vals) / len(psi_vals)
            interp = 'THRIVING' if psi_mean > 1.0 else 'MARGINAL' if abs(psi_mean - 1.0) < 0.1 else 'BREACH'
            writer.writerow(['Œ® (PSI) Vitality', f'{psi_mean:.4f}', interp])
        shadow_vals = [r['shadow'] for r in records if r['shadow'] is not None]
        if shadow_vals:
            shadow_mean = sum(shadow_vals) / len(shadow_vals)
            writer.writerow(['Shadow Level', f'{shadow_mean:.4f}', 'LOW' if shadow_mean < 0.05 else 'ELEVATED'])
        hantu_pass = sum(1 for r in records if r['hantu_scan'] == 'PASS')
        writer.writerow(['Hantu Scan (@EYE)', 'PASS', 'NO_SOUL_PRETENSE' if hantu_pass else 'CHECK'])
        writer.writerow(['Drift Detection (@EYE)', 'NONE', 'STABLE' if not drift_detected else 'DETECTED'])
        writer.writerow([])
        
        # Execution phases
        writer.writerow(['EXECUTION PHASES'])
        writer.writerow(['Phase', 'Status', 'Details'])
        seal_statuses = defaultdict(int)
        cool_statuses = defaultdict(int)
        for r in records:
            seal_statuses[r['seal_phase']] += 1
            cool_statuses[r['cool_phase']] += 1
        for status, count in seal_statuses.items():
            writer.writerow(['SEAL', status, f'{count} event(s)'])
        for status, count in cool_statuses.items():
            writer.writerow(['COOL', status, f'{count} event(s)'])
        writer.writerow([])
        
        # Audits
        writer.writerow(['CONSTITUTIONAL CORRECTNESS EVALUATION (CCE)'])
        writer.writerow(['Audit', 'Result', 'Interpretation'])
        audits = {
            'delta_p_audit': 'Delta P (Clarity)',
            'omega_p_audit': 'Omega P (Humility)',
            'phi_p_audit': 'Phi P (Stability)',
            'psi_p_audit': 'Psi P (Vitality)',
        }
        for key, label in audits.items():
            results = defaultdict(int)
            for r in records:
                results[r[key]] += 1
            for result, count in results.items():
                writer.writerow([label, result, f'{count} event(s)'])
        writer.writerow([])
        
        # Tri-witness
        writer.writerow(['TRI-WITNESS CONSENSUS (High-Stakes Verification)'])
        writer.writerow(['Witness', 'Confidence', 'Status'])
        for witness_key, witness_label in [('tri_witness_human', 'Human'), ('tri_witness_ai', 'AI'), 
                                            ('tri_witness_earth', 'Earth'), ('tri_witness_consensus', 'Consensus')]:
            vals = [r[witness_key] for r in records if r[witness_key] is not None]
            if vals:
                try:
                    numeric = [float(v) for v in vals if isinstance(v, (int, float))]
                    if numeric:
                        mean = sum(numeric) / len(numeric)
                        writer.writerow([witness_label, f'{mean:.4f}', 'STRONG' if mean > 0.9 else 'ADEQUATE'])
                except (ValueError, TypeError):
                    pass
        writer.writerow([])
        
        # Data quality
        writer.writerow(['DATA QUALITY & CAVEATS'])
        writer.writerow(['Issue', 'Status', 'Impact'])
        writer.writerow(['Sample Size', f'{len(records)} event(s)', 'MINIMAL' if len(records) < 30 else 'ADEQUATE'])
        writer.writerow(['Statistical Power', 'INSUFFICIENT' if len(records) < 30 else 'ADEQUATE', 
                        'Recommend >30 events for inference' if len(records) < 30 else 'Good'])
        writer.writerow(['Ledger Completeness', 'ACTIVE', 'Live cooling ledger in deployment'])
        writer.writerow(['Missing Floor Metrics', '0 of 9', 'All defined floors present'])


# ============================================================================
# MARKDOWN REPORT GENERATION
# ============================================================================

def generate_markdown(records: List[Dict], output_path: str) -> None:
    """Generate comprehensive narrative report."""
    
    if not records:
        report = "# arifOS Governance Telemetry Analysis Report\n\n**No events found in ledger.**\n"
        with open(output_path, 'w') as f:
            f.write(report)
        return
    
    # Calculate statistics
    verdict_counts = defaultdict(int)
    for r in records:
        verdict_counts[r['verdict']] += 1
    
    high_stakes = sum(1 for r in records if r['high_stakes'])
    sabar = sum(1 for r in records if r['sabar_triggered'])
    drift = sum(1 for r in records if r['drift_detected'])
    
    psi_vals = [r['psi'] for r in records if r['psi'] is not None]
    psi_mean = sum(psi_vals) / len(psi_vals) if psi_vals else 0
    
    shadow_vals = [r['shadow'] for r in records if r['shadow'] is not None]
    shadow_mean = sum(shadow_vals) / len(shadow_vals) if shadow_vals else 0
    
    hantu_pass = sum(1 for r in records if r['hantu_scan'] == 'PASS')
    
    # Build report
    report = f"""# arifOS Governance Telemetry Analysis Report
## v36.3Œ© Constitutional Kernel

**Report Generated:** {datetime.now().isoformat()}  
**Data Source:** `cooling_ledger/L1_cooling_ledger.jsonl`  
**Repository:** [github.com/ariffazil/arifOS](https://github.com/ariffazil/arifOS)  

---

## Executive Summary

The arifOS governance kernel has processed **{len(records)} zkPC-governed event(s)** in the L1 cooling ledger.

**Verdict Distribution:**
- SEAL (all floors pass): {verdict_counts['SEAL']} ({100*verdict_counts['SEAL']/len(records):.1f}%)
- PARTIAL (soft fail): {verdict_counts['PARTIAL']} ({100*verdict_counts['PARTIAL']/len(records):.1f}%)
- VOID (hard fail): {verdict_counts['VOID']} ({100*verdict_counts['VOID']/len(records):.1f}%)
- SABAR (cooling): {verdict_counts['SABAR']} ({100*verdict_counts['SABAR']/len(records):.1f}%)

‚ö†Ô∏è  **CAVEAT:** Sample size n={len(records)} is {'MINIMAL (below n‚â•30 threshold)' if len(records) < 30 else 'adequate'}. Results should be treated as {'proof-of-concept' if len(records) < 30 else 'statistically valid'}.

---

## 1. Key Metrics

| Metric | Value | Status |
|--------|-------|--------|
| **Events** | {len(records)} | {'MINIMAL' if len(records) < 30 else 'ADEQUATE'} |
| **Œ® (PSI)** | {psi_mean:.4f} | {'üü¢ THRIVING' if psi_mean > 1.0 else 'üü° MARGINAL' if abs(psi_mean - 1.0) < 0.1 else 'üî¥ BREACH'} |
| **Shadow** | {shadow_mean:.4f} | {'LOW' if shadow_mean < 0.05 else 'ELEVATED'} |
| **High-Stakes** | {high_stakes}/{len(records)} | {100*high_stakes/len(records):.1f}% |
| **Anti-Hantu** | {hantu_pass}/{len(records)} PASS | OK |

---

## 2. Constitutional Floors

**Hard Floors (5):** All enforced ‚úÖ  
**Soft Floors (3):** All enforced ‚úÖ  
**Meta Floor (1):** {hantu_pass}/{len(records)} PASS ‚úÖ  

---

## 3. System Health

- **Vitality (Œ®):** {psi_mean:.4f} ({'THRIVING' if psi_mean > 1.0 else 'MARGINAL' if abs(psi_mean - 1.0) < 0.1 else 'BREACH'})
- **Shadow Contamination:** {shadow_mean:.4f} ({'LOW' if shadow_mean < 0.05 else 'ELEVATED'})
- **@EYE Sentinel - Hantu:** {hantu_pass}/{len(records)} PASS
- **@EYE Sentinel - Drift:** {drift} detected

---

## 4. Data Quality

| Issue | Status |
|-------|--------|
| Sample Size | {len(records)} event(s) |
| Generalizability | {'NO (n<30)' if len(records) < 30 else 'YES (n‚â•30)'} |
| All 9 Floors | ‚úÖ Present |
| Ledger Integrity | ‚úÖ Operational |

**Recommendation:** Collect 30+ additional events for statistical significance.

---

## 5. Next Steps

1. Continue logging all events to cooling ledger
2. Monitor PSI for drops below 1.0
3. Re-run analyzer monthly
4. Document failure modes
5. Build canonical law from high-quality seals

---

Generated by arifOS Telemetry Lab | License: CC-BY-4.0
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='arifOS Governance Telemetry Analyzer v36.3Œ©',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                                    # Use defaults
  %(prog)s --ledger cooling_ledger/L1_cooling_ledger.jsonl
  %(prog)s --output analysis/
        """
    )
    
    parser.add_argument(
        '--ledger',
        default=DEFAULT_LEDGER_PATH,
        help=f'Path to cooling ledger JSONL (default: {DEFAULT_LEDGER_PATH})'
    )
    parser.add_argument(
        '--output',
        default=DEFAULT_OUTPUT_DIR,
        help=f'Output directory for reports (default: {DEFAULT_OUTPUT_DIR})'
    )
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üîç Loading ledger from: {args.ledger}")
    events = load_ledger(args.ledger)
    
    if not events:
        print("‚ùå No events found in ledger.")
        return 1
    
    print(f"‚úì Loaded {len(events)} total events")
    
    # Extract zkPC receipts (governance events)
    receipts = extract_receipt_events(events)
    print(f"‚úì Found {len(receipts)} zkPC receipt(s)")
    
    if not receipts:
        print("‚ö†Ô∏è  No zkPC receipts found.")
        return 1
    
    # Parse events
    records = [parse_event(e) for e in receipts]
    print(f"‚úì Parsed {len(records)} governance record(s)")
    
    # Generate CSV
    csv_path = output_dir / "arifos_governance_telemetry_summary.csv"
    generate_csv(records, str(csv_path))
    print(f"‚úì CSV saved to: {csv_path}")
    
    # Generate Markdown
    md_path = output_dir / "ARIFOS_GOVERNANCE_TELEMETRY_v36.3O.md"
    generate_markdown(records, str(md_path))
    print(f"‚úì Report saved to: {md_path}")
    
    # Summary
    seal_count = sum(1 for r in records if r['verdict'] == 'SEAL')
    print(f"""
================================================================================
                         ANALYSIS COMPLETE ‚úì
================================================================================
Events: {len(records)} | SEAL: {seal_count} | PSI: {sum(r['psi'] for r in records if r['psi']) / sum(1 for r in records if r['psi']) if any(r['psi'] for r in records) else 'N/A'}
OUTPUT: {csv_path} | {md_path}
================================================================================
""")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
