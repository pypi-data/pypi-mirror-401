#############################################################
from __future__ import annotations
from typing import List, Tuple, Optional, Union, Any
import torch
import torch.nn as nn
import torch
import itertools
import pickle
import os
import random
import torch.nn.functional as F

################################################################
__all__ = ["get_metrics", "get_ratios", "get_skeleton", "prune"]

#
# def hello():
#     print('FAIR Pruner is working well. hahaha...')


##############################################
def _wasserstein_1d(x, y):
    """
    Compute 1D Wasserstein distance (Earth Mover's Distance) between x and y.
    Supports unequal lengths and GPU tensors.
    """
    # âœ… è½¬æˆ float32ï¼ˆæˆ–è€… float64 éƒ½è¡Œï¼‰
    x = x.flatten().to(torch.float32)
    y = y.flatten().to(torch.float32)

    x = x.sort().values.to(torch.float32)
    y = y.sort().values.to(torch.float32)

    n, m = x.size(0), y.size(0)
    if n != m:
        q = torch.linspace(0, 1, steps=max(n, m), device=x.device)
        xq = torch.quantile(x, q)
        yq = torch.quantile(y, q)
        return torch.mean(torch.abs(xq - yq))
    else:
        return torch.mean(torch.abs(x - y))


def _sliced_wasserstein_distance(X, Y, n_projections=128, p=1):
    """
    Approximate Sliced Wasserstein Distance between two point clouds X, Y.
    Supports unequal sample sizes and 1D fallback.
    """
    device = X.device
    X, Y = X.to(torch.float32), Y.to(torch.float32)
    D = X.shape[1]

    # 1D ç‰¹åŒ–
    if D == 1:
        return _wasserstein_1d(X, Y)

    # ç”Ÿæˆéšæœºæ–¹å‘
    theta = torch.randn((D, n_projections), device=device)
    theta = F.normalize(theta, dim=0)

    proj_X = X @ theta
    proj_Y = Y @ theta

    proj_X_sorted, _ = torch.sort(proj_X, dim=0)
    proj_Y_sorted, _ = torch.sort(proj_Y, dim=0)

    #
    proj_X_sorted = proj_X_sorted.to(torch.float32)
    proj_Y_sorted = proj_Y_sorted.to(torch.float32)
    n = max(proj_X_sorted.size(0), proj_Y_sorted.size(0))
    q = torch.linspace(0, 1, n, device=device)

    #
    proj_X_q = torch.quantile(proj_X_sorted, q, dim=0)
    proj_Y_q = torch.quantile(proj_Y_sorted, q, dim=0)

    dist = torch.mean(torch.abs(proj_X_q - proj_Y_q) ** p)
    return dist ** (1.0 / p)


def _get_prunedata(prune_datasetloader, batch_size, class_num, pruning_samples_num):
    #
    # prune_datasetloader : is ï¼ˆfrom torch.utils.data import DataLoaderï¼‰.
    # class_num : is the number of categories in the dataset.
    # pruning_samples_num : is the upper limit of the sample size for each category used to calculate the distance.
    #
    class_data = {}
    for type in range(class_num):
        class_data[f'{type}'] = []
    n = [0] * class_num
    for inputs, targets in prune_datasetloader:
        if all(num >= pruning_samples_num for num in n):
            break
        class_idx = {}
        for type in range(class_num):
            if n[type] <= pruning_samples_num:
                class_idx[f'{type}'] = torch.where(targets == type)[0]
                class_data[f'{type}'].append(torch.index_select(inputs, 0, class_idx[f'{type}']))
                n[type] += len(class_idx[f'{type}'])
    for type in range(class_num):
        class_data[f'{type}'] = torch.cat(class_data[f'{type}'], dim=0)
    prune_data = {}
    for type in range(class_num):
        bnum = class_data[f'{type}'].shape[0] // batch_size
        prune_data[f'{type}'] = []
        for i in range(bnum):
            prune_data[f'{type}'].append(class_data[f'{type}'][(i * batch_size):((i + 1) * batch_size), :])
    print('The pruning data is collated')
    return prune_data


features = {}


# å®šä¹‰ä¸€ä¸ªé’©å­å‡½æ•°
def _get_features(name):
    def hook(model, input, output):
        if isinstance(model, nn.GRU):
            features[name] = output[0]
        else:
            features[name] = output

    return hook


# def get_Distance(model,prunedata,layer_num,device):
#     #
#     # model: is the model we want to prune.
#     # prunedata :is the output from _get_prunedata.
#     # layer_num : it the aim layer we want to compute Distance.
#     # device: torch.device('cpu') or torch.device('cuda') .
#     #
#     model.to(device)
#     # æ³¨å†Œé’©å­åˆ°æ¯ä¸€å±‚
#     for i,(name, layer) in enumerate(model.named_modules()):
#         if i == layer_num:
#             handle = layer.register_forward_hook(_get_features(f'{i}'))
#             break
# ##############################ä»¥ä¸‹æ˜¯æ”¶é›†è¾“å‡º########################################
#     model.eval()
#     with torch.no_grad():
#         output_res = {}
#         for type in range(len(prunedata)):
#             output_res[f'{type}'] = []
#             for num in range(len(prunedata[f'{type}'])):
#                 model(prunedata[f'{type}'][num].to(device))
#                 if features[f'{layer_num}'].dim() == 3:
#                     output_res[f'{type}'].append(features[f'{layer_num}'])
#                 else:
#                     output_res[f'{type}'].append(features[f'{layer_num}'].view(features[f'{layer_num}'].size(0), features[f'{layer_num}'].size(1), -1).contiguous())
#     #############################ä»¥ä¸‹æ˜¯è®¡ç®—è·ç¦»########################################
#
#         if features[f'{layer_num}'].dim() == 4:
#             channel_num = features[f'{layer_num}'].shape[1]
#             all_distance = [0]*channel_num
#             for combo in itertools.combinations(range(len(prunedata)), 2):
#                 for channel in range(channel_num):
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res[f'{combo[0]}'])):
#                         xjbg0.append(output_res[f'{combo[0]}'][i][:,channel,:].contiguous())
#                     for i in range(len(output_res[f'{combo[1]}'])):
#                         xjbg1.append(output_res[f'{combo[1]}'][i][:,channel,:].contiguous())
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = ot.sliced.sliced_wasserstein_distance(xjbg0.cpu().detach().numpy(),
#                                                                      xjbg1.cpu().detach().numpy(),
#                                                                      n_projections =50)
#                     if distance > all_distance[channel]:
#                         all_distance[channel] = distance
#         elif features[f'{layer_num}'].dim() == 2:
#             neuron_num = features[f'{layer_num}'].shape[1]
#             all_distance = [0] *neuron_num
#             for combo in itertools.combinations(range(len(output_res)), 2):
#                 for neuron in range(neuron_num):
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res[f'{combo[0]}'])):
#                         xjbg0.append(output_res[f'{combo[0]}'][i][:,neuron,0].contiguous())
#                     for i in range(len(output_res[f'{combo[1]}'])):
#                         xjbg1.append(output_res[f'{combo[1]}'][i][:,neuron, 0].contiguous())
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = wasserstein_distance(xjbg0.cpu().detach().numpy(), xjbg1.cpu().detach().numpy())
#                     if distance > all_distance[neuron]:
#                         all_distance[neuron] = distance
#         elif features[f'{layer_num}'].dim() == 3:
#             hidden_num = features[f'{layer_num}'].shape[2]
#             all_distance = [0] * hidden_num
#             for combo in itertools.combinations(range(len(prunedata)), 2):
#                 for hidden in range(hidden_num):
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res[f'{combo[0]}'])):
#                         xjbg0.append(output_res[f'{combo[0]}'][i][:, :, hidden].contiguous())
#                     for i in range(len(output_res[f'{combo[1]}'])):
#                         xjbg1.append(output_res[f'{combo[1]}'][i][:, :, hidden].contiguous())
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = ot.sliced.sliced_wasserstein_distance(xjbg0.cpu().detach().numpy(),
#                                                                      xjbg1.cpu().detach().numpy(),
#                                                                      n_projections=50)
#                     if distance > all_distance[hidden]:
#                         all_distance[hidden] = distance
#
#     print(f'The Distance of the {layer_num}th layer is calculated.')
#     all_distance = torch.tensor(all_distance)
#     features.clear()
#     handle.remove()
#
#     return all_distance
#
#
# ###########################################################################################################
# ###########################################################################################################
#
# #v2ï¼šThis version is the same as the previous version
# # ï¼ˆget_Distanceï¼‰, and in order to reduce the pressure
# # on the video memory and memory, the data is saved on
# # the hard disk. Although it slows down the running
# # speed, it can be adopted in the graphics card and
# # insufficient memory space.
#
# def get_Distance2(model,prunedata,layer_num,device,path):
#     #
#     # model: is the model we want to prune
#     # prunedata :is the output from _get_prunedata
#     # layer_num : it the aim layer we want to compute Distance
#     # device: torch.device('cpu') or 'cuda'
#     # path: A path to save the temporary file
#     #
#     model.to(device)
#     # æ³¨å†Œé’©å­åˆ°æ¯ä¸€å±‚
#     for i,(name, layer) in enumerate(model.named_modules()):
#         if i == layer_num:
#             handle = layer.register_forward_hook(_get_features(f'{i}'))
#             break
# ##############################ä»¥ä¸‹æ˜¯æ”¶é›†è¾“å‡º########################################
#     model.eval()
#     with torch.no_grad():
#         for type in range(len(prunedata)):
#             output_res = []
#             for num in range(len(prunedata[f'{type}'])):
#                 model(prunedata[f'{type}'][num].to(device))
#                 output_res.append(features[f'{layer_num}'].view(features[f'{layer_num}'].size(0), features[f'{layer_num}'].size(1), -1).to(device))
#             with open(path+f'/layer{layer_num}_output_type{type}.pkl', 'wb') as file:
#                 pickle.dump(output_res, file)
#     #############################ä»¥ä¸‹æ˜¯è®¡ç®—è·ç¦»########################################
#         if features[f'{layer_num}'].dim() == 4:
#             channel_num = features[f'{layer_num}'].shape[1]
#             all_distance = [0]*channel_num
#             for combo in itertools.combinations(range(len(prunedata)), 2):
#                 with open(path+f'/layer{layer_num}_output_type{combo[0]}.pkl', 'rb') as file:
#                     output_res0 = pickle.load(file)
#                 with open(path+f'/layer{layer_num}_output_type{combo[1]}.pkl', 'rb') as file:
#                     output_res1 = pickle.load(file)
#                 for channel in range(channel_num):
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res0)):
#                         xjbg0.append(output_res0[i][:,channel,:])
#                     for i in range(len(output_res1)):
#                         xjbg1.append(output_res1[i][:,channel,:])
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = ot.sliced.sliced_wasserstein_distance(xjbg0.cpu().detach().numpy(),
#                                                                      xjbg1.cpu().detach().numpy(),
#                                                                      n_projections =50)
#                     if distance > all_distance[channel]:
#                         all_distance[channel] = distance
#
#         elif features[f'{layer_num}'].dim() == 2:
#             neuron_num = features[f'{layer_num}'].shape[1]
#             all_distance = [0] *neuron_num
#             for combo in itertools.combinations(range(len(output_res)), 2):
#                 with open(path+f'/layer{layer_num}_output_type{combo[0]}.pkl','rb') as file:
#                     output_res0 = pickle.load(file)
#                 with open(path+f'/layer{layer_num}_output_type{combo[1]}.pkl','rb') as file:
#                     output_res1 = pickle.load(file)
#                 for neuron in range(neuron_num):
#                     # print(f'ç¬¬{neuron}ä¸ªç¥ç»å…ƒæ­£åœ¨è®¡ç®—è·ç¦»')
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res0)):
#                         xjbg0.append(output_res0[i][:,neuron,0])
#                     for i in range(len(output_res1)):
#                         xjbg1.append(output_res1[i][:,neuron,0])
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = wasserstein_distance(xjbg0.cpu().detach().numpy(),
#                                                     xjbg1.cpu().detach().numpy())
#                     if distance > all_distance[neuron]:
#                         all_distance[neuron] = distance
#         elif features[f'{layer_num}'].dim() == 3:
#             hidden_num = features[f'{layer_num}'].shape[2]
#             all_distance = [0] * hidden_num
#             for combo in itertools.combinations(range(len(prunedata)), 2):
#                 with open(path+f'/layer{layer_num}_output_type{combo[0]}.pkl','rb') as file:
#                     output_res0 = pickle.load(file)
#                 with open(path+f'/layer{layer_num}_output_type{combo[1]}.pkl','rb') as file:
#                     output_res1 = pickle.load(file)
#                 for hidden in range(hidden_num):
#                     xjbg0 = []
#                     xjbg1 = []
#                     for i in range(len(output_res0)):
#                         xjbg0.append(output_res0[i][:, :, hidden])
#                     for i in range(len(output_res0)):
#                         xjbg1.append(output_res0[i][:, :, hidden])
#                     xjbg0 = torch.cat(xjbg0, dim=0)
#                     xjbg1 = torch.cat(xjbg1, dim=0)
#                     distance = ot.sliced.sliced_wasserstein_distance(xjbg0.cpu().detach().numpy(),
#                                                                      xjbg1.cpu().detach().numpy(),
#                                                                      n_projections=50)
#                     if distance > all_distance[hidden]:
#                         all_distance[hidden] = distance
#     print(f'The Distance of the {layer_num}th layer is calculated.')
#     all_distance = torch.tensor(all_distance)
#     features.clear()
#     handle.remove()
#     for type in range(len(prunedata)):
#         os.remove(path+f'/layer{layer_num}_output_type{type}.pkl')
#
#     return all_distance

###############################################################################################################
def _build_comprehensive_index_cache(dataset, class_num=1000):
    """
    æ„å»ºå¥å£®çš„ç´¢å¼•ç¼“å­˜ï¼Œæ”¯æŒå„ç§æ•°æ®é›†ç±»å‹
    """
    index_cache = {c: [] for c in range(class_num)}

    # å¤„ç† Subset
    if isinstance(dataset, torch.utils.data.Subset):
        base_dataset = dataset.dataset
        indices = dataset.indices
    else:
        base_dataset = dataset
        indices = range(len(dataset))

    # å¤šç§æ–¹å¼è·å–æ ‡ç­¾
    for new_idx, orig_idx in enumerate(indices):
        label = None

        # æ–¹å¼1: ç›´æ¥ä»æ ·æœ¬è·å–
        if hasattr(base_dataset, 'samples'):
            _, label = base_dataset.samples[orig_idx]
        # æ–¹å¼2: ä» targets è·å–
        elif hasattr(base_dataset, 'targets'):
            if isinstance(base_dataset.targets, (list, tuple)):
                label = base_dataset.targets[orig_idx]
            else:  # tensor
                label = base_dataset.targets[orig_idx].item()
        # æ–¹å¼3: é€šè¿‡ __getitem__ è·å–
        else:
            try:
                _, label = base_dataset[orig_idx]
            except (ValueError, IndexError):
                try:
                    sample = base_dataset[orig_idx]
                    if isinstance(sample, (list, tuple)) and len(sample) >= 2:
                        _, label = sample
                except:
                    continue

        if label is not None and 0 <= label < class_num:
            index_cache[label].append(new_idx)

    # ç»Ÿè®¡ä¿¡æ¯
    valid_classes = sum(1 for v in index_cache.values() if len(v) > 0)
    total_samples = sum(len(v) for v in index_cache.values())
    # print(f"âœ… Index cache built: {valid_classes} classes, {total_samples} samples")

    return index_cache


def _sample_data_once(dataloader, index_cache, samples_per_class=50, max_classes=100):
    """
    ä¸€æ¬¡æ€§é‡‡æ ·ï¼Œè¿”å›æ‰€æœ‰é€‰ä¸­çš„æ ·æœ¬
    """
    # é€‰æ‹©ç±»åˆ«
    if max_classes and len(index_cache) > max_classes:
        # class_sizes = {cls: len(indices) for cls, indices in index_cache.items()}
        # selected_classes = sorted(class_sizes, key=class_sizes.get, reverse=True)[:max_classes]
        selected_classes = random.sample(list(index_cache.keys()), max_classes)
    else:
        selected_classes = list(index_cache.keys())

    # print(f"ğŸ¯ Sampling {samples_per_class} samples from {len(selected_classes)} classes...")

    # æ”¶é›†æ ·æœ¬
    class_samples = {}
    for cls in selected_classes:
        if cls in index_cache and len(index_cache[cls]) >= samples_per_class:
            selected_indices = random.sample(index_cache[cls], samples_per_class)
            samples = [dataloader.dataset[idx][0] for idx in selected_indices]
            class_samples[cls] = torch.stack(samples)
        else:
            print(f"âš ï¸ Class {cls} has insufficient samples: {len(index_cache.get(cls, []))}")

    # print(f"âœ… Sampling completed: {sum(len(s) for s in class_samples.values())} total samples")
    return class_samples


def _get_layer_distance_memory_efficient(model, layer_num, class_samples, device):
    """
    å†…å­˜é«˜æ•ˆç‰ˆæœ¬ï¼šåŠ¨æ€æå–ç‰¹å¾ï¼Œä¸ä¿å­˜æ‰€æœ‰ç‰¹å¾
    """
    model.to(device)

    # æ³¨å†Œå½“å‰å±‚çš„é’©å­
    for i, (name, layer) in enumerate(model.named_modules()):
        if i == layer_num:
            handle = layer.register_forward_hook(_get_features(f'{layer_num}'))
            break

    model.eval()

    # è·å–ç‰¹å¾ç»´åº¦ä¿¡æ¯ï¼ˆç”¨ç¬¬ä¸€ä¸ªç±»åˆ«æµ‹è¯•ï¼‰
    first_cls = list(class_samples.keys())[0]
    test_sample = class_samples[first_cls][:1].to(device)
    with torch.no_grad():
        model(test_sample)
        feat = features.get(f'{layer_num}')
        if feat is None:
            print(f"âš ï¸ No features found for layer {layer_num}")
            handle.remove()
            return None

        # ç¡®å®šç‰¹å¾ç»´åº¦
        if feat.dim() == 4:  # CNNç‰¹å¾ [batch, channels, H, W]
            channel_num = feat.shape[1]
            all_distance = [0] * channel_num
            feat_type = 'cnn'
        elif feat.dim() == 2:  # å…¨è¿æ¥ç‰¹å¾ [batch, features]
            neuron_num = feat.shape[1]
            all_distance = [0] * neuron_num
            feat_type = 'fc'
        elif feat.dim() == 3:  # RNNç‰¹å¾ [batch, seq_len, hidden]
            hidden_num = feat.shape[2]
            all_distance = [0] * hidden_num
            feat_type = 'rnn'
        else:
            print(f"âš ï¸ Unsupported feature dimension {feat.dim()}")
            handle.remove()
            return None

    print(f"  Processing {len(all_distance)} units ({feat_type} features)")

    # è·å–ç±»åˆ«åˆ—è¡¨å¹¶æ’åº
    class_list = sorted(class_samples.keys())
    total_pairs = len(class_list) * (len(class_list) - 1) // 2

    print(f"  Computing {total_pairs} class pairs...")

    # å¤–å±‚å¾ªç¯ï¼šk1 ä» 1 åˆ° K-1ï¼ˆå®é™…ç´¢å¼•ä»ç¬¬äºŒä¸ªå¼€å§‹ï¼‰
    for i in range(1, len(class_list)):
        k1 = class_list[i]

        # ğŸ¯ æå– k1 çš„ç‰¹å¾ï¼ˆåªæå–ä¸€æ¬¡ï¼Œç”¨äºæ‰€æœ‰ä¸ k2 çš„é…å¯¹ï¼‰
        features_k1 = _extract_class_features(
            model, class_samples[k1], device, layer_num, feat_type)

        if features_k1 is None:
            continue

        # å†…å±‚å¾ªç¯ï¼šk2 ä» 0 åˆ° i-1
        for j in range(i):
            k2 = class_list[j]

            # ğŸ¯ æå– k2 çš„ç‰¹å¾
            features_k2 = _extract_class_features(
                model, class_samples[k2], device, layer_num, feat_type)

            if features_k2 is None:
                continue

            # ğŸ¯ è®¡ç®—è·ç¦»å¹¶æ›´æ–°æœ€å¤§å€¼
            _update_distances(all_distance, features_k1, features_k2,
                              feat_type, device)

            # ğŸ¯ ç«‹å³ä¸¢å¼ƒ k2 çš„ç‰¹å¾
            del features_k2
            torch.cuda.empty_cache()

        # ğŸ¯ å¤„ç†å®Œæ‰€æœ‰ k2 åï¼Œä¸¢å¼ƒ k1 çš„ç‰¹å¾
        del features_k1
        torch.cuda.empty_cache()

        # è¿›åº¦æ˜¾ç¤º
        if i % 200 == 0:
            completed_pairs = i * (i + 1) // 2
            print(f"    Progress: {completed_pairs}/{total_pairs} pairs "
                  f"({completed_pairs / total_pairs * 100:.1f}%)")

    # æ¸…ç†
    features.clear()
    handle.remove()

    print(f"âœ… Layer {layer_num} completed")
    return torch.tensor(all_distance)


def _get_layer_distance(model, layer_num, class_samples, device,
                        sample_classes=10, num_iterations=500):
    """
    è’™ç‰¹å¡æ´›ç‰ˆæœ¬ï¼šé‡å¤é‡‡æ ·ç±»åˆ«å¯¹ï¼Œè®°å½•æœ€å¤§è·ç¦»
    """
    model.to(device)

    # æ³¨å†Œå½“å‰å±‚çš„é’©å­
    for i, (name, layer) in enumerate(model.named_modules()):
        if i == layer_num:
            handle = layer.register_forward_hook(_get_features(f'{layer_num}'))
            break

    model.eval()

    # è·å–ç‰¹å¾ç»´åº¦ä¿¡æ¯
    first_cls = list(class_samples.keys())[0]
    test_sample = class_samples[first_cls][:1].to(device)
    with torch.no_grad():
        model(test_sample)
        feat = features.get(f'{layer_num}')
        if feat is None:
            print(f"âš ï¸ No features found for layer {layer_num}")
            handle.remove()
            return None

        # ç¡®å®šç‰¹å¾ç»´åº¦
        if feat.dim() == 4:  # CNNç‰¹å¾
            channel_num = feat.shape[1]
            all_distance = [0] * channel_num
            feat_type = 'cnn'
        elif feat.dim() == 2:  # å…¨è¿æ¥ç‰¹å¾
            neuron_num = feat.shape[1]
            all_distance = [0] * neuron_num
            feat_type = 'fc'
        elif feat.dim() == 3:  # RNNç‰¹å¾
            hidden_num = feat.shape[2]
            all_distance = [0] * hidden_num
            feat_type = 'rnn'
        else:
            print(f"âš ï¸ Unsupported feature dimension {feat.dim()}")
            handle.remove()
            return None

    # print(
    #     f"  Processing {len(all_distance)} units, {sample_classes} classes per iteration, {num_iterations} iterations")

    # æ‰€æœ‰å¯ç”¨ç±»åˆ«
    all_classes = list(class_samples.keys())

    for iteration in range(num_iterations):
        # if iteration % 200 == 0:
        #     print(f"    Iteration {iteration + 1}/{num_iterations}")

        # 1. éšæœºé‡‡æ ·ç±»åˆ«
        sampled_classes = random.sample(all_classes, sample_classes)

        # 2. è®¡ç®—è¿™äº›ç±»åˆ«ä¸­æ‰€æœ‰é…å¯¹çš„è·ç¦»
        for k1, k2 in itertools.combinations(sampled_classes, 2):
            # æå–ç‰¹å¾
            features_k1 = _extract_class_features(
                model, class_samples[k1], device, layer_num, feat_type)
            features_k2 = _extract_class_features(
                model, class_samples[k2], device, layer_num, feat_type)

            if features_k1 is None or features_k2 is None:
                continue

            # è®¡ç®—è·ç¦»å¹¶æ›´æ–°æœ€å¤§å€¼
            _update_distances_max(all_distance, features_k1, features_k2, feat_type, device)

            # æ¸…ç†
            del features_k1, features_k2
            torch.cuda.empty_cache()

    # æ¸…ç†
    features.clear()
    handle.remove()

    # print(f"âœ… The Distance of layer {layer_num} is calculated.")
    return torch.tensor(all_distance)


def _update_distances_max(all_distance, features1, features2, feat_type, device):
    """
    è®¡ç®—è·ç¦»å¹¶åªæ›´æ–°æœ€å¤§å€¼ï¼ˆä¸ç´¯åŠ ï¼‰
    """
    features1_gpu = [f.to(device) for f in features1]
    features2_gpu = [f.to(device) for f in features2]

    if feat_type == 'cnn':
        channel_num = features1_gpu[0].shape[1]
        for channel in range(channel_num):
            x0 = torch.cat([f[:, channel, :] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, channel, :] for f in features2_gpu], dim=0)
            distance = _sliced_wasserstein_distance(x0, x1, n_projections=128)
            if distance > all_distance[channel]:
                all_distance[channel] = distance

    elif feat_type == 'fc':
        neuron_num = features1_gpu[0].shape[1]
        for neuron in range(neuron_num):
            x0 = torch.cat([f[:, neuron, 0] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, neuron, 0] for f in features2_gpu], dim=0)
            distance = _wasserstein_1d(x0, x1)
            if distance > all_distance[neuron]:
                all_distance[neuron] = distance

    elif feat_type == 'rnn':
        hidden_num = features1_gpu[0].shape[2]
        for hidden in range(hidden_num):
            x0 = torch.cat([f[:, :, hidden] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, :, hidden] for f in features2_gpu], dim=0)
            distance = _sliced_wasserstein_distance(x0, x1, n_projections=64)
            if distance > all_distance[hidden]:
                all_distance[hidden] = distance

    # æ¸…ç†GPUå†…å­˜
    del features1_gpu, features2_gpu
    torch.cuda.empty_cache()


def _extract_class_features(model, samples, device, layer_num, feat_type):
    """
    æå–å•ä¸ªç±»åˆ«çš„ç‰¹å¾
    """
    features_list = []
    batch_size = min(256, len(samples))
    with torch.no_grad():
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i + batch_size].to(device)
            model(batch)

            feat = features.get(f'{layer_num}')
            if feat is not None:
                # æ ¹æ®ç‰¹å¾ç±»å‹å¤„ç†
                if feat_type == 'cnn':
                    # CNN: [batch, channels, H, W] -> [batch, channels, H*W]
                    feat_processed = feat.view(feat.size(0), feat.size(1), -1)
                elif feat_type == 'fc':
                    # FC: [batch, features] -> [batch, features, 1]
                    feat_processed = feat.unsqueeze(-1)
                elif feat_type == 'rnn':
                    # RNN: [batch, seq_len, hidden] ä¿æŒåŸæ ·
                    feat_processed = feat
                else:
                    feat_processed = feat

                features_list.append(feat_processed)  # ç§»åˆ°CPUä¿å­˜.cpu()

            del batch
            if device.type == 'cuda':
                torch.cuda.empty_cache()

    return features_list if features_list else None


def _update_distances(all_distance, features1, features2, feat_type, device):
    """
    è®¡ç®—ä¸¤ä¸ªç±»åˆ«ç‰¹å¾çš„è·ç¦»å¹¶æ›´æ–°æœ€å¤§å€¼
    """
    # ç§»åŠ¨åˆ°GPUè®¡ç®—
    features1_gpu = [f.to(device) for f in features1]
    features2_gpu = [f.to(device) for f in features2]

    if feat_type == 'cnn':
        # CNN: å¯¹æ¯ä¸ªé€šé“è®¡ç®—è·ç¦»
        channel_num = features1_gpu[0].shape[1]
        for channel in range(channel_num):
            x0 = torch.cat([f[:, channel, :] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, channel, :] for f in features2_gpu], dim=0)
            distance = _sliced_wasserstein_distance(x0, x1, n_projections=128)
            if distance > all_distance[channel]:
                all_distance[channel] = distance

    elif feat_type == 'fc':
        # FC: å¯¹æ¯ä¸ªç¥ç»å…ƒè®¡ç®—è·ç¦»
        neuron_num = features1_gpu[0].shape[1]
        for neuron in range(neuron_num):
            x0 = torch.cat([f[:, neuron, 0] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, neuron, 0] for f in features2_gpu], dim=0)
            distance = _wasserstein_1d(x0, x1)
            if distance > all_distance[neuron]:
                all_distance[neuron] = distance

    elif feat_type == 'rnn':
        # RNN: å¯¹æ¯ä¸ªéšè—å•å…ƒè®¡ç®—è·ç¦»
        hidden_num = features1_gpu[0].shape[2]
        for hidden in range(hidden_num):
            x0 = torch.cat([f[:, :, hidden] for f in features1_gpu], dim=0)
            x1 = torch.cat([f[:, :, hidden] for f in features2_gpu], dim=0)
            distance = _sliced_wasserstein_distance(x0, x1, n_projections=128)
            if distance > all_distance[hidden]:
                all_distance[hidden] = distance

    # æ¸…ç†GPUå†…å­˜
    del features1_gpu, features2_gpu
    torch.cuda.empty_cache()


###################################################################################################################
# åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯ä¸€å±‚çš„æ¢¯åº¦
gradients = {}


# å®šä¹‰ä¸€ä¸ªé’©å­å‡½æ•°æ¥è·å–æ¢¯åº¦
def _get_grad(name):
    def hook(module, grad_input, grad_output):
        # grad_inputæ˜¯è¾“å…¥çš„æ¢¯åº¦ï¼Œgrad_outputæ˜¯è¾“å‡ºçš„æ¢¯åº¦
        gradients[name] = grad_output[0]

    return hook


# å®šä¹‰ä¸€ä¸ªé’©å­å‡½æ•°æ¥æ•è·GRU modelæƒé‡çš„æ¢¯åº¦
gru1_weight_ih_grad = None
gru1_weight_hh_grad = None


def _hook_gru1_weight_ih(grad):
    global gru1_weight_ih_grad
    gru1_weight_ih_grad = grad


def _hook_gru1_weight_hh(grad):
    global gru1_weight_hh_grad
    gru1_weight_hh_grad = grad


# def get_ReconstructionError(model,prune_datasetloader,layer_num,device,loss_function):
#     #
#     # model: is the model we want to prune.
#     # prune_datasetloader : is ï¼ˆfrom torch.utils.data import DataLoaderï¼‰.
#     # layer_num : it the aim layer we want to compute Distance.
#     # device: torch.device('cpu') or torch.device('cuda') .
#     # loss_function: The loss function used for model training.
#     #
#
#     model.to(device)
#     model.train()
#     if isinstance(list(model.named_modules())[layer_num][1],nn.GRU):
#         the_wih = list(model.named_modules())[layer_num][1].weight_ih_l0.data
#         the_whh = list(model.named_modules())[layer_num][1].weight_hh_l0.data
#         the_Bias_ih = list(model.named_modules())[layer_num][1].bias_ih_l0.data
#         the_Bias_hh = list(model.named_modules())[layer_num][1].bias_hh_l0.data
#         loss_fun = loss_function
#         loss_fun.to(device)
#         gradient = torch.zeros(the_wih.shape[0]).to(device)  # shape[0]
#         hinden_num = the_wih.shape[0]
#         for inputs, targets in prune_datasetloader:
#             inputs = inputs.to(device)
#             targets = targets.to(device)
#             output = model(inputs)
#             loss = loss_fun(output, targets)
#             loss.backward()
#             gradient += torch.sum(list(model.named_modules())[layer_num][1].weight_ih_l0.grad * the_wih,dim=1)
#             gradient += torch.sum(list(model.named_modules())[layer_num][1].weight_hh_l0.grad * the_whh,dim=1)
#             gradient += list(model.named_modules())[layer_num][1].bias_ih_l0.grad * the_Bias_ih
#             gradient += list(model.named_modules())[layer_num][1].bias_hh_l0.grad * the_Bias_hh
#         gradient = gradient[0:int(hinden_num/3)] + gradient[int(hinden_num/3):int(hinden_num/3*2)] + gradient[int(hinden_num/3*2):]
#     else:
#         the_weight = list(model.named_modules())[layer_num][1].weight.data
#         dim = the_weight.dim()
#         the_bias = list(model.named_modules())[layer_num][1].bias.data.view(the_weight.shape[0], *([1] * (dim - 1)))
#         # print(the_weight.shape)
#         loss_fun = loss_function
#         loss_fun.to(device)
#         gradient = torch.zeros(the_weight.shape).to(device)#shape[0]
#         # print(gradients[f'{layer_num}'].shape)
#         for inputs, targets in prune_datasetloader:
#             inputs = inputs.to(device)
#             targets = targets.to(device)
#             output = model(inputs)
#             loss = loss_fun(output, targets)
#             loss.backward()
#             gradient += list(model.named_modules())[layer_num][1].weight.grad * the_weight
#             gradient += list(model.named_modules())[layer_num][1].bias.grad.view(the_weight.shape[0], *([1] * (dim - 1))) * the_bias
#         gradient = torch.sum(gradient,dim = list(range(1,gradient.dim())))
#     print(f'The Reconstruction Error of the {layer_num}th layer is calculated.')
#     gradients.clear()
#
#     return gradient
# ################################################
# ################################################
# # The only difference between the two versions is
# # whether the gradient is zeroed out on each
# # calculation, and empirically the first version
# #ï¼ˆget_ReconstructionErrorï¼‰works better
# def get_ReconstructionError2(model,prune_datasetloader,layer_num,device,loss_function):
#     #
#     # model: is the model we want to prune.
#     # prune_datasetloader : is ï¼ˆfrom torch.utils.data import DataLoaderï¼‰.
#     # layer_num : it the aim layer we want to compute Distance.
#     # device: torch.device('cpu') or torch.device('cuda') .
#     # loss_function: The loss function used for model training.
#     #
#     model.to(device)
#     model.train()
#     if isinstance(list(model.named_modules())[layer_num][1],nn.GRU):
#         the_wih = list(model.named_modules())[layer_num][1].weight_ih_l0.data
#         the_whh = list(model.named_modules())[layer_num][1].weight_hh_l0.data
#         the_Bias_ih = list(model.named_modules())[layer_num][1].bias_ih_l0.data
#         the_Bias_hh = list(model.named_modules())[layer_num][1].bias_hh_l0.data
#         loss_fun = loss_function
#         loss_fun.to(device)
#         gradient = torch.zeros(the_wih.shape[0]).to(device)  # shape[0]
#         hinden_num = the_wih.shape[0]
#         for inputs, targets in prune_datasetloader:
#             inputs = inputs.to(device)
#             targets = targets.to(device)
#             output = model(inputs)
#             loss = loss_fun(output, targets)
#             loss.backward()
#             gradient += torch.sum(list(model.named_modules())[layer_num][1].weight_ih_l0.grad * the_wih,dim=1)
#             gradient += torch.sum(list(model.named_modules())[layer_num][1].weight_hh_l0.grad * the_whh,dim=1)
#             gradient += list(model.named_modules())[layer_num][1].bias_ih_l0.grad * the_Bias_ih
#             gradient += list(model.named_modules())[layer_num][1].bias_hh_l0.grad * the_Bias_hh
#         gradient = gradient[0:int(hinden_num/3)] + gradient[int(hinden_num/3):int(hinden_num/3*2)] + gradient[int(hinden_num/3*2):]
#     else:
#         the_weight = list(model.named_modules())[layer_num][1].weight.data
#         dim = the_weight.dim()
#         the_bias = list(model.named_modules())[layer_num][1].bias.data.view(the_weight.shape[0], *([1] * (dim- 1)))
#         loss_fun = nn.CrossEntropyLoss()
#         loss_fun.to(device)
#         model.zero_grad()
#         for inputs, targets in prune_datasetloader:
#             inputs = inputs.to(device)
#             targets = targets.to(device)
#             output = model(inputs)
#             loss = loss_fun(output, targets)
#             loss.backward()
#         gradient = (list(model.named_modules())[layer_num][1].weight.grad * the_weight +
#                     list(model.named_modules())[layer_num][1].bias.grad.view(the_weight.shape[0],*([1] * (dim - 1))) * the_bias)
#         gradient = torch.sum(gradient,dim = list(range(1,gradient.dim())))
#     print(f'The Reconstruction Error of the {layer_num}th layer is calculated.')
#     gradients.clear()
#
#     return gradient
###################################################################################################################################

def _get_reconstructionScore_fast(model, prune_loader, layer_num, device, loss_function):
    model.to(device)
    model.train()

    target_layer = list(model.named_modules())[layer_num][1]

    # å†»ç»“å…¶ä»–å±‚ï¼Œåªä¿ç•™ç›®æ ‡å±‚æ¢¯åº¦
    for name, param in model.named_parameters():
        param.requires_grad_(False)
    for param in target_layer.parameters():
        param.requires_grad_(True)

    grad_accum = None  # ğŸš¨ ä¸è¦ç”¨ Python æ•´æ•°å½“åˆå§‹å€¼
    use_amp = (device.type == 'cuda')
    for inputs, targets in prune_loader:
        inputs = inputs.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)

        model.zero_grad(set_to_none=True)

        try:  # PyTorch (>= 1.14)
            scaler = torch.amp.GradScaler(enabled=use_amp)
            output = model(inputs)
            loss = loss_function(output, targets)
            scaler.scale(loss).backward()
        except AttributeError:
            try:  # PyTorch (>= 1.6)
                from torch.cuda.amp import GradScaler
                scaler = GradScaler(enabled=use_amp)
                output = model(inputs)
                loss = loss_function(output, targets)
                scaler.scale(loss).backward()
            except ImportError:  # older,close AMP
                use_amp = False
                output = model(inputs)
                loss = loss_function(output, targets)
                loss.backward()

        grad_w = target_layer.weight.grad  # ä¸éœ€è¦ grad
        contrib_w = (grad_w * target_layer.weight).sum(dim=list(range(1, grad_w.dim())))

        if target_layer.bias is not None:
            grad_b = target_layer.bias.grad
            contrib_b = grad_b * target_layer.bias

        # 2. âœ… åœ¨ no_grad é‡Œåšç´¯è®¡ / æˆ–è€…ç”¨ detach()
        with torch.no_grad():
            if target_layer.bias is not None:
                contrib = contrib_w + contrib_b
                if grad_accum is None:
                    grad_accum = contrib.clone()
                else:
                    grad_accum.add_(contrib)
            else:
                contrib = contrib_w
                if grad_accum is None:
                    grad_accum = contrib.clone()
                else:
                    grad_accum.add_(contrib)

        # 3. åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡
        del output, loss, contrib_w, contrib_b, contrib
        if device.type == 'cuda':
            torch.cuda.empty_cache()

    # print(f"âœ… The Reconstruction Score of layer {layer_num} is calculated.")

    # 4. æ¢å¤ requires_grad
    for param in model.parameters():
        param.requires_grad_(True)

    # 5. è¿”å› CPU ä¸Šçš„ç»å¯¹å€¼
    return grad_accum.abs().detach().cpu()


########################################################################################################################

def get_ratios(model, results, layers2prune=None, ToD_level=0.005):
    #
    # results: resluts save from main
    # the_list_of_layers_to_compute_Distance: a list of the layer number which we want to get Distance
    # layers2prune: a list of the layer number which we want to get ReconstructionError
    # FDR_level
    if layers2prune is None:
        auto = _auto_prunable_layer_indices(
            model,
            exclude_output_layer=True,
        )
        layers2prune = auto


    # results[f'D_{len(layers2prune) - 1}_s2b_idx'] = list(range(1000))
    ratios = []
    for j in range((len(layers2prune))):
        # if (j + 1) == len(layers2prune):
        #     ratios.append(0)
        #     break
        neuron_number = len(results[f'D_{j}_s2b_idx'])
        # m = int((1-2*FDR_level)*neuron_number)
        for i in range(int(0.99 * neuron_number)):
            k = int(0.99 * neuron_number) - i
            intersection = list(
                set(results[f'D_{j}_s2b_idx'][:k]) & set(results[f'RE_{j}_hat_s2b_idx'][int(neuron_number - k):]))
            # print(len(intersection)/k)
            if len(intersection) / k <= ToD_level:
                # print(f'The prunable set size for layer {layers2prune[j]}th is {k}')
                ratios.append(k)
                break
            if i == (int(0.99 * neuron_number) - 1):
            #     print(f'The {layers2prune[j]}th layer is inspected and does not need to be pruned')
                ratios.append(0)
                break
    print('The pruning ratios for each layer have been obtained.')
    return ratios


#######################################################################################################################
def _get_num_classes(loader: torch.utils.data.DataLoader):
    """
    å°½å¯èƒ½è‡ªåŠ¨å…¼å®¹ ImageFolderã€Subsetã€ConcatDatasetã€è‡ªå®šä¹‰æ•°æ®é›†ã€‚
    è¿”å› (num_classes, class_names)
    """
    ds = loader.dataset

    # 1. ç›´æ¥æœ‰ .classes
    if hasattr(ds, 'classes'):
        return len(ds.classes), ds.classes

    # 2. æœ‰ .class_to_idx ä¹Ÿè¡Œ
    if hasattr(ds, 'class_to_idx'):
        idx2name = {v: k for k, v in ds.class_to_idx.items()}
        names = [idx2name[i] for i in range(len(idx2name))]
        return len(names), names

    # 3. Subsetï¼šé€’å½’åˆ°åº•å±‚
    if hasattr(ds, 'dataset'):  # torch.utils.data.Subset
        return _get_num_classes(torch.utils.data.DataLoader(ds.dataset, batch_size=1))

    # 4. ConcatDatasetï¼šæŠŠå„å­é›†ç±»åˆ«ç´¯åŠ ï¼ˆå‡è®¾æ— é‡å¤ç±»ï¼‰
    if hasattr(ds, 'datasets'):  # torch.utils.data.ConcatDataset
        total_names = []
        for sub_ds in ds.datasets:
            _, names = _get_num_classes(torch.utils.data.DataLoader(sub_ds, batch_size=1))
            total_names.extend(names)
        # å»é‡
        total_names = sorted(set(total_names))
        return len(total_names), total_names

    # 5. è‡ªå®šä¹‰æ•°æ®é›†ï¼šçº¦å®šå®ç° .num_classes æˆ– .classes
    if hasattr(ds, 'num_classes'):
        return ds.num_classes, getattr(ds, 'classes', None)

    raise ValueError(
        "It is impossible to infer the category divisions from the DataLoader."
        "Please ensure that the dataset object has the attributes .classes / .class_to_idx / .num_classes."
        "Or manually pass in the category information."
    )


def _auto_prunable_layer_indices(
    model: nn.Module,
    *,
    num_classes: Optional[int] = None,
    exclude_output_layer: bool = True,
) -> List[int]:
    """
    è‡ªåŠ¨è¿”å›ï¼šmodel.named_modules() é¡ºåºä¸‹ï¼Œæ‰€æœ‰å¯å‰ªæå±‚çš„ indexï¼ˆConv/ConvTranspose/Linearï¼‰ã€‚
    é»˜è®¤æ’é™¤è¾“å‡ºå±‚ï¼ˆä¼˜å…ˆæ’é™¤æœ€åä¸€ä¸ª out_features == num_classes çš„ Linearï¼›å¦åˆ™æ’é™¤æœ€åä¸€ä¸ªå¯å‰ªæå±‚ï¼‰ã€‚
    æ³¨æ„ï¼šè¿™é‡Œçš„ index ä¸ä½ åé¢ get_skeleton/prune é‡Œç”¨çš„ list(model.named_modules()) ä¸€è‡´ï¼ˆåŒ…å« root ''ï¼‰ã€‚
    """
    named = list(model.named_modules())

    prunable_types = (
        nn.Conv1d, nn.Conv2d, nn.Conv3d,
        nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,
        nn.Linear,
    )

    idxs: List[int] = []
    for i, (name, m) in enumerate(named):
        if name == "":  # root module è·³è¿‡
            continue
        if isinstance(m, prunable_types):
            idxs.append(i)

    if not idxs:
        return idxs

    if exclude_output_layer:
        # 1) æ›´ç¨³ï¼šä¼˜å…ˆæ’é™¤ â€œæœ€åä¸€ä¸ª out_features == num_classes çš„ Linearâ€
        if num_classes is not None:
            cand = [
                i for i in idxs
                if isinstance(named[i][1], nn.Linear) and int(named[i][1].out_features) == int(num_classes)
            ]
            if cand:
                idxs.remove(cand[-1])
                return idxs

        # 2) å…œåº•ï¼šæ’é™¤æœ€åä¸€ä¸ªå¯å‰ªæå±‚
        if len(idxs) >= 2:
            idxs = idxs[:-1]
        else:
            # åªæœ‰ä¸€ä¸ªå¯å‰ªæå±‚æ—¶ï¼Œè‡³å°‘ä¿ç•™ä¸ºç©ºï¼ˆè¡¨ç¤ºâ€œæ²¡æ³•å‰ªâ€æ›´å®‰å…¨ï¼‰
            idxs = []

    return idxs


def get_metrics(
        model,
        prune_datasetloader,
        layers2prune:Optional[List[int]]=None,
        layers2analysis: Optional[List[int]]=None,
        *,
        loss_function=None,
        device=None,
        the_samplesize_for_compute_distance=32,
        class_num_for_distance=None,
        num_iterations=1,
        results_save_path="results.pkl",
):
    # 1) resolve device / loss on call-time (not import-time)
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if loss_function is None:
        loss_function = nn.CrossEntropyLoss()

    # 1.5) infer num_classes first (used for auto-excluding output layer)
    class_num = _get_num_classes(prune_datasetloader)[0]
    if class_num >100:
        class_num = 100
    # 1.6) auto-select layers if user didn't provide
    if layers2prune is None and layers2analysis is None:
        auto = _auto_prunable_layer_indices(
            model,
            exclude_output_layer=True,
        )
        layers2prune = auto
        layers2analysis = auto
        # å¯é€‰ï¼šæ‰“å°ä¸€ä¸‹é»˜è®¤é€‰äº†å“ªäº›å±‚
        # print(f"[get_metrics] auto layers2prune/analysis = {layers2prune}")

    elif layers2prune is None and layers2analysis is not None:
        layers2prune = list(layers2analysis)

    elif layers2analysis is None and layers2prune is not None:
        layers2analysis = list(layers2prune)

    # æœ€ç»ˆä¸€è‡´æ€§æ£€æŸ¥
    if layers2prune is None or layers2analysis is None:
        raise RuntimeError("Internal error: layers2prune/layers2analysis should have been resolved.")
    if len(layers2prune) == 0:
        raise RuntimeError("No prunable layers found after excluding output layer. Please pass layers2prune manually.")
    if len(layers2prune) != len(layers2analysis):
        raise ValueError("layers2prune and layers2analysis must have the same length.")


    ok = _safetyCheck(
        model,
        layers2prune=layers2prune,
        analysis_layers=layers2analysis,
        analysis_ds_loader=prune_datasetloader,
    )
    if not ok:
        raise RuntimeError("safetyCheck failed: invalid layer list / model / dataloader.")
    #
    # class_num = _get_num_classes(prune_datasetloader)[0]
    if class_num_for_distance is None:
        class_num_for_distance = class_num
    class_num_for_distance = int(min(class_num_for_distance, class_num))

    # 2) build index cache & sample once (respect class_num_for_distance)
    index_cache = _build_comprehensive_index_cache(prune_datasetloader.dataset, class_num=class_num)
    class_samples = _sample_data_once(
        prune_datasetloader,
        index_cache,
        samples_per_class=the_samplesize_for_compute_distance,
        max_classes=class_num_for_distance,  # âœ… use class_num_for_distance
    )

    results = {}
    for j in range(len(layers2prune)):
        # Reconstruction
        results[f"RE_{j}_hat"] = _get_reconstructionScore_fast(
            model,
            prune_datasetloader,
            layer_num=layers2prune[j],
            device=device,
            loss_function=loss_function,
        )
        results[f"RE_{j}_hat_s2b_idx"] = torch.argsort(results[f"RE_{j}_hat"]).tolist()

        if device.type == "cuda":
            torch.cuda.empty_cache()

        # Distance (Monte Carlo sampling over classes)
        results[f"D_{j}"] = _get_layer_distance(
            model,
            layer_num=layers2analysis[j],
            class_samples=class_samples,
            device=device,
            sample_classes=class_num_for_distance,
            num_iterations=num_iterations,
        )
        results[f"D_{j}_s2b_idx"] = torch.argsort(results[f"D_{j}"]).tolist()

        if device.type == "cuda":
            torch.cuda.empty_cache()

    with open(results_save_path, "wb") as f:
        pickle.dump(results, f)
    print('All the statistical calculations have been completed.')
    return results


import copy
from typing import Optional

import torch
import torch.nn as nn
from torch.optim import SGD
from torch.utils.data import DataLoader


@torch.no_grad()
def _evaluate(model: nn.Module, val_loader: DataLoader, criterion: nn.Module, device: torch.device) -> float:
    """Return average validation loss."""
    model.eval()
    total_loss, total_n = 0.0, 0

    for batch in val_loader:
        # é»˜è®¤ batch=(inputs, targets). å¦‚æœä½ çš„ batch æ˜¯ dictï¼Œåœ¨è¿™é‡Œæ”¹å–å€¼æ–¹å¼
        inputs, targets = batch
        inputs = inputs.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)

        logits = model(inputs)  # [B, C]
        loss = criterion(logits, targets)

        bs = targets.size(0)
        total_loss += loss.item() * bs
        total_n += bs

    return total_loss / max(total_n, 1)


def _finetune_model(
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        *,
        lr: float = 1e-2,
        momentum: float = 0.9,
        weight_decay: float = 0.0,
        nesterov: bool = False,
        device: Optional[str] = None,
        grad_clip_norm: Optional[float] = None,
        amp: bool = True,
        log_every: int = 50,
) -> nn.Module:
    """
    å¾®è°ƒå‡½æ•°ï¼ˆå¤šåˆ†ç±»ï¼‰ï¼š
    - Loss: CrossEntropyLoss
    - Optim: SGD
    - è¿”å›ï¼šéªŒè¯é›†ä¸Š val_loss æœ€ä½³çš„æ¨¡å‹ï¼ˆæƒé‡å·²åŠ è½½ï¼Œä¸ä¿å­˜æœ¬åœ°ï¼‰
    """
    dev = torch.device(device) if device is not None else torch.device(
        "cuda" if torch.cuda.is_available() else "cpu"
    )
    model = model.to(dev)

    criterion = nn.CrossEntropyLoss()
    optimizer = SGD(
        model.parameters(),
        lr=lr,
        momentum=momentum,
        weight_decay=weight_decay,
        nesterov=nesterov,
    )

    use_amp = bool(amp and dev.type == "cuda")
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

    best_state = copy.deepcopy(model.state_dict())
    best_val_loss = float("inf")

    for epoch in range(1, epochs + 1):
        model.train()
        run_loss, run_correct, run_n = 0.0, 0, 0

        for step, batch in enumerate(train_loader, start=1):
            inputs, targets = batch
            inputs = inputs.to(dev, non_blocking=True)
            targets = targets.to(dev, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            with torch.cuda.amp.autocast(enabled=use_amp):
                logits = model(inputs)  # [B, C]
                loss = criterion(logits, targets)

            scaler.scale(loss).backward()

            if grad_clip_norm is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)

            scaler.step(optimizer)
            scaler.update()

            bs = targets.size(0)
            run_loss += loss.item() * bs
            run_correct += (logits.detach().argmax(dim=1) == targets).sum().item()
            run_n += bs

            if log_every > 0 and (step % log_every == 0):
                print(
                    f"[Epoch {epoch}/{epochs}] step {step}/{len(train_loader)} | "
                    f"train_loss={run_loss / max(run_n, 1):.4f} train_acc={run_correct / max(run_n, 1):.4f}"
                )

        # validate & track best
        val_loss = _evaluate(model, val_loader, criterion, dev)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = copy.deepcopy(model.state_dict())

        print(
            f"[Epoch {epoch}/{epochs}] "
            f"train_loss={run_loss / max(run_n, 1):.4f} train_acc={run_correct / max(run_n, 1):.4f} | "
            f"val_loss={val_loss:.4f} (best={best_val_loss:.4f})"
        )

    # load best weights (no local saving)
    model.load_state_dict(best_state)
    return model


import copy
import operator
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Union, Any
from collections import deque

import torch
import torch.nn as nn
import torch.fx as fx
from torch.fx.passes.shape_prop import ShapeProp


# =========================
# Utils: shapes & indices
# =========================

@dataclass
class Spec:
    shape: Tuple[int, ...]
    c_axis: Optional[int]  # NCHW->1, (N,F)->1, (N,T,C)->-1


#

def _get_shape(node: fx.Node) -> Optional[Tuple[int, ...]]:
    tm = node.meta.get("tensor_meta", None)
    if tm is None:
        return None
    shp = getattr(tm, "shape", None)
    if shp is None:
        return None
    return tuple(int(x) for x in shp)


def _default_c_axis(shape: Tuple[int, ...]) -> Optional[int]:
    if len(shape) == 4:
        return 1
    if len(shape) == 2:
        return 1
    if len(shape) == 3:
        return -1
    return None


def _to_long_idx(x: Union[List[int], torch.Tensor], device="cpu") -> torch.Tensor:
    if isinstance(x, torch.Tensor):
        return x.to(device=device, dtype=torch.long)
    return torch.tensor(list(x), dtype=torch.long, device=device)


def _full_idx(n: int, device="cpu") -> torch.Tensor:
    return torch.arange(int(n), dtype=torch.long, device=device)


def _expand_channel_idx_to_flat_features(ch_idx: torch.Tensor, spatial: int) -> torch.Tensor:
    # [c0,c1,...] -> [c0*sp:(c0+1)*sp, c1*sp:(c1+1)*sp, ...]
    if spatial <= 1:
        return ch_idx
    out = []
    for c in ch_idx.tolist():
        out.append(torch.arange(c * spatial, (c + 1) * spatial, dtype=torch.long, device=ch_idx.device))
    return torch.cat(out, dim=0) if out else ch_idx.new_empty((0,), dtype=torch.long)


def _is_depthwise_conv(m: nn.Module) -> bool:
    return isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)) and (m.groups == m.in_channels == m.out_channels)


# =========================
# Trace: find nearest Conv/Linear as "channel source"
# =========================

def _trace_channel_source(node: fx.Node, gm: fx.GraphModule) -> Optional[str]:
    """BFS trace backward to find nearest Conv/Linear module target name."""
    modules = dict(gm.named_modules())
    q = deque([node])
    visited = set()

    def push(v):
        if isinstance(v, fx.Node):
            q.append(v)
        elif isinstance(v, (list, tuple)):
            for vv in v:
                push(vv)

    while q:
        cur = q.popleft()
        if not isinstance(cur, fx.Node) or cur in visited:
            continue
        visited.add(cur)

        if cur.op == "call_module":
            m = modules.get(cur.target, None)
            if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                              nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,
                              nn.Linear)):
                return cur.target
            # passthrough first input
            if cur.args:
                push(cur.args[0])
            continue

        if cur.op == "call_function":
            for a in cur.args:
                push(a)
            for a in cur.kwargs.values():
                push(a)
            continue

        if cur.op == "call_method":
            if cur.args:
                push(cur.args[0])
            continue

    return None


# =========================
# Build keep_out_idx_map from your results + named_modules indices
# =========================

def _build_keep_out_idx_map_from_results(
        original_model: nn.Module,
        layers2prune: List[int],  # indices in list(original_model.named_modules())
        ratios: List[int],
        results: Dict[str, Any],
        key_fmt: str = "D_{j}_s2b_idx",
        device: str = "cpu",
) -> Dict[str, torch.Tensor]:
    """
    results[key_fmt.format(j=j)] should be a list of channel indices sorted small->big by importance.
    We prune first k, keep the rest.
    Returns: {module_path: keep_out_idx_tensor(sorted)}
    """
    named = list(original_model.named_modules())
    assert len(layers2prune) == len(ratios), "prune list and ratios length mismatch"

    keep_out: Dict[str, torch.Tensor] = {}
    for j, (idx, k) in enumerate(zip(layers2prune, ratios)):
        path, m = named[idx]
        key = key_fmt.format(j=j)
        if key not in results:
            raise KeyError(f"results missing key: {key}")

        pos = results[key]
        pos = _to_long_idx(pos, device=device)

        k = int(k)
        if k < 0:
            k = 0
        if k >= pos.numel():
            # keep at least 1
            keep = pos[:1]
        else:
            keep = pos[k:]

        keep = torch.sort(keep).values

        # sanity with out dim
        out0 = None
        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                          nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
            out0 = int(m.out_channels)
        elif isinstance(m, nn.Linear):
            out0 = int(m.out_features)
        else:
            raise RuntimeError(f"Not Conv/Linear at idx={idx}: {path} -> {type(m).__name__}")

        keep = keep[(keep >= 0) & (keep < out0)]
        if keep.numel() == 0:
            keep = _full_idx(out0, device=device)[:1]

        keep_out[path] = keep

    return keep_out


# =========================
# Align keep_out indices for residual add (make both sides use SAME indices)
# =========================

class _UF:
    def __init__(self):
        self.p = {}

    def find(self, x):
        if x not in self.p:
            self.p[x] = x
        while self.p[x] != x:
            self.p[x] = self.p[self.p[x]]
            x = self.p[x]
        return x

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra != rb:
            self.p[rb] = ra

    def groups(self):
        g = {}
        for x in list(self.p.keys()):
            r = self.find(x)
            g.setdefault(r, []).append(x)
        return list(g.values())


def _align_keep_out_idx_for_add(
        model: nn.Module,
        keep_out_idx: Dict[str, torch.Tensor],
        pruned_model: nn.Module,
        example_inputs: Tuple[torch.Tensor, ...],
        verbose: bool = True,
) -> Dict[str, torch.Tensor]:
    """
    For every residual add where both branches have same original shape,
    force the traced source Conv/Linear modules to use the same keep_out_idx.
    Rule: choose a deterministic canonical index set with length = pruned out_dim (å°‘å‰ªä¸å¤šå‰ª in structure already fixed dim).
    """
    gm = fx.symbolic_trace(model)
    ShapeProp(gm).propagate(*example_inputs)
    mods = dict(gm.named_modules())
    uf = _UF()

    _aten_add = None
    try:
        _aten_add = torch.ops.aten.add.Tensor
    except Exception:
        _aten_add = None

    def is_add_node(n: fx.Node) -> bool:
        if n.op == "call_function" and n.target in {operator.add, torch.add}:
            return True
        if _aten_add is not None and n.op == "call_function" and n.target is _aten_add:
            return True
        if n.op == "call_method" and n.target in ("add", "__add__", "__iadd__"):
            return True
        return False

    def get_add_args(n: fx.Node):
        if n.op == "call_method":
            return n.args[0], n.args[1]
        return n.args[0], n.args[1]

    # union sources
    for node in gm.graph.nodes:
        if not is_add_node(node) or len(node.args) < 2:
            continue
        a, b = get_add_args(node)
        if not isinstance(a, fx.Node) or not isinstance(b, fx.Node):
            continue
        sa, sb, so = _get_shape(a), _get_shape(b), _get_shape(node)
        if sa is None or sb is None or so is None:
            continue
        if sa != sb or sa != so:
            continue

        src_a = _trace_channel_source(a, gm)
        src_b = _trace_channel_source(b, gm)
        if src_a is None or src_b is None:
            continue

        ma, mb = mods.get(src_a), mods.get(src_b)
        if ma is None or mb is None:
            continue
        if _outdim_of(ma) != _outdim_of(mb):
            continue

        uf.union(src_a, src_b)

    # adjust keep idx inside each group
    aligned = dict(keep_out_idx)
    pruned_mods = dict(pruned_model.named_modules())

    for group in uf.groups():
        # pruned out_dim should be same for all modules in group (after structure alignment)
        # pick from first that exists in pruned_model
        keep_len = None
        for nm in group:
            if nm in pruned_mods and isinstance(pruned_mods[nm], (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                                                                  nn.ConvTranspose1d, nn.ConvTranspose2d,
                                                                  nn.ConvTranspose3d,
                                                                  nn.Linear)):
                keep_len = _outdim_of(pruned_mods[nm])
                break
        if keep_len is None:
            continue

        # collect candidate sets in original indexing
        candidates = []
        out0 = _outdim_of(mods[group[0]])
        for nm in group:
            if nm in aligned:
                candidates.append(aligned[nm].detach().cpu())
            else:
                candidates.append(_full_idx(out0).cpu())

        # canonical: try intersection; if too small, use sorted union then take first keep_len
        inter = candidates[0]
        for t in candidates[1:]:
            inter = torch.tensor(sorted(set(inter.tolist()).intersection(set(t.tolist()))), dtype=torch.long)
        if inter.numel() >= keep_len:
            canon = inter[:keep_len]
        else:
            uni = sorted(set().union(*[set(t.tolist()) for t in candidates]))
            canon = torch.tensor(uni[:keep_len], dtype=torch.long)

        # write back for members that are actually pruned or used
        changed = False
        for nm in group:
            if nm in aligned:
                if aligned[nm].numel() != keep_len or not torch.equal(aligned[nm].cpu(), canon):
                    aligned[nm] = canon.to(device=aligned[nm].device, dtype=torch.long)
                    changed = True
            else:
                # if not explicitly pruned, still ok not to store
                pass

        if verbose and changed:
            print(f"[add-align-idx] {group}: force keep_idx len={keep_len}")

    return aligned


# =========================
# Main: copy params using FX-propagated in_idx/out_idx
# =========================

@torch.no_grad()
def _copy_params_to_pruned_model_fx(
        original_model: nn.Module,
        pruned_model: nn.Module,
        keep_out_idx: Dict[str, torch.Tensor],
        example_inputs: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
        align_add: bool = True,
        verbose: bool = True,
        strict: bool = True,
) -> nn.Module:
    """
    Copy Conv/Linear/BN/GN parameters from original_model to pruned_model.
    - keep_out_idx: {module_path: kept output indices in ORIGINAL indexing}
    - in_idx is inferred by propagating indices on FX graph of original_model.
    Limitations (reasonable for "general CNNs"):
      - supports common ops: add, cat(dim=channel), relu-like, pool, flatten, view(N,-1).
      - if you have exotic channel-mangling ops, strict=True will raise.
    """
    device = next(pruned_model.parameters()).device if any(True for _ in pruned_model.parameters()) else torch.device(
        "cpu")
    original_model = original_model.to(device).eval()
    pruned_model = pruned_model.to(device).eval()

    if isinstance(example_inputs, torch.Tensor):
        example_inputs = (example_inputs,)
    example_inputs = tuple(t.to(device) for t in example_inputs)

    # optional: align indices for residual add
    if align_add:
        keep_out_idx = _align_keep_out_idx_for_add(original_model, keep_out_idx, pruned_model, example_inputs,
                                                   verbose=verbose)

    # trace original
    gm = fx.symbolic_trace(original_model)
    ShapeProp(gm).propagate(*example_inputs)
    mods = dict(gm.named_modules())

    # spec + keep_idx on channel/feature axis (in ORIGINAL indexing)
    spec: Dict[fx.Node, Spec] = {}
    kidx: Dict[fx.Node, Optional[torch.Tensor]] = {}  # keep indices along channel/feature axis

    placeholders = [n for n in gm.graph.nodes if n.op == "placeholder"]
    if len(placeholders) != len(example_inputs):
        raise RuntimeError("example_inputs æ•°é‡ä¸ forward è¾“å…¥ä¸ä¸€è‡´")

    for n, t in zip(placeholders, example_inputs):
        shp = tuple(int(x) for x in t.shape)
        spec[n] = Spec(shape=shp, c_axis=_default_c_axis(shp))
        if spec[n].c_axis is None:
            kidx[n] = None
        else:
            cax = spec[n].c_axis
            if cax < 0:
                cax = len(shp) + cax
            kidx[n] = _full_idx(shp[cax], device=device)

    def _sp(x) -> Optional[Spec]:
        return spec.get(x, None) if isinstance(x, fx.Node) else None

    def _ki(x) -> Optional[torch.Tensor]:
        return kidx.get(x, None) if isinstance(x, fx.Node) else None

    _aten_add = None
    try:
        _aten_add = torch.ops.aten.add.Tensor
    except Exception:
        _aten_add = None

    # store inferred in_idx/out_idx for modules
    in_idx_map: Dict[str, torch.Tensor] = {}
    out_idx_map: Dict[str, torch.Tensor] = {}

    for node in gm.graph.nodes:
        if node.op == "placeholder":
            continue

        # ========== call_module ==========
        if node.op == "call_module":
            name = node.target
            m = mods[name]
            in_sp = _sp(node.args[0]) if node.args else None
            in_keep = _ki(node.args[0]) if node.args and isinstance(node.args[0], fx.Node) else None

            out_meta = _get_shape(node)
            if in_sp is None:
                spec[node] = Spec(shape=out_meta or (0,), c_axis=None)
                kidx[node] = None
                continue

            # Conv / ConvTranspose
            if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                              nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
                # input keep idx (original)
                if in_keep is None:
                    Cin0 = int(in_sp.shape[1])
                    in_keep = _full_idx(Cin0, device=device)

                # output keep idx (original)
                out0 = int(m.out_channels)
                out_keep = keep_out_idx.get(name, _full_idx(out0, device=device))

                in_idx_map[name] = in_keep
                out_idx_map[name] = out_keep

                # update spec/node keep
                if out_meta is None:
                    # best-effort: preserve spatial from input meta
                    if len(in_sp.shape) == 4:
                        N, _, H, W = in_sp.shape
                        spec[node] = Spec(shape=(N, out0, H, W), c_axis=1)
                    else:
                        spec[node] = Spec(shape=tuple(in_sp.shape), c_axis=in_sp.c_axis)
                else:
                    out_list = list(out_meta)
                    if len(out_list) >= 2:
                        out_list[1] = out0
                    spec[node] = Spec(shape=tuple(out_list), c_axis=1)

                kidx[node] = out_keep
                continue

            # Linear
            if isinstance(m, nn.Linear):
                # input keep idx is on last dim (feature axis)
                if in_keep is None:
                    Fin0 = int(in_sp.shape[-1])
                    in_keep = _full_idx(Fin0, device=device)

                out0 = int(m.out_features)
                out_keep = keep_out_idx.get(name, _full_idx(out0, device=device))

                in_idx_map[name] = in_keep
                out_idx_map[name] = out_keep

                out_shape = list(in_sp.shape)
                out_shape[-1] = out0
                spec[node] = Spec(shape=tuple(out_shape), c_axis=_default_c_axis(tuple(out_shape)))
                kidx[node] = out_keep
                continue

            # BN/GN: preserve channel dim, keep idx same as input
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):
                spec[node] = Spec(shape=in_sp.shape, c_axis=in_sp.c_axis)
                kidx[node] = in_keep
                if in_keep is not None:
                    in_idx_map[name] = in_keep  # for copying BN/GN params
                continue

            # Flatten module
            if isinstance(m, nn.Flatten):
                # common: NCHW -> N, C*H*W
                out_meta = _get_shape(node)
                if out_meta is not None:
                    spec[node] = Spec(shape=out_meta, c_axis=_default_c_axis(out_meta))
                else:
                    # fallback: flatten from dim=1
                    N = in_sp.shape[0]
                    feat = 1
                    for d in in_sp.shape[1:]:
                        feat *= int(d)
                    spec[node] = Spec(shape=(N, feat), c_axis=1)

                # expand keep idx if we flattened channel+spatial
                if in_keep is not None and len(in_sp.shape) == 4:
                    _, _, H, W = in_sp.shape
                    kidx[node] = _expand_channel_idx_to_flat_features(in_keep, int(H * W))
                else:
                    kidx[node] = None
                continue

            # common passthrough modules
            if isinstance(m, (nn.ReLU, nn.ReLU6, nn.GELU, nn.SiLU, nn.Sigmoid, nn.Tanh,
                              nn.Dropout, nn.Dropout2d, nn.Dropout3d, nn.Identity,
                              nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d, nn.Upsample)):
                spec[node] = Spec(shape=out_meta or in_sp.shape, c_axis=in_sp.c_axis)
                kidx[node] = in_keep
                continue

            # fallback
            spec[node] = Spec(shape=out_meta or in_sp.shape, c_axis=in_sp.c_axis)
            kidx[node] = in_keep
            continue

        # ========== call_function ==========
        if node.op == "call_function":

            # torch.flatten
            if node.target is torch.flatten:
                x = node.args[0]
                in_sp = _sp(x);
                in_keep = _ki(x)
                out_meta = _get_shape(node)
                if in_sp is None:
                    spec[node] = Spec(shape=out_meta or (0,), c_axis=None)
                    kidx[node] = None
                    continue

                if out_meta is not None:
                    spec[node] = Spec(shape=out_meta, c_axis=_default_c_axis(out_meta))
                else:
                    # fallback flatten all except batch
                    N = in_sp.shape[0]
                    feat = 1
                    for d in in_sp.shape[1:]:
                        feat *= int(d)
                    spec[node] = Spec(shape=(N, feat), c_axis=1)

                if in_keep is not None and len(in_sp.shape) == 4:
                    _, _, H, W = in_sp.shape
                    kidx[node] = _expand_channel_idx_to_flat_features(in_keep, int(H * W))
                else:
                    kidx[node] = None
                continue

            # add / aten.add
            if node.target in (operator.add, torch.add) or (_aten_add is not None and node.target is _aten_add):
                a, b = node.args[0], node.args[1]
                sa, sb, so = _get_shape(a), _get_shape(b), _get_shape(node)
                in_sp = _sp(a);
                in_keep = _ki(a)
                if in_sp is None:
                    spec[node] = Spec(shape=so or (0,), c_axis=None)
                    kidx[node] = None
                    continue
                spec[node] = Spec(shape=so or in_sp.shape, c_axis=in_sp.c_axis)
                # if mismatch, choose deterministic subset
                ka, kb = _ki(a), _ki(b)
                if ka is None or kb is None:
                    kidx[node] = ka
                else:
                    if ka.numel() != kb.numel() or not torch.equal(torch.sort(ka).values, torch.sort(kb).values):
                        # choose intersection if possible, else take first min length of ka
                        inter = sorted(set(ka.tolist()).intersection(set(kb.tolist())))
                        if len(inter) >= min(ka.numel(), kb.numel()):
                            kidx[node] = torch.tensor(inter[:min(ka.numel(), kb.numel())], dtype=torch.long,
                                                      device=device)
                        else:
                            kidx[node] = ka[:min(ka.numel(), kb.numel())]
                    else:
                        kidx[node] = ka
                continue

            # cat
            if node.target is torch.cat:
                tensors = node.args[0] if node.args else None
                dim = node.args[1] if len(node.args) > 1 else node.kwargs.get("dim", 0)
                dim = int(dim)

                if not isinstance(tensors, (list, tuple)):
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    kidx[node] = None
                    continue

                sps = [_sp(t) for t in tensors]
                kis = [_ki(t) for t in tensors]
                out_meta = _get_shape(node)
                if out_meta is not None:
                    spec[node] = Spec(shape=out_meta, c_axis=_default_c_axis(out_meta))
                else:
                    spec[node] = Spec(shape=(0,), c_axis=None)

                # if cat along channel axis -> concat indices with original offsets
                if sps and sps[0] is not None and spec[node].c_axis is not None:
                    rank = len(sps[0].shape)
                    dd = dim if dim >= 0 else dim + rank
                    cax = spec[node].c_axis
                    if cax < 0:
                        cax = rank + cax

                    if dd == cax:
                        # offsets are ORIGINAL channel sizes of each input (from meta)
                        offsets = []
                        off = 0
                        for t in tensors:
                            sh = _get_shape(t)  # original shape
                            if sh is None:
                                # fallback: cannot compute offsets robustly
                                offsets = None
                                break
                            offsets.append(off)
                            off += int(sh[cax])
                        if offsets is None or any(k is None for k in kis):
                            kidx[node] = None
                        else:
                            out_list = []
                            for k, off in zip(kis, offsets):
                                out_list.append((k + off).to(device))
                            kidx[node] = torch.cat(out_list, dim=0)
                    else:
                        # cat along non-channel dim: channel idx should be same as first
                        kidx[node] = kis[0]
                else:
                    kidx[node] = None
                continue

            # relu
            if node.target in (torch.relu, torch.nn.functional.relu):
                x = node.args[0]
                spec[node] = Spec(shape=_get_shape(node) or (_sp(x).shape if _sp(x) else (0,)),
                                  c_axis=_sp(x).c_axis if _sp(x) else None)
                kidx[node] = _ki(x)
                continue

            # fallback: passthrough
            x0 = node.args[0] if node.args else None
            spec[node] = Spec(shape=_get_shape(node) or (_sp(x0).shape if _sp(x0) else (0,)),
                              c_axis=_sp(x0).c_axis if _sp(x0) else None)
            kidx[node] = _ki(x0) if isinstance(x0, fx.Node) else None
            continue

        # ========== call_method ==========
        if node.op == "call_method":
            method = node.target
            x0 = node.args[0]
            in_sp = _sp(x0);
            in_keep = _ki(x0)
            out_meta = _get_shape(node)

            if in_sp is None:
                spec[node] = Spec(shape=out_meta or (0,), c_axis=None)
                kidx[node] = None
                continue

            if method in ("view", "reshape"):
                # only support common view(N, -1) or reshape(N, -1)
                new_shape = node.args[1:]

                if len(new_shape) == 2 and isinstance(new_shape[0], fx.Node) and int(new_shape[1]) == -1:
                    # âœ… è¿™æ˜¯å…¸å‹çš„ x.view(x.size(0), -1)
                    if out_meta:
                        spec[node] = Spec(shape=out_meta, c_axis=_default_c_axis(out_meta))
                    else:
                        spec[node] = Spec(shape=(in_sp.shape[0], -1), c_axis=1)

                    # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœæ˜¯ NCHW -> (N, -1)ï¼Œéœ€è¦æŠŠé€šé“ idx æ‰©å±•åˆ° flatten åçš„ feature idx
                    if in_keep is not None and len(in_sp.shape) == 4 and out_meta is not None and len(out_meta) == 2:
                        _, _, H, W = in_sp.shape
                        kidx[node] = _expand_channel_idx_to_flat_features(in_keep, int(H * W))
                    else:
                        # åŸé€»è¾‘ï¼šå‡è®¾å·²ç»æ˜¯ feature idx
                        kidx[node] = in_keep

                else:
                    if strict:
                        raise RuntimeError(f"[UNSUPPORTED] view/reshape pattern: {new_shape}")
                    spec[node] = Spec(shape=out_meta or in_sp.shape,
                                      c_axis=_default_c_axis(out_meta) if out_meta else in_sp.c_axis)
                    kidx[node] = None
                continue

            if method == "flatten":
                # treat like flatten
                spec[node] = Spec(shape=out_meta or in_sp.shape,
                                  c_axis=_default_c_axis(out_meta) if out_meta else in_sp.c_axis)
                if in_keep is not None and len(in_sp.shape) == 4 and out_meta is not None and len(out_meta) == 2:
                    _, _, H, W = in_sp.shape
                    kidx[node] = _expand_channel_idx_to_flat_features(in_keep, int(H * W))
                else:
                    kidx[node] = in_keep
                continue

            if method in ("add", "__add__", "__iadd__"):
                b = node.args[1]
                kb = _ki(b)
                spec[node] = Spec(shape=out_meta or in_sp.shape, c_axis=in_sp.c_axis)
                if in_keep is None or kb is None:
                    kidx[node] = in_keep
                else:
                    kidx[node] = in_keep if in_keep.numel() <= kb.numel() else kb
                continue

            # default passthrough
            spec[node] = Spec(shape=out_meta or in_sp.shape, c_axis=in_sp.c_axis)
            kidx[node] = in_keep
            continue

        if node.op == "output":
            break

    # -------------------------
    # Now actually copy weights
    # -------------------------
    o_mods = dict(original_model.named_modules())
    p_mods = dict(pruned_model.named_modules())

    def _copy_conv_like(name: str, m_old: nn.Module, m_new: nn.Module):
        W_old = m_old.weight.data
        W_new = m_new.weight.data

        g = int(getattr(m_old, "groups", 1))
        if _is_depthwise_conv(m_old):
            # depthwise: weight shape [Cout, 1, k, k], only need out_idx
            out_idx = out_idx_map.get(name, _full_idx(W_old.shape[0], device=device))
            out_idx = out_idx.to(device)
            out_idx = out_idx[:W_new.shape[0]]
            sliced = W_old.index_select(0, out_idx)
            if sliced.shape != W_new.shape:
                # fallback: truncate/pad not allowed -> strict
                if strict:
                    raise RuntimeError(f"{name} depthwise weight shape mismatch: {sliced.shape} vs {W_new.shape}")
                sliced = sliced[:W_new.shape[0]]
            W_new.copy_(sliced)
        elif g == 1:
            in_idx = in_idx_map.get(name, _full_idx(W_old.shape[1], device=device)).to(device)
            out_idx = out_idx_map.get(name, _full_idx(W_old.shape[0], device=device)).to(device)

            in_idx = in_idx[:W_new.shape[1]]
            out_idx = out_idx[:W_new.shape[0]]

            sliced = W_old.index_select(0, out_idx).index_select(1, in_idx)
            if sliced.shape != W_new.shape:
                if strict:
                    raise RuntimeError(f"{name} conv weight shape mismatch: {sliced.shape} vs {W_new.shape}")
            W_new.copy_(sliced)
        else:
            # grouped conv (non-depthwise): best-effort group-wise contiguous selection
            Cin0 = int(m_old.in_channels)
            Cout0 = int(m_old.out_channels)
            Cin_pg = Cin0 // g
            Cout_pg = Cout0 // g
            Cin_new = int(getattr(m_new, "in_channels"))
            Cout_new = int(getattr(m_new, "out_channels"))
            Cin_new_pg = Cin_new // g
            Cout_new_pg = Cout_new // g

            # pick first Cin_new_pg per group, first Cout_new_pg per group
            # (this is the most universal safe mapping)
            Wg = []
            for gg in range(g):
                o_start = gg * Cout_pg
                i_start = gg * Cin_pg
                o_sel = torch.arange(o_start, o_start + Cout_new_pg, dtype=torch.long, device=device)
                i_sel_local = torch.arange(0, Cin_new_pg, dtype=torch.long, device=device)
                # weight dim1 is per-group input channels
                part = W_old.index_select(0, o_sel).index_select(1, i_sel_local)
                Wg.append(part)
            sliced = torch.cat(Wg, dim=0)
            if sliced.shape != W_new.shape and strict:
                raise RuntimeError(f"{name} grouped conv weight shape mismatch: {sliced.shape} vs {W_new.shape}")
            W_new.copy_(sliced)

        # bias
        if getattr(m_old, "bias", None) is not None and getattr(m_new, "bias", None) is not None:
            b_old = m_old.bias.data
            b_new = m_new.bias.data
            out_idx = out_idx_map.get(name, _full_idx(b_old.shape[0], device=device)).to(device)
            out_idx = out_idx[:b_new.shape[0]]
            b_new.copy_(b_old.index_select(0, out_idx))

    def _copy_linear(name: str, m_old: nn.Linear, m_new: nn.Linear):
        W_old = m_old.weight.data
        W_new = m_new.weight.data

        in_idx = in_idx_map.get(name, _full_idx(W_old.shape[1], device=device)).to(device)
        out_idx = out_idx_map.get(name, _full_idx(W_old.shape[0], device=device)).to(device)

        in_idx = in_idx[:W_new.shape[1]]
        out_idx = out_idx[:W_new.shape[0]]

        sliced = W_old.index_select(0, out_idx).index_select(1, in_idx)
        if sliced.shape != W_new.shape and strict:
            raise RuntimeError(f"{name} linear weight shape mismatch: {sliced.shape} vs {W_new.shape}")
        W_new.copy_(sliced)

        if m_old.bias is not None and m_new.bias is not None:
            b_old = m_old.bias.data
            b_new = m_new.bias.data
            b_new.copy_(b_old.index_select(0, out_idx))

    def _copy_norm_1d(name: str, m_old: nn.Module, m_new: nn.Module):
        # BN/GN: weight,bias,running stats along channel dim
        idx = in_idx_map.get(name, None)
        if idx is None:
            return
        idx = idx.to(device)
        # weight/bias
        for attr in ("weight", "bias"):
            if hasattr(m_old, attr) and hasattr(m_new, attr):
                t_old = getattr(m_old, attr)
                t_new = getattr(m_new, attr)
                if t_old is not None and t_new is not None and t_old.data.ndim == 1:
                    idx2 = idx[:t_new.data.shape[0]]
                    t_new.data.copy_(t_old.data.index_select(0, idx2))
        # running stats
        for attr in ("running_mean", "running_var"):
            if hasattr(m_old, attr) and hasattr(m_new, attr):
                t_old = getattr(m_old, attr)
                t_new = getattr(m_new, attr)
                if isinstance(t_old, torch.Tensor) and isinstance(t_new, torch.Tensor) and t_old.ndim == 1:
                    idx2 = idx[:t_new.shape[0]]
                    t_new.copy_(t_old.index_select(0, idx2))

    # iterate pruned modules and copy
    for name, m_new in p_mods.items():
        if name not in o_mods:
            continue
        m_old = o_mods[name]

        if isinstance(m_new, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                              nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)) and \
                isinstance(m_old, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                                   nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
            _copy_conv_like(name, m_old, m_new)

        elif isinstance(m_new, nn.Linear) and isinstance(m_old, nn.Linear):
            _copy_linear(name, m_old, m_new)

        elif isinstance(m_new, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)) and \
                isinstance(m_old, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):
            _copy_norm_1d(name, m_old, m_new)

        else:
            # for other modules, if state_dict shapes match, we can copy directly (optional)
            pass

    return pruned_model


# =========================
# Wrapper similar to your original function
# =========================

def prune(
        pruned_model_skeleton: nn.Module,  # already slimmed structure (your universal slimming output)
        original_model: nn.Module,
        results: Dict[str, Any],
        ratios: List[int],
        *,
        layers2prune: List[int]=None,  # indices in original_model.named_modules()
        pruned_model_save_path: str = 'pruned.pht',
        example_inputs: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
        finetune_pruned: bool = False,
        finetune_epochs: int = 10,
        finetunedata=None,
        valdata=None,
        device: str = None,
        verbose: bool = False,
):
    if device == None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if layers2prune is None:
        auto = _auto_prunable_layer_indices(
            original_model,
            exclude_output_layer=True,
        )
        layers2prune = auto
    pruned_model = pruned_model_skeleton.to(device).eval()
    original_model = original_model.to(device).eval()
    keep_out = _build_keep_out_idx_map_from_results(
        original_model=original_model,
        layers2prune=layers2prune,
        ratios=ratios,
        results=results,
        key_fmt="D_{j}_s2b_idx",
        device="cpu",
    )

    pruned_model = _copy_params_to_pruned_model_fx(
        original_model=original_model,
        pruned_model=pruned_model,
        keep_out_idx=keep_out,
        example_inputs=example_inputs,
        align_add=True,
        verbose=verbose,
        strict=True,
    )

    # report
    n_pruned = sum(p.numel() for p in pruned_model.parameters() if p.requires_grad)
    n_orig = sum(p.numel() for p in original_model.parameters() if p.requires_grad)
    report = {
        "parameters number": n_pruned,
        "pruning rate": 1.0 - (n_pruned / max(1, n_orig)),
    }
    if verbose:
        print(f'parameters number: {report["parameters number"]}')
        print(f'pruning rate: {report["pruning rate"]}')

    # optional finetune (keep our own _finetune_model)
    if finetune_pruned:
        pruned_model = _finetune_model(
            model=pruned_model,
            train_loader=finetunedata,
            val_loader=valdata,
            epochs=finetune_epochs,
            lr=0.001,
            momentum=0.9,
            weight_decay=1e-4,
            device=device,
            grad_clip_norm=None,
            amp=True,
            log_every=100,
        )

    # âœ… å¼ºçƒˆå»ºè®®ä¿å­˜ state_dictï¼Œé¿å… PyTorch2.6 çš„ weights_only å®‰å…¨é™åˆ¶
    torch.save(pruned_model.state_dict(), pruned_model_save_path)

    return pruned_model, report


###############################################################################
# å¾—åˆ°å‰ªæåçš„æ¨¡å‹æ¶æ„
import copy
import math
import operator
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Union, Any
from collections import deque

import torch
import torch.nn as nn
import torch.fx as fx
from torch.fx.passes.shape_prop import ShapeProp


# =========================
# Shape spec
# =========================

def _pair(x):
    return x if isinstance(x, tuple) else (x, x)


def _set_submodule(model: nn.Module, path: str, new_module: nn.Module):
    parts = path.split(".")
    parent = model
    for p in parts[:-1]:
        parent = getattr(parent, p)
    setattr(parent, parts[-1], new_module)


def _infer_flatten_shape(in_shape: Tuple[int, ...], start_dim: int, end_dim: int) -> Tuple[int, ...]:
    shp = list(in_shape)
    nd = len(shp)
    if end_dim < 0:
        end_dim += nd
    left = shp[:start_dim]
    mid = shp[start_dim:end_dim + 1]
    right = shp[end_dim + 1:]
    prod = 1
    for v in mid:
        prod *= int(v)
    return tuple(left + [prod] + right)


def _infer_view_shape(in_shape: Tuple[int, ...], new_shape_raw: Tuple[Any, ...]) -> Tuple[int, ...]:
    new_shape: List[int] = []
    for i, d in enumerate(new_shape_raw):
        if isinstance(d, int):
            new_shape.append(d)
        elif isinstance(d, fx.Node):
            if i == 0:
                new_shape.append(int(in_shape[0]))  # batch
            else:
                raise RuntimeError(
                    "view/reshape å½¢çŠ¶åŒ…å«åŠ¨æ€ Nodeï¼ˆé batch ç»´ï¼‰ï¼Œå»ºè®®ç”¨ torch.flatten(x,1) æˆ– view(x.size(0), -1)ã€‚"
                )
        else:
            raise RuntimeError("view/reshape å½¢çŠ¶å‚æ•°ä¸æ˜¯ int/-1/Node(batch)ï¼Œæ— æ³•æ¨å¯¼ã€‚")

    total = math.prod([int(v) for v in in_shape])

    known = 1
    minus1 = None
    for i, d in enumerate(new_shape):
        if d == -1:
            if minus1 is not None:
                raise RuntimeError("view/reshape å‡ºç°å¤šä¸ª -1ï¼Œæ— æ³•æ¨å¯¼ã€‚")
            minus1 = i
        else:
            known *= d

    if minus1 is not None:
        if known == 0 or total % known != 0:
            raise RuntimeError(f"view/reshape æ— æ³•æ¨å¯¼ -1ï¼šin={in_shape}, target={tuple(new_shape)}")
        new_shape[minus1] = total // known

    if math.prod(new_shape) != total:
        raise RuntimeError(
            f"view/reshape å…ƒç´ æ•°ä¸åŒ¹é…ï¼šin={in_shape} numel={total}, target={tuple(new_shape)} numel={math.prod(new_shape)}ã€‚"
            f"å¸¸è§åŸå› ï¼šä½ å†™æ­»äº†æŸç»´ï¼Œå‰ªæåä¸æˆç«‹ã€‚"
        )
    return tuple(int(x) for x in new_shape)


def _gcd_group_adjust(num_groups: int, num_channels: int) -> int:
    g = min(int(num_groups), int(num_channels))
    while g > 1 and (num_channels % g != 0):
        g -= 1
    return max(1, g)


def _outdim_of(m: nn.Module) -> int:
    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                      nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
        return int(m.out_channels)
    if isinstance(m, nn.Linear):
        return int(m.out_features)
    raise TypeError(type(m).__name__)


# =========================
# FX arg/kwarg helpers
# =========================

def _fx_get(node: fx.Node, arg_i: int, kw: str, default=None):
    if len(node.args) > arg_i:
        return node.args[arg_i]
    if kw in node.kwargs:
        return node.kwargs[kw]
    return default


def _fx_get_int(node: fx.Node, arg_i: int, kw: str, default: int) -> int:
    v = _fx_get(node, arg_i, kw, default)
    return int(v)


# =========================
# Conv / ConvTranspose output-shape
# =========================

def _conv_out_shape(in_sp: Spec, orig_out: Optional[Tuple[int, ...]], new_out_channels: int, conv: nn.Module) -> Spec:
    N = int(in_sp.shape[0])

    if orig_out is not None and len(orig_out) == len(in_sp.shape):
        out_list = list(orig_out)
        out_list[0] = N
        out_list[1] = int(new_out_channels)
        return Spec(shape=tuple(int(x) for x in out_list), c_axis=1)

    # no meta -> formula
    if isinstance(conv, nn.Conv1d):
        _, _, L = in_sp.shape
        k = conv.kernel_size[0];
        s = conv.stride[0];
        p = conv.padding[0];
        d = conv.dilation[0]
        L2 = math.floor((L + 2 * p - d * (k - 1) - 1) / s + 1)
        return Spec(shape=(N, int(new_out_channels), int(L2)), c_axis=1)

    if isinstance(conv, nn.Conv2d):
        _, _, H, W = in_sp.shape
        kh, kw = _pair(conv.kernel_size);
        sh, sw = _pair(conv.stride);
        ph, pw = _pair(conv.padding);
        dh, dw = _pair(conv.dilation)
        H2 = math.floor((H + 2 * ph - dh * (kh - 1) - 1) / sh + 1)
        W2 = math.floor((W + 2 * pw - dw * (kw - 1) - 1) / sw + 1)
        return Spec(shape=(N, int(new_out_channels), int(H2), int(W2)), c_axis=1)

    if isinstance(conv, nn.ConvTranspose2d):
        _, _, H, W = in_sp.shape
        kh, kw = _pair(conv.kernel_size);
        sh, sw = _pair(conv.stride);
        ph, pw = _pair(conv.padding);
        dh, dw = _pair(conv.dilation)
        oph, opw = _pair(conv.output_padding)
        H2 = (H - 1) * sh - 2 * ph + dh * (kh - 1) + oph + 1
        W2 = (W - 1) * sw - 2 * pw + dw * (kw - 1) + opw + 1
        return Spec(shape=(N, int(new_out_channels), int(H2), int(W2)), c_axis=1)

    raise RuntimeError(f"Conv shape formula not implemented for {type(conv).__name__}, please rely on shape meta.")


# =========================
# Fallback: ops that preserve channel/feature dim
# =========================

def _adapt_out_shape_preserve_channel(in_sp: Spec, orig_in: Optional[Tuple[int, ...]],
                                      orig_out: Optional[Tuple[int, ...]]) -> Spec:
    if orig_in is None or orig_out is None:
        return Spec(shape=in_sp.shape, c_axis=_default_c_axis(in_sp.shape))

    if len(orig_in) != len(orig_out):
        raise RuntimeError(
            f"[UNSUPPORTED] rank changed: {orig_in} -> {orig_out} (need handler like flatten/view/reshape).")

    out_list = list(orig_out)
    cax = in_sp.c_axis if in_sp.c_axis is not None else _default_c_axis(in_sp.shape)
    if cax is None:
        return Spec(shape=tuple(out_list), c_axis=None)
    if cax < 0:
        cax = len(out_list) + cax

    if orig_in[cax] != orig_out[cax]:
        raise RuntimeError(f"[UNSUPPORTED] channel/feature dim changed: {orig_in} -> {orig_out} (need handler).")

    out_list[cax] = in_sp.shape[cax]
    out_shape = tuple(int(x) for x in out_list)
    return Spec(shape=out_shape, c_axis=in_sp.c_axis if in_sp.c_axis is not None else _default_c_axis(out_shape))


# =========================
# Residual add alignment (å°‘å‰ªä¸å¤šå‰ª)
# =========================
#
# class _UF:
#     def __init__(self):
#         self.p = {}
#     def find(self, x):
#         if x not in self.p:
#             self.p[x] = x
#         while self.p[x] != x:
#             self.p[x] = self.p[self.p[x]]
#             x = self.p[x]
#         return x
#     def union(self, a, b):
#         ra, rb = self.find(a), self.find(b)
#         if ra != rb:
#             self.p[rb] = ra
#     def groups(self):
#         g = {}
#         for x in list(self.p.keys()):
#             r = self.find(x)
#             g.setdefault(r, []).append(x)
#         return list(g.values())


# âœ… æ–°ç‰ˆï¼šå¯¹æ‰€æœ‰ residual add å»ºçº¦æŸï¼ŒæŒ‰â€œå°‘å‰ªä¸å¤šå‰ªâ€å¯¹é½
def _align_keep_for_add(model: nn.Module, keep_cnt: Dict[str, int], example_inputs: Tuple[torch.Tensor, ...],
                        verbose: bool):
    gm = fx.symbolic_trace(model)
    ShapeProp(gm).propagate(*example_inputs)
    mods = dict(gm.named_modules())
    uf = _UF()

    # å…¼å®¹ aten.add å½¢å¼ï¼ˆä¸åŒ pytorch ç‰ˆæœ¬ä¼šå‡ºç°ï¼‰
    _aten_add = None
    try:
        _aten_add = torch.ops.aten.add.Tensor
    except Exception:
        _aten_add = None

    def is_add_node(n: fx.Node) -> bool:
        if n.op == "call_function" and n.target in {operator.add, torch.add}:
            return True
        if _aten_add is not None and n.op == "call_function" and n.target is _aten_add:
            return True
        if n.op == " " and n.target in ("add", "__add__", "__iadd__"):
            return True
        return False

    def get_add_args(n: fx.Node):
        if n.op == "call_method":
            # x.add(y) / x.__add__(y)
            return n.args[0], n.args[1]
        else:
            return n.args[0], n.args[1]

    for node in gm.graph.nodes:
        if not is_add_node(node):
            continue
        if len(node.args) < 2:
            continue
        a, b = get_add_args(node)
        if not isinstance(a, fx.Node) or not isinstance(b, fx.Node):
            continue
        sa, sb, so = _get_shape(a), _get_shape(b), _get_shape(node)
        if sa is None or sb is None or so is None:
            continue
        # åªå¤„ç†çœŸæ­£ residualï¼šä¸¤è¾¹ shape å®Œå…¨ä¸€è‡´
        if sa != sb or sa != so:
            continue

        src_a = _trace_channel_source(a, gm)
        src_b = _trace_channel_source(b, gm)
        if src_a is None or src_b is None:
            continue

        ma, mb = mods.get(src_a), mods.get(src_b)
        if ma is None or mb is None:
            continue

        if _outdim_of(ma) != _outdim_of(mb):
            # è¿™é‡Œä¸€èˆ¬æ„å‘³ç€ä¸æ˜¯â€œé€šé“åŒç»´â€çš„æ®‹å·®ï¼Œæˆ– trace è¿½åˆ°äº†ä¸è¯¥è¿½çš„æ¨¡å—
            # å…ˆè·³è¿‡ï¼Œä¸ç¡¬æŠ¥é”™ï¼ˆæé«˜è¦†ç›–ç‡ï¼‰
            continue

        uf.union(src_a, src_b)

    aligned = dict(keep_cnt)
    for group in uf.groups():
        # ç»„å†…æ‰€æœ‰æ¨¡å— outdim æœ¬åº”ä¸€è‡´ï¼ˆæ®‹å·® addï¼‰
        out0 = _outdim_of(mods[group[0]])
        keeps = []
        for nm in group:
            # è‹¥è¯¥ nm æœ¬æ¥æ²¡å‰ªï¼Œå°±é»˜è®¤ keep=åŸå§‹ outdimï¼ˆä¿è¯å°‘å‰ªä¸å¤šå‰ªèƒ½æŠŠå‰ªå¤šçš„æŠ¬å›å»ï¼‰
            keeps.append(aligned.get(nm, _outdim_of(mods[nm])))
        keep_final = max(keeps)
        keep_final = max(1, min(out0, keep_final))

        changed = False
        for nm in group:
            if nm in aligned and aligned[nm] != keep_final:
                aligned[nm] = keep_final
                changed = True

        if verbose and changed:
            print(f"[add-align] {group}: keep {keeps} -> {keep_final}")

    return aligned


# =========================
# Main API
# =========================

def get_skeleton(
        model: nn.Module,
        ratios: List[int],
        example_inputs: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
        layers2prune: List[int]=None,
        align_residual_add: bool = True,
        verbose: bool = False,
        strict_forward_check: bool = True,
) -> nn.Module:

    device = next(model.parameters()).device if any(True for _ in model.parameters()) else torch.device("cpu")


    if layers2prune is None:
        auto = _auto_prunable_layer_indices(
            model,
            exclude_output_layer=True,
        )
        layers2prune = auto
    assert len(layers2prune) == len(ratios)
    model = model.to(device).eval()
    if isinstance(example_inputs, torch.Tensor):
        example_inputs = (example_inputs,)
    example_inputs = tuple(t.to(device) for t in example_inputs)

    named = list(model.named_modules())
    mods = dict(model.named_modules())

    # keep plan
    keep_cnt: Dict[str, int] = {}
    for idx, k in zip(layers2prune, ratios):
        path, m = named[idx]
        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                          nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
            out = int(m.out_channels)
            keep = max(1, out - int(k))
            keep_cnt[path] = keep
            if verbose:
                print(f"[plan] {idx:4d}  {path:<40}  {type(m).__name__:<18} out {out} -> {keep}")
        elif isinstance(m, nn.Linear):
            out = int(m.out_features)
            keep = max(1, out - int(k))
            keep_cnt[path] = keep
            if verbose:
                print(f"[plan] {idx:4d}  {path:<40}  Linear               out {out} -> {keep}")
        else:
            raise RuntimeError(f"Index {idx} is not Conv/Linear: {path} -> {type(m).__name__}")

    # residual add align
    if align_residual_add:
        keep_cnt = _align_keep_for_add(model, keep_cnt, example_inputs, verbose=verbose)

    # trace original
    gm = fx.symbolic_trace(model)
    ShapeProp(gm).propagate(*example_inputs)

    # clone and rewrite
    new_model = copy.deepcopy(model).to(device).eval()
    spec: Dict[fx.Node, Spec] = {}

    placeholders = [n for n in gm.graph.nodes if n.op == "placeholder"]
    if len(placeholders) != len(example_inputs):
        raise RuntimeError("example_inputs æ•°é‡ä¸ forward è¾“å…¥ä¸ä¸€è‡´")
    for n, t in zip(placeholders, example_inputs):
        shp = tuple(int(x) for x in t.shape)
        spec[n] = Spec(shape=shp, c_axis=_default_c_axis(shp))

    def _sp(x) -> Optional[Spec]:
        return spec.get(x, None) if isinstance(x, fx.Node) else None

    # å…¼å®¹ aten.add
    _aten_add = None
    try:
        _aten_add = torch.ops.aten.add.Tensor
    except Exception:
        _aten_add = None

    for node in gm.graph.nodes:
        if node.op == "placeholder":
            continue

        # ========== call_module ==========
        if node.op == "call_module":
            name = node.target
            m_old = mods[name]
            in_sp = _sp(node.args[0]) if node.args else None
            if in_sp is None:
                spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                continue

            # Conv / ConvTranspose
            if isinstance(m_old, (nn.Conv1d, nn.Conv2d, nn.Conv3d,
                                  nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
                Cin = int(in_sp.shape[1])
                groups = int(getattr(m_old, "groups", 1))

                # depthwise convï¼šä¸èƒ½å•ç‹¬å‰ª outï¼Œåªèƒ½è·Ÿéš Cin
                if isinstance(m_old, (nn.Conv1d, nn.Conv2d, nn.Conv3d)) and _is_depthwise_conv(m_old):
                    new_in, new_out, new_groups = Cin, Cin, Cin
                    if name in keep_cnt and verbose:
                        print(f"[warn] {name}: depthwise conv å¿½ç•¥ out å‰ªæï¼Œå¼ºåˆ¶ out=in={Cin}")
                else:
                    new_in = Cin
                    out0 = int(m_old.out_channels)
                    new_out_req = int(keep_cnt.get(name, out0))
                    new_out_req = max(1, min(out0, new_out_req))

                    # é depthwise grouped conv çš„ä¸€è‡´æ€§è¦æ±‚
                    if isinstance(m_old, (nn.Conv1d, nn.Conv2d, nn.Conv3d)) and groups > 1:
                        if new_in % groups != 0:
                            raise RuntimeError(
                                f"{name}: grouped conv éœ€è¦ in%groups==0ï¼Œä½† in={new_in}, groups={groups}")
                        new_out = min(out0, ((new_out_req + groups - 1) // groups) * groups)
                        if new_out != new_out_req and verbose:
                            print(f"[warn] {name}: grouped conv out {new_out_req} -> {new_out} (align groups={groups})")
                    else:
                        new_out = new_out_req

                    new_groups = groups

                ConvCls = type(m_old)
                kwargs = dict(
                    in_channels=int(new_in),
                    out_channels=int(new_out),
                    kernel_size=m_old.kernel_size,
                    stride=m_old.stride,
                    padding=m_old.padding,
                    dilation=m_old.dilation,
                    groups=int(new_groups),
                    bias=(m_old.bias is not None),
                )
                if isinstance(m_old, (nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):
                    kwargs["output_padding"] = m_old.output_padding
                conv_new = ConvCls(**kwargs).to(device)
                _set_submodule(new_model, name, conv_new)

                spec[node] = _conv_out_shape(in_sp, _get_shape(node), int(new_out), conv_new)
                continue

            # BatchNorm
            if isinstance(m_old, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                C = int(in_sp.shape[1])
                BNCls = type(m_old)
                bn = BNCls(C, eps=m_old.eps, momentum=m_old.momentum, affine=m_old.affine,
                           track_running_stats=m_old.track_running_stats).to(device)
                _set_submodule(new_model, name, bn)
                spec[node] = Spec(shape=in_sp.shape, c_axis=1)
                continue

            # LayerNorm
            if isinstance(m_old, nn.LayerNorm):
                old_norm = m_old.normalized_shape
                n = len(old_norm) if isinstance(old_norm, tuple) else 1
                new_norm = tuple(int(x) for x in in_sp.shape[-n:])
                ln = nn.LayerNorm(new_norm, eps=m_old.eps, elementwise_affine=m_old.elementwise_affine).to(device)
                _set_submodule(new_model, name, ln)
                spec[node] = Spec(shape=in_sp.shape, c_axis=in_sp.c_axis)
                continue

            # GroupNorm
            if isinstance(m_old, nn.GroupNorm):
                C = int(in_sp.shape[1])
                g = _gcd_group_adjust(m_old.num_groups, C)
                gn = nn.GroupNorm(g, C, eps=m_old.eps, affine=m_old.affine).to(device)
                _set_submodule(new_model, name, gn)
                spec[node] = Spec(shape=in_sp.shape, c_axis=1)
                continue

            # Flatten
            if isinstance(m_old, nn.Flatten):
                out_shape = _infer_flatten_shape(in_sp.shape, int(m_old.start_dim), int(m_old.end_dim))
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            # Linear: apply to last dim
            if isinstance(m_old, nn.Linear):
                Fin = int(in_sp.shape[-1])
                out0 = int(m_old.out_features)
                new_out = int(keep_cnt.get(name, out0))
                new_out = max(1, min(out0, new_out))
                fc = nn.Linear(Fin, new_out, bias=(m_old.bias is not None)).to(device)
                _set_submodule(new_model, name, fc)

                out_list = list(in_sp.shape)
                out_list[-1] = new_out
                out_shape = tuple(out_list)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            # common shape-preserving modules
            if isinstance(m_old, (nn.ReLU, nn.ReLU6, nn.GELU, nn.SiLU, nn.Sigmoid, nn.Tanh,
                                  nn.Dropout, nn.Dropout2d, nn.Dropout3d, nn.Identity,
                                  nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d, nn.Upsample)):
                orig_in = _get_shape(node.args[0]) if node.args and isinstance(node.args[0], fx.Node) else None
                orig_out = _get_shape(node)
                spec[node] = _adapt_out_shape_preserve_channel(in_sp, orig_in, orig_out)
                continue

            # fallback
            orig_in = _get_shape(node.args[0]) if node.args and isinstance(node.args[0], fx.Node) else None
            orig_out = _get_shape(node)
            spec[node] = _adapt_out_shape_preserve_channel(in_sp, orig_in, orig_out)
            continue

        # ========== call_function ==========
        if node.op == "call_function":

            # torch.flatten
            if node.target is torch.flatten:
                in_sp = _sp(_fx_get(node, 0, "input", None))
                if in_sp is None:
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue
                start_dim = _fx_get_int(node, 1, "start_dim", 0)
                end_dim = _fx_get_int(node, 2, "end_dim", -1)
                out_shape = _infer_flatten_shape(in_sp.shape, start_dim, end_dim)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            # add / aten.add
            if node.target in (operator.add, torch.add) or (_aten_add is not None and node.target is _aten_add):
                a = _sp(node.args[0]);
                b = _sp(node.args[1])
                if a is None or b is None:
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue
                if a.shape != b.shape:
                    raise RuntimeError(f"add shape mismatch: {a.shape} vs {b.shape}")
                spec[node] = Spec(shape=a.shape, c_axis=a.c_axis)
                continue

            # cat  â€”â€” dim å¯èƒ½åœ¨ kwargs
            if node.target is torch.cat:
                tensors = _fx_get(node, 0, "tensors", None)
                if tensors is None:
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue
                dim = _fx_get_int(node, 1, "dim", 0)

                sps = [_sp(t) for t in tensors]
                if any(s is None for s in sps):
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue

                rank = len(sps[0].shape)
                if dim < 0:
                    dim += rank
                base = list(sps[0].shape)
                base[dim] = sum(s.shape[dim] for s in sps)  # type: ignore
                out_shape = tuple(base)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            # permute â€”â€” dims å¯èƒ½åœ¨ kwargs
            if node.target is torch.permute:
                x = _fx_get(node, 0, "input", None)
                in_sp = _sp(x)
                if in_sp is None:
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue
                dims_raw = _fx_get(node, 1, "dims", None)
                if dims_raw is None:
                    spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                    continue
                dims = tuple(int(d) for d in dims_raw)
                out_shape = tuple(in_sp.shape[d] for d in dims)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            # relu-like
            if node.target in (torch.relu, torch.nn.functional.relu):
                in_sp = _sp(node.args[0])
                spec[node] = Spec(shape=in_sp.shape, c_axis=in_sp.c_axis) if in_sp else Spec(
                    shape=_get_shape(node) or (0,), c_axis=None)
                continue

            # default preserve-channel fallback
            in0 = node.args[0] if node.args else None
            in_sp0 = _sp(in0) if isinstance(in0, fx.Node) else None
            if in_sp0 is None:
                spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
            else:
                orig_in = _get_shape(in0) if isinstance(in0, fx.Node) else None
                orig_out = _get_shape(node)
                spec[node] = _adapt_out_shape_preserve_channel(in_sp0, orig_in, orig_out)
            continue

        # ========== call_method ==========
        if node.op == "call_method":
            method = node.target
            x0 = node.args[0]
            in_sp = _sp(x0) if isinstance(x0, fx.Node) else None
            if in_sp is None:
                spec[node] = Spec(shape=_get_shape(node) or (0,), c_axis=None)
                continue

            if method == "flatten":
                out_meta = _get_shape(node)
                if out_meta is not None:
                    spec[node] = Spec(shape=out_meta, c_axis=_default_c_axis(out_meta))
                else:
                    out_shape = _infer_flatten_shape(in_sp.shape, 1, -1)
                    spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            if method in ("view", "reshape"):
                new_shape_raw = tuple(node.args[1:])
                out_shape = _infer_view_shape(in_sp.shape, new_shape_raw)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            if method == "permute":
                dims = tuple(int(d) for d in node.args[1:])
                out_shape = tuple(in_sp.shape[d] for d in dims)
                spec[node] = Spec(shape=out_shape, c_axis=_default_c_axis(out_shape))
                continue

            if method == "transpose":
                d0 = int(node.args[1]);
                d1 = int(node.args[2])
                shp = list(in_sp.shape)
                shp[d0], shp[d1] = shp[d1], shp[d0]
                spec[node] = Spec(shape=tuple(shp), c_axis=_default_c_axis(tuple(shp)))
                continue

            if method in ("add", "__add__", "__iadd__"):
                # x.add(y)
                b = _sp(node.args[1]) if len(node.args) > 1 and isinstance(node.args[1], fx.Node) else None
                if b is None:
                    spec[node] = Spec(shape=_get_shape(node) or in_sp.shape, c_axis=in_sp.c_axis)
                    continue
                if in_sp.shape != b.shape:
                    raise RuntimeError(f"add(method) shape mismatch: {in_sp.shape} vs {b.shape}")
                spec[node] = Spec(shape=in_sp.shape, c_axis=in_sp.c_axis)
                continue

            # default
            orig_in = _get_shape(x0) if isinstance(x0, fx.Node) else None
            orig_out = _get_shape(node)
            spec[node] = _adapt_out_shape_preserve_channel(in_sp, orig_in, orig_out)
            continue

        if node.op == "output":
            break

    if strict_forward_check:
        with torch.no_grad():
            _ = new_model(*example_inputs)

    return new_model


################################################################################
##_safetyCheck


def _safetyCheck(
        model: nn.Module,
        layers2prune: List[int],
        analysis_layers: List[int],
        analysis_ds_loader: torch.utils.data.DataLoader,
        *,
        strict_layer_type_check: bool = True,
        verbose: bool = True,
) -> bool:
    """
    Safety checks before pruning / analysis.

    Checks:
      1) model is a PyTorch nn.Module
      2) layers2prune and analysis_layers indices are valid (0..N-1) for named_modules() ordering (excluding root)
      3) layer types:
           - layers2prune must be PRUNABLE types (Conv/ConvTranspose/Linear)
           - analysis_layers must be SUPPORTED types (broad whitelist) if strict_layer_type_check=True
             (if False: analysis_layers type check is skipped)
      4) analysis_ds_loader yields data compatible with model input (single-batch forward test)

    Returns:
      True  -> all checks passed
      False -> at least one check failed (prints an explanatory message)

    Notes:
      - Indices are based on model.named_modules() preorder traversal, excluding the root module ("").
      - torch.add / torch.cat etc are not modules, so they won't appear in named_modules().
    """

    # ------------------------ Helpers ------------------------ #
    def _is_not_same_len_list(layers2prune, analysis_layers) -> bool:
        if not len(layers2prune) == len(analysis_layers):
            print('[safetyCheck] FAILED: layers2prune and analysis_layers must be of the same length')
            return True

    def _fail(msg: str) -> bool:
        print(f"[safetyCheck] FAILED: {msg}")
        return False

    def _is_int_list(x: Any, name: str) -> Optional[List[int]]:
        if not isinstance(x, (list, tuple)):
            print(f"[safetyCheck] FAILED: {name} must be a list/tuple of ints, got {type(x).__name__}.")
            return None
        out: List[int] = []
        for i, v in enumerate(x):
            if not isinstance(v, int):
                print(f"[safetyCheck] FAILED: {name}[{i}] must be int, got {type(v).__name__}.")
                return None
            out.append(int(v))
        return out

    def _flatten_batch(batch: Any) -> Any:
        """
        Accepts typical DataLoader batch formats:
          - Tensor
          - (inputs, labels)
          - (inputs, labels, ...)
          - dict with common keys: 'input', 'inputs', 'x', 'data', 'features'
        Returns the 'inputs' portion in the same structure expected by forward.
        """
        if torch.is_tensor(batch):
            return batch

        if isinstance(batch, dict):
            for k in ("inputs", "input", "x", "data", "features"):
                if k in batch:
                    return batch[k]
            return batch

        if isinstance(batch, (list, tuple)):
            if len(batch) == 0:
                return batch
            return batch[0]

        return batch

    def _forward_one_batch(m: nn.Module, batch_inputs: Any) -> Tuple[bool, str]:
        """
        Try a single forward pass with best-effort calling conventions.
        Returns (ok, error_message_if_any).
        """
        device = next(m.parameters(), torch.empty(0)).device  # supports models with no params
        m.eval()

        def _to_device(obj: Any) -> Any:
            if torch.is_tensor(obj):
                return obj.to(device)
            if isinstance(obj, dict):
                return {k: _to_device(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                t = [_to_device(v) for v in obj]
                return type(obj)(t)
            return obj

        batch_inputs = _to_device(batch_inputs)

        try:
            with torch.no_grad():
                if isinstance(batch_inputs, dict):
                    try:
                        _ = m(**batch_inputs)
                    except TypeError:
                        _ = m(batch_inputs)
                elif isinstance(batch_inputs, (tuple, list)):
                    try:
                        _ = m(*batch_inputs)
                    except TypeError:
                        _ = m(batch_inputs)
                else:
                    _ = m(batch_inputs)
            return True, ""
        except Exception as e:
            return False, f"{type(e).__name__}: {e}"

    # def _get_ordered_layers(m: nn.Module) -> List[Tuple[str, nn.Module]]:
    #     """
    #     Create an indexable, stable ordering of modules excluding the root module itself.
    #     Uses named_modules() order (preorder traversal).
    #     """
    #     layers: List[Tuple[str, nn.Module]] = []
    #     for name, mod in m.named_modules():
    #         if name == "":
    #             continue
    #         layers.append((name, mod))
    #     return layers

    def _get_ordered_layers(m: nn.Module) -> List[Tuple[str, nn.Module]]:
        # Create an indexable, stable ordering of modules excluding the root module itself.
        #         Uses named_modules() order (preorder traversal).
        return list(m.named_modules())

    # ------------------------ Type sets ------------------------ #
    def _is_prunable_layer(mod: nn.Module) -> bool:
        """Layers that are intended to be structurally pruned."""
        prunable = (
            nn.Conv1d, nn.Conv2d, nn.Conv3d,
            nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,
            nn.Linear,
        )
        return isinstance(mod, prunable)

    def _is_supported_layer(mod: nn.Module) -> bool:
        """
        Broad whitelist of common modules used in CNN/ResNet/MobileNet/DenseNet/VGG/Transformer-ish models.
        This is a SUPPORT list, not a PRUNE list.
        """
        supported = (
            # Containers / wrappers
            nn.Sequential, nn.ModuleList, nn.ModuleDict,

            # Prunable ops (also supported)
            nn.Conv1d, nn.Conv2d, nn.Conv3d,
            nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,
            nn.Linear,
            nn.Embedding,

            # Normalizations
            nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,
            nn.SyncBatchNorm,
            nn.GroupNorm,
            nn.LayerNorm,
            nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d,
            nn.LocalResponseNorm,

            # Activations
            nn.ReLU, nn.ReLU6,
            nn.LeakyReLU, nn.PReLU,
            nn.ELU, nn.SELU, nn.CELU,
            nn.GELU, nn.SiLU, nn.Mish, nn.Hardswish, nn.Hardtanh,
            nn.Sigmoid, nn.Tanh,
            nn.Softplus, nn.Softsign,
            nn.Softmax, nn.LogSoftmax,

            # Pooling
            nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d,
            nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d,
            nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d,
            nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d,

            # Dropout
            nn.Dropout, nn.Dropout1d, nn.Dropout2d, nn.Dropout3d, nn.AlphaDropout,

            # Shape / misc
            nn.Flatten,
            nn.Identity,
            nn.Upsample,
            nn.UpsamplingNearest2d, nn.UpsamplingBilinear2d,

            # Padding
            nn.ZeroPad2d,
            nn.ConstantPad1d, nn.ConstantPad2d, nn.ConstantPad3d,
            nn.ReflectionPad1d, nn.ReflectionPad2d, nn.ReflectionPad3d,
            nn.ReplicationPad1d, nn.ReplicationPad2d, nn.ReplicationPad3d,

            # RNN family (supported for forward-check; pruning logic may still not prune these)
            nn.RNN, nn.GRU, nn.LSTM,
        )
        return isinstance(mod, supported)

    # ------------------------ 1) Model type ------------------------ #
    if not isinstance(model, nn.Module):
        return _fail(f"model must be a PyTorch nn.Module, got {type(model).__name__}.")

    # ------------------------ 2) Index lists valid ------------------------ #
    if _is_not_same_len_list(layers2prune, analysis_layers):
        return False

    layers2prune_i = _is_int_list(layers2prune, "layers2prune")
    if layers2prune_i is None:
        return False

    analysis_layers_i = _is_int_list(analysis_layers, "analysis_layers")
    if analysis_layers_i is None:
        return False

    ordered_layers = _get_ordered_layers(model)
    n_layers = len(ordered_layers)
    if n_layers == 0:
        return _fail("Model has no submodules (named_modules returned only the root). Cannot index layers.")

    def _check_indices(indices: List[int], name: str) -> bool:
        for idx in indices:
            if idx < 0:
                return _fail(f"{name} contains a negative index ({idx}). Indices must be >= 0.")
            if idx >= n_layers:
                return _fail(
                    f"{name} contains out-of-range index ({idx}). "
                    f"Max valid index is {n_layers - 1} (model has {n_layers} indexed submodules)."
                )
        return True

    if not _check_indices(layers2prune_i, "layers2prune"):
        return False
    if not _check_indices(analysis_layers_i, "analysis_layers"):
        return False

    # ------------------------ 3) Layer type checks ------------------------ #
    def _check_layer_types_prune(indices: List[int], name: str) -> bool:
        bad: List[str] = []
        for idx in indices:
            layer_name, layer_mod = ordered_layers[idx]
            if not _is_prunable_layer(layer_mod):
                bad.append(f"{idx}: {layer_name} ({layer_mod.__class__.__name__})")
        if bad:
            return _fail(
                f"{name} must contain only PRUNABLE layers (Conv/ConvTranspose/Linear). "
                f"Offenders: {', '.join(bad)}"
            )
        return True

    def _check_layer_types_supported(indices: List[int], name: str) -> bool:
        if not strict_layer_type_check:
            return True
        bad: List[str] = []
        for idx in indices:
            layer_name, layer_mod = ordered_layers[idx]
            if not _is_supported_layer(layer_mod):
                bad.append(f"{idx}: {layer_name} ({layer_mod.__class__.__name__})")
        if bad:
            return _fail(
                f"{name} contains unsupported layer types under strict check. "
                f"Offenders: {', '.join(bad)}. "
                f"Tip: set strict_layer_type_check=False to allow custom modules."
            )
        return True

    if not _check_layer_types_prune(layers2prune_i, "layers2prune"):
        return False
    if not _check_layer_types_supported(analysis_layers_i, "analysis_layers"):
        return False

    # Optional: show quick mapping stats
    if verbose:
        prunable_count = 0
        prunable_preview = []
        for i, (nm, md) in enumerate(ordered_layers):
            if _is_prunable_layer(md):
                prunable_count += 1
                if len(prunable_preview) < 10:
                    prunable_preview.append(f"{i}:{nm}({md.__class__.__name__})")
        print(f"[safetyCheck] indexed submodules = {n_layers}, prunable(submodules) = {prunable_count}")
        if prunable_preview:
            print(f"[safetyCheck] prunable preview: {', '.join(prunable_preview)}")

    # ------------------------ 4) DataLoader compatibility (single-batch forward) ------------------------ #
    if analysis_ds_loader is None:
        return _fail("analysis_ds_loader is None.")

    try:
        batch = next(iter(analysis_ds_loader))
    except Exception as e:
        return _fail(f"Could not iterate over analysis_ds_loader to fetch one batch. {type(e).__name__}: {e}")

    batch_inputs = _flatten_batch(batch)
    ok, err = _forward_one_batch(model, batch_inputs)
    if not ok:
        return _fail(
            "analysis_ds_loader batch is not compatible with model forward(). "
            f"Forward test failed with: {err}"
        )

    # ------------------------ Success ------------------------ #
    print("[safetyCheck] PASSED: model type OK, indices OK, layer types OK, and dataloader is forward-compatible.")
    return True


###############################################################################
if __name__ == '__main__':
    model_path = r'C:\Users\Administrator\PycharmProjects\lcq\Data\CIFAR10_vgg16.pht'
    data_path = r'C:\Users\Administrator\PycharmProjects\lcq\DataSet\cifar10_prune_dataset.pkl'
    with open(data_path, 'rb') as f:
        prune_datasetloader = pickle.load(f)
    from torch.utils.data import DataLoader, Subset

    indices = list(range(64))
    subset = Subset(prune_datasetloader.dataset, indices)
    prune_datasetloader = DataLoader(
        subset,
        batch_size=prune_datasetloader.batch_size,  # æƒ³æ”¹å¯æ”¹
        shuffle=False,  # å°æ ·æœ¬ä¸€èˆ¬ä¸å† shuffle
    )
    fortestfinetunedata = prune_datasetloader
    fortestvaldata = prune_datasetloader
    model = torch.load(model_path)
    results_save_path = 'test_res.pkl'
    layers2prune = [2, 4, 7, 9, 12, 14, 16, 19, 21, 23, 26, 28, 30, 35, 38]  # ]
    analysis_layers = [3, 5, 8, 10, 13, 15, 17, 20, 22, 24, 27, 29, 31, 36, 39]  # ]
    loss_function = nn.CrossEntropyLoss()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    results = get_metrics(model, prune_datasetloader, the_samplesize_for_compute_distance=2)
    ratios = get_ratios(model, results,  ToD_level=0.05)
    example_inputs = next(iter(prune_datasetloader))[0]
    pruned_model_skeleton = get_skeleton(model=model, ratios=ratios,
                                        example_inputs=example_inputs, align_residual_add=True, verbose=True,
                                        strict_forward_check=True, )
    pruned_model, report = prune(pruned_model_skeleton, model,
                                 results, ratios,
                                 example_inputs=example_inputs, finetune_pruned=False, finetune_epochs=10,
                                 finetunedata=fortestvaldata, valdata=fortestvaldata)
    print(report)