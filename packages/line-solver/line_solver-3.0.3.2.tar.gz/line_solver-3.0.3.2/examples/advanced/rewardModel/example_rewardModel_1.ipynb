{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced Example: Reward-Based CTMC Analysis\n",
    "\n",
    "This example demonstrates the `setReward` feature for defining custom reward functions on a queueing network model and computing steady-state expected rewards using the CTMC solver.\n",
    "\n",
    "The model is a simple closed queueing network with a delay station and a queue, and we define several reward functions:\n",
    "- Queue length\n",
    "- Utilization\n",
    "- Quadratic queue cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T06:00:58.381994Z",
     "iopub.status.busy": "2026-01-03T06:00:58.381788Z",
     "iopub.status.idle": "2026-01-03T06:00:59.030960Z",
     "shell.execute_reply": "2026-01-03T06:00:59.030450Z"
    }
   },
   "outputs": [],
   "source": [
    "from line_solver import *\n",
    "GlobalConstants.set_verbose(VerboseLevel.STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T06:00:59.032657Z",
     "iopub.status.busy": "2026-01-03T06:00:59.032431Z",
     "iopub.status.idle": "2026-01-03T06:00:59.035692Z",
     "shell.execute_reply": "2026-01-03T06:00:59.035079Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_reward_model():\n",
    "    \"\"\"\n",
    "    Create a simple closed queueing network with reward functions.\n",
    "    \n",
    "    The model has:\n",
    "    - A delay station (think time)\n",
    "    - A single-server FCFS queue\n",
    "    - N=5 jobs circulating in the system\n",
    "    \"\"\"\n",
    "    model = Network('RewardExample')\n",
    "    \n",
    "    # Block 1: nodes\n",
    "    delay = Delay(model, 'Delay')\n",
    "    queue = Queue(model, 'Queue', SchedStrategy.FCFS)\n",
    "    queue.set_number_of_servers(1)\n",
    "    \n",
    "    # Block 2: job classes (closed class with 5 jobs)\n",
    "    cclass = ClosedClass(model, 'Class1', 5, delay)\n",
    "    delay.set_service(cclass, Exp(1.0))   # Think time = 1\n",
    "    queue.set_service(cclass, Exp(2.0))   # Service rate = 2\n",
    "    \n",
    "    # Block 3: topology\n",
    "    model.add_link(delay, queue)\n",
    "    model.add_link(queue, delay)\n",
    "    \n",
    "    # Define Reward Functions\n",
    "    # setReward(name, lambda state, sn: reward_value)\n",
    "    # The function receives the aggregated state vector and network structure\n",
    "    # State format: state is a Matrix row with [jobs_at_delay, jobs_at_queue]\n",
    "    \n",
    "    # Reward 1: Queue length (number of jobs in the queue)\n",
    "    model.set_reward('QueueLength', lambda state, sn: state.get(0, 1))\n",
    "    \n",
    "    # Reward 2: Utilization (1 if server busy, 0 if idle)\n",
    "    model.set_reward('Utilization', lambda state, sn: min(state.get(0, 1), 1.0))\n",
    "    \n",
    "    # Reward 3: Weighted queue cost (quadratic penalty for long queues)\n",
    "    model.set_reward('QueueCost', lambda state, sn: state.get(0, 1) ** 2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_reward_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## About Reward Functions\n",
    "\n",
    "Reward functions allow computing custom metrics from the underlying Markov chain:\n",
    "\n",
    "- **State-based**: Rewards depend on the system state (queue lengths, etc.)\n",
    "- **Steady-state average**: E[R] = Σᵢ πᵢ × r(sᵢ) where πᵢ is steady-state probability\n",
    "- **Flexible**: Can compute any function of the state\n",
    "\n",
    "For a closed network with N=5 jobs, think rate=1, service rate=2:\n",
    "- The queue length varies from 0 to N\n",
    "- Expected metrics computed via CTMC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T06:00:59.037000Z",
     "iopub.status.busy": "2026-01-03T06:00:59.036835Z",
     "iopub.status.idle": "2026-01-03T06:00:59.042780Z",
     "shell.execute_reply": "2026-01-03T06:00:59.042395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving with CTMC solver...\n",
      "\n",
      "\n",
      "=== Steady-State Expected Rewards ===\n",
      "    QueueLength: 5.000000\n",
      "    Utilization: 5.000000\n",
      "      QueueCost: 5.000000\n"
     ]
    }
   ],
   "source": [
    "# Solve with CTMC Solver\n",
    "print(\"Solving with CTMC solver...\\n\")\n",
    "\n",
    "solver = CTMC(model)\n",
    "\n",
    "# Get Steady-State Expected Rewards\n",
    "rewards = solver.get_avg_reward()\n",
    "\n",
    "print(\"\\n=== Steady-State Expected Rewards ===\")\n",
    "for name, value in rewards.items():\n",
    "    print(f\"{name:>15s}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T06:00:59.044045Z",
     "iopub.status.busy": "2026-01-03T06:00:59.043927Z",
     "iopub.status.idle": "2026-01-03T06:00:59.046183Z",
     "shell.execute_reply": "2026-01-03T06:00:59.045829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "The CTMC solver successfully computed steady-state expected rewards\n",
      "for the custom reward functions defined on the closed queueing network.\n",
      "\n",
      "Note: Reward functions are evaluated at each state of the Markov chain,\n",
      "and the expected value is computed as the weighted sum over steady-state\n",
      "probabilities.\n"
     ]
    }
   ],
   "source": [
    "# Print summary of results\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(\"The CTMC solver successfully computed steady-state expected rewards\")\n",
    "print(\"for the custom reward functions defined on the closed queueing network.\")\n",
    "print(\"\\nNote: Reward functions are evaluated at each state of the Markov chain,\")\n",
    "print(\"and the expected value is computed as the weighted sum over steady-state\")\n",
    "print(\"probabilities.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
