import argparse
import json
import os
import requests
import gepa
from datasets import load_dataset as hf_load_dataset
from .adapter import RPCAdapter
from .models import Example


def load_dataset(path_or_name: str) -> list[Example]:
    if path_or_name.endswith(".jsonl") or path_or_name.endswith(".json"):
        ds = hf_load_dataset("json", data_files=path_or_name, split="train")
    elif path_or_name.endswith(".parquet"):
        ds = hf_load_dataset("parquet", data_files=path_or_name, split="train")
    elif path_or_name.endswith(".arrow"):
        ds = hf_load_dataset("arrow", data_files=path_or_name, split="train")
    else:
        # Assume it's a HF dataset path or name
        ds = hf_load_dataset(path_or_name, split="train")

    return ds.to_list()


def parse_simple_yaml(path):
    """
    Very simple YAML parser for the specific format generated by the TS client.
    Handles nesting by looking at indentation.
    """
    config = {}
    with open(path, "r") as f:
        lines = f.readlines()

    def parse_lines(start_idx, current_indent):
        result = {}
        idx = start_idx
        while idx < len(lines):
            line = lines[idx]
            if not line.strip():
                idx += 1
                continue

            indent = len(line) - len(line.lstrip())
            if indent < current_indent:
                break

            if indent == current_indent:
                parts = line.strip().split(":", 1)
                key = parts[0].strip()
                val_str = parts[1].strip() if len(parts) > 1 else ""

                if not val_str and idx + 1 < len(lines):
                    next_indent = len(lines[idx + 1]) - len(lines[idx + 1].lstrip())
                    if next_indent > indent:
                        nested_obj, next_idx = parse_lines(idx + 1, next_indent)
                        result[key] = nested_obj
                        idx = next_idx
                        continue

                try:
                    result[key] = json.loads(val_str) if val_str else None
                except json.JSONDecodeError:
                    result[key] = val_str
            idx += 1
        return result, idx

    config, _ = parse_lines(0, 0)
    return config


def main():
    parser = argparse.ArgumentParser(description="GEPA RPC CLI")
    parser.add_argument("--port", type=int, default=8000, help="Port of the TS server")
    parser.add_argument(
        "--config", type=str, required=True, help="Path to the config.yaml"
    )

    args = parser.parse_args()

    if not os.path.exists(args.config):
        print(f"Config file not found: {args.config}")
        return

    config = parse_simple_yaml(args.config)

    adapter_config = config.get("adapter", {})
    base_url = adapter_config.get("base_url", f"http://localhost:{args.port}")
    adapter = RPCAdapter(base_url)

    trainset = load_dataset(config["trainset"])
    valset = load_dataset(config["valset"])

    seed_candidate = config.get("seed_candidate", {})

    # Extract optimize arguments from config
    optimize_kwargs = {
        "seed_candidate": seed_candidate,
        "trainset": trainset,
        "valset": valset,
        "adapter": adapter,
        "task_lm": config.get("task_lm"),
        "max_metric_calls": config.get("max_metric_calls", 150),
        "reflection_lm": config.get("reflection_lm", "openai/gpt-5"),
    }

    # Add any other config keys that might be valid for optimize
    # (Checking gepa.api.optimize signature)
    valid_keys = [
        "candidate_selection_strategy",
        "skip_perfect_score",
        "batch_sampler",
        "reflection_minibatch_size",
        "perfect_score",
        "reflection_prompt_template",
        "module_selector",
        "use_merge",
        "max_merge_invocations",
        "merge_val_overlap_floor",
        "stop_callbacks",
        "logger",
        "run_dir",
        "use_wandb",
        "wandb_api_key",
        "wandb_init_kwargs",
        "use_mlflow",
        "mlflow_tracking_uri",
        "mlflow_experiment_name",
        "track_best_outputs",
        "display_progress_bar",
        "use_cloudpickle",
        "seed",
        "raise_on_exception",
        "val_evaluation_policy",
    ]

    for key in valid_keys:
        if key in config:
            optimize_kwargs[key] = config[key]

    print(f"Starting GEPA optimization with task_lm={optimize_kwargs['task_lm']}...")
    gepa_result = gepa.optimize(**optimize_kwargs)

    print(f"Optimization finished. Results: {gepa_result}")

    # Post to /finalize
    finalize_url = f"{base_url.rstrip('/')}/finalize"
    finalize_data = {
        "best_candidate": gepa_result.best_candidate,
        "results": {
            "score": getattr(gepa_result, "score", None),
        },
    }

    try:
        response = requests.post(finalize_url, json=finalize_data)
        response.raise_for_status()
        print("Finalization successful.")
    except Exception as e:
        print(f"Failed to call /finalize: {e}")


if __name__ == "__main__":
    main()
