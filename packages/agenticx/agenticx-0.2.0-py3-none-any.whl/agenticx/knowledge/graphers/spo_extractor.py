"""SPO (Subject-Predicate-Object) Extractor for Knowledge Graph Construction

This module provides a unified SPO extraction approach with custom schema support,
extracting entities, relationships, and attributes in a single LLM call.
"""

import json
import os
from typing import Any, Dict, List, Optional, Tuple
from loguru import logger

from .models import Entity, EntityType, Relationship, RelationType


class SPOExtractor:
    """Unified SPO extractor with custom schema and prompt template support"""
    
    def __init__(self, llm_client=None, prompt_manager=None, custom_schema: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None):
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.config = config or {}
        
        # ä»é…ç½®ä¸­è¯»å–ç½®ä¿¡åº¦å‚æ•°
        self.default_entity_confidence = self.config.get('default_confidence', 0.8)
        self.default_relationship_confidence = self.config.get('default_confidence', 0.8)
        self.enable_dynamic_confidence = self.config.get('dynamic_confidence', False)
        
        # Use custom schema if provided, otherwise use default
        if custom_schema:
            self.schema = custom_schema
            logger.info("ä½¿ç”¨å®šåˆ¶Schema")
        else:
            # Default schema
            self.schema = {
                "Nodes": ["person", "organization", "location", "event", "concept", "technology", "product"],
                "Relations": ["related_to", "part_of", "located_in", "works_for", "created_by", "influences", "depends_on"],
                "Attributes": ["name", "description", "type", "status", "date", "profession", "title"]
            }
            logger.info("ğŸ“‹ ä½¿ç”¨é»˜è®¤Schema")
        
        # æå–é¢†åŸŸä¿¡æ¯
        self.domain_info = self.schema.get('domain_info', {})
        self.primary_domain = self.domain_info.get('primary_domain', 'é€šç”¨')
        self.key_concepts = ', '.join(self.domain_info.get('key_concepts', []))
        
        logger.info(f"SPOæŠ½å–å™¨åˆå§‹åŒ–: {len(self.schema['Nodes'])}å®ä½“ç±»å‹, {len(self.schema['Relations'])}å…³ç³»ç±»å‹, {len(self.schema['Attributes'])}å±æ€§ç±»å‹")
        logger.debug(f"ä¸»è¦é¢†åŸŸ: {self.primary_domain}")
    
    def extract(self, text: str, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """Extract entities and relationships in a single call
        
        Args:
            text: Text to extract from
            **kwargs: Additional parameters
            
        Returns:
            Tuple of (entities, relationships)
        """
        logger.info(f"å¼€å§‹SPOæŠ½å–ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        if not self.llm_client:
            raise ValueError("LLM client is required for SPO extraction")
        
        try:
            # Build prompt
            logger.debug("æ„å»ºSPOæŠ½å–æç¤ºè¯...")
            prompt = self._build_spo_prompt(text)
            
            # Call LLM
            logger.debug("è°ƒç”¨LLMè¿›è¡ŒSPOæŠ½å–")
            response = self.llm_client.call(prompt)
            logger.debug(f"LLMå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # Parse response
            logger.debug("è§£æLLMå“åº”...")
            spo_data = self._parse_spo_response(response)
            logger.debug(f"è§£æç»“æœ: {len(spo_data.get('entity_types', {}))} ä¸ªå®ä½“ç±»å‹, {len(spo_data.get('triples', []))} ä¸ªä¸‰å…ƒç»„")
            
            # Convert to entities and relationships
            logger.debug("ğŸ”„ è½¬æ¢ä¸ºå®ä½“å’Œå…³ç³»å¯¹è±¡...")
            entities, relationships = self._convert_spo_to_objects(spo_data, text, **kwargs)
            
            logger.success(f"âœ… SPOæŠ½å–å®Œæˆ: {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")
            
            return entities, relationships
            
        except Exception as e:
            logger.error(f"âŒ SPOæŠ½å–å¤±è´¥: {e}")
            logger.debug(f"âŒ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.debug(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return [], []
    
    async def extract_batch(self, texts: List[str], batch_size: int = 1, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """æ‰¹å¤„ç†SPOæŠ½å–ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            Tuple of (all_entities, all_relationships)
        """
        # è®¡ç®—æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(text) for text in texts)
        avg_chars = total_chars / len(texts) if texts else 0
        
        logger.info(f"å¼€å§‹æ‰¹å¤„ç†SPOæŠ½å–ï¼Œæ€»æ–‡æœ¬æ•°: {len(texts)}, æ‰¹å¤§å°: {batch_size}")
        logger.info(f"æ–‡æœ¬ç»Ÿè®¡: æ€»å­—ç¬¦æ•°={total_chars}, å¹³å‡å­—ç¬¦æ•°={avg_chars:.0f}/æ–‡æœ¬")
        
        all_entities = []
        all_relationships = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å­—ç¬¦æ•°ç»Ÿè®¡
            batch_chars = sum(len(text) for text in batch_texts)
            batch_avg_chars = batch_chars / len(batch_texts) if batch_texts else 0
            
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_texts)}ä¸ªæ–‡æœ¬, {batch_chars}å­—ç¬¦, å¹³å‡{batch_avg_chars:.0f}å­—ç¬¦/æ–‡æœ¬)")
            
            try:
                # æ„å»ºæ‰¹å¤„ç†æç¤ºè¯
                batch_prompt = self._build_batch_spo_prompt(batch_texts)
                
                # è°ƒç”¨LLM
                logger.debug("è°ƒç”¨LLMè¿›è¡Œæ‰¹å¤„ç†SPOæŠ½å–")
                response = self.llm_client.call(batch_prompt)
                
                # è§£ææ‰¹å¤„ç†å“åº”
                batch_entities, batch_relationships = self._parse_batch_spo_response(response, batch_texts, i, **kwargs)
                
                all_entities.extend(batch_entities)
                all_relationships.extend(batch_relationships)
                
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(batch_entities)} ä¸ªå®ä½“, {len(batch_relationships)} ä¸ªå…³ç³»")
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                # å›é€€åˆ°å•ä¸ªå¤„ç†
                for j, text in enumerate(batch_texts):
                    try:
                        entities, relationships = self.extract(text, chunk_id=f"chunk_{i+j}", **kwargs)
                        all_entities.extend(entities)
                        all_relationships.extend(relationships)
                    except Exception as single_e:
                        logger.error(f"âŒ å•ä¸ªæ–‡æœ¬å¤„ç†ä¹Ÿå¤±è´¥: {single_e}")
        
        logger.success(f"ğŸ‰ æ‰¹å¤„ç†SPOæŠ½å–å®Œæˆ: æ€»è®¡ {len(all_entities)} ä¸ªå®ä½“, {len(all_relationships)} ä¸ªå…³ç³»")
        return all_entities, all_relationships
    
    def _build_batch_spo_prompt(self, texts: List[str]) -> str:
        """æ„å»ºæ‰¹å¤„ç†SPOæŠ½å–æç¤ºè¯"""
        schema_str = json.dumps(self.schema, ensure_ascii=False, indent=2)
        
        # æ„å»ºæ‰¹å¤„ç†æ–‡æœ¬
        batch_content = ""
        for i, text in enumerate(texts):
            batch_content += f"\n=== æ–‡æ¡£ç‰‡æ®µ {i+1} ===\n{text}\n"
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µä¸­æŠ½å–å°½å¯èƒ½å¤šçš„å®ä½“ã€å…³ç³»å’Œå±æ€§ã€‚

**é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š**
1. **å®å¯å¤šæŠ½å–ï¼Œä¸è¦é—æ¼**ï¼šå³ä½¿ä¸ç¡®å®šï¼Œä¹Ÿè¦å°è¯•æŠ½å–å¯èƒ½çš„å®ä½“å’Œå…³ç³»
2. **çµæ´»ä½¿ç”¨Schema**ï¼šå¯ä»¥é€‚å½“æ‰©å±•ç±»å‹ï¼Œä¸è¦ä¸¥æ ¼é™åˆ¶
3. **å…³æ³¨éšå«å…³ç³»**ï¼šæŠ½å–æ–‡æœ¬ä¸­éšå«çš„å…³ç³»ï¼Œä¸ä»…ä»…æ˜¯æ˜ç¡®è¡¨è¿°çš„
4. **ç»†ç²’åº¦æŠ½å–**ï¼šå°†å¤åˆæ¦‚å¿µæ‹†åˆ†ä¸ºå¤šä¸ªå®ä½“å’Œå…³ç³»
5. **åŒ…å«æ¨æµ‹æ€§å†…å®¹**ï¼šåŸºäºä¸Šä¸‹æ–‡çš„åˆç†æ¨æµ‹ä¹Ÿè¦æŠ½å–

é¢†åŸŸï¼š{self.primary_domain}
æ ¸å¿ƒæ¦‚å¿µï¼š{self.key_concepts}

å¯ç”¨çš„å®ä½“ç±»å‹ï¼š{', '.join(self.schema.get('Nodes', []))}
å¯ç”¨çš„å…³ç³»ç±»å‹ï¼š{', '.join(self.schema.get('Relations', []))}

æ–‡æ¡£ç‰‡æ®µï¼š
{batch_content}

**æŠ½å–è¦æ±‚ï¼š**
- æ¯ä¸ªå®ä½“éƒ½è¦æœ‰æè¿°å’Œç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0.1-1.0ï¼‰
- æ¯ä¸ªå…³ç³»éƒ½è¦æœ‰æè¿°å’Œç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0.1-1.0ï¼‰
- ç½®ä¿¡åº¦åŸºäºæ–‡æœ¬ä¸­çš„è¯æ®å¼ºåº¦ï¼š
  * 0.9-1.0: æ˜ç¡®ç›´æ¥çš„è¡¨è¿°
  * 0.7-0.8: è¾ƒå¼ºçš„æš—ç¤ºæˆ–æ¨ç†
  * 0.5-0.6: å¼±æš—ç¤ºæˆ–å¯èƒ½çš„å…³ç³»
  * 0.3-0.4: æ¨æµ‹æ€§çš„å…³ç³»
- ä¼˜å…ˆæŠ½å–é«˜ç½®ä¿¡åº¦çš„å†…å®¹ï¼Œä½†ä¹ŸåŒ…å«ä¸€äº›ä½ç½®ä¿¡åº¦çš„æ¨æµ‹

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œç¡®ä¿JSONè¯­æ³•æ­£ç¡®ï¼š

{{
    "entity_types": {{
        "å®ä½“åç§°": {{
            "type": "å®ä½“ç±»å‹",
            "description": "è¯¦ç»†æè¿°",
            "confidence": 0.85,
            "attributes": {{"å±æ€§å": "å±æ€§å€¼"}},
            "source_chunks": ["chunk_0"]
        }}
    }},
    "triples": [
        {{
            "subject": "ä¸»ä½“å®ä½“",
            "predicate": "å…³ç³»ç±»å‹",
            "object": "å®¢ä½“å®ä½“",
            "description": "å…³ç³»æè¿°",
            "confidence": 0.75,
            "evidence": "æ”¯æŒè¯æ®",
            "source_chunks": ["chunk_0"]
        }}
    ]
}}

è¦æ±‚ï¼š
1. æŠ½å–å°½å¯èƒ½å¤šçš„å®ä½“å’Œå…³ç³»ï¼Œå®å¯å¤šæŠ½å–
2. å¯ä»¥çµæ´»æ‰©å±•Schemaä¸­çš„ç±»å‹
3. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œæ³¨æ„é€—å·å’Œå¼•å·
4. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—"""
        
        return prompt
    
    def _parse_batch_spo_response(self, response: str, texts: List[str], start_index: int, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """è§£ææ‰¹å¤„ç†SPOå“åº”"""
        try:
            # æ¸…ç†å’Œè§£æå“åº”
            cleaned_response = self._clean_llm_response(response)
            logger.debug(f"æ¸…ç†åçš„å“åº”é•¿åº¦: {len(cleaned_response)}")
            
            try:
                # åº”ç”¨å’Œquery_decomposer.pyç›¸åŒçš„æ¸…ç†é€»è¾‘
                raw_content = cleaned_response.strip()
                
                # ç§»é™¤markdownä»£ç å—æ ‡è®°ï¼ˆæ›´å½»åº•çš„æ¸…ç†ï¼‰
                if raw_content.startswith('```json'):
                    raw_content = raw_content[7:]  # ç§»é™¤ ```json
                if raw_content.startswith('```'):
                    raw_content = raw_content[3:]   # ç§»é™¤ ```
                if raw_content.endswith('```'):
                    raw_content = raw_content[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
                
                raw_content = raw_content.strip()
                logger.debug(f"äºŒæ¬¡æ¸…ç†åçš„JSONå†…å®¹é•¿åº¦: {len(raw_content)}")
                
                spo_data = json.loads(raw_content)
                logger.debug("âœ… JSONè§£ææˆåŠŸ")
            except json.JSONDecodeError as json_error:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {json_error}")
                logger.error(f"ğŸ” å®Œæ•´åŸå§‹å“åº”å†…å®¹:\n{response}")
                logger.error(f"ğŸ” å®Œæ•´æ¸…ç†åå†…å®¹:\n{raw_content if 'raw_content' in locals() else cleaned_response}")
                
                # å°è¯•æ›´æ¿€è¿›çš„ä¿®å¤
                fixed_response = self._aggressive_json_fix(raw_content if 'raw_content' in locals() else cleaned_response)
                logger.debug(f"ğŸ”§ æ¿€è¿›ä¿®å¤åçš„å†…å®¹:\n{fixed_response}")
                try:
                    spo_data = json.loads(fixed_response)
                    logger.info("âœ… æ¿€è¿›ä¿®å¤æˆåŠŸ")
                except Exception as fix_error:
                    logger.error(f"âŒ æ¿€è¿›ä¿®å¤ä¹Ÿå¤±è´¥: {fix_error}")
                    logger.warning("âŒ æ¿€è¿›ä¿®å¤ä¹Ÿå¤±è´¥ï¼Œè¿”å›æœ€å°æœ‰æ•ˆJSONç»“æ„")
                    return [], []
            
            # è½¬æ¢ä¸ºå®ä½“å’Œå…³ç³»å¯¹è±¡
            entities, relationships = self._convert_spo_to_objects(spo_data, "\n".join(texts), **kwargs)
            
            # æ›´æ–°chunk_idæ˜ å°„
            for entity in entities:
                if hasattr(entity, 'source_chunks'):
                    # å°†ç›¸å¯¹chunkç¼–å·è½¬æ¢ä¸ºç»å¯¹ç¼–å·
                    updated_chunks = set()
                    for chunk_ref in entity.source_chunks:
                        if chunk_ref.startswith('chunk_'):
                            chunk_num = int(chunk_ref.split('_')[1])
                            updated_chunks.add(f"chunk_{start_index + chunk_num}")
                        else:
                            updated_chunks.add(chunk_ref)
                    entity.source_chunks = updated_chunks
            
            for relationship in relationships:
                if hasattr(relationship, 'source_chunks'):
                    # å°†ç›¸å¯¹chunkç¼–å·è½¬æ¢ä¸ºç»å¯¹ç¼–å·
                    updated_chunks = set()
                    for chunk_ref in relationship.source_chunks:
                        if chunk_ref.startswith('chunk_'):
                            chunk_num = int(chunk_ref.split('_')[1])
                            updated_chunks.add(f"chunk_{start_index + chunk_num}")
                        else:
                            updated_chunks.add(chunk_ref)
                    relationship.source_chunks = updated_chunks
            
            return entities, relationships
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹å¤„ç†å“åº”è§£æå¤±è´¥: {e}")
            return [], []
    
    def _find_entity_id(self, entity_name: str, entity_id_map: Dict[str, str]) -> Optional[str]:
        """æŸ¥æ‰¾å®ä½“IDï¼Œæ”¯æŒæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…"""
        # 1. ç²¾ç¡®åŒ¹é…
        if entity_name in entity_id_map:
            return entity_id_map[entity_name]
        
        # 2. æ ‡å‡†åŒ–åç§°åŒ¹é…
        normalized_target = self._normalize_entity_name(entity_name)
        for name, entity_id in entity_id_map.items():
            if self._normalize_entity_name(name) == normalized_target:
                logger.debug(f"æ ‡å‡†åŒ–åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{name}'")
                return entity_id
        
        # 3. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¤„ç†å¤åˆè¯ã€ç¼©å†™ç­‰ï¼‰
        best_match = None
        best_score = 0.0
        
        for name, entity_id in entity_id_map.items():
            score = self._calculate_similarity(entity_name, name)
            if score > best_score and score >= 0.8:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                best_score = score
                best_match = (name, entity_id)
        
        if best_match:
            logger.debug(f"ç›¸ä¼¼åº¦åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{best_match[0]}' (ç›¸ä¼¼åº¦: {best_score:.2f})")
            return best_match[1]
        
        # 4. åŒ…å«å…³ç³»åŒ¹é…ï¼ˆé™ä½ä¼˜å…ˆçº§ï¼‰
        for name, entity_id in entity_id_map.items():
            if len(normalized_target) > 3:  # é¿å…çŸ­è¯è¯¯åŒ¹é…
                if normalized_target in self._normalize_entity_name(name) or self._normalize_entity_name(name) in normalized_target:
                    logger.debug(f"åŒ…å«åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{name}'")
                    return entity_id
        
        return None
    
    def _normalize_entity_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å®ä½“åç§°"""
        import re
        # è½¬æ¢ä¸ºå°å†™
        normalized = name.lower().strip()
        # æ›¿æ¢è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿ä¸ºç©ºæ ¼
        normalized = re.sub(r'[-_]', ' ', normalized)
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼ï¼‰
        normalized = re.sub(r'[^\w\s]', '', normalized)
        # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', normalized)
        return normalized.strip()
    
    def _select_template(self, text: str) -> str:
        """æ™ºèƒ½é€‰æ‹©SPOæŠ½å–æ¨¡æ¿"""
        text_length = len(text)
        
        # 1. æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©
        if text_length < 500:
            logger.debug(f"ğŸ“ æ–‡æœ¬è¾ƒçŸ­({text_length}å­—ç¬¦)ï¼Œé€‰æ‹©ç®€åŒ–æ¨¡æ¿")
            return "simple_template"
        
        # 2. æ ¹æ®é¢†åŸŸä¿¡æ¯é€‰æ‹©é¢†åŸŸç‰¹å®šæ¨¡æ¿
        if hasattr(self, 'primary_domain') and self.primary_domain:
            domain_lower = self.primary_domain.lower()
            
            # æŠ€æœ¯é¢†åŸŸ
            if any(keyword in domain_lower for keyword in ['æŠ€æœ¯', 'ç§‘æŠ€', 'äººå·¥æ™ºèƒ½', 'ai', 'technology', 'tech']):
                logger.debug(f"æ£€æµ‹åˆ°æŠ€æœ¯é¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.technology"
            
            # å•†ä¸šé¢†åŸŸ
            elif any(keyword in domain_lower for keyword in ['å•†ä¸š', 'ä¸šåŠ¡', 'ç®¡ç†', 'business', 'management']):
                logger.debug(f"ğŸ’¼ æ£€æµ‹åˆ°å•†ä¸šé¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.business"
            
            # å­¦æœ¯é¢†åŸŸ
            elif any(keyword in domain_lower for keyword in ['å­¦æœ¯', 'ç ”ç©¶', 'ç§‘å­¦', 'academic', 'research', 'science']):
                logger.debug(f"ğŸ“ æ£€æµ‹åˆ°å­¦æœ¯é¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.academic"
        
        # 3. æ ¹æ®æ–‡æœ¬å†…å®¹ç‰¹å¾é€‰æ‹©
        text_lower = text.lower()
        
        # æŠ€æœ¯æ–‡æ¡£ç‰¹å¾
        tech_keywords = ['ç®—æ³•', 'æ¨¡å‹', 'æ¡†æ¶', 'ç³»ç»Ÿ', 'ä»£ç ', 'algorithm', 'model', 'framework', 'system']
        if any(keyword in text_lower for keyword in tech_keywords):
            logger.debug("æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©æŠ€æœ¯æ¨¡æ¿")
            return "domain_templates.technology"
        
        # å•†ä¸šæ–‡æ¡£ç‰¹å¾
        business_keywords = ['å…¬å¸', 'å¸‚åœº', 'é”€å”®', 'å®¢æˆ·', 'ä¸šç»©', 'company', 'market', 'sales', 'customer']
        if any(keyword in text_lower for keyword in business_keywords):
            logger.debug("ğŸ’¼ æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©å•†ä¸šæ¨¡æ¿")
            return "domain_templates.business"
        
        # å­¦æœ¯æ–‡æ¡£ç‰¹å¾
        academic_keywords = ['è®ºæ–‡', 'ç ”ç©¶', 'å®éªŒ', 'ç†è®º', 'paper', 'research', 'experiment', 'theory']
        if any(keyword in text_lower for keyword in academic_keywords):
            logger.debug("ğŸ“ æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©å­¦æœ¯æ¨¡æ¿")
            return "domain_templates.academic"
        
        # 4. é»˜è®¤ä½¿ç”¨ä¸»æ¨¡æ¿
        logger.debug("ä½¿ç”¨é»˜è®¤ä¸»æ¨¡æ¿")
        return "template"
    
    def _calculate_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå®ä½“åç§°çš„ç›¸ä¼¼åº¦"""
        # æ ‡å‡†åŒ–åç§°
        norm1 = self._normalize_entity_name(name1)
        norm2 = self._normalize_entity_name(name2)
        
        # å¦‚æœå®Œå…¨ç›¸åŒ
        if norm1 == norm2:
            return 1.0
        
        # åˆ†è¯å¤„ç†
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­é›†
        if words1.issubset(words2) or words2.issubset(words1):
            return 0.9
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
        
        jaccard_score = intersection / union
        
        # å¤„ç†ç¼©å†™æƒ…å†µï¼ˆå¦‚ LLMs vs Large Language Modelsï¼‰
        if self._is_abbreviation_match(norm1, norm2):
            jaccard_score = max(jaccard_score, 0.85)
        
        # å¤„ç†ç¼–è¾‘è·ç¦»
        edit_distance_score = self._calculate_edit_distance_similarity(norm1, norm2)
        
        # ç»¼åˆè¯„åˆ†
        final_score = max(jaccard_score, edit_distance_score * 0.8)
        
        return final_score
    
    def _calculate_dynamic_confidence(self, entity_name: str, entity_description: str, source_text: str) -> float:
        """åŠ¨æ€è®¡ç®—å®ä½“ç½®ä¿¡åº¦"""
        if not self.enable_dynamic_confidence:
            return self.default_entity_confidence
            
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦
        
        # åŸºäºåç§°é•¿åº¦å’Œå¤æ‚åº¦
        if len(entity_name) >= 2:
            confidence += 0.1
        if len(entity_name) >= 4:
            confidence += 0.1
            
        # åŸºäºæè¿°è´¨é‡
        if entity_description and len(entity_description) > 10:
            confidence += 0.1
        if entity_description and len(entity_description) > 30:
            confidence += 0.1
            
        # åŸºäºåœ¨åŸæ–‡ä¸­çš„å‡ºç°é¢‘ç‡
        occurrences = source_text.lower().count(entity_name.lower())
        if occurrences > 1:
            confidence += min(0.2, occurrences * 0.05)
            
        # åŸºäºä¸Šä¸‹æ–‡è´¨é‡
        if self._has_strong_context(entity_name, source_text):
            confidence += 0.1
            
        return min(1.0, confidence)
    
    def _calculate_relationship_confidence(self, subject: str, predicate: str, object_name: str, source_text: str) -> float:
        """åŠ¨æ€è®¡ç®—å…³ç³»ç½®ä¿¡åº¦"""
        if not self.enable_dynamic_confidence:
            return self.default_relationship_confidence
            
        confidence = 0.4  # åŸºç¡€ç½®ä¿¡åº¦
        
        # æ£€æŸ¥ä¸»ä½“å’Œå®¢ä½“æ˜¯å¦éƒ½åœ¨æ–‡æœ¬ä¸­
        subject_in_text = subject.lower() in source_text.lower()
        object_in_text = object_name.lower() in source_text.lower()
        
        if subject_in_text and object_in_text:
            confidence += 0.3
        elif subject_in_text or object_in_text:
            confidence += 0.1
            
        # æ£€æŸ¥å…³ç³»è¯çš„å¼ºåº¦
        strong_relation_words = ["æ˜¯", "ä¸º", "å±äº", "åŒ…å«", "ç®¡ç†", "è´Ÿè´£", "åˆ›å»º", "å¼€å‘"]
        weak_relation_words = ["ç›¸å…³", "æ¶‰åŠ", "å¯èƒ½", "ä¼¼ä¹"]
        
        for word in strong_relation_words:
            if word in source_text:
                confidence += 0.1
                break
                
        for word in weak_relation_words:
            if word in source_text:
                confidence -= 0.05
                break
                
        # åŸºäºè·ç¦»ï¼ˆä¸»ä½“å’Œå®¢ä½“åœ¨æ–‡æœ¬ä¸­çš„è·ç¦»ï¼‰
        try:
            subject_pos = source_text.lower().find(subject.lower())
            object_pos = source_text.lower().find(object_name.lower())
            if subject_pos != -1 and object_pos != -1:
                distance = abs(subject_pos - object_pos)
                if distance < 100:  # è·ç¦»å¾ˆè¿‘
                    confidence += 0.1
                elif distance < 300:  # è·ç¦»é€‚ä¸­
                    confidence += 0.05
        except:
            pass
            
        return min(1.0, max(0.1, confidence))
    
    def _has_strong_context(self, entity_name: str, source_text: str) -> bool:
        """æ£€æŸ¥å®ä½“æ˜¯å¦æœ‰å¼ºä¸Šä¸‹æ–‡"""
        import re
        # æŸ¥æ‰¾å®ä½“å‘¨å›´çš„æè¿°æ€§è¯æ±‡
        descriptive_patterns = [
            rf"{re.escape(entity_name)}[æ˜¯ä¸º]([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            rf"([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+){re.escape(entity_name)}",
            rf"{re.escape(entity_name)}ï¼š([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            rf"{re.escape(entity_name)}\s*\(([^)]+)\)"
        ]
        
        for pattern in descriptive_patterns:
            if re.search(pattern, source_text, re.IGNORECASE):
                return True
        return False
    
    def _is_abbreviation_match(self, name1: str, name2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™åŒ¹é…"""
        words1 = name1.split()
        words2 = name2.split()
        
        # æ£€æŸ¥ä¸€ä¸ªæ˜¯å¦ä¸ºå¦ä¸€ä¸ªçš„é¦–å­—æ¯ç¼©å†™
        if len(words1) == 1 and len(words2) > 1:
            abbrev = ''.join([w[0] for w in words2 if w])
            return words1[0].replace('s', '') == abbrev.lower()  # å¤„ç†å¤æ•°å½¢å¼
        elif len(words2) == 1 and len(words1) > 1:
            abbrev = ''.join([w[0] for w in words1 if w])
            return words2[0].replace('s', '') == abbrev.lower()
        
        return False
    
    def _calculate_edit_distance_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦"""
        if len(s1) == 0 or len(s2) == 0:
            return 0.0
        
        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        
        # è®¡ç®—å…¬å…±å­åºåˆ—é•¿åº¦
        common_chars = 0
        for char in set(s1):
            common_chars += min(s1.count(char), s2.count(char))
        
        return common_chars / max_len
    
    def _create_missing_entity(self, entity_name: str, entities: List, entity_id_map: Dict[str, str], source_text: str = "") -> Optional[str]:
        """åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„å®ä½“ï¼Œä»åŸæ–‡ä¸­æå–æè¿°"""
        import uuid
        import re
        from .models import Entity, EntityType
        
        # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„å®ä½“åç§°
        if len(entity_name.strip()) < 2:
            return None
        
        # è¿‡æ»¤æ‰å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡
        meaningless_words = {
            'information', 'data', 'system', 'method', 'approach', 'way', 'means',
            'process', 'technique', 'strategy', 'solution', 'result', 'output'
        }
        
        normalized_name = self._normalize_entity_name(entity_name)
        if normalized_name in meaningless_words:
            return None
        
        # ç”Ÿæˆæ–°çš„å®ä½“ID
        entity_id = str(uuid.uuid4())
        
        # æ¨æ–­å®ä½“ç±»å‹ï¼ˆç®€å•çš„å¯å‘å¼è§„åˆ™ï¼‰
        try:
            entity_type = self._infer_entity_type(entity_name)
            logger.debug(f"æ¨æ–­å®ä½“ç±»å‹: {entity_name} -> {entity_type.value}")
        except Exception as e:
            logger.warning(f"âš ï¸ å®ä½“ç±»å‹æ¨æ–­å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
            from .models import EntityType
            entity_type = EntityType.CONCEPT
        
        # ä»åŸæ–‡ä¸­æå–å®ä½“æè¿°
        entity_description = self._extract_entity_description_from_text(entity_name, source_text)
        
        # åˆ›å»ºæ–°å®ä½“
        try:
            # è®¡ç®—åŠ¨æ€ç½®ä¿¡åº¦
            entity_confidence = self._calculate_dynamic_confidence(
                entity_name, 
                entity_description, 
                source_text
            )
            # åŠ¨æ€åˆ›å»ºçš„å®ä½“é™ä½ç½®ä¿¡åº¦
            entity_confidence *= 0.8
            
            new_entity = Entity(
                id=entity_id,
                name=entity_name,
                entity_type=entity_type,
                description=entity_description,
                confidence=entity_confidence  # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨åŠ¨æ€ç½®ä¿¡åº¦
            )
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºå®ä½“: {entity_name} ({entity_type.value}) - {entity_description[:50]}...")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå®ä½“å¤±è´¥: {e}")
            return None
        
        # æ·»åŠ åˆ°å®ä½“åˆ—è¡¨å’Œæ˜ å°„
        entities.append(new_entity)
        entity_id_map[entity_name] = entity_id
        
        return entity_id
    
    def _infer_entity_type(self, entity_name: str) -> 'EntityType':
        """æ¨æ–­å®ä½“ç±»å‹"""
        from .models import EntityType
        
        name_lower = entity_name.lower()
        
        # äººå‘˜ç›¸å…³
        if any(word in name_lower for word in ['äºº', 'è€…', 'å‘˜', 'person', 'researcher', 'author', 'developer']):
            return EntityType.PERSON
        
        # ç»„ç»‡ç›¸å…³
        if any(word in name_lower for word in ['å…¬å¸', 'ç»„ç»‡', 'æœºæ„', 'company', 'organization', 'institution']):
            return EntityType.ORGANIZATION
        
        # åœ°ç‚¹ç›¸å…³
        if any(word in name_lower for word in ['åœ°', 'å¸‚', 'å›½', 'location', 'city', 'country', 'place']):
            return EntityType.LOCATION
        
        # äº‹ä»¶ç›¸å…³
        if any(word in name_lower for word in ['è¿‡ç¨‹', 'æµç¨‹', 'æ“ä½œ', 'ä»»åŠ¡', 'process', 'procedure', 'operation', 'task', 'event']):
            return EntityType.EVENT
        
        # å¯¹è±¡ç›¸å…³ï¼ˆæŠ€æœ¯äº§å“ã€å·¥å…·ç­‰ï¼‰
        if any(word in name_lower for word in ['ç³»ç»Ÿ', 'å¹³å°', 'å·¥å…·', 'è½¯ä»¶', 'æ¨¡å‹', 'system', 'platform', 'tool', 'software', 'model']):
            return EntityType.OBJECT
        
        # æ—¶é—´ç›¸å…³
        if any(word in name_lower for word in ['æ—¶é—´', 'æ—¥æœŸ', 'å¹´', 'æœˆ', 'time', 'date', 'year', 'month']):
            return EntityType.TIME
        
        # æ¦‚å¿µç›¸å…³ï¼ˆç®—æ³•ã€æ–¹æ³•ã€ç†è®ºç­‰ï¼‰
        if any(word in name_lower for word in ['ç®—æ³•', 'æ–¹æ³•', 'æŠ€æœ¯', 'ç†è®º', 'æ¦‚å¿µ', 'algorithm', 'method', 'technique', 'theory', 'concept', 'approach']):
            return EntityType.CONCEPT
        
        # é»˜è®¤è¿”å›æ¦‚å¿µç±»å‹
        return EntityType.CONCEPT
    
    def _extract_entity_description_from_text(self, entity_name: str, source_text: str) -> str:
        """ä»åŸæ–‡ä¸­æå–å®ä½“çš„æè¿°ä¿¡æ¯"""
        import re
        
        if not source_text or not entity_name:
            return f"å®ä½“: {entity_name}"
        
        # å°è¯•å¤šç§æ¨¡å¼æå–å®ä½“æè¿°
        patterns = [
            # æ¨¡å¼1: "å®ä½“åç§°æ˜¯/ä¸º/æŒ‡..."
            rf"{re.escape(entity_name)}(?:æ˜¯|ä¸º|æŒ‡|è¡¨ç¤º|ä»£è¡¨)([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            # æ¨¡å¼2: "å®ä½“åç§°ï¼Œæè¿°..."
            rf"{re.escape(entity_name)}ï¼Œ([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            # æ¨¡å¼3: "å®ä½“åç§°ï¼šæè¿°..."
            rf"{re.escape(entity_name)}ï¼š([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            # æ¨¡å¼4: "å®ä½“åç§° - æè¿°..."
            rf"{re.escape(entity_name)}\s*-\s*([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]+)",
            # æ¨¡å¼5: "å®ä½“åç§°(æè¿°)"
            rf"{re.escape(entity_name)}\s*\(([^)]+)\)",
            # æ¨¡å¼6: è‹±æ–‡æ¨¡å¼ "EntityName is/are..."
            rf"{re.escape(entity_name)}\s+(?:is|are|refers to|represents?)\s+([^.;,!?\n]+)",
            # æ¨¡å¼7: å‰åæ–‡æè¿°
            rf"([^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]*{re.escape(entity_name)}[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ\n]*)"
        ]
        
        best_description = ""
        max_length = 0
        
        for pattern in patterns:
            try:
                matches = re.finditer(pattern, source_text, re.IGNORECASE)
                for match in matches:
                    if len(match.groups()) > 0:
                        desc = match.group(1).strip()
                        # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„æè¿°
                        if len(desc) > max_length and len(desc) > 5:
                            # æ¸…ç†æè¿°æ–‡æœ¬
                            desc = re.sub(r'\s+', ' ', desc)  # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
                            desc = desc.strip('ï¼Œã€‚ï¼›ï¼ï¼Ÿ.,;!?')  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
                            if desc and not desc.lower() in ['æ˜¯', 'is', 'are', 'the', 'a', 'an']:
                                best_description = desc
                                max_length = len(desc)
            except Exception as e:
                logger.debug(f"æ¨¡å¼åŒ¹é…å¤±è´¥: {pattern}, é”™è¯¯: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„æè¿°ï¼Œå°è¯•æå–å®ä½“å‘¨å›´çš„ä¸Šä¸‹æ–‡
        if not best_description and source_text:
            try:
                # æŸ¥æ‰¾å®ä½“åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                entity_pos = source_text.lower().find(entity_name.lower())
                if entity_pos != -1:
                    # æå–å‰åå„50ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                    start = max(0, entity_pos - 50)
                    end = min(len(source_text), entity_pos + len(entity_name) + 50)
                    context = source_text[start:end].strip()
                    
                    # æ¸…ç†ä¸Šä¸‹æ–‡
                    context = re.sub(r'\s+', ' ', context)
                    if len(context) > 20:
                        best_description = f"ä¸Šä¸‹æ–‡: {context}"
            except Exception as e:
                logger.debug(f"ä¸Šä¸‹æ–‡æå–å¤±è´¥: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æè¿°ï¼Œä½¿ç”¨å®ä½“åç§°æœ¬èº«
        if not best_description:
            best_description = f"å®ä½“: {entity_name}"
        
        return best_description
    
    def _build_spo_prompt(self, text: str) -> str:
        """Build SPO extraction prompt using prompt manager and custom schema"""
        
        if self.prompt_manager:
            # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨ï¼Œæ™ºèƒ½é€‰æ‹©æ¨¡æ¿
            try:
                custom_schema_str = json.dumps(self.schema, ensure_ascii=False, indent=2)
                
                # æ™ºèƒ½é€‰æ‹©æ¨¡æ¿
                template_name = self._select_template(text)
                logger.info(f"é€‰æ‹©æ¨¡æ¿: {template_name}")
                
                # å¤„ç†é¢†åŸŸæ¨¡æ¿è·¯å¾„
                if template_name.startswith("domain_templates."):
                    domain_type = template_name.split(".")[-1]
                    prompt = self.prompt_manager.format_prompt(
                        "spo_extraction",
                        template_key=f"domain_templates.{domain_type}.template",
                        custom_schema=custom_schema_str,
                        node_types=', '.join(self.schema.get('Nodes', [])),
                        relation_types=', '.join(self.schema.get('Relations', [])),
                        attribute_types=', '.join(self.schema.get('Attributes', [])),
                        primary_domain=self.primary_domain,
                        key_concepts=self.key_concepts,
                        text=text
                    )
                else:
                    prompt = self.prompt_manager.format_prompt(
                        "spo_extraction",
                        template_key=template_name,
                        custom_schema=custom_schema_str,
                        node_types=', '.join(self.schema.get('Nodes', [])),
                        relation_types=', '.join(self.schema.get('Relations', [])),
                        attribute_types=', '.join(self.schema.get('Attributes', [])),
                        primary_domain=self.primary_domain,
                        key_concepts=self.key_concepts,
                        text=text
                    )
                
                if prompt:
                    logger.debug(f"ä½¿ç”¨{template_name}æ¨¡æ¿ç”ŸæˆSPOæŠ½å–æç¤ºè¯")
                    return prompt
                else:
                    logger.warning("âš ï¸ æç¤ºè¯æ¨¡æ¿åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
                    
            except Exception as e:
                logger.error(f"âŒ æç¤ºè¯æ¨¡æ¿å¤„ç†å¤±è´¥: {e}")
                logger.warning("ğŸ”„ å›é€€åˆ°é»˜è®¤æç¤ºè¯")
        
        # å›é€€åˆ°é»˜è®¤æç¤ºè¯
        schema_str = json.dumps(self.schema, ensure_ascii=False, indent=2)
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å°½å¯èƒ½å¤šçš„å®ä½“ã€å…³ç³»å’Œå±æ€§ã€‚

**é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š**
1. **å®å¯å¤šæŠ½å–ï¼Œä¸è¦é—æ¼**ï¼šå³ä½¿ä¸ç¡®å®šï¼Œä¹Ÿè¦å°è¯•æŠ½å–å¯èƒ½çš„å®ä½“å’Œå…³ç³»
2. **çµæ´»ä½¿ç”¨Schema**ï¼šå¯ä»¥é€‚å½“æ‰©å±•Schemaä¸­çš„ç±»å‹ï¼Œä¸è¦ä¸¥æ ¼é™åˆ¶
3. **å…³æ³¨éšå«å…³ç³»**ï¼šæŠ½å–æ–‡æœ¬ä¸­éšå«çš„å…³ç³»ï¼Œä¸ä»…ä»…æ˜¯æ˜ç¡®è¡¨è¿°çš„
4. **ç»†ç²’åº¦æŠ½å–**ï¼šå°†å¤åˆæ¦‚å¿µæ‹†åˆ†ä¸ºå¤šä¸ªå®ä½“å’Œå…³ç³»
5. **åŒ…å«æ¨æµ‹æ€§å†…å®¹**ï¼šåŸºäºä¸Šä¸‹æ–‡çš„åˆç†æ¨æµ‹ä¹Ÿè¦æŠ½å–

å¯ç”¨Schemaï¼š
```json
{schema_str}
```

é¢†åŸŸä¿¡æ¯ï¼š
- ä¸»è¦é¢†åŸŸï¼š{self.primary_domain}
- æ ¸å¿ƒæ¦‚å¿µï¼š{self.key_concepts}

æ–‡æœ¬å†…å®¹ï¼š
```
{text}
```

**æŠ½å–è¦æ±‚ï¼š**
- æ¯ä¸ªå®ä½“éƒ½è¦æœ‰æè¿°å’Œç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0.1-1.0ï¼‰
- æ¯ä¸ªå…³ç³»éƒ½è¦æœ‰æè¿°å’Œç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0.1-1.0ï¼‰
- ç½®ä¿¡åº¦åŸºäºæ–‡æœ¬ä¸­çš„è¯æ®å¼ºåº¦ï¼š
  * 0.9-1.0: æ˜ç¡®ç›´æ¥çš„è¡¨è¿°
  * 0.7-0.8: è¾ƒå¼ºçš„æš—ç¤ºæˆ–æ¨ç†
  * 0.5-0.6: å¼±æš—ç¤ºæˆ–å¯èƒ½çš„å…³ç³»
  * 0.3-0.4: æ¨æµ‹æ€§çš„å…³ç³»
- ä¼˜å…ˆæŠ½å–é«˜ç½®ä¿¡åº¦çš„å†…å®¹ï¼Œä½†ä¹ŸåŒ…å«ä¸€äº›ä½ç½®ä¿¡åº¦çš„æ¨æµ‹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
```json
{{
    "entity_types": {{
        "å®ä½“åç§°": {{
            "type": "å®ä½“ç±»å‹",
            "description": "è¯¦ç»†æè¿°",
            "confidence": 0.85,
            "attributes": {{"å±æ€§å": "å±æ€§å€¼"}}
        }}
    }},
    "triples": [
        {{
            "subject": "ä¸»ä½“å®ä½“",
            "predicate": "å…³ç³»ç±»å‹", 
            "object": "å®¢ä½“å®ä½“",
            "description": "å…³ç³»æè¿°",
            "confidence": 0.75,
            "evidence": "æ”¯æŒè¯æ®"
        }}
    ]
}}
```

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt.strip()
    
    def _parse_spo_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into SPO data"""
        try:
            # Clean response
            cleaned_response = self._clean_llm_response(response)
            
            # Parse JSON
            spo_data = json.loads(cleaned_response)
            
            # Validate required fields
            required_fields = ['attributes', 'triples', 'entity_types']
            for field in required_fields:
                if field not in spo_data:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å­—æ®µ: {field}")
                    spo_data[field] = {} if field != 'triples' else []
            
            return spo_data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response}")
            return {"attributes": {}, "triples": [], "entity_types": {}}
    
    def _clean_llm_response(self, response: str) -> str:
        """Clean LLM response to extract valid JSON with enhanced error handling"""
        import re
        
        # Remove markdown code blocks
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        elif response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        
        # Find JSON content - look for the first complete JSON object
        start_idx = response.find('{')
        if start_idx == -1:
            return '{"entity_types": {}, "triples": []}'
        
        # Find the matching closing brace
        brace_count = 0
        end_idx = start_idx
        
        for i in range(start_idx, len(response)):
            if response[i] == '{':
                brace_count += 1
            elif response[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i
                    break
        
        if brace_count == 0:
            json_content = response[start_idx:end_idx+1]
        else:
            # Fallback to original method
            end_idx = response.rfind('}')
            if end_idx > start_idx:
                json_content = response[start_idx:end_idx+1]
            else:
                json_content = '{"entity_types": {}, "triples": []}'
        
        # å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
        json_content = self._fix_json_errors(json_content)
        
        return json_content.strip()
    
    def _fix_json_errors(self, json_str: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯"""
        import re
        
        # ä¿®å¤å°¾éšé€—å·
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜
        json_str = re.sub(r'"\s*\n\s*"', '",\n"', json_str)
        json_str = re.sub(r'}\s*\n\s*"', '},\n"', json_str)
        json_str = re.sub(r']\s*\n\s*"', '],\n"', json_str)
        
        # ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·
        json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)
        
        # ç¡®ä¿åŸºæœ¬ç»“æ„å­˜åœ¨
        if '"entity_types"' not in json_str:
            json_str = json_str.replace('{', '{"entity_types": {},', 1)
        if '"triples"' not in json_str:
            json_str = json_str.replace('}', ', "triples": []}', 1)
        
        return json_str
    
    def _aggressive_json_fix(self, json_str: str) -> str:
        """æ›´æ¿€è¿›çš„JSONä¿®å¤æ–¹æ³•ï¼Œå¤„ç†å¤æ‚çš„æ ¼å¼é”™è¯¯"""
        import re
        
        # ç§»é™¤æ‰€æœ‰æ³¨é‡Š
        json_str = re.sub(r'//.*?\n', '\n', json_str)
        json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
        
        # ä¿®å¤å±æ€§åæ²¡æœ‰åŒå¼•å·çš„é—®é¢˜
        json_str = re.sub(r'(\w+)(\s*:)', r'"\1"\2', json_str)
        
        # ä¿®å¤å­—ç¬¦ä¸²å€¼æ²¡æœ‰åŒå¼•å·çš„é—®é¢˜ï¼ˆä½†è¦é¿å…æ•°å­—å’Œå¸ƒå°”å€¼ï¼‰
        json_str = re.sub(r':\s*([^"\d\[\{][^,\}\]]*?)([,\}\]])', r': "\1"\2', json_str)
        
        # ä¿®å¤å¤šä½™çš„é€—å·
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜
        json_str = re.sub(r'([}\]"])\s*\n\s*(["\[{])', r'\1,\n\2', json_str)
        
        # ç¡®ä¿åŸºæœ¬ç»“æ„
        if not json_str.strip().startswith('{'):
            json_str = '{' + json_str
        if not json_str.strip().endswith('}'):
            json_str = json_str + '}'
        
        # å¦‚æœè¿˜æ˜¯æœ‰é—®é¢˜ï¼Œè¿”å›æœ€å°æœ‰æ•ˆJSON
        try:
            json.loads(json_str)
            return json_str
        except:
            logger.warning("è¿”å›æœ€å°æœ‰æ•ˆJSONç»“æ„")
            return '{"entity_types": {}, "triples": []}'
    
    def _convert_spo_to_objects(self, spo_data: Dict[str, Any], source_text: str, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """Convert SPO data to Entity and Relationship objects"""
        entities = []
        relationships = []
        entity_id_map = {}  # name -> id mapping
        
        # Create entities
        entity_types = spo_data.get('entity_types', {})
        attributes = spo_data.get('attributes', {})
        
        for entity_name, entity_data in entity_types.items():
            # Generate unique ID
            entity_id = f"entity_{len(entities) + 1}"
            entity_id_map[entity_name] = entity_id
            
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(entity_data, dict):
                # æ–°çš„æ‰¹å¤„ç†æ ¼å¼
                entity_type = entity_data.get('type', 'concept')
                entity_description = entity_data.get('description', '')
                entity_attrs = entity_data.get('attributes', {})
                
                # è½¬æ¢å±æ€§æ ¼å¼
                attr_dict = entity_attrs if isinstance(entity_attrs, dict) else {}
                description_parts = [entity_description] if entity_description else []
            else:
                # æ—§çš„ç®€å•æ ¼å¼
                entity_type = entity_data
                entity_attrs = attributes.get(entity_name, [])
                attr_dict = {}
                description_parts = []
                
                for attr in entity_attrs:
                    if ':' in str(attr):
                        key, value = str(attr).split(':', 1)
                        attr_dict[key.strip()] = value.strip()
                        description_parts.append(str(attr))
                    else:
                        description_parts.append(str(attr))
            
            # Create entity
            try:
                # ç¡®ä¿entity_typeæ˜¯å­—ç¬¦ä¸²
                if isinstance(entity_type, dict):
                    entity_type = entity_type.get('type', 'concept')
                entity_type_str = str(entity_type).lower()
                entity_type_enum = EntityType(entity_type_str)
            except (ValueError, AttributeError):
                entity_type_enum = EntityType.CONCEPT  # Default fallback
            
            # è·å–ç½®ä¿¡åº¦ï¼ˆä¼˜å…ˆä½¿ç”¨LLMæä¾›çš„ï¼Œå¦åˆ™åŠ¨æ€è®¡ç®—ï¼‰
            if isinstance(entity_data, dict) and 'confidence' in entity_data:
                entity_confidence = float(entity_data['confidence'])
            else:
                entity_confidence = self._calculate_dynamic_confidence(
                    entity_name, 
                    '; '.join(description_parts), 
                    source_text
                )
            
            entity = Entity(
                id=entity_id,
                name=entity_name,
                entity_type=entity_type_enum,
                description='; '.join(description_parts),
                confidence=entity_confidence,  # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨åŠ¨æ€ç½®ä¿¡åº¦
                attributes=attr_dict,
                source_chunks={kwargs.get('chunk_id', 'unknown')}
            )
            
            entities.append(entity)
            logger.debug(f"ğŸ“ åˆ›å»ºå®ä½“: {entity_name} ({entity_type}) -> {entity_id}")
        
        # Create relationships
        triples = spo_data.get('triples', [])
        
        for triple in triples:
            # å¤„ç†ä¸åŒçš„ä¸‰å…ƒç»„æ ¼å¼
            if isinstance(triple, dict):
                # æ–°çš„æ‰¹å¤„ç†æ ¼å¼
                source_name = triple.get('subject', '')
                relation = triple.get('predicate', '')
                target_name = triple.get('object', '')
            elif isinstance(triple, (list, tuple)) and len(triple) == 3:
                # æ—§çš„ç®€å•æ ¼å¼
                source_name, relation, target_name = triple
            else:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆä¸‰å…ƒç»„æ ¼å¼: {triple}")
                continue
            
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ˜¯å­—ç¬¦ä¸²
            source_name = str(source_name).strip()
            target_name = str(target_name).strip()
            
            if not source_name or not target_name:
                logger.warning(f"âš ï¸ è·³è¿‡ç©ºå®ä½“åç§°çš„ä¸‰å…ƒç»„: {triple}")
                continue
            
            # Get entity IDs with fuzzy matching
            source_id = self._find_entity_id(source_name, entity_id_map)
            target_id = self._find_entity_id(target_name, entity_id_map)
            
            if not source_id:
                # åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„æºå®ä½“
                source_id = self._create_missing_entity(source_name, entities, entity_id_map, source_text)
                if not source_id:
                    logger.warning(f"âš ï¸ æºå®ä½“æœªæ‰¾åˆ°ä¸”æ— æ³•åˆ›å»º: {source_name}")
                    continue
                else:
                    logger.info(f"åŠ¨æ€åˆ›å»ºæºå®ä½“: {source_name}")
                    
            if not target_id:
                # åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„ç›®æ ‡å®ä½“
                target_id = self._create_missing_entity(target_name, entities, entity_id_map, source_text)
                if not target_id:
                    logger.warning(f"âš ï¸ ç›®æ ‡å®ä½“æœªæ‰¾åˆ°ä¸”æ— æ³•åˆ›å»º: {target_name}")
                    continue
                else:
                    logger.info(f"åŠ¨æ€åˆ›å»ºç›®æ ‡å®ä½“: {target_name}")
            
            # Create relationship
            try:
                # ç¡®ä¿relationæ˜¯å­—ç¬¦ä¸²
                if isinstance(relation, dict):
                    relation = relation.get('type', 'related_to')
                relation_str = str(relation).lower().replace(' ', '_')
                relation_type_enum = RelationType(relation_str)
            except (ValueError, AttributeError):
                relation_type_enum = RelationType.RELATED_TO  # Default fallback
            
            # è·å–ç½®ä¿¡åº¦ï¼ˆä¼˜å…ˆä½¿ç”¨LLMæä¾›çš„ï¼Œå¦åˆ™åŠ¨æ€è®¡ç®—ï¼‰
            if isinstance(triple, dict) and 'confidence' in triple:
                relationship_confidence = float(triple['confidence'])
            else:
                relationship_confidence = self._calculate_relationship_confidence(
                    source_name, 
                    relation, 
                    target_name, 
                    source_text
                )
            
            relationship = Relationship(
                source_entity_id=source_id,
                target_entity_id=target_id,
                relation_type=relation_type_enum,
                description=f"{source_name} {relation} {target_name}",
                confidence=relationship_confidence,  # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨åŠ¨æ€ç½®ä¿¡åº¦
                source_chunks={kwargs.get('chunk_id', 'unknown')}
            )
            
            relationships.append(relationship)
            logger.debug(f"ğŸ”— åˆ›å»ºå…³ç³»: {source_name} --[{relation}]--> {target_name}")
        
        return entities, relationships