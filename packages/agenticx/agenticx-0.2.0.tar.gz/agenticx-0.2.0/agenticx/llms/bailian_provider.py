from typing import Any, Optional, Dict, List, AsyncGenerator, Generator, Union
import openai
import json
import requests
import aiohttp
from pydantic import Field
from loguru import logger
from .base import BaseLLMProvider
from .response import LLMResponse, TokenUsage, LLMChoice

class BailianProvider(BaseLLMProvider):
    """
    Bailian (Dashscope) LLM provider that uses OpenAI-compatible API.
    Supports the latest Bailian models through Aliyun's API.
    """
    
    api_key: str = Field(description="Bailian API key")
    base_url: str = Field(default="https://dashscope.aliyuncs.com/compatible-mode/v1", description="Bailian API base URL")
    timeout: Optional[float] = Field(default=60.0, description="Request timeout in seconds")
    max_retries: Optional[int] = Field(default=3, description="Maximum number of retries")
    temperature: Optional[float] = Field(default=0.6, description="Sampling temperature")
    client: Optional[Any] = Field(default=None, exclude=True, description="OpenAI client instance")
    
    def __init__(self, **data):
        super().__init__(**data)
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
            max_retries=self.max_retries or 3
        )
    
    def _needs_native_request(self, model_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨åŸç”ŸHTTPè¯·æ±‚ï¼ˆå› ä¸ºæœ‰ç™¾ç‚¼ç‰¹æœ‰å‚æ•°ï¼‰"""
        model_lower = model_name.lower()
        return any(model in model_lower for model in ["qwen3-32b", "qwen3-8b", "qwen3-235", "qwen-plus", "qwen-turbo"])
    
    def _make_native_request(self, request_params: Dict[str, Any]) -> Any:
        """ä½¿ç”¨åŸç”ŸHTTPè¯·æ±‚è°ƒç”¨ç™¾ç‚¼APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        import time
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # æ·»åŠ ç™¾ç‚¼ç‰¹æœ‰å‚æ•°
        if self._needs_native_request(request_params.get("model", "")):
            request_params["enable_thinking"] = False
            logger.debug(f"ä¸ºæ¨¡å‹ {request_params.get('model')} è®¾ç½® enable_thinking=false")
        
        url = f"{self.base_url}/chat/completions"
        
        # ç¦ç”¨ä»£ç†ä»¥é¿å…è¿æ¥é—®é¢˜
        proxies = {
            'http': None,
            'https': None
        }
        
        # é‡è¯•é€»è¾‘
        max_retries = self.max_retries or 3
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                response = requests.post(
                    url,
                    headers=headers,
                    json=request_params,
                    timeout=self.timeout,
                    proxies=proxies,
                    verify=True  # ä¿æŒSSLéªŒè¯
                )
                
                # æ£€æŸ¥çŠ¶æ€ç 
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 500:
                    # 500é”™è¯¯å¯èƒ½æ˜¯ä¸´æ—¶æ€§çš„ï¼Œè¿›è¡Œé‡è¯•
                    if attempt < max_retries:
                        wait_time = (2 ** attempt) * 1.0  # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s
                        error_text = response.text[:200] if response.text else "No error details"
                        logger.warning(f"ç™¾ç‚¼APIè¿”å›500é”™è¯¯ï¼Œ{wait_time:.1f}ç§’åé‡è¯• ({attempt + 1}/{max_retries}): {error_text}")
                        time.sleep(wait_time)
                        continue
                    else:
                        error_text = response.text[:500] if response.text else "No error details"
                        raise Exception(f"ç™¾ç‚¼APIè¿”å›500é”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {error_text}")
                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    response.raise_for_status()
                    return response.json()
                    
            except requests.exceptions.Timeout as e:
                last_error = e
                if attempt < max_retries:
                    wait_time = (2 ** attempt) * 1.0
                    logger.warning(f"ç™¾ç‚¼APIè¯·æ±‚è¶…æ—¶ï¼Œ{wait_time:.1f}ç§’åé‡è¯• ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    raise Exception(f"Native Bailian API call timeout after {max_retries} retries: {str(e)}")
            except requests.exceptions.RequestException as e:
                last_error = e
                if attempt < max_retries:
                    wait_time = (2 ** attempt) * 1.0
                    logger.warning(f"ç™¾ç‚¼APIè¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯• ({attempt + 1}/{max_retries}): {str(e)}")
                    time.sleep(wait_time)
                    continue
                else:
                    raise Exception(f"Native Bailian API call failed after {max_retries} retries: {str(e)}")
            except Exception as e:
                last_error = e
                if attempt < max_retries:
                    wait_time = (2 ** attempt) * 1.0
                    logger.warning(f"ç™¾ç‚¼APIè°ƒç”¨å¼‚å¸¸ï¼Œ{wait_time:.1f}ç§’åé‡è¯• ({attempt + 1}/{max_retries}): {str(e)}")
                    time.sleep(wait_time)
                    continue
                else:
                    raise Exception(f"Native Bailian API call failed: {str(e)}")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise Exception(f"Native Bailian API call failed after {max_retries} retries. Last error: {str(last_error)}")
    
    async def _make_native_request_async(self, request_params: Dict[str, Any]) -> Any:
        """å¼‚æ­¥ç‰ˆæœ¬çš„åŸç”ŸHTTPè¯·æ±‚"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # ä¸ºéœ€è¦ç‰¹æ®Šå‚æ•°çš„æ¨¡å‹æ·»åŠ enable_thinking=false
        request_params["enable_thinking"] = False
        logger.debug(f"ä¸ºæ¨¡å‹ {request_params.get('model')} è®¾ç½® enable_thinking=false (å¼‚æ­¥åŸç”Ÿè¯·æ±‚)")
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=request_params,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"HTTP {response.status}: {error_text}")
                
                response_data = await response.json()
                return response_data

    def _convert_native_response(self, response_data: Dict[str, Any]) -> Any:
         """å°†åŸç”Ÿå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å¯¹è±¡"""
         # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¯¹è±¡æ¥æ¨¡æ‹ŸOpenAIå“åº”æ ¼å¼
         class MockResponse:
             def __init__(self, data):
                 self.id = data.get('id', '')
                 self.model = data.get('model', '')
                 self.created = data.get('created', 0)
                 self.choices = []
                 self.usage = None
                 
                 # å¤„ç†choices
                 for choice_data in data.get('choices', []):
                     choice = type('Choice', (), {})()
                     choice.index = choice_data.get('index', 0)
                     choice.finish_reason = choice_data.get('finish_reason', '')
                     
                     # å¤„ç†message
                     message_data = choice_data.get('message', {})
                     message = type('Message', (), {})()
                     message.content = message_data.get('content', '')
                     message.role = message_data.get('role', 'assistant')
                     choice.message = message
                     
                     self.choices.append(choice)
                 
                 # å¤„ç†usage
                 usage_data = data.get('usage', {})
                 if usage_data:
                     usage = type('Usage', (), {})()
                     usage.prompt_tokens = usage_data.get('prompt_tokens', 0)
                     usage.completion_tokens = usage_data.get('completion_tokens', 0)
                     usage.total_tokens = usage_data.get('total_tokens', 0)
                     self.usage = usage
         
         return MockResponse(response_data)
     
    def _prepare_bailian_params(self, request_params: Dict[str, Any]) -> Dict[str, Any]:
         """å¤„ç†ç™¾ç‚¼ç‰¹å®šçš„å‚æ•°ï¼Œç¡®ä¿ä¸OpenAIå®¢æˆ·ç«¯å…¼å®¹"""
         # åˆ›å»ºå‚æ•°å‰¯æœ¬
         params = request_params.copy()
         
         # ä¸ºéœ€è¦ç‰¹æ®Šå‚æ•°çš„æ¨¡å‹æ·»åŠ enable_thinking=false
         if self._needs_native_request(params.get("model", "")):
             params["enable_thinking"] = False
             logger.debug(f"ä¸ºæ¨¡å‹ {params.get('model')} è®¾ç½® enable_thinking=false (OpenAIå®¢æˆ·ç«¯)")
         
         return params
    
    def invoke(
        self, prompt: Union[str, List[Dict]], tools: Optional[List[Dict]] = None, **kwargs
    ) -> LLMResponse:
        """Invoke the Bailian model synchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                **kwargs
            }
            
            if tools:
                request_params["tools"] = tools
            
            # è®°å½•è¯·æ±‚è¯¦æƒ…
            logger.info(f"å‘é€è¯·æ±‚åˆ°ç™¾ç‚¼API: æ¨¡å‹={self.model}, æ¸©åº¦={request_params.get('temperature', self.temperature)}, æ¶ˆæ¯æ•°={len(messages)}")
            
            # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆæˆªæ–­é•¿æ¶ˆæ¯ï¼‰
            for i, msg in enumerate(messages):
                content = msg.get('content', '')
                if isinstance(content, str):
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    logger.debug(f"ğŸ“¨ æ¶ˆæ¯[{i}] ({msg.get('role', 'unknown')}): {content_preview}")
                else:
                    logger.debug(f"ğŸ“¨ æ¶ˆæ¯[{i}] ({msg.get('role', 'unknown')}): [å¤æ‚å†…å®¹]")
            
            if tools:
                logger.debug(f"å·¥å…·æ•°é‡: {len(tools)}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨åŸç”ŸHTTPè¯·æ±‚
            if self._needs_native_request(self.model):
                logger.debug("ä½¿ç”¨åŸç”ŸHTTPè¯·æ±‚è°ƒç”¨ç™¾ç‚¼API")
                response_data = self._make_native_request(request_params)
                # å°†åŸç”Ÿå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å¯¹è±¡
                response = self._convert_native_response(response_data)
            else:
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
                final_params = self._prepare_bailian_params(request_params)
                logger.trace(f"å®Œæ•´è¯·æ±‚å‚æ•°: {json.dumps(final_params, ensure_ascii=False, indent=2)}")
                
                if self.client is None:
                    raise ValueError("Client not initialized")
                
                logger.debug(f"æ­£åœ¨è°ƒç”¨ç™¾ç‚¼API: å‚æ•°={list(final_params.keys())}")
                response = self.client.chat.completions.create(**final_params)
            
            # è®°å½•å“åº”è¯¦æƒ…
            logger.info("âœ… ç™¾ç‚¼APIå“åº”æˆåŠŸ")
            if hasattr(response, 'usage') and response.usage:
                logger.debug(f"Tokenä½¿ç”¨: è¾“å…¥={response.usage.prompt_tokens}, è¾“å‡º={response.usage.completion_tokens}, æ€»è®¡={response.usage.total_tokens}")
            
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    content = choice.message.content or ""
                    content_preview = content[:300] + "..." if len(content) > 300 else content
                    logger.debug(f"å“åº”å†…å®¹: {len(content)}å­—ç¬¦ - {content_preview}")
            
            # è®°å½•å®Œæ•´å“åº”ï¼ˆtraceçº§åˆ«ï¼‰
            logger.trace(f"å®Œæ•´APIå“åº”: {response}")
            
            parsed_response = self._parse_response(response)
            logger.debug(f"âœ¨ å“åº”è§£æå®Œæˆ")
            return parsed_response
        except Exception as e:
            logger.error(f"âŒ ç™¾ç‚¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Bailian API call failed: {str(e)}")
    
    async def ainvoke(
        self, prompt: Union[str, List[Dict]], tools: Optional[List[Dict]] = None, **kwargs
    ) -> LLMResponse:
        """Invoke the Bailian model asynchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                **kwargs
            }
            
            if tools:
                request_params["tools"] = tools
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨åŸç”ŸHTTPè¯·æ±‚
            if self._needs_native_request(self.model):
                response_data = await self._make_native_request_async(request_params)
                response = self._convert_native_response(response_data)
                return self._parse_response(response)
            else:
                async_client = openai.AsyncOpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url,
                    timeout=self.timeout,
                    max_retries=self.max_retries or 3
                )
                
                # å¤„ç†ç™¾ç‚¼ç‰¹å®šå‚æ•°
                final_params = self._prepare_bailian_params(request_params)
                
                response = await async_client.chat.completions.create(**final_params)
                return self._parse_response(response)
        except Exception as e:
            raise Exception(f"Bailian API async call failed: {str(e)}")
    
    def stream(self, prompt: Union[str, List[Dict]], **kwargs) -> Generator[str, None, None]:
        """Stream the Bailian model's response synchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                "stream": True,
                **kwargs
            }
            
            if self.client is None:
                raise ValueError("Client not initialized")
                
            response_stream = self.client.chat.completions.create(**request_params)
            
            for chunk in response_stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise Exception(f"Bailian API stream call failed: {str(e)}")
    
    async def astream(self, prompt: Union[str, List[Dict]], **kwargs):  # type: ignore
        """Stream the Bailian model's response asynchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            async_client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=self.timeout,
                max_retries=self.max_retries or 3
            )
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                "stream": True,
                **kwargs
            }
            
            response_stream = await async_client.chat.completions.create(**request_params)
            
            async for chunk in response_stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise Exception(f"Bailian API async stream call failed: {str(e)}")
    
    def _parse_response(self, response) -> LLMResponse:
        """Parse OpenAI response into AgenticX LLMResponse format."""
        usage = response.usage
        token_usage = TokenUsage(
            prompt_tokens=usage.prompt_tokens if usage else 0,
            completion_tokens=usage.completion_tokens if usage else 0,
            total_tokens=usage.total_tokens if usage else 0
        )
        
        choices = [
            LLMChoice(
                index=choice.index,
                content=choice.message.content or "",
                finish_reason=choice.finish_reason
            ) for choice in response.choices
        ]
        
        main_content = choices[0].content if choices else ""
        
        return LLMResponse(
            id=response.id,
            model_name=response.model,
            created=response.created,
            content=main_content,
            choices=choices,
            token_usage=token_usage,
            cost=None,
            metadata={
                "provider": "bailian",
                "api_version": "v1"
            }
        )
    
    def generate(self, prompt: str, **kwargs) -> str:
        """Generate text response from a simple prompt string.
        
        Args:
            prompt: The input prompt string
            **kwargs: Additional generation parameters
            
        Returns:
            Generated text content as string
        """
        response = self.invoke(prompt, **kwargs)
        return response.content

    def call(self, prompt: Union[str, List[Dict]], **kwargs) -> str:
        """Call method for compatibility with extractors.
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Returns:
            Generated text content as string
        """
        logger.debug("ğŸ”„ è°ƒç”¨callæ–¹æ³•ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰")
        response = self.invoke(prompt, **kwargs)
        logger.debug(f"ğŸ“¤ è¿”å›æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦: {len(response.content)} å­—ç¬¦")
        return response.content

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> "BailianProvider":
        """Create BailianProvider from configuration dictionary."""
        return cls(
            model=config.get("model", "qwen-plus"),
            api_key=config.get("api_key"),
            base_url=config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
            timeout=config.get("timeout", 60.0),
            max_retries=config.get("max_retries", 3),
            temperature=config.get("temperature", 0.6)
        )

    def create_multimodal_message(self, text: str, image_url: Optional[str] = None, 
                                image_base64: Optional[str] = None) -> Dict:
        """åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            image_url: å›¾ç‰‡URLï¼ˆå¯é€‰ï¼‰
            image_base64: Base64ç¼–ç çš„å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„å¤šæ¨¡æ€æ¶ˆæ¯
        """
        content: List[Dict[str, Any]] = [{"type": "text", "text": text}]
        
        if image_url:
            content.append({
                "type": "image_url",
                "image_url": {"url": image_url}
            })
        elif image_base64:
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
            })
            
        return {"role": "user", "content": content}