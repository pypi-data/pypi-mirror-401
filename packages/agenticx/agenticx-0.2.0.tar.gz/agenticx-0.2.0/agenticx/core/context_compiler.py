"""
Context Compiler (ä¸Šä¸‹æ–‡ç¼–è¯‘å™¨) - å¢å¼ºç‰ˆ

å‚è€ƒè‡ª Google ADK çš„ "Compiled View" æœºåˆ¶ã€‚
æ ¸å¿ƒæ€æƒ³ï¼šä¸Šä¸‹æ–‡ä¸æ˜¯äº‹ä»¶çš„ç®€å•æ‹¼æ¥ï¼Œè€Œæ˜¯å¯¹ EventLog çš„æŒ‰éœ€"ç¼–è¯‘"ã€‚

å¢å¼ºåŠŸèƒ½ï¼ˆv2ï¼‰ï¼š
- ç²¾ç¡® Token è®¡æ•°ï¼šä½¿ç”¨ tiktoken è¿›è¡Œç²¾ç¡®ç»Ÿè®¡
- å¤šç­–ç•¥å‹ç¼©ï¼šæ»‘åŠ¨çª—å£ã€ä¸»é¢˜åˆ†å—ã€ç´§æ€¥å‹ç¼©ç­‰
- æŒ–æ˜ä»»åŠ¡ä¸“ç”¨ Promptï¼šä¿ç•™å¤±è´¥è·¯å¾„å’Œæ¢ç´¢çº¿ç´¢
- å¯è§‚æµ‹æ€§ï¼šåŸå§‹è§†å›¾ vs ç¼–è¯‘è§†å›¾å¯¹ç…§
"""

from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any, Callable, Literal
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
import logging
import math
import json

from .event import (
    EventLog, AnyEvent, CompactedEvent, CompactionConfig,
    ToolCallEvent, ToolResultEvent, ErrorEvent, LLMCallEvent, LLMResponseEvent,
    TaskStartEvent, TaskEndEvent, FinishTaskEvent, HumanRequestEvent, HumanResponseEvent
)
from .token_counter import TokenCounter, TokenStats, count_tokens

logger = logging.getLogger(__name__)


# =============================================================================
# å‹ç¼©ç­–ç•¥æšä¸¾
# =============================================================================

class CompactionStrategy(str, Enum):
    """å‹ç¼©ç­–ç•¥"""
    SLIDING_WINDOW = "sliding_window"    # æ»‘åŠ¨çª—å£ï¼ˆé»˜è®¤ï¼‰
    TOPIC_BASED = "topic_based"          # æŒ‰ä¸»é¢˜åˆ†å—
    TIME_BASED = "time_based"            # æŒ‰æ—¶é—´çª—å£
    EMERGENCY = "emergency"              # ç´§æ€¥å‹ç¼©ï¼ˆè¶…è¿‡ token é˜ˆå€¼æ—¶ï¼‰
    HYBRID = "hybrid"                    # æ··åˆç­–ç•¥


# =============================================================================
# ä¸“ç”¨ Prompt æ¨¡æ¿åº“
# =============================================================================

# é»˜è®¤é€šç”¨ Prompt
DEFAULT_COMPACTION_PROMPT = """You are a context summarizer for an AI agent system. Your job is to compress a sequence of events into a concise summary while preserving critical information.

## Events to Summarize
{events}

## Requirements
1. Preserve key information:
   - What tools were called and their results (especially errors and failures)
   - Important decisions made by the agent
   - Any user inputs or human feedback
   - Critical state changes

2. Be concise but complete. Aim for 30-50% compression ratio.

3. Use structured format:
   - Start with a one-sentence overall summary
   - List key actions and their outcomes
   - Note any important lessons learned

## Output
Provide your summary directly, without any preamble."""


# æ™ºèƒ½ä½“è‡ªåŠ¨æŒ–æ˜ä¸“ç”¨ Promptï¼ˆä¼˜åŒ–ç‰ˆï¼‰
MINING_TASK_PROMPT = """You are summarizing the execution history of an AI agent performing an **automatic exploration/mining task**.

## Events to Summarize
{events}

## Critical Requirements for Mining Tasks

### 1. MUST Preserve (Critical for avoiding repeated failures):
- **Failed Paths**: List every approach that was tried and failed, with reasons
- **Dead Ends**: Resources/APIs/methods that were found to be unusable
- **Error Patterns**: Common error types encountered and their triggers
- **Blocked Directions**: Paths that were explicitly blocked or rate-limited

### 2. MUST Preserve (Critical for continued exploration):
- **Discovered Patterns**: Any patterns or insights found during exploration
- **Successful Techniques**: Methods that worked, even partially
- **Unexplored Leads**: Promising directions not yet fully investigated
- **Resource States**: Current status of external resources (APIs, files, etc.)

### 3. Progress Tracking:
- Overall exploration coverage estimate (e.g., "~40% of potential paths explored")
- Priority ranking for remaining directions
- Key milestones achieved

## Output Format
```
ğŸ“Š EXPLORATION SUMMARY
[One-sentence progress summary]

âŒ TRIED & FAILED:
- [path/method]: [why it failed]
- ...

âœ… WORKING APPROACHES:
- [method]: [what worked and results]
- ...

ğŸ” UNEXPLORED LEADS:
- [direction]: [priority: high/medium/low]
- ...

âš ï¸ IMPORTANT NOTES:
- [any critical warnings or constraints]
```

Provide your summary now:"""


# å¯¹è¯å†å²å‹ç¼© Prompt
CONVERSATION_PROMPT = """Summarize this conversation history between a user and an AI agent.

## Conversation Events
{events}

## Requirements
1. Preserve the key topics discussed
2. Keep important user requests and agent responses
3. Maintain the logical flow of the conversation
4. Note any unresolved questions or pending tasks

## Output
A concise summary that would allow the conversation to continue seamlessly."""


# å·¥å…·æ‰§è¡Œåºåˆ—å‹ç¼© Prompt
TOOL_SEQUENCE_PROMPT = """Summarize this sequence of tool executions.

## Tool Events
{events}

## Requirements
1. Group related tool calls together
2. Highlight successes and failures
3. Note any patterns or dependencies between tools
4. Preserve error details for failed calls

## Output
A structured summary of what tools were used and their outcomes."""


# Prompt æ¨¡æ¿æ˜ å°„
PROMPT_TEMPLATES = {
    "default": DEFAULT_COMPACTION_PROMPT,
    "mining": MINING_TASK_PROMPT,
    "conversation": CONVERSATION_PROMPT,
    "tool_sequence": TOOL_SEQUENCE_PROMPT,
}


# =============================================================================
# äº‹ä»¶æ‘˜è¦å™¨
# =============================================================================

class EventSummarizer(ABC):
    """
    äº‹ä»¶æ‘˜è¦ç”Ÿæˆå™¨çš„æŠ½è±¡åŸºç±»ã€‚
    """
    
    @abstractmethod
    async def summarize(
        self, 
        events: List[AnyEvent], 
        prompt_template: Optional[str] = None,
        task_type: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆäº‹ä»¶åˆ—è¡¨çš„æ‘˜è¦ã€‚
        
        Args:
            events: éœ€è¦æ‘˜è¦çš„äº‹ä»¶åˆ—è¡¨ã€‚
            prompt_template: è‡ªå®šä¹‰çš„æç¤ºè¯æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰ã€‚
            task_type: ä»»åŠ¡ç±»å‹ï¼Œç”¨äºé€‰æ‹©ä¸“ç”¨ Promptï¼ˆå¦‚ 'mining'ï¼‰ã€‚
            
        Returns:
            æ‘˜è¦æ–‡æœ¬ã€‚
        """
        pass


class LLMEventSummarizer(EventSummarizer):
    """
    åŸºäº LLM çš„äº‹ä»¶æ‘˜è¦ç”Ÿæˆå™¨ã€‚
    
    ä½¿ç”¨ LLM æ¥ç†è§£äº‹ä»¶è¯­ä¹‰å¹¶ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦ã€‚
    """
    
    def __init__(
        self, 
        llm_provider: Any,
        model: Optional[str] = None,
        default_task_type: str = "default",
        token_counter: Optional[TokenCounter] = None
    ):
        """
        Args:
            llm_provider: LLM æä¾›è€…å®ä¾‹ã€‚
            model: æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰ã€‚
            default_task_type: é»˜è®¤ä»»åŠ¡ç±»å‹ï¼ˆç”¨äºé€‰æ‹© Promptï¼‰ã€‚
            token_counter: Token è®¡æ•°å™¨ï¼ˆå¯é€‰ï¼‰ã€‚
        """
        self.llm_provider = llm_provider
        self.model = model
        self.default_task_type = default_task_type
        self.token_counter = token_counter or TokenCounter(model=model)
        self.stats = TokenStats(model=model)
    
    async def summarize(
        self, 
        events: List[AnyEvent], 
        prompt_template: Optional[str] = None,
        task_type: Optional[str] = None
    ) -> str:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦ã€‚"""
        if not events:
            return "No events to summarize."
        
        # å°†äº‹ä»¶æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        events_text = self._format_events_for_llm(events)
        
        # é€‰æ‹© Prompt æ¨¡æ¿
        if prompt_template:
            template = prompt_template
        else:
            task = task_type or self.default_task_type
            template = PROMPT_TEMPLATES.get(task, DEFAULT_COMPACTION_PROMPT)
        
        prompt = template.format(events=events_text)
        
        # è®°å½•è¾“å…¥ token
        input_tokens = self.token_counter.count_tokens(prompt)
        logger.debug(f"Summarization prompt: {input_tokens} tokens")
        
        # è°ƒç”¨ LLM
        try:
            response = self.llm_provider.invoke([{"role": "user", "content": prompt}])
            summary = response.content
            
            # è®°å½•ç»Ÿè®¡
            self.stats.record(prompt, summary, {"event_count": len(events)})
            
            return summary
        except Exception as e:
            logger.error(f"LLM summarization failed: {e}")
            return self._fallback_summary(events)
    
    def _format_events_for_llm(self, events: List[AnyEvent]) -> str:
        """å°†äº‹ä»¶åˆ—è¡¨æ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„æ–‡æœ¬ã€‚"""
        lines = []
        for i, event in enumerate(events, 1):
            line = self._format_single_event(event, i)
            if line:
                lines.append(line)
        return "\n".join(lines)
    
    def _format_single_event(self, event: AnyEvent, index: int) -> str:
        """æ ¼å¼åŒ–å•ä¸ªäº‹ä»¶ã€‚"""
        if isinstance(event, ToolCallEvent):
            args_str = str(event.tool_args)[:100] + "..." if len(str(event.tool_args)) > 100 else str(event.tool_args)
            return f"{index}. [TOOL_CALL] {event.tool_name}({args_str}) - Intent: {event.intent}"
        elif isinstance(event, ToolResultEvent):
            status = "âœ“" if event.success else "âœ—"
            result = event.result if event.success else event.error
            result_str = str(result)[:200] + "..." if result and len(str(result)) > 200 else str(result)
            return f"{index}. [TOOL_RESULT] {event.tool_name} {status}: {result_str}"
        elif isinstance(event, ErrorEvent):
            return f"{index}. [ERROR] {event.error_type}: {event.error_message}"
        elif isinstance(event, LLMCallEvent):
            return f"{index}. [LLM_CALL] Model: {event.model}"
        elif isinstance(event, LLMResponseEvent):
            tokens = event.token_usage.get('total_tokens', 'N/A') if event.token_usage else 'N/A'
            response_preview = event.response[:200] + "..." if len(event.response) > 200 else event.response
            return f"{index}. [LLM_RESPONSE] Tokens: {tokens}, Preview: {response_preview}"
        elif isinstance(event, TaskStartEvent):
            return f"{index}. [TASK_START] {event.task_description}"
        elif isinstance(event, TaskEndEvent):
            return f"{index}. [TASK_END] Success: {event.success}"
        elif isinstance(event, FinishTaskEvent):
            result_str = str(event.final_result)[:200] + "..." if len(str(event.final_result)) > 200 else str(event.final_result)
            return f"{index}. [FINISH] Result: {result_str}"
        elif isinstance(event, HumanRequestEvent):
            return f"{index}. [HUMAN_REQUEST] {event.question}"
        elif isinstance(event, HumanResponseEvent):
            return f"{index}. [HUMAN_RESPONSE] {event.response}"
        else:
            return f"{index}. [{event.type.upper()}] {event.data}"
    
    def _fallback_summary(self, events: List[AnyEvent]) -> str:
        """é™çº§æ‘˜è¦ï¼šå½“ LLM è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ã€‚"""
        tool_calls = [e for e in events if isinstance(e, ToolCallEvent)]
        tool_results = [e for e in events if isinstance(e, ToolResultEvent)]
        errors = [e for e in events if isinstance(e, ErrorEvent)]
        
        parts = [f"ğŸ“Š Summary of {len(events)} events"]
        
        if tool_calls:
            tools_used = set(e.tool_name for e in tool_calls)
            parts.append(f"Tools: {', '.join(sorted(tools_used))}")
        
        if tool_results:
            successes = sum(1 for e in tool_results if e.success)
            failures = len(tool_results) - successes
            parts.append(f"Results: {successes} success, {failures} failures")
        
        if errors:
            parts.append(f"Errors: {len(errors)}")
            # åŒ…å«æœ€åä¸€ä¸ªé”™è¯¯
            last_error = errors[-1]
            parts.append(f"Last error: {last_error.error_message[:100]}")
        
        return ". ".join(parts) + "."
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        return self.stats.get_summary()


class SimpleEventSummarizer(EventSummarizer):
    """
    ç®€å•çš„äº‹ä»¶æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆä¸ä½¿ç”¨ LLMï¼‰ã€‚
    
    é€‚ç”¨äºæµ‹è¯•æˆ–ä½æˆæœ¬åœºæ™¯ã€‚
    """
    
    async def summarize(
        self, 
        events: List[AnyEvent], 
        prompt_template: Optional[str] = None,
        task_type: Optional[str] = None
    ) -> str:
        """ç”ŸæˆåŸºäºè§„åˆ™çš„ç®€å•æ‘˜è¦ã€‚"""
        if not events:
            return "No events."
        
        # ç»Ÿè®¡å„ç±»äº‹ä»¶
        stats = {
            "tool_calls": 0,
            "tool_successes": 0,
            "tool_failures": 0,
            "errors": 0,
            "llm_calls": 0
        }
        
        tool_names = set()
        error_messages = []
        failed_tools = []
        
        for event in events:
            if isinstance(event, ToolCallEvent):
                stats["tool_calls"] += 1
                tool_names.add(event.tool_name)
            elif isinstance(event, ToolResultEvent):
                if event.success:
                    stats["tool_successes"] += 1
                else:
                    stats["tool_failures"] += 1
                    failed_tools.append(f"{event.tool_name}: {event.error}")
            elif isinstance(event, ErrorEvent):
                stats["errors"] += 1
                error_messages.append(event.error_message)
            elif isinstance(event, LLMCallEvent):
                stats["llm_calls"] += 1
        
        # æ„å»ºæ‘˜è¦
        parts = []
        parts.append(f"ğŸ“Š Period summary ({len(events)} events)")
        
        if tool_names:
            parts.append(f"Tools: {', '.join(sorted(tool_names))}")
        
        if stats["tool_calls"] > 0:
            success_rate = stats["tool_successes"] / stats["tool_calls"] * 100
            parts.append(f"Tool calls: {stats['tool_calls']} ({success_rate:.0f}% success)")
        
        if failed_tools:
            parts.append(f"âŒ Failed: {'; '.join(failed_tools[:3])}")
        
        if stats["errors"] > 0:
            parts.append(f"âš ï¸ Errors: {stats['errors']}")
            if error_messages:
                parts.append(f"Last error: {error_messages[-1][:100]}")
        
        return ". ".join(parts) + "."


# =============================================================================
# å¿«é€Ÿå¯å‘å¼å‹ç¼©å™¨ï¼ˆå‚è€ƒè‡ª DeerFlow ContextManagerï¼‰
# =============================================================================

class FastHeuristicCompressor:
    """
    å¿«é€Ÿå¯å‘å¼å‹ç¼©å™¨ï¼ˆé›¶ LLM è°ƒç”¨ï¼‰ã€‚
    
    çµæ„Ÿæ¥è‡ª DeerFlow çš„ ContextManagerï¼Œç”¨äºç´§æ€¥æƒ…å†µä¸‹çš„å¿«é€Ÿå‹ç¼©ã€‚
    ä½¿ç”¨å¯å‘å¼ token ä¼°ç®—å’Œæˆªæ–­ç­–ç•¥ï¼Œé¿å… LLM è°ƒç”¨å¼€é”€ã€‚
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. å¯å‘å¼ token ä¼°ç®—ï¼ˆè‹±æ–‡ 4 char/tokenï¼Œä¸­æ–‡ 1 char/tokenï¼‰
    2. ä¿ç•™å‰ç¼€æ¶ˆæ¯ï¼ˆç³»ç»Ÿ promptsã€åˆå§‹ç›®æ ‡ï¼‰
    3. ä»å°¾éƒ¨æ·»åŠ æ¶ˆæ¯ç›´åˆ°è¾¾åˆ° token é™åˆ¶
    4. å•æ¡æ¶ˆæ¯è¿‡é•¿æ—¶æˆªæ–­è€Œéä¸¢å¼ƒ
    
    é€‚ç”¨åœºæ™¯ï¼š
    - Token å³å°†æº¢å‡ºï¼ˆç´§æ€¥å‹ç¼©ï¼‰
    - æˆæœ¬æ•æ„Ÿå‹ä»»åŠ¡
    - å¿«é€Ÿè¿­ä»£å¼€å‘/æµ‹è¯•
    
    ä¸ AgenticX ContextCompiler çš„å¯¹æ¯”ï¼š
    - ContextCompiler: è¯­ä¹‰æ‘˜è¦ï¼ˆLLMï¼‰ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œæˆæœ¬è¾ƒé«˜
    - FastHeuristicCompressor: æˆªæ–­ï¼ˆæ—  LLMï¼‰ï¼Œé€Ÿåº¦å¿«ï¼Œæˆæœ¬é›¶ï¼Œå¯èƒ½ä¸¢å¤±ç»†èŠ‚
    """
    
    def __init__(
        self, 
        token_limit: int = 8000,
        preserve_prefix_count: int = 2,
        max_message_tokens: int = 2000
    ):
        """
        Args:
            token_limit: Token ä¸Šé™ï¼ˆè¶…è¿‡æ­¤å€¼è§¦å‘å‹ç¼©ï¼‰
            preserve_prefix_count: ä¿ç•™çš„å‰ç¼€äº‹ä»¶æ•°é‡ï¼ˆé€šå¸¸æ˜¯ç³»ç»Ÿæ¶ˆæ¯å’Œåˆå§‹ç›®æ ‡ï¼‰
            max_message_tokens: å•æ¡æ¶ˆæ¯çš„æœ€å¤§ token æ•°ï¼ˆè¶…è¿‡åˆ™æˆªæ–­ï¼‰
        """
        self.token_limit = token_limit
        self.preserve_prefix_count = preserve_prefix_count
        self.max_message_tokens = max_message_tokens
        logger.info(
            f"FastHeuristicCompressor initialized: "
            f"limit={token_limit}, preserve_prefix={preserve_prefix_count}"
        )
    
    def compress(self, event_log: EventLog) -> List[AnyEvent]:
        """
        å¿«é€Ÿå‹ç¼©äº‹ä»¶æ—¥å¿—ã€‚
        
        ç­–ç•¥ï¼š
        1. ä¿ç•™å‰ N ä¸ªäº‹ä»¶ï¼ˆprefixï¼‰
        2. ä»å°¾éƒ¨å‘å‰æ·»åŠ äº‹ä»¶ï¼Œç›´åˆ°è¾¾åˆ° token é™åˆ¶
        3. è¿”å›å‹ç¼©åçš„äº‹ä»¶åˆ—è¡¨
        
        Args:
            event_log: éœ€è¦å‹ç¼©çš„äº‹ä»¶æ—¥å¿—
            
        Returns:
            å‹ç¼©åçš„äº‹ä»¶åˆ—è¡¨
        """
        events = event_log.events
        
        if not events:
            return []
        
        # 1. è®¡ç®—å¯ç”¨ token é¢„ç®—
        available_tokens = self.token_limit
        
        # 2. ä¿ç•™å‰ç¼€äº‹ä»¶ï¼ˆç³»ç»Ÿæ¶ˆæ¯ã€åˆå§‹ç›®æ ‡ç­‰ï¼‰
        prefix_count = min(self.preserve_prefix_count, len(events))
        prefix_events = events[:prefix_count]
        
        # è®¡ç®—å‰ç¼€æ¶ˆè€—çš„ tokens
        for event in prefix_events:
            event_tokens = self._estimate_event_tokens(event)
            available_tokens -= event_tokens
        
        logger.debug(
            f"Preserved {prefix_count} prefix events, "
            f"remaining budget: {available_tokens} tokens"
        )
        
        # 3. ä»å°¾éƒ¨æ·»åŠ äº‹ä»¶ï¼Œç›´åˆ°è¾¾åˆ°é™åˆ¶
        suffix_events = []
        remaining_events = events[prefix_count:]
        
        for event in reversed(remaining_events):
            event_tokens = self._estimate_event_tokens(event)
            
            # å¦‚æœå•æ¡æ¶ˆæ¯è¿‡é•¿ï¼Œæˆªæ–­è€Œéå®Œå…¨ä¸¢å¼ƒ
            if event_tokens > self.max_message_tokens:
                truncated_event = self._truncate_event(event, self.max_message_tokens)
                event_tokens = self._estimate_event_tokens(truncated_event)
                event = truncated_event
            
            if event_tokens <= available_tokens:
                suffix_events.insert(0, event)
                available_tokens -= event_tokens
            else:
                # Token é¢„ç®—è€—å°½ï¼Œåœæ­¢æ·»åŠ 
                break
        
        # 4. åˆå¹¶å‰ç¼€å’Œåç¼€
        result = prefix_events + suffix_events
        
        # æ—¥å¿—è®°å½•å‹ç¼©ç»Ÿè®¡
        original_count = len(events)
        result_count = len(result)
        dropped_count = original_count - result_count
        
        logger.info(
            f"Fast compression: {original_count} -> {result_count} events "
            f"({dropped_count} dropped, {available_tokens} tokens remaining)"
        )
        
        return result
    
    def _estimate_event_tokens(self, event: AnyEvent) -> int:
        """
        å¯å‘å¼ token ä¼°ç®—ï¼ˆæ¥è‡ª DeerFlowï¼‰ã€‚
        
        è§„åˆ™ï¼š
        - è‹±æ–‡å­—ç¬¦ï¼š4 char/token
        - éè‹±æ–‡å­—ç¬¦ï¼ˆä¸­æ–‡ç­‰ï¼‰ï¼š1 char/token
        
        æ³¨æ„ï¼šè¿™æ˜¯è¿‘ä¼¼ä¼°ç®—ï¼Œè¯¯å·®çº¦ Â±20%ï¼Œä½†é€Ÿåº¦å¿«ï¼ˆæ— éœ€ tiktokenï¼‰
        
        Args:
            event: äº‹ä»¶å¯¹è±¡
            
        Returns:
            ä¼°ç®—çš„ token æ•°
        """
        # å°†äº‹ä»¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        content = str(event.model_dump())
        
        # ç»Ÿè®¡è‹±æ–‡å’Œéè‹±æ–‡å­—ç¬¦
        english_chars = sum(1 for c in content if ord(c) < 128)
        non_english_chars = len(content) - english_chars
        
        # è®¡ç®— tokensï¼ˆè‹±æ–‡ 4 char/tokenï¼Œä¸­æ–‡ 1 char/tokenï¼‰
        estimated_tokens = (english_chars // 4) + non_english_chars
        
        return max(1, estimated_tokens)  # è‡³å°‘ 1 token
    
    def _truncate_event(self, event: AnyEvent, max_tokens: int) -> AnyEvent:
        """
        æˆªæ–­å•æ¡äº‹ä»¶å†…å®¹ã€‚
        
        ç­–ç•¥ï¼šä¿ç•™äº‹ä»¶çš„å…³é”®å­—æ®µï¼Œæˆªæ–­è¾ƒé•¿çš„å­—æ®µï¼ˆå¦‚ result, responseï¼‰
        
        Args:
            event: åŸå§‹äº‹ä»¶
            max_tokens: æœ€å¤§å…è®¸ token æ•°
            
        Returns:
            æˆªæ–­åçš„äº‹ä»¶
        """
        # å¤åˆ¶äº‹ä»¶æ•°æ®
        event_data = event.model_dump()
        
        # è¯†åˆ«éœ€è¦æˆªæ–­çš„é•¿å­—æ®µ
        truncatable_fields = ["result", "response", "error", "description", "content"]
        
        for field in truncatable_fields:
            if field in event_data and isinstance(event_data[field], str):
                original_value = event_data[field]
                
                # æ ¹æ® token é™åˆ¶è®¡ç®—å…è®¸çš„å­—ç¬¦æ•°
                # ä¿å®ˆä¼°è®¡ï¼šå‡è®¾å…¨è‹±æ–‡ï¼ˆ4 char/tokenï¼‰
                max_chars = max_tokens * 4
                
                if len(original_value) > max_chars:
                    truncated_value = original_value[:max_chars] + "... [truncated]"
                    event_data[field] = truncated_value
                    logger.debug(f"Truncated field '{field}': {len(original_value)} -> {len(truncated_value)} chars")
        
        # é‡å»ºäº‹ä»¶å¯¹è±¡
        event_type = type(event)
        try:
            truncated_event = event_type(**event_data)
            return truncated_event
        except Exception as e:
            logger.warning(f"Failed to rebuild truncated event: {e}, returning original")
            return event
    
    def estimate_total_tokens(self, events: List[AnyEvent]) -> int:
        """
        ä¼°ç®—äº‹ä»¶åˆ—è¡¨çš„æ€» token æ•°ã€‚
        
        Args:
            events: äº‹ä»¶åˆ—è¡¨
            
        Returns:
            ä¼°ç®—çš„æ€» token æ•°
        """
        return sum(self._estimate_event_tokens(e) for e in events)
    
    def is_over_limit(self, events: List[AnyEvent]) -> bool:
        """
        åˆ¤æ–­äº‹ä»¶åˆ—è¡¨æ˜¯å¦è¶…è¿‡ token é™åˆ¶ã€‚
        
        Args:
            events: äº‹ä»¶åˆ—è¡¨
            
        Returns:
            True å¦‚æœè¶…è¿‡é™åˆ¶
        """
        total = self.estimate_total_tokens(events)
        return total > self.token_limit
    
    def get_compression_ratio(self, original_events: List[AnyEvent], compressed_events: List[AnyEvent]) -> float:
        """
        è®¡ç®—å‹ç¼©æ¯”ç‡ã€‚
        
        Args:
            original_events: åŸå§‹äº‹ä»¶åˆ—è¡¨
            compressed_events: å‹ç¼©åäº‹ä»¶åˆ—è¡¨
            
        Returns:
            å‹ç¼©æ¯”ç‡ï¼ˆ0-1ï¼Œè¶Šå°å‹ç¼©è¶Šå¤šï¼‰
        """
        original_tokens = self.estimate_total_tokens(original_events)
        compressed_tokens = self.estimate_total_tokens(compressed_events)
        
        if original_tokens == 0:
            return 1.0
        
        return compressed_tokens / original_tokens


# =============================================================================
# ä¸Šä¸‹æ–‡ç¼–è¯‘å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
# =============================================================================

class ContextCompiler:
    """
    ä¸Šä¸‹æ–‡ç¼–è¯‘å™¨ï¼šå®ç° EventLog çš„æ™ºèƒ½å‹ç¼©ï¼ˆå¢å¼ºç‰ˆï¼‰ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç²¾ç¡® Token è®¡æ•°ï¼ˆä½¿ç”¨ tiktokenï¼‰
    2. å¤šç­–ç•¥å‹ç¼©æ”¯æŒ
    3. ä»»åŠ¡ç±»å‹æ„ŸçŸ¥ï¼ˆé’ˆå¯¹æŒ–æ˜ä»»åŠ¡ä¼˜åŒ–ï¼‰
    4. å¯è§‚æµ‹æ€§å¢å¼º
    
    è®¾è®¡åŸåˆ™ï¼ˆå€Ÿé‰´ ADKï¼‰ï¼š
    - æ»‘åŠ¨çª—å£ï¼šä½¿ç”¨ overlap ä¿æŒè¯­ä¹‰è¿ç»­æ€§
    - æŒ‰éœ€ç¼–è¯‘ï¼šä»…åœ¨å¿…è¦æ—¶è§¦å‘å‹ç¼©
    - æ¸è¿›å‹ç¼©ï¼šæ¯æ¬¡å‹ç¼©å›ºå®šæ•°é‡çš„äº‹ä»¶
    """
    
    def __init__(
        self,
        summarizer: Optional[EventSummarizer] = None,
        config: Optional[CompactionConfig] = None,
        strategy: CompactionStrategy = CompactionStrategy.SLIDING_WINDOW,
        task_type: str = "default",
        model: Optional[str] = None,
        enable_fast_fallback: bool = True
    ):
        """
        Args:
            summarizer: äº‹ä»¶æ‘˜è¦ç”Ÿæˆå™¨ã€‚
            config: å‹ç¼©é…ç½®ã€‚
            strategy: å‹ç¼©ç­–ç•¥ã€‚
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'default', 'mining', 'conversation', 'tool_sequence'ï¼‰ã€‚
            model: æ¨¡å‹åç§°ï¼ˆç”¨äºç²¾ç¡® token è®¡æ•°ï¼‰ã€‚
            enable_fast_fallback: æ˜¯å¦å¯ç”¨å¿«é€Ÿå‹ç¼©é™çº§ï¼ˆDeerFlow é£æ ¼ï¼‰ã€‚
        """
        self.summarizer = summarizer or SimpleEventSummarizer()
        self.config = config or CompactionConfig()
        self.strategy = strategy
        self.task_type = task_type
        self.token_counter = TokenCounter(model=model)
        self.enable_fast_fallback = enable_fast_fallback
        
        # å¿«é€Ÿå‹ç¼©å™¨ï¼ˆç”¨äºç´§æ€¥æƒ…å†µï¼‰
        self.fast_compressor: Optional[FastHeuristicCompressor] = None
        if enable_fast_fallback:
            self.fast_compressor = FastHeuristicCompressor(
                token_limit=config.max_context_tokens if config else 8000,
                preserve_prefix_count=2
            )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.compaction_history: List[Dict[str, Any]] = []
        self.total_tokens_saved = 0
        self.emergency_compressions = 0  # ç´§æ€¥å‹ç¼©æ¬¡æ•°
    
    async def maybe_compact(self, event_log: EventLog) -> Optional[CompactedEvent]:
        """
        æ£€æŸ¥å¹¶æ‰§è¡Œå‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚
        """
        should_compact, reason = self._should_compact(event_log)
        
        if not should_compact:
            return None
        
        logger.info(f"Triggering compaction. Reason: {reason}")
        return await self.compact(event_log, reason=reason)
    
    def _should_compact(self, event_log: EventLog) -> tuple:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å‹ç¼©ã€‚
        
        Returns:
            (should_compact, reason)
        """
        if not self.config.enabled:
            return False, None
        
        # è·å–æ–°äº‹ä»¶
        new_events = event_log.get_events_since_last_compaction()
        
        # ç­–ç•¥ 1ï¼šäº‹ä»¶æ•°é˜ˆå€¼
        if len(new_events) >= self.config.compaction_interval:
            return True, f"event_count ({len(new_events)} >= {self.config.compaction_interval})"
        
        # ç­–ç•¥ 2ï¼šToken é˜ˆå€¼ï¼ˆç´§æ€¥å‹ç¼©ï¼‰
        total_tokens = self._count_event_log_tokens(event_log)
        if total_tokens > self.config.max_context_tokens:
            return True, f"token_overflow ({total_tokens} > {self.config.max_context_tokens})"
        
        return False, None
    
    def _count_event_log_tokens(self, event_log: EventLog) -> int:
        """ç²¾ç¡®è®¡ç®— EventLog çš„ token æ•°ã€‚"""
        total = 0
        for event in event_log.events:
            if isinstance(event, CompactedEvent):
                total += self.token_counter.count_tokens(event.summary)
            else:
                # åºåˆ—åŒ–äº‹ä»¶å¹¶è®¡æ•°
                event_str = self._event_to_string(event)
                total += self.token_counter.count_tokens(event_str)
        return total
    
    def _event_to_string(self, event: AnyEvent) -> str:
        """å°†äº‹ä»¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆç”¨äº token è®¡æ•°ï¼‰ã€‚"""
        if isinstance(event, ToolCallEvent):
            return f"Tool: {event.tool_name}, Args: {event.tool_args}, Intent: {event.intent}"
        elif isinstance(event, ToolResultEvent):
            return f"Result: {event.tool_name}, Success: {event.success}, Data: {event.result or event.error}"
        elif isinstance(event, ErrorEvent):
            return f"Error: {event.error_type} - {event.error_message}"
        elif isinstance(event, LLMResponseEvent):
            return f"LLM Response: {event.response}"
        else:
            return str(event.model_dump())
    
    async def compact(
        self, 
        event_log: EventLog,
        reason: Optional[str] = None
    ) -> Optional[CompactedEvent]:
        """
        æ‰§è¡Œå‹ç¼©æ“ä½œï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒç´§æ€¥å¿«é€Ÿå‹ç¼©ï¼‰ã€‚
        """
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç´§æ€¥å‹ç¼©
        current_tokens = self._count_event_log_tokens(event_log)
        is_emergency = self._is_emergency(current_tokens)
        
        # ç´§æ€¥æƒ…å†µä¸‹ä½¿ç”¨å¿«é€Ÿå‹ç¼©å™¨ï¼ˆDeerFlow é£æ ¼ï¼‰
        if is_emergency and self.enable_fast_fallback and self.fast_compressor:
            logger.warning(
                f"EMERGENCY compaction triggered: {current_tokens} tokens "
                f"(limit: {self.config.max_context_tokens}). Using fast heuristic compression."
            )
            return self._fast_compress(event_log, reason="emergency_token_overflow")
        
        # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨è¯­ä¹‰æ‘˜è¦ï¼ˆåŸ AgenticX æ–¹å¼ï¼‰
        # æ ¹æ®ç­–ç•¥è·å–å¾…å‹ç¼©çš„äº‹ä»¶
        events_to_compact = self._get_events_to_compact(event_log)
        
        if not events_to_compact:
            logger.debug("No events to compact.")
            return None
        
        logger.info(f"Compacting {len(events_to_compact)} events (strategy: {self.strategy.value})...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        start_ts = self._get_event_timestamp(events_to_compact[0])
        end_ts = self._get_event_timestamp(events_to_compact[-1])
        
        # ç²¾ç¡®è®¡ç®—å‹ç¼©å‰çš„ token æ•°
        token_count_before = sum(
            self.token_counter.count_tokens(self._event_to_string(e))
            for e in events_to_compact
        )
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆä¼ é€’ä»»åŠ¡ç±»å‹ï¼‰
        summary = await self.summarizer.summarize(
            events_to_compact,
            self.config.summarizer_prompt,
            task_type=self.task_type
        )
        
        # ç²¾ç¡®è®¡ç®—å‹ç¼©åçš„ token æ•°
        token_count_after = self.token_counter.count_tokens(summary)
        
        # è®¡ç®—èŠ‚çœçš„ token
        tokens_saved = token_count_before - token_count_after
        self.total_tokens_saved += max(0, tokens_saved)
        
        # åˆ›å»º CompactedEvent
        compacted_event = CompactedEvent(
            summary=summary,
            start_timestamp=start_ts,
            end_timestamp=end_ts,
            compressed_event_ids=[e.id for e in events_to_compact],
            token_count_before=token_count_before,
            token_count_after=token_count_after,
            agent_id=event_log.agent_id,
            task_id=event_log.task_id
        )
        
        # è¿½åŠ åˆ° EventLog
        event_log.append(compacted_event)
        
        # è®°å½•å†å²
        self.compaction_history.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "events_compacted": len(events_to_compact),
            "token_before": token_count_before,
            "token_after": token_count_after,
            "compression_ratio": compacted_event.get_compression_ratio(),
            "strategy": self.strategy.value,
            "reason": reason
        })
        
        logger.info(
            f"Compaction complete. "
            f"Tokens: {token_count_before} -> {token_count_after} "
            f"(saved: {tokens_saved}, ratio: {compacted_event.get_compression_ratio():.2f})"
        )
        
        return compacted_event
    
    def _is_emergency(self, current_tokens: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç´§æ€¥æƒ…å†µï¼ˆæ¥è¿‘ token é™åˆ¶ï¼‰ã€‚
        
        Args:
            current_tokens: å½“å‰ token æ•°
            
        Returns:
            True å¦‚æœè¶…è¿‡é™åˆ¶çš„ 95%
        """
        threshold = self.config.max_context_tokens * 0.95
        return current_tokens >= threshold
    
    def _fast_compress(self, event_log: EventLog, reason: str = "emergency") -> Optional[CompactedEvent]:
        """
        å¿«é€Ÿå‹ç¼©ï¼ˆä½¿ç”¨ FastHeuristicCompressorï¼‰ã€‚
        
        æ³¨æ„ï¼šè¿™æ˜¯åŒæ­¥æ“ä½œï¼Œä¸è°ƒç”¨ LLM
        
        Args:
            event_log: äº‹ä»¶æ—¥å¿—
            reason: å‹ç¼©åŸå› 
            
        Returns:
            CompactedEvent æˆ– None
        """
        if not self.fast_compressor:
            logger.error("Fast compressor not initialized, cannot perform emergency compression")
            return None
        
        original_events = event_log.events.copy()
        
        # æ‰§è¡Œå¿«é€Ÿå‹ç¼©
        compressed_events = self.fast_compressor.compress(event_log)
        
        # æ›¿æ¢ EventLog çš„äº‹ä»¶åˆ—è¡¨
        event_log.events = compressed_events
        
        # ç»Ÿè®¡
        self.emergency_compressions += 1
        original_count = len(original_events)
        compressed_count = len(compressed_events)
        dropped_count = original_count - compressed_count
        
        # ä¼°ç®— token èŠ‚çœï¼ˆä½¿ç”¨å¯å‘å¼ï¼‰
        original_tokens = self.fast_compressor.estimate_total_tokens(original_events)
        compressed_tokens = self.fast_compressor.estimate_total_tokens(compressed_events)
        tokens_saved = original_tokens - compressed_tokens
        
        # è®°å½•å†å²
        self.compaction_history.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "events_compacted": dropped_count,
            "token_before": original_tokens,
            "token_after": compressed_tokens,
            "compression_ratio": compressed_tokens / original_tokens if original_tokens > 0 else 1.0,
            "strategy": "emergency",
            "reason": reason,
            "fast_compression": True
        })
        
        logger.warning(
            f"Emergency fast compression complete. "
            f"Events: {original_count} -> {compressed_count} (dropped: {dropped_count}). "
            f"Tokens: {original_tokens} -> {compressed_tokens} (saved: {tokens_saved})"
        )
        
        # å¿«é€Ÿå‹ç¼©ä¸ç”Ÿæˆ CompactedEventï¼Œè€Œæ˜¯ç›´æ¥æˆªæ–­äº‹ä»¶åˆ—è¡¨
        return None
    
    def _get_events_to_compact(self, event_log: EventLog) -> List[AnyEvent]:
        """
        æ ¹æ®ç­–ç•¥ç¡®å®šæœ¬æ¬¡å‹ç¼©çš„äº‹ä»¶èŒƒå›´ã€‚
        """
        if self.strategy == CompactionStrategy.SLIDING_WINDOW:
            return self._sliding_window_events(event_log)
        elif self.strategy == CompactionStrategy.EMERGENCY:
            return self._emergency_events(event_log)
        elif self.strategy == CompactionStrategy.TIME_BASED:
            return self._time_based_events(event_log)
        elif self.strategy == CompactionStrategy.TOPIC_BASED:
            return self._topic_based_events(event_log)
        elif self.strategy == CompactionStrategy.HYBRID:
            return self._hybrid_events(event_log)
        else:
            # é»˜è®¤ä½¿ç”¨æ»‘åŠ¨çª—å£
            return self._sliding_window_events(event_log)
    
    def _sliding_window_events(self, event_log: EventLog) -> List[AnyEvent]:
        """æ»‘åŠ¨çª—å£ç­–ç•¥ï¼šæ ‡å‡†çš„æ¸è¿›å‹ç¼©ã€‚"""
        last_compaction = event_log.get_last_compaction()
        
        if not last_compaction:
            # é¦–æ¬¡å‹ç¼©
            events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
            if len(events) > self.config.compaction_interval:
                return events[:-self.config.overlap_size] if self.config.overlap_size > 0 else events
            return events
        
        # æ‰¾åˆ° overlap äº‹ä»¶å’Œæ–°äº‹ä»¶
        overlap_events = []
        new_events = []
        
        for event in event_log.events:
            if isinstance(event, CompactedEvent):
                continue
            
            event_ts = self._get_event_timestamp(event)
            
            if event_ts <= last_compaction.end_timestamp:
                overlap_events.append(event)
            else:
                new_events.append(event)
        
        overlap_selected = overlap_events[-self.config.overlap_size:] if overlap_events else []
        return overlap_selected + new_events
    
    def _emergency_events(self, event_log: EventLog) -> List[AnyEvent]:
        """ç´§æ€¥å‹ç¼©ç­–ç•¥ï¼šå‹ç¼©æ›´å¤šäº‹ä»¶ä»¥å¿«é€Ÿé™ä½ token æ•°ã€‚"""
        events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
        
        # ç´§æ€¥æ¨¡å¼ä¸‹ï¼Œä¿ç•™æ›´å°‘çš„æœ€è¿‘äº‹ä»¶
        keep_recent = max(3, self.config.overlap_size)
        if len(events) > keep_recent:
            return events[:-keep_recent]
        
        return events
    
    def _time_based_events(self, event_log: EventLog) -> List[AnyEvent]:
        """
        åŸºäºæ—¶é—´çš„å‹ç¼©ç­–ç•¥ï¼šæŒ‰æ—¶é—´çª—å£åˆ†ç»„å‹ç¼©ã€‚
        
        ç®—æ³•ï¼š
        1. æ ¹æ®é…ç½®çš„æ—¶é—´çª—å£å¤§å°ï¼ˆé»˜è®¤ 5 åˆ†é’Ÿï¼‰ï¼Œå°†äº‹ä»¶åˆ†ç»„
        2. é€‰æ‹©æœ€æ—©çš„ã€å·²å®Œæˆçš„æ—¶é—´çª—å£è¿›è¡Œå‹ç¼©
        3. ä¿ç•™å½“å‰æ´»è·ƒæ—¶é—´çª—å£å†…çš„äº‹ä»¶
        """
        events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
        if not events:
            return []
        
        # æ—¶é—´çª—å£å¤§å°ï¼ˆç§’ï¼‰
        window_size = self.config.time_window_seconds
        
        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
        time_buckets: Dict[int, List[AnyEvent]] = {}
        for event in events:
            event_ts = self._get_event_timestamp(event)
            bucket_key = int(event_ts // window_size)
            if bucket_key not in time_buckets:
                time_buckets[bucket_key] = []
            time_buckets[bucket_key].append(event)
        
        if len(time_buckets) < 2:
            # åªæœ‰ä¸€ä¸ªæ—¶é—´çª—å£ï¼Œä¸å‹ç¼©
            return []
        
        # å¯¹ bucket_key æ’åºï¼Œå‹ç¼©æœ€æ—©çš„çª—å£ï¼ˆä¿ç•™æœ€è¿‘çš„çª—å£ï¼‰
        sorted_keys = sorted(time_buckets.keys())
        
        # å‹ç¼©æ‰€æœ‰å·²å®Œæˆçš„æ—¶é—´çª—å£ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
        events_to_compact = []
        for key in sorted_keys[:-1]:
            events_to_compact.extend(time_buckets[key])
        
        return events_to_compact
    
    def _topic_based_events(self, event_log: EventLog) -> List[AnyEvent]:
        """
        åŸºäºä¸»é¢˜çš„å‹ç¼©ç­–ç•¥ï¼šå°†ç›¸å…³äº‹ä»¶èšç±»åå‹ç¼©ã€‚
        
        ç®—æ³•ï¼š
        1. æŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„ï¼ˆToolCall/ToolResult, LLM, Error, Humanï¼‰
        2. åœ¨åŒä¸€ç±»å‹å†…ï¼ŒæŒ‰å·¥å…·åæˆ–å…¶ä»–ç‰¹å¾è¿›ä¸€æ­¥èšç±»
        3. é€‰æ‹©å¯ä»¥å®‰å…¨å‹ç¼©çš„èšç±»ï¼ˆå·²å®Œæˆçš„å·¥å…·è°ƒç”¨é“¾ç­‰ï¼‰
        """
        events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
        if not events:
            return []
        
        # æŒ‰ä¸»é¢˜åˆ†ç±»
        topic_groups: Dict[str, List[AnyEvent]] = {
            "tool_chains": [],      # å·¥å…·è°ƒç”¨é“¾ï¼ˆToolCall + ToolResultï¼‰
            "llm_interactions": [], # LLM äº¤äº’
            "errors": [],           # é”™è¯¯äº‹ä»¶
            "human_io": [],         # äººç±»äº¤äº’
            "others": []            # å…¶ä»–
        }
        
        # è¿½è¸ªæœªå®Œæˆçš„å·¥å…·è°ƒç”¨
        pending_tool_calls: Dict[str, ToolCallEvent] = {}
        completed_tool_chains: List[AnyEvent] = []
        
        for event in events:
            if isinstance(event, ToolCallEvent):
                pending_tool_calls[event.tool_name] = event
            elif isinstance(event, ToolResultEvent):
                # æ‰¾åˆ°å¯¹åº”çš„ ToolCallï¼Œå½¢æˆå®Œæ•´é“¾
                if event.tool_name in pending_tool_calls:
                    completed_tool_chains.append(pending_tool_calls.pop(event.tool_name))
                    completed_tool_chains.append(event)
                else:
                    topic_groups["tool_chains"].append(event)
            elif isinstance(event, (LLMCallEvent, LLMResponseEvent)):
                topic_groups["llm_interactions"].append(event)
            elif isinstance(event, ErrorEvent):
                topic_groups["errors"].append(event)
            elif isinstance(event, (HumanRequestEvent, HumanResponseEvent)):
                topic_groups["human_io"].append(event)
            else:
                topic_groups["others"].append(event)
        
        # åªå‹ç¼©å·²å®Œæˆçš„å·¥å…·è°ƒç”¨é“¾
        topic_groups["tool_chains"] = completed_tool_chains
        
        # ä¿ç•™æœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼ˆä¸å‹ç¼©ï¼‰
        # pending_tool_calls ä¸­çš„äº‹ä»¶ä¸ä¼šè¢«å‹ç¼©
        
        # å†³å®šå‹ç¼©å“ªäº›ä¸»é¢˜ï¼ˆä¼˜å…ˆå‹ç¼©å·²å®Œæˆçš„å·¥å…·é“¾å’Œ LLM äº¤äº’ï¼‰
        events_to_compact = []
        
        # å·¥å…·é“¾ï¼šåªæœ‰å½“é“¾è¶³å¤Ÿé•¿æ—¶æ‰å‹ç¼©
        if len(topic_groups["tool_chains"]) >= self.config.compaction_interval:
            # ä¿ç•™æœ€è¿‘çš„å‡ ä¸ªäº‹ä»¶ä½œä¸º overlap
            events_to_compact.extend(topic_groups["tool_chains"][:-self.config.overlap_size])
        
        # LLM äº¤äº’ï¼šé€šå¸¸å¯ä»¥å®‰å…¨å‹ç¼©
        if len(topic_groups["llm_interactions"]) >= self.config.compaction_interval:
            events_to_compact.extend(topic_groups["llm_interactions"][:-self.config.overlap_size])
        
        return events_to_compact
    
    def _hybrid_events(self, event_log: EventLog) -> List[AnyEvent]:
        """
        æ··åˆç­–ç•¥ï¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€é€‰æ‹©æœ€ä¼˜å‹ç¼©æ–¹æ¡ˆã€‚
        
        å†³ç­–é€»è¾‘ï¼š
        1. å¦‚æœäº‹ä»¶è·¨åº¦è¶…è¿‡æ—¶é—´é˜ˆå€¼ï¼Œä½¿ç”¨ TIME_BASED
        2. å¦‚æœå­˜åœ¨æ˜æ˜¾çš„ä¸»é¢˜èšç±»ï¼Œä½¿ç”¨ TOPIC_BASED
        3. å¦åˆ™ä½¿ç”¨ SLIDING_WINDOW
        """
        events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
        if not events:
            return []
        
        # è®¡ç®—æ—¶é—´è·¨åº¦
        timestamps = [self._get_event_timestamp(e) for e in events]
        time_span = max(timestamps) - min(timestamps) if timestamps else 0
        
        # è®¡ç®—ä¸»é¢˜å¤šæ ·æ€§
        topic_diversity = self._calculate_topic_diversity(events)
        
        # å†³ç­–é˜ˆå€¼
        time_threshold = self.config.time_window_seconds * 2  # 2 ä¸ªæ—¶é—´çª—å£
        diversity_threshold = 0.6  # ä¸»é¢˜å¤šæ ·æ€§é˜ˆå€¼
        
        logger.debug(
            f"Hybrid strategy analysis: time_span={time_span:.0f}s, "
            f"topic_diversity={topic_diversity:.2f}"
        )
        
        # é€‰æ‹©ç­–ç•¥
        if time_span > time_threshold:
            logger.info("Hybrid: Selecting TIME_BASED strategy (large time span)")
            return self._time_based_events(event_log)
        elif topic_diversity > diversity_threshold:
            logger.info("Hybrid: Selecting TOPIC_BASED strategy (high topic diversity)")
            return self._topic_based_events(event_log)
        else:
            logger.info("Hybrid: Selecting SLIDING_WINDOW strategy (default)")
            return self._sliding_window_events(event_log)
    
    def _calculate_topic_diversity(self, events: List[AnyEvent]) -> float:
        """
        è®¡ç®—äº‹ä»¶çš„ä¸»é¢˜å¤šæ ·æ€§ï¼ˆ0-1ï¼‰ã€‚
        
        Returns:
            0 = æ‰€æœ‰äº‹ä»¶åŒä¸€ä¸»é¢˜ï¼Œ1 = å®Œå…¨ä¸åŒçš„ä¸»é¢˜
        """
        if not events:
            return 0.0
        
        # ç»Ÿè®¡å„ç±»äº‹ä»¶
        type_counts: Dict[str, int] = {}
        for event in events:
            event_type = type(event).__name__
            type_counts[event_type] = type_counts.get(event_type, 0) + 1
        
        # å¦‚æœåªæœ‰ä¸€ç§ç±»å‹ï¼Œå¤šæ ·æ€§ä¸º 0
        if len(type_counts) == 1:
            return 0.0
        
        # è®¡ç®— Shannon ç†µä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
        total = len(events)
        entropy = 0.0
        for count in type_counts.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)
        
        # å½’ä¸€åŒ–åˆ° 0-1ï¼ˆæœ€å¤§ç†µ = log2(ç±»å‹æ•°)ï¼‰
        max_entropy = math.log2(len(type_counts)) if len(type_counts) > 1 else 1
        return entropy / max_entropy if max_entropy > 0 else 0.0
    
    def _get_event_timestamp(self, event: AnyEvent) -> float:
        """è·å–äº‹ä»¶æ—¶é—´æˆ³ã€‚"""
        if hasattr(event, 'timestamp'):
            ts = event.timestamp
            if isinstance(ts, datetime):
                return ts.timestamp()
            return float(ts)
        return 0.0
    
    # =========================================================================
    # å¯è§‚æµ‹æ€§æ–¹æ³•
    # =========================================================================
    
    def get_compaction_stats(self) -> Dict[str, Any]:
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        if not self.compaction_history:
            return {
                "total_compactions": 0,
                "total_tokens_saved": 0,
                "average_compression_ratio": 0.0
            }
        
        avg_ratio = sum(h["compression_ratio"] for h in self.compaction_history) / len(self.compaction_history)
        total_events = sum(h["events_compacted"] for h in self.compaction_history)
        
        return {
            "total_compactions": len(self.compaction_history),
            "total_events_compacted": total_events,
            "total_tokens_saved": self.total_tokens_saved,
            "average_compression_ratio": round(avg_ratio, 3),
            "history": self.compaction_history[-10:]  # æœ€è¿‘ 10 æ¬¡
        }
    
    # =========================================================================
    # æŒä¹…åŒ–æ–¹æ³•
    # =========================================================================
    
    def export_stats(self, file_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å¯¼å‡ºå‹ç¼©ç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶æˆ–è¿”å›å­—å…¸ã€‚
        
        Args:
            file_path: å¯é€‰çš„æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œå°†å¯¼å‡ºåˆ°æ–‡ä»¶ã€‚
            
        Returns:
            å¯¼å‡ºçš„ç»Ÿè®¡æ•°æ®å­—å…¸ã€‚
        """
        export_data = {
            "version": "2.2",
            "exported_at": datetime.now(timezone.utc).isoformat(),
            "config": {
                "enabled": self.config.enabled,
                "compaction_interval": self.config.compaction_interval,
                "overlap_size": self.config.overlap_size,
                "max_context_tokens": self.config.max_context_tokens,
                "time_window_seconds": self.config.time_window_seconds,
            },
            "strategy": self.strategy.value,
            "task_type": self.task_type,
            "statistics": {
                "total_compactions": len(self.compaction_history),
                "total_tokens_saved": self.total_tokens_saved,
                "average_compression_ratio": (
                    sum(h["compression_ratio"] for h in self.compaction_history) / 
                    len(self.compaction_history) if self.compaction_history else 0.0
                ),
            },
            "history": self.compaction_history,
        }
        
        if file_path:
            path = Path(file_path)
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            logger.info(f"Compaction stats exported to {file_path}")
        
        return export_data
    
    def import_stats(self, file_path: str) -> None:
        """
        ä» JSON æ–‡ä»¶å¯¼å…¥å‹ç¼©ç»Ÿè®¡ä¿¡æ¯ã€‚
        
        è¿™å…è®¸åœ¨é‡å¯åæ¢å¤å†å²ç»Ÿè®¡ã€‚
        
        Args:
            file_path: JSON æ–‡ä»¶è·¯å¾„ã€‚
        """
        path = Path(file_path)
        if not path.exists():
            logger.warning(f"Stats file not found: {file_path}")
            return
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ¢å¤å†å²è®°å½•
            if "history" in data:
                self.compaction_history = data["history"]
            
            # æ¢å¤ç»Ÿè®¡
            if "statistics" in data:
                self.total_tokens_saved = data["statistics"].get("total_tokens_saved", 0)
            
            logger.info(f"Imported {len(self.compaction_history)} compaction records from {file_path}")
            
        except Exception as e:
            logger.error(f"Failed to import stats from {file_path}: {e}")
    
    def reset_stats(self) -> None:
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        self.compaction_history = []
        self.total_tokens_saved = 0
        logger.info("Compaction stats reset")
    
    def compare_views(self, event_log: EventLog) -> Dict[str, Any]:
        """
        å¯¹æ¯”åŸå§‹è§†å›¾å’Œç¼–è¯‘è§†å›¾ã€‚
        
        Returns:
            åŒ…å«ä¸¤ç§è§†å›¾ token ç»Ÿè®¡çš„å¯¹æ¯”ä¿¡æ¯ã€‚
        """
        # åŸå§‹è§†å›¾ï¼šæ‰€æœ‰éå‹ç¼©äº‹ä»¶
        original_events = [e for e in event_log.events if not isinstance(e, CompactedEvent)]
        original_tokens = sum(
            self.token_counter.count_tokens(self._event_to_string(e))
            for e in original_events
        )
        
        # ç¼–è¯‘è§†å›¾ï¼šå‹ç¼©äº‹ä»¶çš„æ‘˜è¦ + æœªè¢«è¦†ç›–çš„åŸå§‹äº‹ä»¶
        compiled_tokens = self._count_compiled_view_tokens(event_log)
        
        return {
            "original_view": {
                "event_count": len(original_events),
                "token_count": original_tokens
            },
            "compiled_view": {
                "event_count": len(event_log.events),
                "token_count": compiled_tokens
            },
            "savings": {
                "tokens_saved": original_tokens - compiled_tokens,
                "compression_ratio": round(compiled_tokens / max(original_tokens, 1), 3)
            }
        }
    
    def _count_compiled_view_tokens(self, event_log: EventLog) -> int:
        """è®¡ç®—ç¼–è¯‘è§†å›¾çš„ token æ•°ã€‚"""
        # å®ç°é€†åºç¼–è¯‘ç®—æ³•æ¥è®¡ç®—å®é™…ä¼šè¢«æ¸²æŸ“çš„ token æ•°
        compaction_boundaries = []
        for event in event_log.events:
            if isinstance(event, CompactedEvent):
                compaction_boundaries.append((event.start_timestamp, event.end_timestamp, event))
        
        if not compaction_boundaries:
            return self._count_event_log_tokens(event_log)
        
        compaction_boundaries.sort(key=lambda x: x[1], reverse=True)
        
        total_tokens = 0
        mask_end_time = float('inf')
        
        for event in reversed(event_log.events):
            event_ts = self._get_event_timestamp(event)
            
            if isinstance(event, CompactedEvent):
                if event_ts < mask_end_time:
                    total_tokens += self.token_counter.count_tokens(event.summary)
                    mask_end_time = min(mask_end_time, event.start_timestamp)
            else:
                if event_ts < mask_end_time:
                    total_tokens += self.token_counter.count_tokens(self._event_to_string(event))
        
        return total_tokens


# =============================================================================
# ä¾¿æ·å·¥å‚å‡½æ•°
# =============================================================================

def create_context_compiler(
    llm_provider: Optional[Any] = None,
    config: Optional[CompactionConfig] = None,
    use_simple_summarizer: bool = False,
    strategy: CompactionStrategy = CompactionStrategy.SLIDING_WINDOW,
    task_type: str = "default",
    model: Optional[str] = None
) -> ContextCompiler:
    """
    åˆ›å»º ContextCompiler å®ä¾‹çš„ä¾¿æ·å·¥å‚å‡½æ•°ã€‚
    
    Args:
        llm_provider: LLM æä¾›è€…ï¼ˆå¦‚æœä½¿ç”¨ LLM æ‘˜è¦ï¼‰ã€‚
        config: å‹ç¼©é…ç½®ã€‚
        use_simple_summarizer: æ˜¯å¦ä½¿ç”¨ç®€å•æ‘˜è¦ï¼ˆä¸è°ƒç”¨ LLMï¼‰ã€‚
        strategy: å‹ç¼©ç­–ç•¥ã€‚
        task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'default', 'mining', 'conversation'ï¼‰ã€‚
        model: æ¨¡å‹åç§°ï¼ˆç”¨äºç²¾ç¡® token è®¡æ•°ï¼‰ã€‚
        
    Returns:
        é…ç½®å¥½çš„ ContextCompiler å®ä¾‹ã€‚
    """
    if use_simple_summarizer or llm_provider is None:
        summarizer = SimpleEventSummarizer()
    else:
        summarizer = LLMEventSummarizer(
            llm_provider, 
            model=model,
            default_task_type=task_type
        )
    
    return ContextCompiler(
        summarizer=summarizer,
        config=config,
        strategy=strategy,
        task_type=task_type,
        model=model
    )


def create_mining_compiler(
    llm_provider: Any,
    model: Optional[str] = None,
    compaction_interval: int = 15,
    overlap_size: int = 3
) -> ContextCompiler:
    """
    åˆ›å»ºé’ˆå¯¹"è‡ªåŠ¨æŒ–æ˜"ä»»åŠ¡ä¼˜åŒ–çš„ ContextCompilerã€‚
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ MINING_TASK_PROMPT ä¿ç•™å¤±è´¥è·¯å¾„
    - è¾ƒå¤§çš„ overlap ç¡®ä¿æ¢ç´¢çº¿ç´¢ä¸ä¸¢å¤±
    """
    config = CompactionConfig(
        enabled=True,
        compaction_interval=compaction_interval,
        overlap_size=overlap_size,
        max_context_tokens=12000,  # æŒ–æ˜ä»»åŠ¡é€šå¸¸éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
    )
    
    return create_context_compiler(
        llm_provider=llm_provider,
        config=config,
        strategy=CompactionStrategy.SLIDING_WINDOW,
        task_type="mining",
        model=model
    )
