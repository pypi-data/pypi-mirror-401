"""Semantic Chunker for AgenticX Knowledge Management System

This module provides semantic chunking that groups content by semantic similarity.
"""

import logging
import re
import time
from typing import List, Optional, Dict, Any

from ..base import ChunkingConfig
from ..document import Document, DocumentMetadata, ChunkMetadata
from .framework import AdvancedBaseChunker, ChunkingResult, ChunkMetrics

logger = logging.getLogger(__name__)


class SemanticChunker(AdvancedBaseChunker):
    """Semantic chunker that groups content by semantic similarity"""
    
    def __init__(self, config: Optional[ChunkingConfig] = None, **kwargs):
        super().__init__(config, **kwargs)
        self.embedding_model = kwargs.get('embedding_model')
        self.similarity_threshold = kwargs.get('similarity_threshold', 0.7)
        self.min_chunk_size = kwargs.get('min_chunk_size', 100)
        self.max_chunk_size = kwargs.get('max_chunk_size', self.config.chunk_size * 2)
    
    def chunk_text(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Split text into chunks using semantic similarity
        
        Args:
            text: Text to chunk
            metadata: Optional metadata to attach to chunks
            
        Returns:
            List of chunk dictionaries with 'content' and 'metadata' keys
        """
        # Create a temporary document for processing
        doc_metadata = DocumentMetadata(
            name=metadata.get('name', 'temp_doc') if metadata else 'temp_doc',
            source=metadata.get('source', 'text') if metadata else 'text',
            source_type='text'
        )
        document = Document(content=text, metadata=doc_metadata)
        
        # Use the async method synchronously
        chunks = self.chunk_document(document)
        
        # Convert to the expected format
        result = []
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                'chunk_index': i,
                'chunk_size': len(chunk.content),
                'chunker': 'SemanticChunker'
            })
            
            result.append({
                'content': chunk.content,
                'metadata': chunk_metadata
            })
        
        return result
    
    async def chunk_document_async(self, document: Document) -> ChunkingResult:
        """Chunk document using semantic similarity"""
        start_time = time.time()
        
        logger.info(f"å¼€å§‹è¯­ä¹‰åˆ†å—: {document.metadata.name}")
        logger.info(f"åˆ†å—å™¨é…ç½®: ç›¸ä¼¼åº¦é˜ˆå€¼={self.similarity_threshold}, æœ€å°å—={self.min_chunk_size}, æœ€å¤§å—={self.max_chunk_size}")
        
        try:
            # Split into sentences first
            logger.info("âœ‚ï¸ å¼€å§‹å¥å­åˆ†å‰²...")
            sentences = self._split_into_sentences(document.content)
            logger.info(f"å¥å­åˆ†å‰²å®Œæˆ: {len(sentences)} ä¸ªå¥å­")
            
            if not sentences:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å¥å­ï¼Œè¿”å›åŸæ–‡æ¡£")
                return ChunkingResult(
                    chunks=[document],
                    strategy_used="semantic",
                    processing_time=time.time() - start_time
                )
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå¥å­ä½œä¸ºç¤ºä¾‹
            if sentences:
                logger.debug(f"å¥å­ç¤ºä¾‹: {sentences[0][:100]}...")
            
            # Group sentences by semantic similarity
            logger.info("å¼€å§‹è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†ç»„...")
            chunks = await self._group_by_semantic_similarity(sentences, document)
            
            # Evaluate chunk quality
            metrics = await self._evaluate_chunks(chunks)
            
            return ChunkingResult(
                chunks=chunks,
                strategy_used="semantic",
                processing_time=time.time() - start_time,
                metrics=metrics,
                metadata={
                    'original_sentences': len(sentences),
                    'similarity_threshold': self.similarity_threshold
                }
            )
            
        except Exception as e:
            logger.error(f"Semantic chunking failed: {e}")
            return ChunkingResult(
                strategy_used="semantic",
                processing_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        # Simple sentence splitting - can be enhanced with NLP libraries
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences
    
    async def _group_by_semantic_similarity(self, sentences: List[str], document: Document) -> List[Document]:
        """Group sentences by semantic similarity with optimization"""
        logger.info(f"è¿›å…¥è¯­ä¹‰åˆ†ç»„ï¼Œå¥å­æ•°é‡: {len(sentences)}")
        
        if not self.embedding_model:
            logger.warning("âŒ æœªæ‰¾åˆ°åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨å›é€€åˆ†ç»„ç­–ç•¥")
            return self._fallback_grouping(sentences, document)
        
        logger.info(f"âœ… åµŒå…¥æ¨¡å‹å·²é…ç½®: {type(self.embedding_model).__name__}")
        
        # ç®—åŠ›ä¼˜åŒ–ï¼šå¦‚æœå¥å­å¤ªå¤šï¼Œä½¿ç”¨æ··åˆç­–ç•¥
        if len(sentences) > 50:
            logger.info(f"âš¡ å¥å­æ•°é‡è¾ƒå¤š({len(sentences)})ï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ··åˆåˆ†å—ç­–ç•¥")
            return await self._optimized_hybrid_grouping(sentences, document)
        
        logger.info(f"å¥å­æ•°é‡é€‚ä¸­({len(sentences)})ï¼Œä½¿ç”¨å®Œæ•´è¯­ä¹‰åˆ†å—")
        
        try:
            # Get embeddings for all sentences
            logger.info(f"è¯­ä¹‰åˆ†å—ï¼šå¼€å§‹ä¸º {len(sentences)} ä¸ªå¥å­ç”ŸæˆåµŒå…¥å‘é‡")
            
            if hasattr(self.embedding_model, 'aembed_texts'):
                embeddings = await self.embedding_model.aembed_texts(sentences)
            elif hasattr(self.embedding_model, 'embed_texts'):
                embeddings = self.embedding_model.embed_texts(sentences)
            elif hasattr(self.embedding_model, 'embed'):
                embeddings = self.embedding_model.embed(sentences)
            else:
                logger.warning("åµŒå…¥æ¨¡å‹æ²¡æœ‰æ”¯æŒçš„åµŒå…¥æ–¹æ³•ï¼Œå›é€€åˆ°ç®€å•åˆ†ç»„")
                return self._fallback_grouping(sentences, document)
            
            logger.info(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ")
            
            # Group sentences by similarity
            groups = self._cluster_by_similarity(sentences, embeddings)
            
            # Convert groups to chunks
            chunks = []
            for i, group in enumerate(groups):
                chunk_content = ' '.join(group)
                
                chunk_metadata = ChunkMetadata(
                    name=f"{document.metadata.name}_semantic_{i+1}",
                    source=document.metadata.source,
                    source_type=document.metadata.source_type,
                    content_type=document.metadata.content_type,
                    parent_id=document.metadata.document_id,
                    chunk_index=i,
                    chunker_name="SemanticChunker"
                )
                
                chunk = Document(content=chunk_content, metadata=chunk_metadata)
                chunks.append(chunk)
            
            return chunks
            
        except Exception as e:
            logger.warning(f"è¯­ä¹‰åˆ†ç»„å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥: {e}")
            return self._fallback_grouping(sentences, document)
    
    def _fallback_grouping(self, sentences: List[str], document: Document) -> List[Document]:
        """Fallback grouping when embeddings are not available"""
        logger.warning("âš ï¸ ä½¿ç”¨å›é€€åˆ†ç»„ç­–ç•¥ï¼ˆåŸºäºé•¿åº¦çš„ç®€å•åˆ†ç»„ï¼Œéè¯­ä¹‰åˆ†ç»„ï¼‰")
        logger.info(f"ğŸ“ å›é€€åˆ†ç»„å‚æ•°: ç›®æ ‡å¤§å°={self.config.chunk_size}, å¥å­æ•°={len(sentences)}")
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.config.chunk_size and current_chunk:
                # Create chunk from current group
                chunk_content = ' '.join(current_chunk)
                chunk_metadata = ChunkMetadata(
                    name=f"{document.metadata.name}_fallback_{len(chunks)+1}",
                    source=document.metadata.source,
                    source_type=document.metadata.source_type,
                    content_type=document.metadata.content_type,
                    parent_id=document.metadata.document_id,
                    chunk_index=len(chunks),
                    chunker_name="SemanticChunker"
                )
                
                chunk = Document(content=chunk_content, metadata=chunk_metadata)
                chunks.append(chunk)
                
                current_chunk = [sentence]
                current_size = sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        # Add remaining sentences as final chunk
        if current_chunk:
            chunk_content = ' '.join(current_chunk)
            chunk_metadata = ChunkMetadata(
                name=f"{document.metadata.name}_fallback_{len(chunks)+1}",
                source=document.metadata.source,
                source_type=document.metadata.source_type,
                content_type=document.metadata.content_type,
                parent_id=document.metadata.document_id,
                chunk_index=len(chunks),
                chunker_name="SemanticChunker"
            )
            
            chunk = Document(content=chunk_content, metadata=chunk_metadata)
            chunks.append(chunk)
        
        return chunks
    
    async def _optimized_hybrid_grouping(self, sentences: List[str], document: Document) -> List[Document]:
        """ä¼˜åŒ–çš„æ··åˆåˆ†å—ç­–ç•¥ï¼šå…ˆæŒ‰æ®µè½åˆ†ç»„ï¼Œå†è¿›è¡Œå±€éƒ¨è¯­ä¹‰åˆ†æ"""
        logger.info("ğŸš€ ä½¿ç”¨ä¼˜åŒ–æ··åˆç­–ç•¥ï¼šæ®µè½é¢„åˆ†ç»„ + å±€éƒ¨è¯­ä¹‰ä¼˜åŒ–")
        logger.info(f"æ··åˆç­–ç•¥è¾“å…¥: {len(sentences)} ä¸ªå¥å­")
        
        # 1. å…ˆæŒ‰æ®µè½æˆ–é•¿åº¦è¿›è¡Œç²—åˆ†ç»„
        logger.info("ğŸ“‹ ç¬¬1æ­¥: æ®µè½é¢„åˆ†ç»„...")
        rough_groups = self._paragraph_based_grouping(sentences)
        logger.info(f"æ®µè½é¢„åˆ†ç»„å®Œæˆï¼š{len(rough_groups)} ä¸ªç²—åˆ†ç»„")
        
        # æ˜¾ç¤ºåˆ†ç»„å¤§å°åˆ†å¸ƒ
        group_sizes = [len(group) for group in rough_groups]
        logger.info(f"ğŸ“ˆ åˆ†ç»„å¤§å°åˆ†å¸ƒ: æœ€å°={min(group_sizes)}, æœ€å¤§={max(group_sizes)}, å¹³å‡={sum(group_sizes)/len(group_sizes):.1f}")
        
        # 2. å¯¹æ¯ä¸ªç²—åˆ†ç»„è¿›è¡Œå±€éƒ¨è¯­ä¹‰ä¼˜åŒ–
        logger.info("ç¬¬2æ­¥: å±€éƒ¨è¯­ä¹‰ä¼˜åŒ–...")
        optimized_chunks = []
        semantic_optimized_count = 0
        
        for i, group in enumerate(rough_groups):
            logger.debug(f"å¤„ç†åˆ†ç»„ {i+1}/{len(rough_groups)}: {len(group)} ä¸ªå¥å­")
            
            if len(group) <= 10:  # å°ç»„ç›´æ¥ä½¿ç”¨
                logger.debug(f"å°ç»„ç›´æ¥ä½¿ç”¨: {len(group)} ä¸ªå¥å­")
                chunk_content = ' '.join(group)
                chunk_metadata = ChunkMetadata(
                    name=f"{document.metadata.name}_hybrid_{i+1}",
                    source=document.metadata.source,
                    source_type=document.metadata.source_type,
                    content_type=document.metadata.content_type,
                    parent_id=document.metadata.document_id,
                    chunk_index=i,
                    chunker_name="SemanticChunker"
                )
                chunk = Document(content=chunk_content, metadata=chunk_metadata)
                optimized_chunks.append(chunk)
            else:
                # å¤§ç»„è¿›è¡Œå±€éƒ¨è¯­ä¹‰ä¼˜åŒ–
                logger.debug(f"å¤§ç»„è¿›è¡Œè¯­ä¹‰ä¼˜åŒ–: {len(group)} ä¸ªå¥å­")
                try:
                    # ç”ŸæˆåµŒå…¥å‘é‡
                    if hasattr(self.embedding_model, 'aembed_texts'):
                        logger.debug("ä½¿ç”¨å¼‚æ­¥åµŒå…¥æ–¹æ³•")
                        embeddings = await self.embedding_model.aembed_texts(group)
                    else:
                        logger.debug("ä½¿ç”¨åŒæ­¥åµŒå…¥æ–¹æ³•")
                        embeddings = self.embedding_model.embed(group)
                    
                    logger.debug(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
                    
                    # å±€éƒ¨èšç±»
                    logger.debug("å¼€å§‹å±€éƒ¨è¯­ä¹‰èšç±»...")
                    local_groups = self._cluster_by_similarity(group, embeddings)
                    logger.debug(f"å±€éƒ¨èšç±»ç»“æœ: {len(local_groups)} ä¸ªå­ç»„")
                    semantic_optimized_count += 1
                    
                    # è½¬æ¢ä¸ºæ–‡æ¡£å—
                    for j, local_group in enumerate(local_groups):
                        chunk_content = ' '.join(local_group)
                        chunk_metadata = ChunkMetadata(
                            name=f"{document.metadata.name}_hybrid_{i+1}_{j+1}",
                            source=document.metadata.source,
                            source_type=document.metadata.source_type,
                            content_type=document.metadata.content_type,
                            parent_id=document.metadata.document_id,
                            chunk_index=len(optimized_chunks),
                            chunker_name="SemanticChunker"
                        )
                        chunk = Document(content=chunk_content, metadata=chunk_metadata)
                        optimized_chunks.append(chunk)
                        
                except Exception as e:
                    logger.warning(f"å±€éƒ¨è¯­ä¹‰ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸåˆ†ç»„: {e}")
                    chunk_content = ' '.join(group)
                    chunk_metadata = ChunkMetadata(
                        name=f"{document.metadata.name}_hybrid_{i+1}",
                        source=document.metadata.source,
                        source_type=document.metadata.source_type,
                        content_type=document.metadata.content_type,
                        parent_id=document.metadata.document_id,
                        chunk_index=len(optimized_chunks),
                        chunker_name="SemanticChunker"
                    )
                    chunk = Document(content=chunk_content, metadata=chunk_metadata)
                    optimized_chunks.append(chunk)
        
        logger.info(f"âœ… æ··åˆåˆ†å—å®Œæˆï¼š{len(optimized_chunks)} ä¸ªæœ€ç»ˆåˆ†å—")
        logger.info(f"è¯­ä¹‰ä¼˜åŒ–ç»Ÿè®¡: {semantic_optimized_count}/{len(rough_groups)} ä¸ªåˆ†ç»„ä½¿ç”¨äº†è¯­ä¹‰ä¼˜åŒ–")
        return optimized_chunks
    
    def _paragraph_based_grouping(self, sentences: List[str]) -> List[List[str]]:
        """åŸºäºæ®µè½å’Œé•¿åº¦çš„é¢„åˆ†ç»„"""
        groups = []
        current_group = []
        current_size = 0
        target_size = self.config.chunk_size * 0.8  # é¢„ç•™ç©ºé—´ç»™è¯­ä¹‰è°ƒæ•´
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼€å§‹æ–°ç»„
            should_start_new = (
                current_size + sentence_size > target_size and current_group
            ) or (
                # æ£€æŸ¥æ®µè½åˆ†éš”ç¬¦
                len(current_group) > 0 and 
                (sentence.strip().startswith(('ç¬¬', 'ä¸€ã€', 'äºŒã€', 'ä¸‰ã€', 'å››ã€', 'äº”ã€', '1.', '2.', '3.', '4.', '5.')) or
                 current_group[-1].endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')))
            )
            
            if should_start_new:
                if current_group:
                    groups.append(current_group)
                current_group = [sentence]
                current_size = sentence_size
            else:
                current_group.append(sentence)
                current_size += sentence_size
        
        # æ·»åŠ æœ€åä¸€ç»„
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def _cluster_by_similarity(self, sentences: List[str], embeddings: List[List[float]]) -> List[List[str]]:
        """ä¼˜åŒ–çš„è¯­ä¹‰èšç±»ç®—æ³•ï¼šä½¿ç”¨è´ªå¿ƒç­–ç•¥å‡å°‘è®¡ç®—å¤æ‚åº¦"""
        if not embeddings or len(embeddings) != len(sentences):
            logger.warning("åµŒå…¥å‘é‡æ•°é‡ä¸å¥å­æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨å•å¥åˆ†ç»„")
            return [[sentence] for sentence in sentences]
        
        logger.info(f"å¼€å§‹ä¼˜åŒ–è¯­ä¹‰èšç±»ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
        
        # ä¼˜åŒ–çš„è´ªå¿ƒèšç±»ç®—æ³•
        groups = []
        used = set()
        similarity_matches = 0
        total_comparisons = 0
        
        for i, sentence in enumerate(sentences):
            if i in used:
                continue
            
            group = [sentence]
            used.add(i)
            current_size = len(sentence)
            group_similarities = []
            
            # ä¼˜åŒ–ï¼šåªæ£€æŸ¥é™„è¿‘çš„å¥å­ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
            window_size = min(20, len(sentences) - i - 1)  # é™åˆ¶æœç´¢çª—å£
            
            for offset in range(1, window_size + 1):
                j = i + offset
                if j >= len(sentences) or j in used:
                    continue
                
                other_sentence = sentences[j]
                if current_size + len(other_sentence) > self.max_chunk_size:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(embeddings[i], embeddings[j])
                total_comparisons += 1
                
                if similarity > self.similarity_threshold:
                    group.append(other_sentence)
                    used.add(j)
                    current_size += len(other_sentence)
                    group_similarities.append(similarity)
                    similarity_matches += 1
                    
                    # æ—©åœä¼˜åŒ–ï¼šå¦‚æœæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„å¥å­ï¼Œåœæ­¢æœç´¢
                    if len(group) >= 5:  # é™åˆ¶æ¯ç»„æœ€å¤§å¥å­æ•°
                        break
            
            if len(group) > 1:
                avg_similarity = sum(group_similarities) / len(group_similarities) if group_similarities else 0
                logger.debug(f"è¯­ä¹‰ç»„ {len(groups)+1}: {len(group)} ä¸ªå¥å­, å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            
            groups.append(group)
        
        logger.info(f"âœ… ä¼˜åŒ–èšç±»å®Œæˆ: {len(groups)} ä¸ªç»„, {similarity_matches} ä¸ªåŒ¹é…, {total_comparisons} æ¬¡æ¯”è¾ƒ")
        return groups
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    async def _evaluate_chunks(self, chunks: List[Document]) -> ChunkMetrics:
        """Evaluate semantic chunk quality"""
        metrics = ChunkMetrics()
        
        if not chunks:
            return metrics
        
        # Size evaluation
        target_size = self.config.chunk_size
        size_scores = []
        for chunk in chunks:
            size = len(chunk.content)
            if target_size * 0.5 <= size <= target_size * 1.5:
                size_scores.append(1.0)
            elif size < target_size * 0.5:
                size_scores.append(size / (target_size * 0.5))
            else:
                size_scores.append((target_size * 1.5) / size)
        
        metrics.size_score = sum(size_scores) / len(size_scores)
        
        # Coherence evaluation (semantic chunks should be highly coherent)
        metrics.coherence_score = 0.85  # Assume high coherence for semantic chunks
        
        # Completeness evaluation
        completeness_scores = []
        for chunk in chunks:
            content = chunk.content.strip()
            # Check for complete sentences
            ends_with_punctuation = content and content[-1] in '.!?'
            starts_with_capital = content and content[0].isupper()
            completeness_scores.append(0.5 * ends_with_punctuation + 0.5 * starts_with_capital)
        
        metrics.completeness_score = sum(completeness_scores) / len(completeness_scores)
        
        # Overlap score (semantic chunks typically have minimal overlap)
        metrics.overlap_score = 0.9
        
        # Boundary score (semantic boundaries are natural)
        metrics.boundary_score = 0.9
        
        metrics.calculate_overall_score()
        return metrics