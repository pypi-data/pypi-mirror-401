"""Knowledge Graph Builder - Main orchestrator for knowledge graph construction"""

import json
import os
from typing import Any, Dict, List, Optional, Union
from loguru import logger

from .config import GraphRagConfig, LLMConfig
from .models import Entity, Relationship, KnowledgeGraph, EntityType, RelationType
# Note: Traditional extractors removed - using SPO extraction only
from .spo_extractor import SPOExtractor
from .schema_generator import SchemaGenerator
from .validators import GraphQualityValidator
from .community import CommunityDetector
from .optimizer import GraphOptimizer


class KnowledgeGraphBuilder:
    """Main orchestrator for knowledge graph construction"""
    
    def __init__(self, config: GraphRagConfig, llm_config: LLMConfig):
        self.config = config
        self.llm_config = llm_config
        
        # Delayed import to avoid circular dependency
        from agenticx.llms import LlmFactory
        llm_client = LlmFactory.create_llm(self.llm_config)
        self.llm_client = llm_client  # ä¿å­˜ä¸ºå®žä¾‹å±žæ€§ï¼ˆè½»é‡æ¨¡åž‹ï¼‰
        
        # åˆ›å»ºå¼ºæ¨¡åž‹å®¢æˆ·ç«¯ï¼ˆç”¨äºŽæ–‡æ¡£åˆ†æžå’ŒSchemaç”Ÿæˆï¼‰
        try:
            # å°è¯•ä»Žé…ç½®ä¸­èŽ·å–å¼ºæ¨¡åž‹é…ç½®
            strong_model_config = getattr(self.config, 'strong_model_config', None)
            if strong_model_config:
                self.strong_llm_client = LlmFactory.create_llm(strong_model_config)
                # logger.info("ðŸš€ å¼ºæ¨¡åž‹å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            else:
                self.strong_llm_client = llm_client  # å›žé€€åˆ°é»˜è®¤æ¨¡åž‹
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å¼ºæ¨¡åž‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹")
        except Exception as e:
            logger.warning(f"âš ï¸ å¼ºæ¨¡åž‹åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹: {e}")
            self.strong_llm_client = llm_client
        
        # Note: Traditional extractors removed - using SPO extraction only
        
        # Initialize extraction method
        self.extraction_method = getattr(self.config, 'extraction_method', 'separate')
        
        # Initialize prompt manager - ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç›¸å¯¹è·¯å¾„
        prompts_dir = os.path.join(os.getcwd(), 'prompts')
        
        # åŠ¨æ€å¯¼å…¥PromptManagerï¼ˆé¿å…å¾ªçŽ¯å¯¼å…¥ï¼‰
        try:
            import sys
            sys.path.append(os.getcwd())
            from prompt_manager import PromptManager
            self.prompt_manager = PromptManager(prompts_dir)
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥PromptManager: {e}")
            self.prompt_manager = None
        
        # Initialize schema generator
        base_schema_path = os.path.join(os.getcwd(), 'schema.json')
        self.schema_generator = SchemaGenerator(
            llm_client=llm_client,
            strong_llm_client=self.strong_llm_client,  # ä¼ å…¥å¼ºæ¨¡åž‹å®¢æˆ·ç«¯
            prompt_manager=self.prompt_manager,
            base_schema_path=base_schema_path if os.path.exists(base_schema_path) else None
        )
        
        # Initialize SPO extractor (will be configured with custom schema later)
        if self.extraction_method == 'spo':
            self.spo_extractor = None  # Will be initialized with custom schema
            logger.info(f"ä½¿ç”¨ä¸¤é˜¶æ®µSPOæŠ½å–æ–¹æ³•ï¼ˆSchemaç”Ÿæˆ + SPOæŠ½å–ï¼‰")
        else:
            logger.info(f"ä½¿ç”¨ä¼ ç»Ÿåˆ†ç¦»æŠ½å–æ–¹æ³•")
        
        self.quality_validator = GraphQualityValidator(
            config=self.config.quality_validation.to_dict()
        )
        
        community_config = self.config.community_detection.to_dict()
        community_config["llm_client"] = llm_client
        self.community_detector = CommunityDetector(
            algorithm="louvain",
            config=community_config
        )
        
        self.graph_optimizer = GraphOptimizer(
            config=self.config.graph_optimization.to_dict()
        )
    
    async def build_from_texts(
        self, 
        texts: List[str], 
        metadata: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> KnowledgeGraph:
        """Build knowledge graph from a list of texts"""
        logger.info(f"å¼€å§‹æž„å»ºçŸ¥è¯†å›¾è°±ï¼Œè¾“å…¥æ–‡æœ¬æ•°é‡: {len(texts)}")
        
        # Initialize graph
        logger.debug("åˆå§‹åŒ–çŸ¥è¯†å›¾è°±")
        graph = KnowledgeGraph()
        
        # Stage 1: Generate custom schema if using SPO method
        custom_schema = None
        if self.extraction_method == 'spo':
            logger.info("é˜¶æ®µ1: æ™ºèƒ½Schemaç”Ÿæˆ")
            logger.info(f"æŠ½å–æ–¹æ³•: {self.extraction_method} (ä¸¤é˜¶æ®µSPOæŠ½å–)")
            
            # ç›´æŽ¥åŸºäºŽå®Œæ•´æ–‡æ¡£ç”Ÿæˆå®šåˆ¶schemaï¼ˆæ–°æ–¹æ³•ï¼‰
            logger.info("å¼€å§‹åŸºäºŽå®Œæ•´æ–‡æ¡£ç”Ÿæˆå®šåˆ¶Schema...")
            custom_schema = self.schema_generator.generate_custom_schema_from_documents(texts)
            
            # Save custom schema for reference
            custom_schema_path = os.path.join(os.getcwd(), 'custom_schema.json')
            self.schema_generator.save_custom_schema(custom_schema, custom_schema_path)
            logger.info(f"å®šåˆ¶Schemaå·²ä¿å­˜: {custom_schema_path}")
            
            # Initialize SPO extractor with custom schema
            logger.info("åˆå§‹åŒ–SPOæŠ½å–å™¨...")
            self.spo_extractor = SPOExtractor(
                llm_client=self.llm_client,
                prompt_manager=self.prompt_manager,
                custom_schema=custom_schema,
                config=self.config.entity_extraction.to_dict()
            )
            
            logger.success(f"âœ… é˜¶æ®µ1å®Œæˆ - å®šåˆ¶Schemaç”Ÿæˆï¼Œé¢†åŸŸ: {custom_schema.get('domain_info', {}).get('primary_domain', 'é€šç”¨')}")
        else:
            logger.info(f"æŠ½å–æ–¹æ³•: {self.extraction_method} (ä¼ ç»Ÿåˆ†ç¦»æŠ½å–)")
        
        # Stage 2: Extract entities and relationships
        logger.info("é˜¶æ®µ2: çŸ¥è¯†æŠ½å–")
        
        if self.extraction_method == 'spo':
            # ä½¿ç”¨æ‰¹å¤„ç†SPOæŠ½å–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            # logger.info("ðŸš€ ä½¿ç”¨æ‰¹å¤„ç†SPOæŠ½å–ï¼Œæ˜¾è‘—æå‡æ€§èƒ½")
            batch_size = getattr(self.config, 'spo_batch_size', 1)  # ä»Žé…ç½®èŽ·å–æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1é¿å…ç½‘ç»œé—®é¢˜
            
            try:
                entities, relationships = await self.spo_extractor.extract_batch(
                    texts=texts, 
                    batch_size=batch_size,
                    **kwargs
                )
                
                logger.info(f"æ‰¹å¤„ç†SPOæŠ½å–å®Œæˆ: {len(entities)} ä¸ªå®žä½“, {len(relationships)} ä¸ªå…³ç³»")
                
                # æ‰¹é‡æ·»åŠ å®žä½“åˆ°å›¾è°±
                for entity in entities:
                    graph.add_entity(entity)
                
                # æ‰¹é‡æ·»åŠ å…³ç³»åˆ°å›¾è°±
                for relationship in relationships:
                    try:
                        graph.add_relationship(relationship)
                    except Exception as e:
                        logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†SPOæŠ½å–å¤±è´¥ï¼Œå›žé€€åˆ°é€ä¸ªå¤„ç†: {e}")
                # å›žé€€åˆ°åŽŸæ¥çš„é€ä¸ªå¤„ç†æ–¹å¼
                for i, text in enumerate(texts):
                    chunk_id = f"chunk_{i}"
                    logger.debug(f"å¤„ç†æ–‡æœ¬å— {i+1}/{len(texts)} (ID: {chunk_id})")
                    
                    entities, relationships = self.spo_extractor.extract(text, chunk_id=chunk_id)
                    
                    for entity in entities:
                        graph.add_entity(entity)
                    
                    for relationship in relationships:
                        try:
                            graph.add_relationship(relationship)
                        except Exception as rel_e:
                            logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {rel_e}")
        
        else:
            # ä¼ ç»Ÿåˆ†ç¦»æŠ½å–æ¨¡å¼å·²ç§»é™¤ï¼Œå¼ºåˆ¶ä½¿ç”¨SPOæ¨¡å¼
            logger.error(f"âŒ ä¸æ”¯æŒçš„æŠ½å–æ–¹æ³•: {self.extraction_method}")
            logger.error("ðŸ’¡ ä¼ ç»Ÿåˆ†ç¦»æŠ½å–æ¨¡å¼å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨ 'spo' æ¨¡å¼")
            raise ValueError(f"ä¸æ”¯æŒçš„æŠ½å–æ–¹æ³•: {self.extraction_method}ï¼Œè¯·ä½¿ç”¨ 'spo' æ¨¡å¼")
        
        # Post-processing
        logger.info("å¼€å§‹åŽå¤„ç†")
        
        # Merge duplicate entities
        if kwargs.get("merge_entities", True):
            logger.debug("ðŸ”„ åˆå¹¶é‡å¤å®žä½“")
            merged_count = self._merge_duplicate_entities(graph)
            logger.debug(f"âœ… åˆå¹¶äº† {merged_count} ä¸ªé‡å¤å®žä½“")
        
        # Validate quality
        if kwargs.get("validate_quality", True):
            logger.debug("è¿›è¡Œè´¨é‡éªŒè¯")
            quality_report = self.quality_validator.validate(graph)
            logger.info(f"è´¨é‡éªŒè¯ç»“æžœ: {quality_report.summary()}")
        
        # Detect communities
        if kwargs.get("detect_communities", False):
            logger.debug("ðŸ‘¥ æ£€æµ‹ç¤¾åŒº")
            self.community_detector.detect_communities(graph)
        
        # Optimize graph
        if kwargs.get("optimize_graph", True):
            logger.debug("âš¡ ä¼˜åŒ–å›¾è°±")
            optimization_stats = self.graph_optimizer.optimize(graph)
            logger.info(f"âš¡ å›¾è°±ä¼˜åŒ–ç»“æžœ: {optimization_stats}")
        
        logger.success(f"ðŸŽ‰ çŸ¥è¯†å›¾è°±æž„å»ºå®Œæˆï¼å®žä½“æ•°é‡: {len(graph.entities)}, å…³ç³»æ•°é‡: {len(graph.relationships)}")
        
        # Auto export to Neo4j if enabled
        if self.config.neo4j.enabled and self.config.neo4j.auto_export:
            try:
                logger.info("ðŸ—„ï¸ è‡ªåŠ¨å¯¼å‡ºåˆ°Neo4j...")
                graph.export_to_neo4j(
                    uri=self.config.neo4j.uri,
                    username=self.config.neo4j.username,
                    password=self.config.neo4j.password,
                    database=self.config.neo4j.database,
                    clear_existing=self.config.neo4j.clear_on_export,
                    tenant_id=graph.name  # Use graph name as tenant_id
                )
                logger.success("âœ… Neo4jå¯¼å‡ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Neo4jå¯¼å‡ºå¤±è´¥: {e}")
                logger.warning("ðŸ’¡ è¯·æ£€æŸ¥Neo4jæœåŠ¡æ˜¯å¦è¿è¡Œï¼Œä»¥åŠè¿žæŽ¥é…ç½®æ˜¯å¦æ­£ç¡®")
        
        return graph
    
    def build_from_documents(
        self, 
        documents: List[Dict[str, Any]], 
        **kwargs
    ) -> KnowledgeGraph:
        """Build knowledge graph from structured documents"""
        texts = [doc.get("content", "") for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        
        return self.build_from_texts(texts, metadata, **kwargs)
    
    async def build_incremental(
        self, 
        existing_graph: KnowledgeGraph,
        new_texts: List[str],
        **kwargs
    ) -> KnowledgeGraph:
        """Incrementally build upon existing knowledge graph"""
        logger.info(f"ðŸ”„ å¢žé‡æž„å»º: å‘çŽ°æœ‰å›¾è°±({len(existing_graph.entities)}ä¸ªå®žä½“)æ·»åŠ {len(new_texts)}ä¸ªæ–°æ–‡æœ¬")
        
        # Create new graph from existing one
        new_graph = KnowledgeGraph()
        new_graph.entities = existing_graph.entities.copy()
        new_graph.relationships = existing_graph.relationships.copy()
        new_graph.metadata = existing_graph.metadata.copy()
        
        # Copy NetworkX graph
        new_graph.graph = existing_graph.graph.copy()
        
        # Process new texts using SPO extraction
        if self.spo_extractor:
            try:
                # Use batch SPO extraction for incremental texts
                entities, relationships = await self.spo_extractor.extract_batch(
                    texts=new_texts,
                    batch_size=1,  # Conservative batch size for incremental
                    **kwargs
                )
                
                # Add extracted entities and relationships
                for entity in entities:
                    new_graph.add_entity(entity)
                
                for relationship in relationships:
                    try:
                        new_graph.add_relationship(relationship)
                    except Exception as e:
                        logger.error(f"âŒ å¢žé‡æ·»åŠ å…³ç³»å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.error(f"âŒ å¢žé‡SPOæŠ½å–å¤±è´¥: {e}")
                logger.warning("ðŸ’¡ å¢žé‡æž„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥SPOæŠ½å–å™¨é…ç½®")
        else:
            logger.error("âŒ SPOæŠ½å–å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œå¢žé‡æž„å»º")
            raise ValueError("SPOæŠ½å–å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œå¢žé‡æž„å»º")
        
        # Post-processing for incremental build
        if kwargs.get("merge_entities", True):
            self._merge_duplicate_entities(new_graph)
        
        if kwargs.get("validate_quality", True):
            quality_report = self.quality_validator.validate(new_graph)
            logger.info(f"å¢žé‡è´¨é‡éªŒè¯: {quality_report.summary()}")
        
        if kwargs.get("optimize_graph", True):
            optimization_stats = self.graph_optimizer.optimize(new_graph)
            logger.info(f"âš¡ å¢žé‡å›¾è°±ä¼˜åŒ–: {optimization_stats}")
        
        logger.success(f"âœ… å¢žé‡æž„å»ºå®Œæˆ: {len(new_graph.entities)} ä¸ªå®žä½“, {len(new_graph.relationships)} ä¸ªå…³ç³»")
        
        return new_graph
    
    def _find_entity_by_name(self, graph: KnowledgeGraph, name: str) -> Optional[Entity]:
        """é€šè¿‡åç§°æŸ¥æ‰¾å®žä½“"""
        for entity in graph.entities.values():
            if entity.name == name:
                return entity
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆåŽ»é™¤ç©ºæ ¼å’Œå¤§å°å†™ï¼‰
            if entity.name.strip().lower() == name.strip().lower():
                return entity
        return None
    
    def _merge_duplicate_entities(self, graph: KnowledgeGraph) -> int:
        """Merge duplicate entities based on name similarity"""
        merged_count = 0
        processed_pairs = set()
        
        entity_list = list(graph.entities.values())
        
        for i, entity1 in enumerate(entity_list):
            for j, entity2 in enumerate(entity_list[i+1:], i+1):
                pair_key = tuple(sorted([entity1.id, entity2.id]))
                
                if pair_key in processed_pairs:
                    continue
                
                processed_pairs.add(pair_key)
                
                # Check if entities are similar enough to merge
                if self._should_merge_entities(entity1, entity2):
                    self._merge_two_entities(graph, entity1.id, entity2.id)
                    merged_count += 1
        
        logger.debug(f"ðŸ”„ åˆå¹¶äº† {merged_count} ä¸ªé‡å¤å®žä½“")
        return merged_count
    
    def _should_merge_entities(self, entity1: Entity, entity2: Entity) -> bool:
        """Determine if two entities should be merged"""
        # Check name similarity
        name_similarity = self._calculate_name_similarity(entity1.name, entity2.name)
        
        # Check type compatibility
        type_compatible = entity1.entity_type == entity2.entity_type
        
        # Check if they have similar contexts (simple heuristic)
        context_similarity = self._calculate_context_similarity(entity1, entity2)
        
        # Merge if names are very similar and types are compatible
        return name_similarity >= 0.8 and type_compatible and context_similarity >= 0.5
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between two entity names"""
        name1_lower = name1.lower().strip()
        name2_lower = name2.lower().strip()
        
        # Exact match
        if name1_lower == name2_lower:
            return 1.0
        
        # One is substring of another
        if name1_lower in name2_lower or name2_lower in name1_lower:
            return 0.9
        
        # Calculate Jaccard similarity of words
        words1 = set(name1_lower.split())
        words2 = set(name2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_context_similarity(self, entity1: Entity, entity2: Entity) -> float:
        """Calculate context similarity between entities"""
        # Simple context similarity based on attributes
        attr1 = set(entity1.attributes.keys())
        attr2 = set(entity2.attributes.keys())
        
        if not attr1 or not attr2:
            return 0.5
        
        intersection = len(attr1.intersection(attr2))
        union = len(attr1.union(attr2))
        
        return intersection / union if union > 0 else 0.0
    
    def _merge_two_entities(self, graph: KnowledgeGraph, entity_id1: str, entity_id2: str) -> None:
        """Merge two entities into one"""
        entity1 = graph.get_entity(entity_id1)
        entity2 = graph.get_entity(entity_id2)
        
        if not entity1 or not entity2:
            return
        
        # Keep the entity with higher confidence
        if entity1.confidence >= entity2.confidence:
            keep_entity = entity1
            remove_entity = entity2
            keep_id = entity_id1
            remove_id = entity_id2
        else:
            keep_entity = entity2
            remove_entity = entity1
            keep_id = entity_id2
            remove_id = entity_id1
        
        # Merge attributes
        keep_entity.attributes.update(remove_entity.attributes)
        
        # Merge source chunks
        keep_entity.source_chunks.update(remove_entity.source_chunks)
        
        # Update confidence if merged
        keep_entity.confidence = max(entity1.confidence, entity2.confidence)
        
        # Update relationships
        for rel in graph.relationships.values():
            if rel.source_entity_id == remove_id:
                rel.source_entity_id = keep_id
            if rel.target_entity_id == remove_id:
                rel.target_entity_id = keep_id
        
        # Remove the merged entity
        del graph.entities[remove_id]
        graph.graph.remove_node(remove_id)
    
    def add_metadata(self, graph: KnowledgeGraph, metadata: Dict[str, Any]) -> None:
        """Add metadata to knowledge graph"""
        graph.metadata.update(metadata)
    
    def get_build_statistics(self, graph: KnowledgeGraph) -> Dict[str, Any]:
        """Get statistics about the built knowledge graph"""
        return {
            "num_entities": len(graph.entities),
            "num_relationships": len(graph.relationships),
            "num_entity_types": len(set(entity.entity_type for entity in graph.entities.values())),
            "num_relation_types": len(set(rel.relation_type for rel in graph.relationships.values())),
            "average_entity_confidence": sum(entity.confidence for entity in graph.entities.values()) / len(graph.entities) if graph.entities else 0,
            "average_relationship_confidence": sum(rel.confidence for rel in graph.relationships.values()) / len(graph.relationships) if graph.relationships else 0,
            "num_communities": len([entity for entity in graph.entities.values() if entity.entity_type == EntityType.COMMUNITY]),
            "graph_density": nx.density(graph.graph) if graph.graph.number_of_nodes() > 0 else 0,
            "num_connected_components": nx.number_connected_components(graph.graph) if graph.graph.number_of_nodes() > 0 else 0
        }