# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import enum
import numpy
import numpy.typing
import typing

class Dataset:
    r"""
    Internal dataset holding features, labels, and optional metadata.
    
    This is a low-level binding that accepts pre-validated numpy arrays.
    The Python `Dataset` class extends this to provide user-friendly
    constructors with DataFrame support, type conversion, and validation.
    
    Note:
        This class expects C-contiguous float32 arrays. The Python subclass
        handles all type conversion, validation, and DataFrame support.
    
    Attributes:
        n_samples: Number of samples in the dataset.
        n_features: Number of features in the dataset.
        has_labels: Whether labels are present.
        has_weights: Whether weights are present.
        feature_names: Feature names if provided.
        categorical_features: Indices of categorical features.
        shape: Shape as (n_samples, n_features).
    """
    @property
    def n_samples(self) -> builtins.int:
        r"""
        Number of samples in the dataset.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features in the dataset.
        """
    @property
    def has_labels(self) -> builtins.bool:
        r"""
        Whether labels are present.
        """
    @property
    def has_weights(self) -> builtins.bool:
        r"""
        Whether weights are present.
        """
    @property
    def feature_names(self) -> typing.Optional[builtins.list[builtins.str]]:
        r"""
        Feature names if provided.
        """
    @property
    def categorical_features(self) -> builtins.list[builtins.int]:
        r"""
        Indices of categorical features.
        """
    @property
    def shape(self) -> tuple[builtins.int, builtins.int]:
        r"""
        Shape of the features array as (n_samples, n_features).
        """
    def __new__(cls, features: numpy.typing.NDArray[numpy.float32], labels: typing.Optional[numpy.typing.NDArray[numpy.float32]] = None, weights: typing.Optional[numpy.typing.NDArray[numpy.float32]] = None, feature_names: typing.Optional[typing.Sequence[builtins.str]] = None, categorical_features: typing.Optional[typing.Sequence[builtins.int]] = None) -> Dataset:
        r"""
        Create a new Dataset from pre-validated numpy arrays.
        
        Args:
            features: C-contiguous float32 array of shape (n_samples, n_features).
            labels: C-contiguous float32 array of shape (n_outputs, n_samples), or None.
            weights: C-contiguous float32 array of shape (n_samples,), or None.
            feature_names: List of feature names, or None.
            categorical_features: List of categorical feature indices, or None.
        
        Returns:
            Dataset ready for training or prediction.
        """
    def __repr__(self) -> builtins.str: ...

class DatasetBuilder:
    r"""
    Builder for constructing datasets with flexible feature types.
    
    DatasetBuilder collects features (via `add_feature`) and builds a Dataset.
    Features can be dense or sparse, with per-feature metadata.
    
    Example:
        >>> from boosters import DatasetBuilder, Feature
        >>> builder = DatasetBuilder()
        >>> builder.add_feature(Feature.dense(age_array, name="age"))
        >>> builder.add_feature(Feature.sparse(idx, vals, n, name="rare"))
        >>> builder.labels(y)
        >>> dataset = builder.build()
    """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features added so far.
        """
    def __new__(cls) -> DatasetBuilder:
        r"""
        Create a new empty DatasetBuilder.
        """
    def add_feature(self, feature: Feature) -> DatasetBuilder:
        r"""
        Add a feature to the builder.
        
        Args:
            feature: A Feature object (created via Feature.dense() or Feature.sparse()).
        
        Returns:
            Self for method chaining.
        """
    def labels_1d(self, labels: numpy.typing.NDArray[numpy.float32]) -> DatasetBuilder:
        r"""
        Set 1D target labels.
        
        Args:
            labels: 1D float32 array of labels (one per sample).
        
        Returns:
            Self for method chaining.
        """
    def labels_2d(self, labels: numpy.typing.NDArray[numpy.float32]) -> DatasetBuilder:
        r"""
        Set 2D multi-output target labels.
        
        Args:
            labels: 2D float32 array of shape (n_outputs, n_samples).
        
        Returns:
            Self for method chaining.
        """
    def weights(self, weights: numpy.typing.NDArray[numpy.float32]) -> DatasetBuilder:
        r"""
        Set sample weights.
        
        Args:
            weights: 1D float32 array of weights (one per sample).
        
        Returns:
            Self for method chaining.
        """
    def build(self) -> Dataset:
        r"""
        Build the dataset.
        
        Validates all features have the same sample count, indices are valid,
        and targets/weights match the sample count.
        
        Returns:
            Constructed Dataset ready for training or prediction.
        
        Raises:
            ValueError: If validation fails (shape mismatch, invalid sparse indices, etc.)
        """
    def __repr__(self) -> builtins.str: ...

class Feature:
    r"""
    A single feature column with metadata.
    
    Features can be dense (array of values) or sparse (indices + values + default).
    The Python wrapper handles all type conversions; this class just holds validated data.
    
    Attributes:
        name: Optional feature name.
        categorical: Whether this feature is categorical.
        n_samples: Number of samples in this feature.
        is_sparse: Whether this is a sparse feature.
    
    Example:
        >>> from boosters import Feature
        >>> # Dense feature
        >>> f = Feature.dense(np.array([1.0, 2.0, 3.0], dtype=np.float32), name="age")
        >>> # Sparse feature
        >>> f = Feature.sparse(indices, values, n_samples=1000, name="rare", default=0.0)
    """
    @property
    def name(self) -> typing.Optional[builtins.str]:
        r"""
        Feature name (if provided).
        """
    @property
    def categorical(self) -> builtins.bool:
        r"""
        Whether this feature is categorical.
        """
    @property
    def n_samples(self) -> builtins.int:
        r"""
        Number of samples in this feature.
        """
    @property
    def is_sparse(self) -> builtins.bool:
        r"""
        Whether this is a sparse feature.
        """
    @staticmethod
    def dense(values: numpy.typing.NDArray[numpy.float32], name: typing.Optional[builtins.str] = None, categorical: builtins.bool = False) -> Feature:
        r"""
        Create a dense feature from a 1D float32 array.
        
        Args:
            values: 1D float32 array of values (one per sample).
            name: Optional feature name.
            categorical: Whether this feature is categorical. Default: False.
        
        Returns:
            A new dense Feature.
        """
    @staticmethod
    def sparse(indices: numpy.typing.NDArray[numpy.uint32], values: numpy.typing.NDArray[numpy.float32], n_samples: builtins.int, name: typing.Optional[builtins.str] = None, default: builtins.float = 0.0, categorical: builtins.bool = False) -> Feature:
        r"""
        Create a sparse feature.
        
        Args:
            indices: 1D uint32 array of row indices with non-default values.
                Must be sorted in ascending order with no duplicates.
            values: 1D float32 array of values at those indices.
            n_samples: Total number of samples in the dataset.
            name: Optional feature name.
            default: Default value for unspecified indices. Default: 0.0.
            categorical: Whether this feature is categorical. Default: False.
        
        Returns:
            A new sparse Feature.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTConfig:
    r"""
    Main configuration for GBDT model.
    
    This is the primary configuration class for gradient boosted decision trees.
    All parameters are flat (no nested config objects) matching the core Rust API.
    
    # Arguments
    
    * `n_estimators` - Number of boosting rounds (trees to train). Default: 100.
    * `learning_rate` - Step size shrinkage (0.01 - 0.3 typical). Default: 0.3.
    * `objective` - Loss function for training. Default: Objective.Squared().
    * `metric` - Evaluation metric. None uses objective's default.
    * `growth_strategy` - Tree growth strategy. Default: GrowthStrategy.Depthwise.
    * `max_depth` - Maximum tree depth (only for depthwise). Default: 6.
    * `n_leaves` - Maximum leaves (only for leafwise). Default: 31.
    * `max_onehot_cats` - Max categories for one-hot encoding. Default: 4.
    * `l1` - L1 regularization on leaf weights. Default: 0.0.
    * `l2` - L2 regularization on leaf weights. Default: 1.0.
    * `min_gain_to_split` - Minimum gain required to make a split. Default: 0.0.
    * `min_child_weight` - Minimum sum of hessians in a leaf. Default: 1.0.
    * `min_samples_leaf` - Minimum samples in a leaf. Default: 1.
    * `subsample` - Row subsampling ratio per tree. Default: 1.0.
    * `colsample_bytree` - Column subsampling per tree. Default: 1.0.
    * `colsample_bylevel` - Column subsampling per level. Default: 1.0.
    * `linear_leaves` - Enable linear models in leaves (experimental). Default: False.
    * `linear_l2` - L2 regularization for linear coefficients. Default: 0.01.
    * `linear_l1` - L1 regularization for linear coefficients. Default: 0.0.
    * `early_stopping_rounds` - Stop if no improvement for this many rounds.
    * `seed` - Random seed for reproducibility. Default: 42.
    
    # Example (Python)
    
    ```text
    config = GBDTConfig(
        n_estimators=500,
        learning_rate=0.1,
        objective=Objective.logistic(),
        max_depth=6,
        l2=1.0,
    )
    ```
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size shrinkage).
        """
    @property
    def growth_strategy(self) -> GrowthStrategy:
        r"""
        Growth strategy for tree building.
        """
    @property
    def max_depth(self) -> builtins.int:
        r"""
        Maximum depth of tree (for depthwise growth).
        """
    @property
    def n_leaves(self) -> builtins.int:
        r"""
        Maximum number of leaves (for leafwise growth).
        """
    @property
    def max_onehot_cats(self) -> builtins.int:
        r"""
        Maximum categories for one-hot encoding categorical splits.
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization on leaf weights.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization on leaf weights.
        """
    @property
    def min_gain_to_split(self) -> builtins.float:
        r"""
        Minimum gain required to make a split.
        """
    @property
    def min_child_weight(self) -> builtins.float:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    @property
    def min_samples_leaf(self) -> builtins.int:
        r"""
        Minimum number of samples required in a leaf.
        """
    @property
    def subsample(self) -> builtins.float:
        r"""
        Row subsampling ratio per tree.
        """
    @property
    def colsample_bytree(self) -> builtins.float:
        r"""
        Column subsampling ratio per tree.
        """
    @property
    def colsample_bylevel(self) -> builtins.float:
        r"""
        Column subsampling ratio per level.
        """
    @property
    def linear_leaves(self) -> builtins.bool:
        r"""
        Enable linear models in leaves.
        """
    @property
    def linear_l2(self) -> builtins.float:
        r"""
        L2 regularization for linear coefficients.
        """
    @property
    def linear_l1(self) -> builtins.float:
        r"""
        L1 regularization for linear coefficients.
        """
    @property
    def linear_max_iterations(self) -> builtins.int:
        r"""
        Maximum coordinate descent iterations for linear leaves.
        """
    @property
    def linear_tolerance(self) -> builtins.float:
        r"""
        Convergence tolerance for linear leaves.
        """
    @property
    def linear_min_samples(self) -> builtins.int:
        r"""
        Minimum samples required to fit linear model in leaf.
        """
    @property
    def linear_coefficient_threshold(self) -> builtins.float:
        r"""
        Threshold for pruning small coefficients.
        """
    @property
    def linear_max_features(self) -> builtins.int:
        r"""
        Maximum features in linear model per leaf.
        """
    @property
    def linear_use_global_features(self) -> builtins.bool:
        r"""
        Use global features instead of path features for linear models.
        When true, uses the top-k most frequently split features for all leaves.
        This can improve extrapolation by ensuring important features are always included.
        """
    @property
    def linear_skip_first_n_trees(self) -> builtins.int:
        r"""
        Number of initial trees to skip linear leaf fitting.
        Default is 1 (first tree has homogeneous gradients). Set to 0 to enable from first tree.
        """
    @property
    def max_bins(self) -> builtins.int:
        r"""
        Maximum bins per feature for binning (1-256).
        """
    @property
    def enable_bundling(self) -> builtins.bool:
        r"""
        Enable exclusive feature bundling for sparse data. Default: true.
        Bundles sparse/one-hot features to reduce memory and speed up training.
        """
    @property
    def sparsity_threshold(self) -> builtins.float:
        r"""
        Sparsity threshold (fraction of zeros to use sparse storage).
        Features with density â‰¤ (1 - threshold) are considered sparse.
        """
    @property
    def max_categorical_cardinality(self) -> builtins.int:
        r"""
        Max cardinality to auto-detect as categorical.
        Features with â‰¤ this many unique integer values may be treated as categorical.
        """
    @property
    def binning_sample_cnt(self) -> builtins.int:
        r"""
        Number of samples for computing bin boundaries (for large datasets).
        """
    @property
    def cache_size(self) -> builtins.int:
        r"""
        Histogram cache size (number of slots). Default: 8.
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def verbosity(self) -> Verbosity:
        r"""
        Verbosity level for training output.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.30000001192092896, objective: Objective | None = None, metric: Metric | None = None, growth_strategy: GrowthStrategy = GrowthStrategy.Depthwise, max_depth: builtins.int = 6, n_leaves: builtins.int = 31, max_onehot_cats: builtins.int = 4, l1: builtins.float = 0.0, l2: builtins.float = 1.0, min_gain_to_split: builtins.float = 0.0, min_child_weight: builtins.float = 1.0, min_samples_leaf: builtins.int = 1, subsample: builtins.float = 1.0, colsample_bytree: builtins.float = 1.0, colsample_bylevel: builtins.float = 1.0, linear_leaves: builtins.bool = False, linear_l2: builtins.float = 0.009999999776482582, linear_l1: builtins.float = 0.0, linear_max_iterations: builtins.int = 10, linear_tolerance: builtins.float = 1e-06, linear_min_samples: builtins.int = 50, linear_coefficient_threshold: builtins.float = 9.999999974752427e-07, linear_max_features: builtins.int = 10, linear_use_global_features: builtins.bool = False, linear_skip_first_n_trees: builtins.int = 1, max_bins: builtins.int = 256, enable_bundling: builtins.bool = True, sparsity_threshold: builtins.float = 0.8999999761581421, max_categorical_cardinality: builtins.int = 0, binning_sample_cnt: builtins.int = 200000, cache_size: builtins.int = 8, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42, verbosity: Verbosity = Verbosity.Silent) -> GBDTConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTModel:
    r"""
    Gradient Boosted Decision Tree model.
    
    This is the main model class for training and prediction with gradient
    boosted decision trees.
    
    Attributes:
        is_fitted: Whether the model has been fitted.
        n_trees: Number of trees in the fitted model.
        n_features: Number of features the model was trained on.
        config: Model configuration.
    
    Examples:
        >>> from boosters import GBDTModel, Dataset
        >>> train = Dataset(X, y)
        >>> model = GBDTModel().fit(train)
        >>> predictions = model.predict(train)
    """
    @property
    def n_trees(self) -> builtins.int:
        r"""
        Number of trees in the fitted model.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        """
    @property
    def feature_names(self) -> typing.Optional[builtins.list[builtins.str]]:
        r"""
        Feature names from training dataset (if provided).
        """
    @staticmethod
    def train(train: Dataset, config: typing.Optional[GBDTConfig] = None, val_set: typing.Optional[Dataset] = None, n_threads: builtins.int = 0) -> GBDTModel:
        r"""
        Train a new GBDT model.
        
        This matches the Rust API style: training is a class-level constructor.
        
        Args:
            train: Training dataset containing features and labels.
            config: Optional GBDTConfig. If not provided, uses default config.
            val_set: Optional validation dataset for early stopping and evaluation.
            n_threads: Number of threads for parallel training (0 = auto).
        
        Returns:
            Trained GBDTModel.
        """
    def feature_importance(self, importance_type: ImportanceType = ImportanceType.Split) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Get feature importance scores.
        
        Args:
            importance_type: Type of importance (ImportanceType.Split or ImportanceType.Gain).
        
        Returns:
            Array of importance scores, one per feature.
        """
    def shap_values(self, data: Dataset) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Compute SHAP values for feature contribution analysis.
        
        Args:
            data: Dataset containing features for SHAP computation.
        
        Returns:
            Array with shape (n_samples, n_features + 1, n_outputs).
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def __reduce__(self) -> tuple[typing.Any, tuple[bytes]]:
        r"""
        Pickle support: return (callable, args) for reconstruction.
        
        Returns:
            Tuple of (from_bytes function, (serialized_bytes,)).
        """
    def predict(self, data: Dataset, n_threads: builtins.int = 0) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make predictions on data.
        
        Returns transformed predictions (e.g., probabilities for classification).
        Output shape is (n_samples, n_outputs) - sklearn convention.
        
        Args:
            data: Dataset containing features for prediction.
            n_threads: Number of threads for parallel prediction (0 = auto).
        
        Returns:
            Predictions array with shape (n_samples, n_outputs).
        """
    def predict_raw(self, data: Dataset, n_threads: builtins.int = 0) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make raw (untransformed) predictions on data.
        
        Returns raw margin scores without transformation.
        Output shape is (n_samples, n_outputs) - sklearn convention.
        
        Args:
            data: Dataset containing features for prediction.
            n_threads: Number of threads for parallel prediction (0 = auto).
        
        Returns:
            Raw scores array with shape (n_samples, n_outputs).
        """
    def to_bytes(self) -> bytes:
        r"""
        Serialize model to binary bytes.
        
        Returns:
            Binary representation of the model (.bstr format).
        
        Raises:
            RuntimeError: If model is not fitted or serialization fails.
        """
    def to_json_bytes(self) -> bytes:
        r"""
        Serialize model to JSON bytes.
        
        Returns:
            UTF-8 JSON representation of the model (.bstr.json format).
        
        Raises:
            RuntimeError: If model is not fitted or serialization fails.
        """
    @staticmethod
    def from_bytes(data: bytes) -> GBDTModel:
        r"""
        Load model from binary bytes.
        
        Args:
            data: Binary bytes in .bstr format.
        
        Returns:
            Loaded GBDTModel instance.
        
        Raises:
            ValueError: If bytes are invalid or corrupted.
        """
    @staticmethod
    def from_json_bytes(data: bytes) -> GBDTModel:
        r"""
        Load model from JSON bytes.
        
        Args:
            data: UTF-8 JSON bytes in .bstr.json format.
        
        Returns:
            Loaded GBDTModel instance.
        
        Raises:
            ValueError: If JSON is invalid.
        """

@typing.final
class GBLinearConfig:
    r"""
    Main configuration for GBLinear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Args:
        n_estimators: Number of boosting rounds. Default: 100.
        learning_rate: Step size for weight updates. Default: 0.5.
        objective: Loss function for training. Default: Objective.Squared().
        metric: Evaluation metric. None uses objective's default.
        l1: L1 regularization (alpha). Encourages sparse weights. Default: 0.0.
        l2: L2 regularization (lambda). Prevents large weights. Default: 0.0.
        early_stopping_rounds: Stop if no improvement for this many rounds.
        seed: Random seed for reproducibility. Default: 42.
    
    Examples:
        >>> config = GBLinearConfig(
        ...     n_estimators=200,
        ...     learning_rate=0.3,
        ...     objective=Objective.logistic(),
        ...     l2=0.1,
        ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size).
        """
    @property
    def update_strategy(self) -> GBLinearUpdateStrategy:
        r"""
        Coordinate descent update strategy.
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization (alpha).
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization (lambda).
        """
    @property
    def max_delta_step(self) -> builtins.float:
        r"""
        Maximum per-coordinate Newton step (stability), in absolute value.
        
        Set to `0.0` to disable.
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def verbosity(self) -> Verbosity:
        r"""
        Verbosity level for training output.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.5, objective: Objective | None = None, metric: Metric | None = None, l1: builtins.float = 0.0, l2: builtins.float = 0.0, update_strategy: GBLinearUpdateStrategy = GBLinearUpdateStrategy.Shotgun, max_delta_step: builtins.float = 0.0, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42, verbosity: Verbosity = Verbosity.Silent) -> GBLinearConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBLinearModel:
    r"""
    Gradient Boosted Linear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Attributes:
        coef_: Model coefficients after fitting.
        intercept_: Model intercept after fitting.
        is_fitted: Whether the model has been trained.
        n_features_in_: Number of features seen during fit.
    
    Examples:
        >>> config = GBLinearConfig(n_estimators=50, learning_rate=0.3)
        >>> model = GBLinearModel(config=config).fit(train)
        >>> predictions = model.predict(X_test)
    """
    @property
    def n_features_in_(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        """
    @property
    def feature_names(self) -> typing.Optional[builtins.list[builtins.str]]:
        r"""
        Feature names from training dataset (if provided).
        """
    @property
    def coef_(self) -> typing.Any:
        r"""
        Model coefficients (weights).
        
        Returns:
            Array with shape (n_features,) for single-output models
            or (n_features, n_outputs) for multi-output models.
        """
    @property
    def intercept_(self) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Model intercept (bias).
        
        Returns:
            Array of shape (n_outputs,).
        """
    @staticmethod
    def train(train: Dataset, config: typing.Optional[GBLinearConfig] = None, val_set: typing.Optional[Dataset] = None, n_threads: builtins.int = 0) -> GBLinearModel:
        r"""
        Train a new GBLinear model.
        
        This matches the Rust API style: training is a class-level constructor.
        
        Args:
            train: Training dataset containing features and labels.
            config: Optional GBLinearConfig. If not provided, uses default config.
            val_set: Optional validation dataset for early stopping and evaluation.
            n_threads: Number of threads for parallel training (0 = auto).
        
        Returns:
            Trained GBLinearModel.
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def __reduce__(self) -> tuple[typing.Any, tuple[bytes]]:
        r"""
        Pickle support: return (callable, args) for reconstruction.
        
        Returns:
            Tuple of (from_bytes function, (serialized_bytes,)).
        """
    def predict(self, data: Dataset, n_threads: builtins.int = 0) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        Output shape is (n_samples, n_outputs) - sklearn convention.
        
        Args:
            data: Dataset containing features.
            n_threads: Number of threads (unused, for API consistency).
        
        Returns:
            Predictions array with shape (n_samples, n_outputs).
        """
    def predict_raw(self, data: Dataset, n_threads: builtins.int = 0) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        Output shape is (n_samples, n_outputs) - sklearn convention.
        
        Args:
            data: Dataset containing features.
            n_threads: Number of threads (unused, for API consistency).
        
        Returns:
            Raw scores array with shape (n_samples, n_outputs).
        """
    def to_bytes(self) -> bytes:
        r"""
        Serialize model to binary bytes.
        
        Returns:
            Binary representation of the model (.bstr format).
        
        Raises:
            RuntimeError: If model is not fitted or serialization fails.
        """
    def to_json_bytes(self) -> bytes:
        r"""
        Serialize model to JSON bytes.
        
        Returns:
            UTF-8 JSON representation of the model (.bstr.json format).
        
        Raises:
            RuntimeError: If model is not fitted or serialization fails.
        """
    @staticmethod
    def from_bytes(data: bytes) -> GBLinearModel:
        r"""
        Load model from binary bytes.
        
        Args:
            data: Binary bytes in .bstr format.
        
        Returns:
            Loaded GBLinearModel instance.
        
        Raises:
            ValueError: If bytes are invalid or corrupted.
        """
    @staticmethod
    def from_json_bytes(data: bytes) -> GBLinearModel:
        r"""
        Load model from JSON bytes.
        
        Args:
            data: UTF-8 JSON bytes in .bstr.json format.
        
        Returns:
            Loaded GBLinearModel instance.
        
        Raises:
            ValueError: If JSON is invalid.
        """

@typing.final
class Model:
    r"""
    Namespace for polymorphic model loading and inspection.
    
    Prefer this over a global `loads()` function.
    
    Examples:
        >>> data = model.to_bytes()
        >>> loaded = boosters.Model.load_from_bytes(data)
        >>> info = boosters.Model.inspect_bytes(data)
    """
    @staticmethod
    def load_from_bytes(data: bytes) -> GBDTModel | GBLinearModel:
        r"""
        Load a model from binary or JSON bytes, auto-detecting the format and model type.
        
        Args:
            data: Binary bytes (.bstr format) or UTF-8 JSON bytes (.bstr.json format).
        
        Returns:
            The loaded model (either GBDTModel or GBLinearModel).
        
        Raises:
            ValueError: If the data is invalid, corrupted, or uses an unsupported format.
        """
    @staticmethod
    def inspect_bytes(data: bytes) -> ModelInfo:
        r"""
        Inspect serialized model bytes without fully deserializing.
        
        Args:
            data: Binary bytes (.bstr format) or UTF-8 JSON bytes (.bstr.json format).
        
        Returns:
            ModelInfo object with schema_version, model_type, format, and is_compressed.
        
        Raises:
            ValueError: If the data is invalid or cannot be inspected.
        """

@typing.final
class ModelInfo:
    r"""
    Information about a serialized model, obtained without full deserialization.
    
    This is useful for quick inspection of model files to determine their type,
    format, and schema version before loading the full model.
    
    Attributes:
        schema_version: The bstr schema version (e.g., 1).
        model_type: The type of model ("gbdt" or "gblinear").
        format: The serialization format ("binary" or "json").
        is_compressed: Whether the binary payload is compressed (always False for JSON).
    
    Examples:
        >>> data = model.to_bytes()
        >>> info = boosters.Model.inspect_bytes(data)
        >>> print(f"Model type: {info.model_type}, version: {info.schema_version}")
    """
    @property
    def schema_version(self) -> builtins.int:
        r"""
        Schema version (e.g., 1).
        """
    @property
    def model_type(self) -> builtins.str:
        r"""
        Model type string ("gbdt" or "gblinear").
        """
    @property
    def format(self) -> builtins.str:
        r"""
        Format string ("binary" or "json").
        """
    @property
    def is_compressed(self) -> builtins.bool:
        r"""
        Whether the payload is compressed (binary only).
        """
    def __repr__(self) -> builtins.str: ...

class ReadError(builtins.ValueError):
    r"""
    Exception raised when reading a serialized model fails.
    
    This exception is raised when model deserialization encounters:
    - Invalid file format (wrong magic number)
    - Corrupted data (checksum mismatch)
    - Unsupported schema version
    - Invalid JSON structure
    
    Examples:
        >>> try:
        ...     model = boosters.Model.load_from_bytes(invalid_bytes)
        ... except boosters.ReadError as e:
        ...     print(f'Failed to load model: {e}')
    """
    ...

@typing.final
class GBLinearUpdateStrategy(enum.Enum):
    r"""
    Coordinate descent update strategy for GBLinear.
    
    Attributes:
        Shotgun: Parallel (shotgun) coordinate descent (fast, approximate).
        Sequential: Sequential coordinate descent (slower, deterministic).
    """
    Shotgun = ...
    r"""
    Parallel (shotgun) coordinate descent.
    """
    Sequential = ...
    r"""
    Sequential coordinate descent.
    """

    def __str__(self) -> builtins.str: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GrowthStrategy(enum.Enum):
    r"""
    Tree growth strategy for building decision trees.
    
    Determines the order in which nodes are expanded during tree construction.
    Acts like a Python StrEnum - can be compared to strings.
    
    Attributes:
        Depthwise: Grow tree level-by-level (like XGBoost).
            All nodes at depth d are expanded before any node at depth d+1.
            More balanced trees, better for shallow trees.
        Leafwise: Grow tree by best-first split (like LightGBM).
            Always expands the leaf with highest gain.
            Can produce deeper, more accurate trees but risks overfitting.
    
    Examples:
        >>> from boosters import GrowthStrategy
        >>> strategy = GrowthStrategy.Depthwise
        >>> strategy == "depthwise"
        True
        >>> str(strategy)
        'depthwise'
    """
    Depthwise = ...
    r"""
    Grow tree level-by-level (like XGBoost).
    """
    Leafwise = ...
    r"""
    Grow tree by best-first split (like LightGBM).
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation like StrEnum.
        """
    def __repr__(self) -> builtins.str: ...
    def __hash__(self) -> builtins.int:
        r"""
        Hash using the string value for StrEnum-like behavior.
        """
    def __reduce__(self) -> tuple[typing.Any, tuple[builtins.str]]:
        r"""
        Pickle support: reduce to module path and variant name.
        """
    def __deepcopy__(self, _memo: typing.Any) -> GrowthStrategy:
        r"""
        Deepcopy support - return self since enum variants are singletons.
        """
    def __copy__(self) -> GrowthStrategy:
        r"""
        Copy support - return self since enum variants are singletons.
        """

@typing.final
class ImportanceType(enum.Enum):
    r"""
    Type of feature importance to compute.
    
    Attributes:
        Split: Number of times each feature is used in splits.
        Gain: Total gain from splits using each feature.
        AverageGain: Average gain per split (Gain / Split count).
        Cover: Total cover (hessian sum) at nodes splitting on each feature.
        AverageCover: Average cover per split (Cover / Split count).
    
    Examples:
        >>> from boosters import ImportanceType
        >>> importance = model.feature_importance(ImportanceType.Gain)
    """
    Split = ...
    r"""
    Number of times each feature is used in splits.
    """
    Gain = ...
    r"""
    Total gain from splits using each feature.
    """
    AverageGain = ...
    r"""
    Average gain per split (Gain / Split count).
    """
    Cover = ...
    r"""
    Total cover (hessian sum) at nodes splitting on each feature.
    """
    AverageCover = ...
    r"""
    Average cover per split (Cover / Split count).
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Metric(enum.Enum):
    r"""
    Evaluation metrics for gradient boosting.
    
    Each variant represents a different metric for evaluating model performance.
    Use the static constructor methods for validation.
    
    Regression:
        - Metric.Rmse(): Root Mean Squared Error
        - Metric.Mae(): Mean Absolute Error
        - Metric.Mape(): Mean Absolute Percentage Error
    
    Classification:
        - Metric.LogLoss(): Binary cross-entropy
        - Metric.Auc(): Area Under ROC Curve
        - Metric.Accuracy(): Classification accuracy
    
    Note: ranking metrics are not implemented in core yet.
    
    Examples
    --------
    >>> from boosters import Metric
    >>> metric = Metric.rmse()  # Regression
    >>> metric = Metric.auc()  # Binary classification
    >>> metric = Metric.Accuracy(threshold=0.7)
    
    Pattern matching:
    >>> match metric:
    ...     case Metric.Rmse():
    ...         print("RMSE")
    ...     case Metric.Accuracy(threshold=t):
    ...         print(f"Accuracy@{t}")
    """
    None = ...
    r"""
    No metric - skips evaluation entirely.
    """
    Rmse = ...
    r"""
    Root Mean Squared Error for regression.
    """
    Mae = ...
    r"""
    Mean Absolute Error for regression.
    """
    Mape = ...
    r"""
    Mean Absolute Percentage Error for regression.
    """
    LogLoss = ...
    r"""
    Binary Log Loss (cross-entropy) for classification.
    """
    Auc = ...
    r"""
    Area Under ROC Curve for binary classification.
    """
    Accuracy = ...
    r"""
    Classification accuracy (binary or multiclass).
    
    Parameters:
        threshold: Probability threshold in (0, 1] for positive class. Default: 0.5.
    """
    MarginAccuracy = ...
    r"""
    Accuracy computed on raw margins (no sigmoid/softmax).
    """
    MulticlassLogLoss = ...
    r"""
    Multiclass log loss (cross-entropy).
    """
    MulticlassAccuracy = ...
    r"""
    Multiclass accuracy.
    """
    Quantile = ...
    r"""
    Quantile loss metric.
    
    Parameters:
        alpha: List of quantiles to evaluate. Each value must be in (0, 1).
    """
    PoissonDeviance = ...
    r"""
    Poisson deviance for count regression.
    """

    @staticmethod
    def none() -> Metric:
        r"""
        Create no-metric.
        """
    @staticmethod
    def rmse() -> Metric:
        r"""
        Create RMSE metric.
        """
    @staticmethod
    def mae() -> Metric:
        r"""
        Create MAE metric.
        """
    @staticmethod
    def mape() -> Metric:
        r"""
        Create MAPE metric.
        """
    @staticmethod
    def logloss() -> Metric:
        r"""
        Create log loss metric.
        """
    @staticmethod
    def auc() -> Metric:
        r"""
        Create AUC metric.
        """
    @staticmethod
    def accuracy_at(threshold: builtins.float = 0.5) -> Metric:
        r"""
        Create accuracy metric with validation.
        
        Use `Metric.Accuracy()` for default threshold, `Metric.Accuracy(threshold=...)`
        for direct construction, or this helper for validation.
        """
    @staticmethod
    def margin_accuracy() -> Metric:
        r"""
        Create margin accuracy metric.
        """
    @staticmethod
    def multiclass_logloss() -> Metric:
        r"""
        Create multiclass log loss metric.
        """
    @staticmethod
    def multiclass_accuracy() -> Metric:
        r"""
        Create multiclass accuracy metric.
        """
    @staticmethod
    def quantile(alpha: typing.Sequence[builtins.float]) -> Metric:
        r"""
        Create quantile metric with validation.
        """
    @staticmethod
    def poisson_deviance() -> Metric:
        r"""
        Create Poisson deviance metric.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Objective(enum.Enum):
    r"""
    Objective (loss) functions for gradient boosting.
    
    Each variant represents a different loss function for training GBDT and
    GBLinear models. Use the static constructor methods for validation.
    
    Regression:
        - Objective.Squared(): Mean squared error (L2)
        - Objective.Absolute(): Mean absolute error (L1)
        - Objective.Huber(delta): Pseudo-Huber loss (robust)
        - Objective.Pinball(alpha): Quantile regression
        - Objective.Poisson(): Poisson deviance for count data
    
    Classification:
        - Objective.Logistic(): Binary cross-entropy
        - Objective.Hinge(): SVM-style hinge loss
        - Objective.Softmax(n_classes): Multiclass cross-entropy
    
    Note: ranking objectives are not implemented in core yet.
    
    Examples
    --------
    >>> from boosters import Objective
    >>> obj = Objective.squared()  # L2 regression
    >>> obj = Objective.logistic()  # Binary classification
    >>> obj = Objective.pinball([0.1, 0.5, 0.9])  # Quantile regression
    >>> obj = Objective.softmax(10)  # Multiclass classification
    
    Pattern matching:
    >>> match obj:
    ...     case Objective.Squared():
    ...         print("L2 loss")
    ...     case Objective.Pinball(alpha=a):
    ...         print(f"Quantile: {a}")
    """
    Squared = ...
    r"""
    Squared error loss (L2) for regression.
    """
    Absolute = ...
    r"""
    Absolute error loss (L1) for robust regression.
    """
    Poisson = ...
    r"""
    Poisson loss for count regression.
    """
    Logistic = ...
    r"""
    Logistic loss for binary classification.
    """
    Hinge = ...
    r"""
    Hinge loss for binary classification (SVM-style).
    """
    Huber = ...
    r"""
    Pseudo-Huber loss for robust regression.
    
    Parameters:
        delta: Transition point between quadratic and linear loss. Default: 1.0.
    """
    Pinball = ...
    r"""
    Pinball loss for quantile regression.
    
    Parameters:
        alpha: List of quantiles to predict. Each value must be in (0, 1).
    """
    Softmax = ...
    r"""
    Softmax loss for multiclass classification.
    
    Parameters:
        n_classes: Number of classes. Must be >= 2.
    """

    @staticmethod
    def squared() -> Objective:
        r"""
        Create squared error loss (L2).
        """
    @staticmethod
    def absolute() -> Objective:
        r"""
        Create absolute error loss (L1).
        """
    @staticmethod
    def poisson() -> Objective:
        r"""
        Create Poisson loss.
        """
    @staticmethod
    def logistic() -> Objective:
        r"""
        Create logistic loss for binary classification.
        """
    @staticmethod
    def hinge() -> Objective:
        r"""
        Create hinge loss for binary classification.
        """
    @staticmethod
    def huber(delta: builtins.float = 1.0) -> Objective:
        r"""
        Create Huber loss with validation.
        """
    @staticmethod
    def pinball(alpha: typing.Sequence[builtins.float]) -> Objective:
        r"""
        Create pinball loss with validation.
        """
    @staticmethod
    def softmax(n_classes: builtins.int) -> Objective:
        r"""
        Create softmax loss with validation.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Verbosity(enum.Enum):
    r"""
    Verbosity level for training output.
    
    Controls the amount of information logged during training.
    
    Attributes:
        Silent: No output.
        Warning: Errors and warnings only.
        Info: Progress and important information.
        Debug: Detailed debugging information.
    
    Examples:
        >>> from boosters import Verbosity, GBDTConfig
        >>> config = GBDTConfig(verbosity=Verbosity.Info)
    """
    Silent = ...
    r"""
    No output.
    """
    Warning = ...
    r"""
    Errors and warnings only.
    """
    Info = ...
    r"""
    Progress and important information.
    """
    Debug = ...
    r"""
    Detailed debugging information.
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation.
        """
    def __repr__(self) -> builtins.str: ...

