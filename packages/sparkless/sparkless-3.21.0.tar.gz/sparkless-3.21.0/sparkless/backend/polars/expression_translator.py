"""
Expression translator for converting Column expressions to Polars expressions.

This module translates Sparkless column expressions (Column, ColumnOperation)
to Polars expressions (pl.Expr) for DataFrame operations.
"""

from typing import Any, Optional, cast
from datetime import datetime, date
import logging
import polars as pl
import math
import threading
from collections import OrderedDict
from sparkless import config
from sparkless.functions import Column, ColumnOperation, Literal
from sparkless.functions.base import AggregateFunction
from sparkless.functions.window_execution import WindowFunction

logger = logging.getLogger(__name__)


def _is_mock_case_when(expr: Any) -> bool:
    """Check if expression is a CaseWhen instance.

    Args:
        expr: Expression to check

    Returns:
        True if expr is a CaseWhen instance
    """
    # Use isinstance if available, otherwise check by class name to avoid import issues
    try:
        from sparkless.functions.conditional import CaseWhen

        return isinstance(expr, CaseWhen)
    except (ImportError, AttributeError):
        # Fallback: check by class name
        return (
            hasattr(expr, "__class__")
            and expr.__class__.__name__ == "CaseWhen"
            and hasattr(expr, "conditions")
        )


class PolarsExpressionTranslator:
    """Translates Column expressions to Polars expressions."""

    def __init__(self) -> None:
        self._cache_enabled = config.is_feature_enabled(
            "enable_expression_translation_cache"
        )
        self._cache_lock = threading.Lock()
        self._translation_cache: OrderedDict[Any, pl.Expr] = OrderedDict()
        self._cache_size = 512

    def translate(self, expr: Any, input_col_dtype: Any = None) -> pl.Expr:
        """Translate Column expression to Polars expression.

        Args:
            expr: Column, ColumnOperation, or other expression
            input_col_dtype: Optional Polars dtype of input column (for to_timestamp optimization)

        Returns:
            Polars expression (pl.Expr)
        """
        cache_key = self._build_cache_key(expr) if self._cache_enabled else None
        if cache_key is not None:
            cached = self._cache_get(cache_key)
            if cached is not None:
                return cached

        if isinstance(expr, ColumnOperation):
            result = self._translate_operation(expr, input_col_dtype=input_col_dtype)
        elif isinstance(expr, Column):
            result = self._translate_column(expr)
        elif isinstance(expr, Literal):
            result = self._translate_literal(expr)
        elif isinstance(expr, AggregateFunction):
            result = self._translate_aggregate_function(expr)
        elif isinstance(expr, WindowFunction):
            # Window functions are handled separately in window_handler.py
            raise ValueError("Window functions should be handled by WindowHandler")
        elif isinstance(expr, str):
            # String column name
            result = pl.col(expr)
        elif isinstance(expr, (int, float, bool)):
            # Literal value
            result = pl.lit(expr)
        elif isinstance(expr, (datetime, date)):
            # Datetime or date literal value
            result = pl.lit(expr)
        elif isinstance(expr, tuple):
            # Tuple - this is likely a function argument tuple, not a literal
            # Don't try to create a literal from it - tuples as literals are not supported in Polars
            # This should be handled by the function that uses it (e.g., concat_ws, substring)
            # If we reach here, it means a tuple was passed where it shouldn't be
            raise ValueError(
                f"Cannot translate tuple as literal: {expr}. This should be handled by the function that uses it."
            )
        elif expr is None:
            result = pl.lit(None)
        elif _is_mock_case_when(expr):
            result = self._translate_case_when(expr)
        else:
            raise ValueError(f"Unsupported expression type: {type(expr)}")

        if cache_key is not None:
            self._cache_set(cache_key, result)

        return result

    def _translate_column(self, col: Column) -> pl.Expr:
        """Translate Column to Polars column expression.

        Args:
            col: Column instance

        Returns:
            Polars column expression
        """
        # If column has an alias, use the original column name for translation
        # The alias will be applied when the expression is used in select
        if hasattr(col, "_original_column") and col._original_column is not None:
            # Use the original column's name for the actual column reference
            return pl.col(col._original_column.name)
        # Use the column's name directly
        return pl.col(col.name)

    def _translate_literal(self, lit: Literal) -> pl.Expr:
        """Translate Literal to Polars literal expression.

        Args:
            lit: Literal instance

        Returns:
            Polars literal expression
        """
        # Resolve lazy literals (session-aware functions) before translating
        if hasattr(lit, "_is_lazy") and lit._is_lazy:
            value = lit._resolve_lazy_value()
        else:
            value = lit.value
        return pl.lit(value)

    def _translate_operation(
        self, op: ColumnOperation, input_col_dtype: Any = None
    ) -> pl.Expr:
        """Translate ColumnOperation to Polars expression.

        Args:
            op: ColumnOperation instance

        Returns:
            Polars expression
        """
        operation = op.operation
        column = op.column
        value = op.value

        # Translate left side
        # Check ColumnOperation before Column since ColumnOperation is a subclass of Column
        if isinstance(column, ColumnOperation):
            left = self._translate_operation(column, input_col_dtype=None)
        elif isinstance(column, Column):
            left = pl.col(column.name)
        elif isinstance(column, Literal):
            # Resolve lazy literals before translating
            if hasattr(column, "_is_lazy") and column._is_lazy:
                lit_value = column._resolve_lazy_value()
            else:
                lit_value = column.value

            # For cast operations with None literals, we'll handle dtype in _translate_cast
            # For now, create the literal - the cast will handle the dtype
            left = pl.lit(lit_value)
        elif isinstance(column, str):
            left = pl.col(column)
        elif isinstance(column, (int, float, bool)):
            left = pl.lit(column)
        else:
            left = self.translate(column)

        # Special handling for cast operation - value should be a type name, not a column
        if operation == "cast":
            # Special case: if casting a None literal, create typed None directly
            # This handles F.lit(None).cast(TimestampType()) correctly
            if isinstance(column, Literal) and column.value is None:
                from .type_mapper import mock_type_to_polars_dtype

                polars_dtype = mock_type_to_polars_dtype(value)
                return pl.lit(None, dtype=polars_dtype)
            return self._translate_cast(left, value)

        # Special handling for isin - value is a list, don't translate it
        if operation == "isin":
            if isinstance(value, list):
                return left.is_in(value)
            else:
                return left.is_in([value])

        # Check if this is a binary operator first (must be handled as binary operation, not function)
        binary_operators = [
            "==",
            "!=",
            "<",
            "<=",
            ">",
            ">=",
            "+",
            "-",
            "*",
            "/",
            "%",
            "&",
            "|",
        ]
        if operation in binary_operators:
            # Binary operators should NOT be routed to function calls - handle as binary operation below
            pass
        # Check if this is a string operation (must be handled as binary operation, not function)
        elif operation in [
            "contains",
            "startswith",
            "endswith",
            "like",
            "rlike",
            "isin",
        ]:
            # String operations and isin should NOT be routed to function calls - handle as binary operation below
            pass
        # Check if this is a unary operator (must be handled as unary operation, not function)
        elif value is None and operation in ["!", "-"]:
            # Unary operators should NOT be routed to function calls - handle as unary operation below
            pass
        # Check if this is a function call (not a binary or unary operation)
        # Functions like concat_ws, substring, etc. have values but are not binary operations
        elif hasattr(op, "function_name") or operation in [
            "substring",
            "regexp_replace",
            "regexp_extract",
            "split",
            "concat",
            "concat_ws",
            "like",
            "rlike",
            "round",
            "pow",
            "to_date",
            "to_timestamp",
            "date_format",
            "date_add",
            "date_sub",
            "datediff",
            "lpad",
            "rpad",
            "repeat",
            "instr",
            "locate",
            "add_months",
            "last_day",
            "bin",
            "bround",
            "conv",
            "factorial",
            "map_keys",
            "map_values",
            "map_entries",
            "map_concat",
        ]:
            return self._translate_function_call(op, input_col_dtype=input_col_dtype)

        # Handle unary operations
        if value is None:
            # Handle operators first (before function calls)
            if operation == "!":
                return ~left
            elif operation == "-":
                return -left
            elif operation in ["isnull", "isNull"]:
                return left.is_null()
            elif operation in ["isnotnull", "isNotNull"]:
                return left.is_not_null()
            # Check if it's a function call (e.g., upper, lower, length)
            # Also check for datetime functions and other unary functions
            elif hasattr(op, "function_name") or operation in [
                "upper",
                "lower",
                "length",
                "trim",
                "ltrim",
                "rtrim",
                "btrim",
                "bit_length",
                "octet_length",
                "char",
                "ucase",
                "lcase",
                "positive",
                "negative",
                "power",
                "now",
                "curdate",
                "days",
                "hours",
                "months",
                "equal_null",
                "substr",
                "split_part",
                "position",
                "elt",
                "abs",
                "ceil",
                "floor",
                "sqrt",
                "exp",
                "log",
                "log10",
                "sin",
                "cos",
                "tan",
                "round",
                "bin",
                "bround",
                "conv",
                "factorial",
                "year",
                "month",
                "day",
                "dayofmonth",
                "hour",
                "minute",
                "second",
                "dayofweek",
                "dayofyear",
                "weekofyear",
                "quarter",
                "to_date",
                "current_timestamp",
                "current_date",
                "now",
                "curdate",
                "map_keys",
                "map_values",
                "map_entries",
                "map_concat",
            ]:
                return self._translate_function_call(op)
            else:
                raise ValueError(f"Unsupported unary operation: {operation}")

        # Translate right side
        # Check ColumnOperation before Column since ColumnOperation is a subclass of Column
        if isinstance(value, ColumnOperation):
            right = self._translate_operation(value, input_col_dtype=None)
        elif isinstance(value, Column):
            right = pl.col(value.name)
        elif isinstance(value, Literal):
            # Resolve lazy literals before translating
            if hasattr(value, "_is_lazy") and value._is_lazy:
                right = pl.lit(value._resolve_lazy_value())
            else:
                right = pl.lit(value.value)
        elif isinstance(value, (int, float, bool, str)):
            right = pl.lit(value)
        elif isinstance(value, (datetime, date)):
            # Datetime or date literal value
            right = pl.lit(value)
        elif value is None:
            right = pl.lit(None)
        else:
            right = self.translate(value)

        # Handle binary operations
        if operation == "==":
            return left == right
        elif operation == "!=":
            return left != right
        elif operation == "<":
            return left < right
        elif operation == "<=":
            return left <= right
        elif operation == ">":
            return left > right
        elif operation == ">=":
            return left >= right
        elif operation == "+":
            return left + right
        elif operation == "-":
            return left - right
        elif operation == "*":
            return left * right
        elif operation == "/":
            return left / right
        elif operation == "%":
            return left % right
        elif operation == "&":
            return left & right
        elif operation == "|":
            return left | right
        elif operation == "cast":
            # Handle cast operation
            return self._translate_cast(left, value)
        # isin is handled earlier, before value translation
        elif operation in ["startswith", "endswith"]:
            # operation is guaranteed to be a string in ColumnOperation
            op_str = cast("str", operation)
            return self._translate_string_operation(left, op_str, value)
        elif operation == "contains":
            # Handle contains as a function call
            return self._translate_function_call(op, input_col_dtype=input_col_dtype)
        elif hasattr(op, "function_name"):
            # Handle function calls (e.g., upper, lower, sum, etc.)
            return self._translate_function_call(op, input_col_dtype=input_col_dtype)
        else:
            raise ValueError(f"Unsupported operation: {operation}")

    def _translate_cast(self, expr: pl.Expr, target_type: Any) -> pl.Expr:
        """Translate cast operation.

        Args:
            expr: Polars expression to cast
            target_type: Target data type (DataType or string type name)

        Returns:
            Casted Polars expression
        """
        from .type_mapper import mock_type_to_polars_dtype
        from sparkless.spark_types import (
            StringType,
            IntegerType,
            LongType,
            DoubleType,
            FloatType,
            BooleanType,
            DateType,
            TimestampType,
            ShortType,
            ByteType,
        )

        # Handle string type names (e.g., "string", "int", "long")
        if isinstance(target_type, str):
            type_name_map = {
                "string": StringType(),
                "str": StringType(),
                "int": IntegerType(),
                "integer": IntegerType(),
                "long": LongType(),
                "bigint": LongType(),
                "double": DoubleType(),
                "float": FloatType(),
                "boolean": BooleanType(),
                "bool": BooleanType(),
                "date": DateType(),
                "timestamp": TimestampType(),
                "short": ShortType(),
                "byte": ByteType(),
            }
            target_type = type_name_map.get(target_type.lower())
            if target_type is None:
                raise ValueError(f"Unsupported cast type: {target_type}")

        # Special handling for casting to StringType
        if isinstance(target_type, StringType):
            # For datetime/date types, use direct cast to string
            # This fixes issue #145 where explicit string casts weren't working correctly
            # Use cast(pl.Utf8, strict=False) which works for all types including datetime
            return expr.cast(pl.Utf8, strict=False)

        polars_dtype = mock_type_to_polars_dtype(target_type)

        # Special handling for None literals - create literal with target dtype directly
        # This handles F.lit(None).cast(TimestampType()) correctly
        # Check if expr is a None literal by trying to evaluate it
        # If it's a constant None, create pl.lit(None, dtype=target_dtype) directly
        try:
            # Try to get the value if it's a literal
            # For None literals, Polars needs the dtype specified at creation time
            # Check if this is a literal expression that evaluates to None
            if hasattr(expr, "meta"):
                import contextlib

                with contextlib.suppress(Exception):
                    # Try to see if this is a literal None
                    # For Polars, we need to create pl.lit(None, dtype=...) for typed nulls
                    # This is a workaround - we'll handle it by creating the literal with dtype
                    pass
        except Exception:
            logger.debug("Exception in cast type detection, continuing", exc_info=True)
            pass

        # For string to int/long casting, Polars needs float intermediate step
        # PySpark handles "10.5" -> 10 by converting to float first, then int
        if isinstance(target_type, (IntegerType, LongType)):
            # Check if source is string - need float intermediate step
            # For other types, direct cast is fine
            return expr.cast(pl.Float64, strict=False).cast(polars_dtype, strict=False)

        # For string to date/timestamp casting
        if isinstance(target_type, (DateType, TimestampType)):
            # Try to parse string to date/timestamp
            # Use map_elements to handle both string and non-string inputs
            if isinstance(target_type, DateType):
                # Parse date string
                def parse_date(val: Any) -> Any:
                    if val is None:
                        return None
                    from datetime import datetime

                    val_str = str(val)
                    try:
                        return datetime.strptime(val_str, "%Y-%m-%d").date()
                    except ValueError:
                        return None

                # Try strptime first (works for string columns), fall back to map_elements
                try:
                    return expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
                except Exception:
                    logger.debug(
                        "strptime failed for date parsing, using map_elements fallback",
                        exc_info=True,
                    )
                    return expr.map_elements(parse_date, return_dtype=pl.Date)
            else:  # TimestampType
                # Parse timestamp string
                def parse_timestamp(val: Any) -> Any:
                    if val is None:
                        return None
                    from datetime import datetime

                    val_str = str(val)
                    try:
                        return datetime.strptime(val_str, "%Y-%m-%d %H:%M:%S")
                    except ValueError:
                        try:
                            return datetime.strptime(val_str, "%Y-%m-%d")
                        except ValueError:
                            return None

                # Try strptime first (works for string columns), fall back to map_elements
                try:
                    return expr.str.strptime(
                        pl.Datetime, "%Y-%m-%d %H:%M:%S", strict=False
                    )
                except Exception:
                    logger.debug(
                        "strptime failed for timestamp parsing, using map_elements fallback",
                        exc_info=True,
                    )
                    return expr.map_elements(
                        parse_timestamp, return_dtype=pl.Datetime(time_unit="us")
                    )

        # For other types, use strict=False to return null for invalid casts (PySpark behavior)
        # Special handling: if expr is a None literal (pl.lit(None)), create typed None
        # This handles F.lit(None).cast(TimestampType()) correctly
        try:
            # Check if this is a literal None by trying to get its value
            # If it's pl.lit(None), we need to create it with the target dtype
            # Polars requires dtype to be specified when creating None literals for typed columns
            if hasattr(expr, "meta"):
                # Try to detect if this is a literal None
                # For now, we'll use a workaround: try casting, and if it fails with schema error,
                # create a new literal with dtype
                try:
                    return expr.cast(polars_dtype, strict=False)
                except Exception as e:
                    # If casting fails due to null type, create typed None literal
                    if "null" in str(e).lower() or "dtype" in str(e).lower():
                        return pl.lit(None, dtype=polars_dtype)
                    raise
            else:
                return expr.cast(polars_dtype, strict=False)
        except Exception:
            # Fallback: try to create typed None if cast fails
            # This handles the case where pl.lit(None) can't be cast directly
            logger.debug(
                "Initial cast failed, trying typed None fallback", exc_info=True
            )
            try:
                # Check if expr represents a None value
                # For Polars, we need pl.lit(None, dtype=...) for typed nulls
                return pl.lit(None, dtype=polars_dtype)
            except Exception:
                # Last resort: try regular cast
                logger.debug(
                    "Typed None creation failed, using regular cast", exc_info=True
                )
                return expr.cast(polars_dtype, strict=False)

    def _translate_string_operation(
        self, expr: pl.Expr, operation: str, value: Any
    ) -> pl.Expr:
        """Translate string operations.

        Args:
            expr: Polars expression (string column)
            operation: String operation name
            value: Operation value

        Returns:
            Polars expression for string operation
        """
        if operation == "contains":
            if isinstance(value, str):
                return expr.str.contains(value)
            else:
                value_expr = self.translate(value)
                return expr.str.contains(value_expr)
        elif operation == "startswith":
            if isinstance(value, str):
                return expr.str.starts_with(value)
            else:
                value_expr = self.translate(value)
                return expr.str.starts_with(value_expr)
        elif operation == "endswith":
            if isinstance(value, str):
                return expr.str.ends_with(value)
            else:
                value_expr = self.translate(value)
                return expr.str.ends_with(value_expr)
        else:
            raise ValueError(f"Unsupported string operation: {operation}")

    def _build_cache_key(self, expr: Any) -> Optional[tuple[Any, ...]]:
        try:
            return self._serialize_expression(expr)
        except Exception:
            logger.debug("Failed to build cache key for expression", exc_info=True)
            return None

    def _serialize_expression(self, expr: Any) -> tuple[Any, ...]:
        if isinstance(expr, Column):
            alias = getattr(expr, "_alias_name", None)
            original = getattr(expr, "_original_column", None)
            original_name = getattr(original, "name", None)
            return ("column", expr.name, alias, original_name)
        if isinstance(expr, ColumnOperation):
            column_repr = self._serialize_value(getattr(expr, "column", None))
            value_repr = self._serialize_value(getattr(expr, "value", None))
            return (
                "operation",
                expr.operation,
                column_repr,
                value_repr,
                getattr(expr, "name", None),
                getattr(expr, "function_name", None),
            )
        if isinstance(expr, Literal):
            # Resolve lazy literals before serializing
            if hasattr(expr, "_is_lazy") and expr._is_lazy:
                return ("literal", expr._resolve_lazy_value())
            return ("literal", expr.value)
        if isinstance(expr, tuple):
            return ("tuple",) + tuple(self._serialize_value(item) for item in expr)
        if isinstance(expr, list):
            return ("list",) + tuple(self._serialize_value(item) for item in expr)
        if isinstance(expr, dict):
            return (
                "dict",
                tuple(
                    sorted(
                        (self._serialize_value(k), self._serialize_value(v))
                        for k, v in expr.items()
                    )
                ),
            )
        if isinstance(expr, (int, float, bool, str)):
            return ("scalar", expr)
        if expr is None:
            return ("none",)
        return ("repr", repr(expr))

    def _serialize_value(self, value: Any) -> Any:
        if isinstance(value, (Column, ColumnOperation, Literal)):
            return self._serialize_expression(value)
        if isinstance(value, (list, tuple)):
            return tuple(self._serialize_value(item) for item in value)
        if isinstance(value, dict):
            return tuple(
                sorted(
                    (self._serialize_value(k), self._serialize_value(v))
                    for k, v in value.items()
                )
            )
        if isinstance(value, (int, float, bool, str)) or value is None:
            return value
        return repr(value)

    def _cache_get(self, key: tuple[Any, ...]) -> Optional[pl.Expr]:
        with self._cache_lock:
            cached = self._translation_cache.get(key)
            if cached is not None:
                self._translation_cache.move_to_end(key)
            return cached

    def _cache_set(self, key: tuple[Any, ...], expr: pl.Expr) -> None:
        with self._cache_lock:
            self._translation_cache[key] = expr
            self._translation_cache.move_to_end(key)
            if len(self._translation_cache) > self._cache_size:
                self._translation_cache.popitem(last=False)

    def clear_cache(self) -> None:
        """Clear the expression translation cache.

        This should be called when columns are dropped to invalidate cached
        expressions that reference those columns.
        """
        with self._cache_lock:
            self._translation_cache.clear()

    def _translate_function_call(
        self, op: ColumnOperation, input_col_dtype: Any = None
    ) -> pl.Expr:
        """Translate function call operations.

        Args:
            op: ColumnOperation with function call

        Returns:
            Polars expression for function call
        """
        # op.operation is guaranteed to be a string in ColumnOperation
        op_operation = cast("str", op.operation)
        function_name = getattr(op, "function_name", op_operation)
        if function_name is None:
            function_name = op_operation
        function_name = function_name.lower()
        column = op.column

        # Handle functions without column first (e.g., current_timestamp, current_date, monotonically_increasing_id)
        if column is None:
            operation = op.operation  # Extract operation for use in comparisons
            if operation == "current_timestamp":
                # Use datetime.now() which returns current timestamp
                from datetime import datetime

                return pl.lit(datetime.now())
            elif operation == "current_date":
                # Use date.today() which returns current date
                from datetime import date

                return pl.lit(date.today())
            elif operation == "now":
                # Alias for current_timestamp
                from datetime import datetime

                return pl.lit(datetime.now())
            elif operation == "curdate":
                # Alias for current_date
                from datetime import date

                return pl.lit(date.today())
            elif operation == "localtimestamp":
                # Local timestamp (without timezone)
                from datetime import datetime

                return pl.lit(datetime.now())
            elif function_name == "monotonically_increasing_id":
                # monotonically_increasing_id() - generate row numbers
                # Use int_range to generate sequential IDs
                return pl.int_range(pl.len())

        # Extract operation for use in comparisons
        operation = op.operation  # Extract operation for use in comparisons

        # SPECIAL CASE: Check for nested to_date(to_timestamp(...)) BEFORE translating col_expr
        # This allows us to detect the nested structure and handle it specially
        if (
            operation == "to_date"
            and isinstance(column, ColumnOperation)
            and column.operation == "to_timestamp"
        ):
            # For to_date(to_timestamp(...)), the input is already datetime
            # Use map_elements with a simple datetime->date conversion
            # This avoids schema validation issues that dt.date() might cause
            # First translate the nested to_timestamp to get the datetime expression
            nested_ts_expr = self._translate_operation(column, input_col_dtype=None)

            def datetime_to_date(val: Any) -> Any:
                from datetime import datetime, date

                if val is None:
                    return None
                if isinstance(val, datetime):
                    return val.date()
                if isinstance(val, date):
                    return val
                return None

            return nested_ts_expr.map_elements(
                datetime_to_date,
                return_dtype=pl.Date,
            )

        # Handle unix_timestamp() without arguments (current timestamp) BEFORE translating column
        if operation == "unix_timestamp":
            from sparkless.functions.core.literals import Literal

            is_current_timestamp = False
            if (
                column is None
                or isinstance(column, str)
                and column == "current_timestamp"
                or isinstance(column, Literal)
                and column.value == "current_timestamp"
            ):
                is_current_timestamp = True

            if is_current_timestamp:
                # Return current Unix timestamp
                from datetime import datetime

                return pl.lit(int(datetime.now().timestamp()))

        # Translate column expression
        # Check ColumnOperation BEFORE Column since ColumnOperation inherits from Column
        if isinstance(column, ColumnOperation):
            col_expr = self._translate_operation(column, input_col_dtype=None)
        elif isinstance(column, Column):
            col_expr = pl.col(column.name)
        elif isinstance(column, str):
            col_expr = pl.col(column)
        else:
            col_expr = self.translate(column)

        # Handle array_sort before other checks since it can have op.value=None or op.value=bool
        if operation == "array_sort":
            # array_sort(col, asc) - sort array elements
            # op.value can be None (default ascending) or a boolean
            asc = True  # Default to ascending
            if op.value is not None:
                asc = op.value if isinstance(op.value, bool) else bool(op.value)
            # Polars list.sort() with descending=False for ascending, descending=True for descending
            return col_expr.list.sort(descending=not asc)

        # Handle to_timestamp before other checks since it can have op.value=None or op.value=format
        # to_timestamp needs special handling for multiple input types
        # Note: We can optionally pass the input column dtype to help choose the right method
        if operation == "to_timestamp":
            # to_timestamp(col, format) or to_timestamp(col)
            # PySpark accepts multiple input types:
            # - StringType: parse with format (or default format)
            # - TimestampType: pass-through (return as-is)
            # - IntegerType/LongType: Unix timestamp in seconds
            # - DateType: convert Date to Timestamp
            # - DoubleType: Unix timestamp with decimal seconds
            from datetime import datetime, timezone, date

            if op.value is not None:
                # With format string
                format_str = op.value
                # Handle optional fractional seconds like [.SSSSSS]
                import re

                # Check if format includes microseconds/fractional seconds
                # PySpark supports [.SSSSSS] for optional fractional seconds
                # Remove optional fractional pattern from format string for now
                # We'll handle microseconds automatically in the parsing function
                format_str = re.sub(r"\[\.S+\]", "", format_str)
                # Handle single-quoted literals (e.g., 'T' in yyyy-MM-dd'T'HH:mm:ss)
                # Remove quotes but keep the literal characters
                format_str = re.sub(r"'([^']*)'", r"\1", format_str)
                # Convert Java/Spark format to Python format (Polars str.strptime uses Python format)
                format_map = {
                    "yyyy": "%Y",
                    "MM": "%m",
                    "dd": "%d",
                    "HH": "%H",
                    "mm": "%M",
                    "ss": "%S",
                }
                # Sort by length descending to process longest matches first
                for java_pattern, python_pattern in sorted(
                    format_map.items(), key=lambda x: len(x[0]), reverse=True
                ):
                    format_str = format_str.replace(java_pattern, python_pattern)

                # Use str.strptime() for string columns to avoid schema inference issues
                # This is the most efficient approach and avoids Polars incorrectly inferring
                # the input column type as datetime
                # For non-string inputs, fall back to map_elements

                def convert_to_timestamp_single_with_format(
                    val: Any, fmt: str = format_str
                ) -> Any:
                    """Convert single value to timestamp with format."""
                    from datetime import datetime, timezone, date

                    if val is None:
                        return None
                    # If already a datetime, return as-is (TimestampType pass-through)
                    if isinstance(val, datetime):
                        return val
                    # If date, convert to datetime at midnight
                    if isinstance(val, date) and not isinstance(val, datetime):
                        return datetime.combine(val, datetime.min.time())
                    # If numeric (int/long/double), treat as Unix timestamp
                    if isinstance(val, (int, float)):
                        try:
                            timestamp = float(val)
                            # Interpret as UTC and convert to local timezone (PySpark behavior)
                            dt_utc = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                            return dt_utc.astimezone().replace(tzinfo=None)
                        except (ValueError, TypeError, OverflowError, OSError):
                            return None
                    # If string, parse with format
                    if isinstance(val, str):
                        import re

                        # PySpark's to_timestamp is lenient and automatically handles microseconds
                        # even if they're not in the format string. Strip microseconds before parsing
                        # if the format doesn't include them.
                        val_cleaned = val

                        # Check if format includes microseconds (look for %f or similar patterns)
                        has_microseconds_in_format = "%f" in fmt or "S" in fmt.upper()

                        # If format doesn't include microseconds, strip them from the value
                        # Match microseconds pattern: . followed by digits (1-6 digits typical)
                        # This pattern can appear after seconds but before timezone or end of string
                        if not has_microseconds_in_format:
                            # Remove microseconds pattern: . followed by 1-6 digits
                            # Match: .123456 or .123 (before timezone or end of string)
                            val_cleaned = re.sub(
                                r"\.\d{1,6}(?=[+-]|\d{2}:\d{2}|Z|$)", "", val_cleaned
                            )

                        # Remove timezone patterns (e.g., +00:00, Z) if not in format
                        if "%z" not in fmt and "%Z" not in fmt:
                            val_cleaned = re.sub(r"[+-]\d{2}:\d{2}$", "", val_cleaned)
                            val_cleaned = val_cleaned.rstrip("Z")

                        try:
                            return datetime.strptime(val_cleaned, fmt)
                        except (ValueError, TypeError):
                            # If parsing still fails, try original value as fallback
                            try:
                                return datetime.strptime(val, fmt)
                            except (ValueError, TypeError):
                                return None
                    # For other types, try converting to string and parsing
                    try:
                        return datetime.strptime(str(val), fmt)
                    except (ValueError, TypeError):
                        return None

                # Check if the input is a string type (from dtype or string operation).
                # For string types, use str.strptime() which works correctly and avoids
                # schema inference issues with map_elements.
                # For other types (datetime, date, numeric), use map_elements which
                # handles all types correctly at runtime.
                is_string_type = False

                # Check if we have dtype information from the DataFrame
                # input_col_dtype is a Polars dtype (e.g., pl.Utf8 for String)
                if input_col_dtype is not None and input_col_dtype == pl.Utf8:
                    is_string_type = True
                # Also check if it's a string operation or cast to string
                if not is_string_type and isinstance(op.column, ColumnOperation):
                    string_ops = [
                        "regexp_replace",
                        "substring",
                        "concat",
                        "upper",
                        "lower",
                        "trim",
                        "ltrim",
                        "rtrim",
                    ]
                    # Check if it's a string operation
                    if op.column.operation in string_ops:
                        is_string_type = True
                    # Check if it's a cast to string
                    elif op.column.operation == "cast":
                        cast_target = op.column.value
                        if isinstance(cast_target, str) and cast_target.lower() in [
                            "string",
                            "varchar",
                        ]:
                            is_string_type = True
                    # For nested ColumnOperations, check recursively
                    elif isinstance(op.column.column, ColumnOperation):
                        inner_op = op.column.column
                        if inner_op.operation in string_ops:
                            is_string_type = True
                        elif inner_op.operation == "cast":
                            cast_target = inner_op.value
                            if isinstance(cast_target, str) and cast_target.lower() in [
                                "string",
                                "varchar",
                            ]:
                                is_string_type = True

                if is_string_type:
                    # For string types, preprocess to strip microseconds if format doesn't include them,
                    # then use str.strptime() directly. This avoids map_elements schema validation issues.
                    # PySpark's to_timestamp automatically handles microseconds even if not in format.
                    has_microseconds_in_format = "%f" in format_str

                    if not has_microseconds_in_format:
                        # Strip microseconds from the string column before parsing
                        # PySpark's to_timestamp automatically handles microseconds even if not in format
                        # Use Polars string operations to remove microseconds pattern
                        # Pattern: Remove .\d+ after seconds (HH:mm:ss.123456 -> HH:mm:ss)
                        # Use a single pattern that handles most cases: (:\d{2})\.\d+ -> :\d{2}
                        cleaned_expr = col_expr.str.replace_all(
                            r"(:\d{2})\.\d+", r"$1", literal=False
                        )
                        # Also remove any remaining .\d+ at the end (handles edge cases)
                        cleaned_expr = cleaned_expr.str.replace_all(
                            r"\.\d+$", "", literal=False
                        )
                        # Now use str.strptime on the cleaned expression
                        # This should work without schema validation issues
                        return cleaned_expr.str.strptime(
                            pl.Datetime, format_str, strict=False
                        )
                    else:
                        # Format includes microseconds, use str.strptime directly
                        return col_expr.str.strptime(
                            pl.Datetime, format_str, strict=False
                        )
                else:
                    # Use map_elements for non-string types (datetime, date, numeric)
                    # This handles all types correctly at runtime
                    def to_timestamp_with_format(val: Any) -> Any:
                        return convert_to_timestamp_single_with_format(val, format_str)

                    result_expr = col_expr.map_elements(
                        to_timestamp_with_format,
                        return_dtype=pl.Datetime(time_unit="us"),
                    )
                    # Explicitly cast to ensure Polars recognizes the type during schema validation
                    return result_expr.cast(pl.Datetime(time_unit="us"))
            else:
                # Without format - handle all types
                def convert_to_timestamp_no_format(val: Any) -> Any:
                    if val is None:
                        return None
                    # If already a datetime, return as-is (TimestampType pass-through)
                    if isinstance(val, datetime):
                        return val
                    # If date, convert to datetime at midnight
                    if isinstance(val, date) and not isinstance(val, datetime):
                        return datetime.combine(val, datetime.min.time())
                    # If numeric (int/long/double), treat as Unix timestamp
                    if isinstance(val, (int, float)):
                        try:
                            timestamp = float(val)
                            # Interpret as UTC and convert to local timezone (PySpark behavior)
                            dt_utc = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                            return dt_utc.astimezone().replace(tzinfo=None)
                        except (ValueError, TypeError, OverflowError, OSError):
                            return None
                    # If string, try parsing with common formats
                    if isinstance(val, str):
                        for fmt in [
                            "%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%dT%H:%M:%S",
                            "%Y-%m-%d",
                        ]:
                            try:
                                return datetime.strptime(val, fmt)
                            except ValueError:
                                continue
                        return None
                    # For other types, try converting to string and parsing
                    val_str = str(val)
                    for fmt in [
                        "%Y-%m-%d %H:%M:%S",
                        "%Y-%m-%dT%H:%M:%S",
                        "%Y-%m-%d",
                    ]:
                        try:
                            return datetime.strptime(val_str, fmt)
                        except ValueError:
                            continue
                    return None

                # Use map_batches instead of map_elements for better lazy evaluation support
                def convert_to_timestamp_batch_no_format(
                    series: pl.Series,
                ) -> pl.Series:
                    """Convert batch of values to timestamps without format."""
                    from datetime import datetime, timezone, date

                    def convert_single(val: Any) -> Any:
                        if val is None:
                            return None
                        # If already a datetime, return as-is (TimestampType pass-through)
                        if isinstance(val, datetime):
                            return val
                        # If date, convert to datetime at midnight
                        if isinstance(val, date) and not isinstance(val, datetime):
                            return datetime.combine(val, datetime.min.time())
                        # If numeric (int/long/double), treat as Unix timestamp
                        if isinstance(val, (int, float)):
                            try:
                                timestamp = float(val)
                                # Interpret as UTC and convert to local timezone (PySpark behavior)
                                dt_utc = datetime.fromtimestamp(
                                    timestamp, tz=timezone.utc
                                )
                                return dt_utc.astimezone().replace(tzinfo=None)
                            except (ValueError, TypeError, OverflowError, OSError):
                                return None
                        # If string, try parsing with common formats
                        if isinstance(val, str):
                            for fmt in [
                                "%Y-%m-%d %H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S",
                                "%Y-%m-%d",
                            ]:
                                try:
                                    return datetime.strptime(val, fmt)
                                except ValueError:
                                    continue
                            return None
                        # For other types, try converting to string and parsing
                        val_str = str(val)
                        for fmt in [
                            "%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%dT%H:%M:%S",
                            "%Y-%m-%d",
                        ]:
                            try:
                                return datetime.strptime(val_str, fmt)
                            except ValueError:
                                continue
                        return None

                    return series.map_elements(
                        convert_single, return_dtype=pl.Datetime(time_unit="us")
                    )

                return col_expr.map_batches(
                    convert_to_timestamp_batch_no_format,
                    return_dtype=pl.Datetime(time_unit="us"),
                )

        # Map function names to Polars expressions
        # Handle functions with arguments (operation is already extracted above)
        if op.value is not None:
            if operation == "substring":
                # substring(col, start, length) - Polars uses 0-indexed, PySpark uses 1-indexed
                if isinstance(op.value, tuple):
                    start = op.value[0]
                    length = op.value[1] if len(op.value) > 1 else None
                    # Convert 1-indexed to 0-indexed
                    start_idx = start - 1 if start > 0 else 0
                    if length is not None:
                        return col_expr.str.slice(start_idx, length)
                    else:
                        return col_expr.str.slice(start_idx)
                else:
                    return col_expr.str.slice(op.value - 1 if op.value > 0 else 0)
            elif operation == "regexp_replace":
                # regexp_replace(col, pattern, replacement)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    pattern = op.value[0]
                    replacement = op.value[1]
                    return col_expr.str.replace_all(pattern, replacement, literal=True)
                else:
                    raise ValueError(
                        "regexp_replace requires (pattern, replacement) tuple"
                    )
            elif operation == "regexp_extract":
                # regexp_extract(col, pattern, idx)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    pattern = op.value[0]
                    idx = op.value[1] if len(op.value) > 1 else 0
                    # Polars extract_all returns a list, we need to get the first match
                    return col_expr.str.extract(pattern, idx)
                else:
                    raise ValueError("regexp_extract requires (pattern, idx) tuple")
            elif operation == "split":
                # split(col, delimiter)
                delimiter = op.value
                return col_expr.str.split(delimiter)
            elif operation == "btrim":
                # btrim(col, trim_string) or btrim(col)
                if isinstance(op.value, str):
                    return col_expr.str.strip_chars(op.value)
                else:
                    # No trim_string specified, trim whitespace
                    return col_expr.str.strip_chars()
            elif operation == "left":
                # left(col, length)
                n = op.value if isinstance(op.value, int) else int(op.value)
                return col_expr.str.slice(0, n)
            elif operation == "right":
                # right(col, length)
                n = op.value if isinstance(op.value, int) else int(op.value)
                return col_expr.str.slice(-n) if n > 0 else col_expr.str.slice(0, 0)
            elif operation == "contains":
                # contains(col, substring)
                if isinstance(op.value, str):
                    return col_expr.str.contains(op.value)
                else:
                    value_expr = self.translate(op.value)
                    return col_expr.str.contains(value_expr)
            elif operation == "startswith":
                # startswith(col, substring)
                if isinstance(op.value, str):
                    return col_expr.str.starts_with(op.value)
                else:
                    value_expr = self.translate(op.value)
                    return col_expr.str.starts_with(value_expr)
            elif operation == "endswith":
                # endswith(col, substring)
                if isinstance(op.value, str):
                    return col_expr.str.ends_with(op.value)
                else:
                    value_expr = self.translate(op.value)
                    return col_expr.str.ends_with(value_expr)
            elif operation == "like":
                # like(col, pattern) - SQL LIKE pattern matching
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                # Convert SQL LIKE pattern to regex (simplified: % -> .*, _ -> .)
                regex_pattern = pattern.replace("%", ".*").replace("_", ".")
                return col_expr.str.contains(regex_pattern, literal=False)
            elif operation == "rlike":
                # rlike(col, pattern) - Regular expression pattern matching
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                return col_expr.str.contains(pattern, literal=False)
            elif operation == "regexp":
                # regexp(col, pattern) - Alias for rlike
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                return col_expr.str.contains(pattern, literal=False)
            elif operation == "ilike":
                # ilike(col, pattern) - Case-insensitive LIKE
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                regex_pattern = pattern.replace("%", ".*").replace("_", ".")
                return col_expr.str.to_lowercase().str.contains(
                    regex_pattern, literal=False
                )
            elif operation == "regexp_like":
                # regexp_like(col, pattern) - Alias for rlike
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                return col_expr.str.contains(pattern, literal=False)
            elif operation == "regexp_count":
                # regexp_count(col, pattern) - Count regex matches
                pattern = op.value if isinstance(op.value, str) else str(op.value)
                # Use regex to find all matches and count them
                return col_expr.str.count_matches(pattern, literal=False)
            elif operation == "regexp_substr":
                # regexp_substr(col, pattern, pos, occurrence) - Extract substring matching regex
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    pattern = op.value[0]
                    pos = op.value[1] if len(op.value) > 1 else 1
                    # Simplified implementation - extract first match
                    return col_expr.str.extract(pattern, 0)
                else:
                    pattern = op.value if isinstance(op.value, str) else str(op.value)
                    return col_expr.str.extract(pattern, 0)
            elif operation == "regexp_instr":
                # regexp_instr(col, pattern, pos, occurrence) - Find position of regex match
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    pattern = op.value[0]
                    # Simplified implementation - find first match position
                    return col_expr.str.find(pattern)
                else:
                    pattern = op.value if isinstance(op.value, str) else str(op.value)
                    return col_expr.str.find(pattern)
            elif operation == "find_in_set":
                # find_in_set(value, str_list) - Find position in comma-separated list
                # Simplified implementation
                return pl.lit(0)  # Placeholder
            elif operation == "pmod":
                # pmod(dividend, divisor) - Positive modulo
                if isinstance(op.value, (Column, ColumnOperation)):
                    divisor = self.translate(op.value)
                else:
                    divisor = pl.lit(op.value)
                # pmod always returns positive: ((dividend % divisor) + divisor) % divisor
                return ((col_expr % divisor) + divisor) % divisor
            elif operation == "shiftleft":
                # shiftleft(col, num_bits) - Bitwise left shift
                if isinstance(op.value, (Column, ColumnOperation)):
                    num_bits = self.translate(op.value)
                else:
                    num_bits = pl.lit(op.value)
                return col_expr << num_bits
            elif operation == "shiftright":
                # shiftright(col, num_bits) - Bitwise right shift (signed)
                if isinstance(op.value, (Column, ColumnOperation)):
                    num_bits = self.translate(op.value)
                else:
                    num_bits = pl.lit(op.value)
                return col_expr >> num_bits
            elif operation == "shiftrightunsigned":
                # shiftrightunsigned(col, num_bits) - Bitwise unsigned right shift
                # In Python, >> is already unsigned for positive numbers
                if isinstance(op.value, (Column, ColumnOperation)):
                    num_bits = self.translate(op.value)
                else:
                    num_bits = pl.lit(op.value)
                return col_expr >> num_bits
            elif operation == "replace":
                # replace(col, old, new)
                if isinstance(op.value, tuple) and len(op.value) == 2:
                    old, new = op.value
                    return col_expr.str.replace(old, new)
                else:
                    raise ValueError("replace requires (old, new) tuple")
            elif operation == "split_part":
                # split_part(col, delimiter, part) - Extract part of string split by delimiter
                if isinstance(op.value, tuple) and len(op.value) == 2:
                    delimiter, part = op.value
                    # Split and get the part (1-indexed, so subtract 1)
                    return col_expr.str.split(delimiter).list.get(part - 1)
                else:
                    raise ValueError("split_part requires (delimiter, part) tuple")
            elif operation == "position":
                # position(substring, col) - Find position of substring in string (1-indexed)
                # Note: op.value is the substring, op.column is the string to search in
                substring = op.value if isinstance(op.value, str) else str(op.value)
                # Polars find returns 0-based index, add 1 for 1-based
                return col_expr.str.find(substring) + 1
            elif operation == "substr":
                # substr(col, start, length) - Alias for substring
                if isinstance(op.value, tuple):
                    start, length = (
                        op.value[0],
                        op.value[1] if len(op.value) > 1 else None,
                    )
                else:
                    start, length = op.value, None
                # Convert to 0-based index for Polars
                start_idx = start - 1 if start > 0 else 0
                if length is not None:
                    return col_expr.str.slice(start_idx, length)
                else:
                    return col_expr.str.slice(start_idx)
            elif operation == "elt":
                # elt(n, *columns) - Return element at index from list of columns
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    n, columns = op.value[0], op.value[1:]
                    # Translate n and columns
                    n_expr = self.translate(n) if not isinstance(n, int) else pl.lit(n)
                    # Create a list of translated columns
                    col_list = [col_expr] + [self.translate(col) for col in columns]
                    # Use Polars list indexing (1-indexed, so subtract 1)
                    # This is complex - we'll use a when/otherwise chain
                    result = None
                    for i, col in enumerate(col_list, 1):
                        if result is None:
                            result = pl.when(n_expr == i).then(col)
                        else:
                            result = result.when(n_expr == i).then(col)
                    return (
                        result.otherwise(None) if result is not None else pl.lit(None)
                    )
                else:
                    raise ValueError("elt requires (n, *columns) tuple")
            elif operation == "days":
                # days(n) - Convert number to days interval (for date arithmetic)
                # This is a numeric multiplier for date operations
                return col_expr  # Return as-is, will be used in date arithmetic
            elif operation == "hours":
                # hours(n) - Convert number to hours interval
                return col_expr  # Return as-is, will be used in date arithmetic
            elif operation == "months":
                # months(n) - Convert number to months interval
                return col_expr  # Return as-is, will be used in date arithmetic
            elif operation == "equal_null":
                # equal_null(col1, col2) - Equality check that treats NULL as equal
                col2_expr = self.translate(op.value)
                # Return True if both are NULL, or if both are equal
                return (col_expr.is_null() & col2_expr.is_null()) | (
                    col_expr == col2_expr
                )
            elif operation == "concat":
                # concat(*columns) - op.value is tuple/list of additional columns/literals
                # The first column is in op.column, the rest are in op.value
                if op.value and (
                    isinstance(op.value, (list, tuple)) and len(op.value) > 0
                ):
                    # Translate all columns/literals
                    all_cols = [col_expr]  # Start with the first column
                    for col in op.value:
                        if isinstance(col, str):
                            # Try to translate as column first
                            # If it fails or doesn't exist, we'll treat as literal
                            # For now, we'll try pl.col() and catch errors, but a better approach
                            # is to check if it's a valid identifier (column names are identifiers)
                            # Strings with spaces or special chars are likely literals
                            if (
                                col.strip() != col
                                or not col.replace("_", "").replace("-", "").isalnum()
                            ):
                                # String has spaces or special chars - treat as literal
                                all_cols.append(pl.lit(col))
                            else:
                                # Try as column name
                                try:
                                    all_cols.append(pl.col(col))
                                except Exception:
                                    # If it fails, treat as literal
                                    logger.debug(
                                        f"Failed to create column reference for '{col}', treating as literal",
                                        exc_info=True,
                                    )
                                    all_cols.append(pl.lit(col))
                        elif hasattr(col, "value"):  # Literal
                            # Resolve lazy literals before translating
                            if hasattr(col, "_is_lazy") and col._is_lazy:
                                all_cols.append(pl.lit(col._resolve_lazy_value()))
                            else:
                                all_cols.append(pl.lit(col.value))
                        else:
                            # Column or expression
                            all_cols.append(self.translate(col))
                    # Cast all to string and concatenate
                    str_cols = [col.cast(pl.Utf8) for col in all_cols]
                    result = str_cols[0]
                    for other_col in str_cols[1:]:
                        result = result + other_col
                    return result
                else:
                    # Single column - just cast to string
                    return col_expr.cast(pl.Utf8)
            elif operation == "concat_ws":
                # concat_ws(sep, *columns) - op.value is (sep, [columns])
                if isinstance(op.value, tuple) and len(op.value) >= 1:
                    sep = op.value[0]
                    other_cols = op.value[1] if len(op.value) > 1 else []
                    # Translate all columns - ensure they're properly translated
                    translated_cols = []
                    # First column is already in col_expr
                    translated_cols.append(col_expr)
                    # Translate other columns
                    for col in other_cols:
                        if isinstance(col, str):
                            # String column name
                            translated_cols.append(pl.col(col))
                        elif isinstance(col, (int, float, bool)):
                            # Literal value
                            translated_cols.append(pl.lit(col))
                        else:
                            # Expression or Column
                            translated_cols.append(self.translate(col))
                    # Join with separator using Polars
                    # Ensure all columns are strings to avoid nested Objects error
                    if len(translated_cols) == 1:
                        return translated_cols[0].cast(pl.Utf8)
                    # Cast all to string first
                    str_cols = [col.cast(pl.Utf8) for col in translated_cols]
                    result = str_cols[0]
                    for other_col in str_cols[1:]:
                        result = result + pl.lit(str(sep)) + other_col
                    return result
                else:
                    raise ValueError("concat_ws requires (sep, [columns]) tuple")
            elif operation == "like":
                # SQL LIKE pattern - convert to Polars regex
                pattern = op.value
                # Convert SQL LIKE to regex: % -> .*, _ -> .
                regex_pattern = pattern.replace("%", ".*").replace("_", ".")
                return col_expr.str.contains(regex_pattern, literal=False)
            elif operation == "rlike":
                # Regular expression pattern matching
                pattern = op.value
                return col_expr.str.contains(pattern, literal=False)
            elif operation == "round":
                # round(col, decimals)
                decimals = op.value if isinstance(op.value, int) else 0
                if decimals < 0:
                    # Negative decimals: round to nearest 10^|decimals|
                    # e.g., round(12345, -3) = round(12345/1000) * 1000 = 12000
                    factor = 10 ** abs(decimals)
                    return (col_expr / factor).round() * factor
                else:
                    return col_expr.round(decimals)
            elif operation == "pow":
                # pow(col, exponent)
                exponent = (
                    self.translate(op.value)
                    if not isinstance(op.value, (int, float))
                    else pl.lit(op.value)
                )
                return col_expr.pow(exponent)
            elif operation == "power":
                # power(col, exponent) - Alias for pow
                exponent = (
                    self.translate(op.value)
                    if not isinstance(op.value, (int, float))
                    else pl.lit(op.value)
                )
                return col_expr.pow(exponent)
            elif operation == "to_date":
                # to_date(col, format) or to_date(col)
                # PySpark accepts StringType, TimestampType, or DateType
                # If input is already TimestampType or DateType, convert directly
                # If input is StringType, parse with format

                # IMPORTANT: Check for nested to_timestamp BEFORE translating col_expr
                # This allows us to detect the nested structure before it's converted to a Polars expression
                is_nested_to_timestamp = (
                    isinstance(op.column, ColumnOperation)
                    and op.column.operation == "to_timestamp"
                )

                if is_nested_to_timestamp:
                    # For to_date(to_timestamp(...)), the input is already datetime
                    # Use .dt.date() directly for datetime columns to avoid schema validation issues
                    # First translate the nested to_timestamp to get the datetime expression
                    nested_ts_expr = self._translate_operation(
                        op.column, input_col_dtype=None
                    )
                    # Use .dt.date() for datetime columns - this avoids schema validation issues
                    return nested_ts_expr.dt.date()

                # Use map_elements to handle both StringType and TimestampType/DateType inputs
                # This avoids the issue where .str.strptime fails on datetime columns
                def convert_to_date(val: Any, format_str: Optional[str] = None) -> Any:
                    from datetime import datetime, date

                    if val is None:
                        return None
                    # If already a date, return as-is
                    if isinstance(val, date) and not isinstance(val, datetime):
                        return val
                    # If datetime, convert to date
                    if isinstance(val, datetime):
                        return val.date()
                    # If string, parse with format
                    if isinstance(val, str):
                        if format_str:
                            try:
                                dt = datetime.strptime(val, format_str)
                                return dt.date()
                            except (ValueError, TypeError):
                                return None
                        else:
                            # Try common formats
                            for fmt in [
                                "%Y-%m-%d %H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S",
                                "%Y-%m-%d",
                            ]:
                                try:
                                    dt = datetime.strptime(val, fmt)
                                    return dt.date()
                                except ValueError:
                                    continue
                            return None
                    return None

                if op.value is not None:
                    # With format string - convert Java SimpleDateFormat to Polars format
                    format_str = op.value
                    import re

                    # Handle single-quoted literals (e.g., 'T' in yyyy-MM-dd'T'HH:mm:ss)
                    format_str = re.sub(r"'([^']*)'", r"\1", format_str)
                    # Convert Java format to Polars format
                    format_map = {
                        "yyyy": "%Y",
                        "MM": "%m",
                        "dd": "%d",
                        "HH": "%H",
                        "mm": "%M",
                        "ss": "%S",
                    }
                    # Sort by length descending to process longest matches first
                    for java_pattern, polars_pattern in sorted(
                        format_map.items(), key=lambda x: len(x[0]), reverse=True
                    ):
                        format_str = format_str.replace(java_pattern, polars_pattern)
                    # Use map_elements to handle both StringType and TimestampType inputs
                    # Wrap in a lambda that captures format_str to avoid closure issues
                    return col_expr.map_elements(
                        lambda x, fmt=format_str: convert_to_date(x, fmt),
                        return_dtype=pl.Date,
                    )
                else:
                    # Without format - use map_elements which checks type at runtime
                    return col_expr.map_elements(
                        lambda x: convert_to_date(x),
                        return_dtype=pl.Date,
                    )
            elif operation == "date_format":
                # date_format(col, format) - format a date/timestamp column
                if isinstance(op.value, str):
                    format_str = op.value
                    # Convert Java SimpleDateFormat to Polars strftime format
                    # Common conversions: yyyy -> %Y, MM -> %m, dd -> %d, HH -> %H, mm -> %M, ss -> %S
                    import re

                    format_map = {
                        "yyyy": "%Y",
                        "MM": "%m",
                        "dd": "%d",
                        "HH": "%H",
                        "mm": "%M",
                        "ss": "%S",
                        "EEE": "%a",
                        "EEEE": "%A",
                        "MMM": "%b",
                        "MMMM": "%B",
                    }
                    polars_format = format_str
                    for java_pattern, polars_pattern in sorted(
                        format_map.items(), key=lambda x: len(x[0]), reverse=True
                    ):
                        polars_format = polars_format.replace(
                            java_pattern, polars_pattern
                        )

                    # If column is string, parse it first; if already date/timestamp, use directly
                    # Try to parse as datetime first (handles timestamps), then fall back to date
                    # For string columns, try datetime format first (handles "2024-01-15 10:30:00")
                    # then fall back to date format (handles "2024-01-15")
                    # Use map_elements to handle both formats
                    def parse_and_format(val: Optional[str]) -> Optional[str]:
                        if val is None:
                            return None
                        from datetime import datetime

                        # Try datetime format first
                        try:
                            dt = datetime.strptime(val, "%Y-%m-%d %H:%M:%S")
                            return dt.strftime(polars_format)
                        except ValueError:
                            # Fall back to date format
                            try:
                                dt = datetime.strptime(val, "%Y-%m-%d")
                                return dt.strftime(polars_format)
                            except ValueError:
                                return None

                    # Use map_elements for flexible parsing
                    return col_expr.map_elements(parse_and_format, return_dtype=pl.Utf8)
                else:
                    raise ValueError("date_format requires format string")
            elif operation == "date_add":
                # date_add(col, days) - add days to a date column
                # Handle both string dates and date columns
                if isinstance(op.value, int):
                    days = op.value
                    days_expr = pl.duration(days=days)
                else:
                    days_expr = self.translate(op.value)
                    # If it's a literal, extract the value for duration
                    if isinstance(days_expr, pl.Expr):
                        # It's an expression - try to use it directly with duration
                        # For literals, we can extract the value
                        # For now, assume it's a literal integer
                        # Actually, we need to handle this differently - use the expression value if available
                        # For expressions, we'll need to convert to duration
                        # Simplest: assume days is an integer literal
                        days = op.value if isinstance(op.value, int) else int(op.value)
                        days_expr = pl.duration(days=days)
                    else:
                        days = (
                            int(days_expr)
                            if not isinstance(days_expr, int)
                            else days_expr
                        )
                        days_expr = pl.duration(days=days)
                # Parse string dates first, then add duration
                # Always try parsing as string first (most common case)
                date_col = col_expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
                return date_col + days_expr
            elif operation == "date_sub":
                # date_sub(col, days) - subtract days from a date column
                if isinstance(op.value, int):
                    days = op.value
                    days_expr = pl.duration(days=days)
                else:
                    days_expr = self.translate(op.value)
                    if isinstance(days_expr, pl.Expr):
                        days = op.value if isinstance(op.value, int) else int(op.value)
                        days_expr = pl.duration(days=days)
                    else:
                        days = (
                            int(days_expr)
                            if not isinstance(days_expr, int)
                            else days_expr
                        )
                        days_expr = pl.duration(days=days)
                # Parse string dates first, then subtract duration
                date_col = col_expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
                return date_col - days_expr
            elif operation == "datediff":
                # datediff(end, start) - note: in PySpark, end comes first
                # In ColumnOperation: column is end, value is start
                # Handle Literal objects in value
                from ...functions.core.literals import Literal

                if isinstance(op.value, Literal):
                    start_date = pl.lit(op.value.value)
                else:
                    start_date = self.translate(op.value)
                # Handle both string dates and date columns
                # Polars str.strptime() only works on string columns, so it fails on date columns
                # Use cast to Date which works for both: strings are parsed, dates are unchanged
                end_parsed = col_expr.cast(pl.Date)
                start_parsed = start_date.cast(pl.Date)
                return (end_parsed - start_parsed).dt.total_days()
            elif operation == "lpad":
                # lpad(col, len, pad)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    target_len = op.value[0]
                    pad_str = op.value[1]
                    return col_expr.str.pad_start(target_len, pad_str)
                else:
                    raise ValueError("lpad requires (len, pad) tuple")
            elif operation == "rpad":
                # rpad(col, len, pad)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    target_len = op.value[0]
                    pad_str = op.value[1]
                    return col_expr.str.pad_end(target_len, pad_str)
                else:
                    raise ValueError("rpad requires (len, pad) tuple")
            elif operation == "repeat":
                # repeat(col, n) - repeat string n times
                # Polars doesn't have str.repeat(), use string concatenation
                n = op.value if isinstance(op.value, int) else int(op.value)
                if n <= 0:
                    return pl.lit("")
                # Build expression: col + col + ... + col (n times)
                result = col_expr
                for _ in range(n - 1):
                    result = result + col_expr
                return result
            elif operation == "instr":
                # instr(col, substr) - returns 1-based position, or 0 if not found
                substr = op.value if isinstance(op.value, str) else str(op.value)
                # Polars str.find() returns -1 if not found, we need 0
                # So we check if it's -1, return 0, otherwise add 1 for 1-based indexing
                # Add fill_null(0) as fallback for any nulls
                find_result = col_expr.str.find(substr)
                return (
                    pl.when(find_result == -1)
                    .then(0)
                    .otherwise(find_result + 1)
                    .fill_null(0)
                )
            elif operation == "locate":
                # locate(substr, col, pos) - op.value is (substr, pos)
                if isinstance(op.value, tuple) and len(op.value) >= 1:
                    substr = op.value[0]
                    pos = op.value[1] if len(op.value) > 1 else 1
                    # Find substring starting from pos (1-indexed)
                    return (
                        col_expr.str.slice(pos - 1).str.find(substr) + pos
                    ).fill_null(0)
                else:
                    substr = op.value
                    return col_expr.str.find(substr) + 1
            elif operation == "add_months":
                # add_months(col, months) - add months to a date column
                months = op.value if isinstance(op.value, int) else int(op.value)
                # Parse string dates first, or use directly if already a date
                # Try parsing as string first (most common case)
                try:
                    date_col = col_expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
                except AttributeError:
                    # Already a date column, use directly
                    date_col = col_expr.cast(pl.Date)
                # Convert to datetime for offset_by, then back to date
                datetime_col = date_col.cast(pl.Datetime)
                # Use offset_by with months
                return datetime_col.dt.offset_by(f"{months}mo").cast(pl.Date)
            elif operation == "last_day":
                # last_day(col) - get last day of month
                # Parse string dates first, or use directly if already a date
                # Try parsing as string first (most common case)
                try:
                    date_col = col_expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
                except AttributeError:
                    # Already a date column, use directly
                    date_col = col_expr.cast(pl.Date)
                # Get first day of current month
                first_of_month = date_col.dt.replace(day=1)
                # Add 1 month to get first of next month (using string offset)
                first_of_next_month = first_of_month.dt.offset_by("1mo")
                # Subtract 1 day to get last day of current month
                return first_of_next_month.dt.offset_by("-1d")
            elif operation == "array_contains":
                # array_contains(col, value) - check if array contains value
                value_expr = (
                    pl.lit(op.value)
                    if not isinstance(op.value, (Column, ColumnOperation))
                    else self.translate(op.value)
                )
                return col_expr.list.contains(value_expr)
            elif operation == "array_position":
                # array_position(col, value) - find 1-based position of value in array
                # Polars doesn't have list.index(), so we use list.eval to find position
                value_expr = (
                    pl.lit(op.value)
                    if not isinstance(op.value, (Column, ColumnOperation))
                    else self.translate(op.value)
                )
                # Use list.eval to create indices where element equals value, get first, add 1 for 1-based
                # If not found, returns null, which we convert to 0 (PySpark returns 0 if not found)
                return (
                    col_expr.list.eval(
                        pl.int_range(pl.len()).filter(pl.element() == value_expr)
                    ).list.first()
                ).fill_null(-1) + 1
            elif operation == "element_at":
                # element_at(col, index) - get element at 1-based index (negative for reverse)
                index = op.value if isinstance(op.value, int) else int(op.value)
                # Polars list.get() uses 0-based indexing, but element_at is 1-based
                # For negative indices, count from end
                if index > 0:
                    return col_expr.list.get(index - 1)
                else:
                    # Negative index: count from end
                    return col_expr.list.get(index)
            elif operation == "array_append":
                # array_append(col, value) - append value to array
                # Polars doesn't have list.append(), use list.eval with concat
                value_expr = (
                    pl.lit(op.value)
                    if not isinstance(op.value, (Column, ColumnOperation))
                    else self.translate(op.value)
                )
                return col_expr.list.eval(pl.concat([pl.element(), value_expr]))
            elif operation == "array_remove":
                # array_remove(col, value) - remove all occurrences of value from array
                value_expr = (
                    pl.lit(op.value)
                    if not isinstance(op.value, (Column, ColumnOperation))
                    else self.translate(op.value)
                )
                return col_expr.list.eval(
                    pl.element().filter(pl.element() != value_expr)
                )
            elif operation == "array":
                # array(*cols) - create array containing values from each column as elements
                # So array(arr1, arr2) where arr1=[1,2,3] and arr2=[4,5] creates [[1,2,3], [4,5]]
                # NOT [1,2,3,4,5] (which would be concatenation)
                # Polars concat_list concatenates arrays, so we need Python evaluation
                # Raise ValueError to trigger Python evaluation fallback
                raise ValueError(
                    "array function requires Python evaluation to create array of arrays"
                )
            elif operation == "timestamp_seconds":
                # timestamp_seconds needs to return formatted string, not datetime object
                # Force Python evaluation to format correctly
                raise ValueError(
                    "timestamp_seconds requires Python evaluation to format timestamp string"
                )
            elif operation == "to_utc_timestamp":
                # to_utc_timestamp needs timezone conversion
                # Force Python evaluation for proper timezone handling
                raise ValueError(
                    "to_utc_timestamp requires Python evaluation for timezone conversion"
                )
            elif operation == "from_utc_timestamp":
                # from_utc_timestamp needs timezone conversion
                # Force Python evaluation for proper timezone handling
                raise ValueError(
                    "from_utc_timestamp requires Python evaluation for timezone conversion"
                )
            elif operation == "nanvl":
                # nanvl(col1, col2) - returns col1 if not NaN, col2 if col1 is NaN
                # PySpark generates: CASE WHEN (NOT (col1 = col1)) THEN col2 ELSE col1 END
                # Polars: use is_nan() check
                col2_expr = self.translate(op.value)
                # Check if col1 is NaN: return col2 if col1 is NaN, otherwise return col1
                return pl.when(col_expr.is_nan()).then(col2_expr).otherwise(col_expr)
            elif operation == "array_intersect":
                # array_intersect(col1, col2) - intersection of two arrays
                col2_expr = self.translate(op.value)
                return col_expr.list.set_intersection(col2_expr)
            elif operation == "array_union":
                # array_union(col1, col2) - union of two arrays (duplicates removed)
                col2_expr = self.translate(op.value)
                return col_expr.list.set_union(col2_expr)
            elif operation == "array_except":
                # array_except(col1, col2) - elements in col1 but not in col2
                col2_expr = self.translate(op.value)
                return col_expr.list.set_difference(col2_expr)
            elif operation == "array_join":
                # array_join(col, delimiter, null_replacement) - join array elements with delimiter
                # op.value is a tuple: (delimiter, null_replacement)
                if isinstance(op.value, tuple) and len(op.value) >= 1:
                    delimiter = op.value[0]
                    null_replacement = op.value[1] if len(op.value) > 1 else None
                    # Polars list.join() takes a separator string
                    # Handle null_replacement by filtering nulls and replacing them before joining
                    if null_replacement is not None:
                        # Replace nulls with null_replacement string, then join
                        return col_expr.list.eval(
                            pl.element()
                            .fill_null(pl.lit(null_replacement))
                            .cast(pl.Utf8)
                        ).list.join(str(delimiter))
                    else:
                        # Filter out nulls and join with delimiter
                        return col_expr.list.eval(
                            pl.element()
                            .filter(pl.element().is_not_null())
                            .cast(pl.Utf8)
                        ).list.join(str(delimiter))
                else:
                    # Fallback: just delimiter
                    delimiter = op.value if isinstance(op.value, str) else str(op.value)
                    return col_expr.list.eval(
                        pl.element().filter(pl.element().is_not_null()).cast(pl.Utf8)
                    ).list.join(delimiter)
            elif operation == "arrays_overlap":
                # arrays_overlap(col1, col2) - check if arrays have common elements
                col2_expr = self.translate(op.value)
                # Check if intersection is non-empty
                intersection = col_expr.list.set_intersection(col2_expr)
                return intersection.list.len() > 0
            elif operation == "array_repeat":
                # array_repeat(col, count) - repeat value to create array
                # Polars doesn't have a direct repeat for columns, use map_elements
                count = op.value if isinstance(op.value, int) else int(op.value)
                # Use map_elements to create array by repeating value
                # Polars will infer the list type from the element type
                return col_expr.map_elements(lambda x: [x] * count)
            elif operation == "slice":
                # slice(col, start, length) - get slice of array (1-based start)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    start = op.value[0]
                    length = op.value[1]
                    # Convert 1-based to 0-based for Polars
                    start_idx = start - 1 if start > 0 else 0
                    return col_expr.list.slice(start_idx, length)
                else:
                    raise ValueError("slice requires (start, length) tuple")
            elif operation == "str_to_map":
                # str_to_map(col, pair_delim, key_value_delim)
                if isinstance(op.value, tuple) and len(op.value) >= 2:
                    pair_delim, key_value_delim = op.value[0], op.value[1]
                    return col_expr.map_elements(
                        lambda x, pd=pair_delim, kvd=key_value_delim: (
                            {
                                kv.split(kvd, 1)[0].strip(): kv.split(kvd, 1)[1].strip()
                                for kv in x.split(pd)
                                if kvd in kv
                            }
                            if isinstance(x, str) and x
                            else {}
                        ),
                        return_dtype=pl.Object,
                    )
                else:
                    raise ValueError(
                        "str_to_map requires (pair_delim, key_value_delim) tuple"
                    )
            # New crypto functions (PySpark 3.5+)
            elif operation == "aes_encrypt":
                # aes_encrypt(data, key, mode, padding)
                # Simplified: return NULL for now (encryption requires external library)
                return pl.lit(None).cast(pl.Binary)
            elif operation == "aes_decrypt":
                # aes_decrypt(data, key, mode, padding)
                # Simplified: return NULL for now (decryption requires external library)
                return pl.lit(None).cast(pl.Utf8)
            elif operation == "try_aes_decrypt":
                # try_aes_decrypt(data, key, mode, padding) - null-safe version
                # Simplified: return NULL for now (decryption requires external library)
                return pl.lit(None).cast(pl.Utf8)
            # New string functions (PySpark 3.5+)
            elif operation == "sha":
                # sha(col) - alias for sha1
                import hashlib

                return col_expr.map_elements(
                    lambda x: hashlib.sha1(
                        x.encode("utf-8")
                        if isinstance(x, str)
                        else str(x).encode("utf-8")
                    ).hexdigest()
                    if x is not None
                    else "",
                    return_dtype=pl.Utf8,
                )
            elif operation == "mask":
                # mask(col, upperChar='X', lowerChar='x', digitChar='n', otherChar='-')
                import re

                params = op.value if isinstance(op.value, dict) else {}
                upper_char = params.get("upperChar", "X")
                lower_char = params.get("lowerChar", "x")
                digit_char = params.get("digitChar", "n")
                other_char = params.get("otherChar", "-")
                return col_expr.map_elements(
                    lambda x,
                    uc=upper_char,
                    lc=lower_char,
                    dc=digit_char,
                    oc=other_char: (
                        "".join(
                            uc
                            if c.isupper()
                            else lc
                            if c.islower()
                            else dc
                            if c.isdigit()
                            else oc
                            for c in x
                        )
                        if isinstance(x, str) and x
                        else x
                    ),
                    return_dtype=pl.Utf8,
                )
            elif operation == "json_array_length":
                # json_array_length(col, path)
                import json

                path = op.value if op.value else None
                return col_expr.map_elements(
                    lambda x, p=path: (
                        len(json.loads(x).get(p.lstrip("$."), []))
                        if p and isinstance(json.loads(x), dict)
                        else len(json.loads(x))
                        if isinstance(json.loads(x), list)
                        else 0
                        if isinstance(x, str)
                        else 0
                    ),
                    return_dtype=pl.Int64,
                )
            elif operation == "json_object_keys":
                # json_object_keys(col, path)
                import json

                path = op.value if op.value else None
                return col_expr.map_elements(
                    lambda x, p=path: (
                        list(json.loads(x).get(p.lstrip("$."), {}).keys())
                        if p and isinstance(json.loads(x), dict)
                        else list(json.loads(x).keys())
                        if isinstance(json.loads(x), dict)
                        else []
                        if isinstance(x, str)
                        else []
                    ),
                    return_dtype=pl.List(pl.Utf8),
                )
            elif operation == "xpath_number":
                # xpath_number(col, path) - simplified XML parsing
                # Note: Full XPath support requires lxml or similar library
                return pl.lit(None).cast(pl.Float64)
            elif operation == "user":
                # user() - get current user name
                import os

                return pl.lit(os.getenv("USER", os.getenv("USERNAME", "unknown")))
            # New math functions (PySpark 3.5+)
            elif operation == "getbit":
                # getbit(col, bit) - get bit at position
                bit_expr = (
                    self.translate(op.value)
                    if not isinstance(op.value, (int, float))
                    else pl.lit(int(op.value))
                )
                return (col_expr.cast(pl.Int64) >> bit_expr.cast(pl.Int64)) & 1
            elif operation == "width_bucket":
                # width_bucket(value, min_value, max_value, num_buckets)
                if isinstance(op.value, tuple) and len(op.value) >= 3:
                    min_val, max_val, num_buckets = (
                        op.value[0],
                        op.value[1],
                        op.value[2],
                    )
                    min_expr = (
                        self.translate(min_val)
                        if not isinstance(min_val, (int, float))
                        else pl.lit(float(min_val))
                    )
                    max_expr = (
                        self.translate(max_val)
                        if not isinstance(max_val, (int, float))
                        else pl.lit(float(max_val))
                    )
                    num_buckets_expr = (
                        self.translate(num_buckets)
                        if not isinstance(num_buckets, int)
                        else pl.lit(int(num_buckets))
                    )
                    # Compute bucket: floor((value - min) / (max - min) * num_buckets) + 1
                    # Clamp to [1, num_buckets]
                    bucket = (
                        (col_expr.cast(pl.Float64) - min_expr)
                        / (max_expr - min_expr)
                        * num_buckets_expr
                    ).floor() + 1
                    return pl.max_horizontal(
                        [pl.min_horizontal([bucket, num_buckets_expr]), pl.lit(1)]
                    )
                else:
                    raise ValueError(
                        "width_bucket requires (min_value, max_value, num_buckets) tuple"
                    )
            # New datetime functions (PySpark 3.5+)
            elif operation == "date_from_unix_date":
                # date_from_unix_date(days) - convert days since epoch to date
                # Convert days to date by adding days to epoch
                return (
                    pl.datetime(1970, 1, 1) + pl.duration(days=col_expr.cast(pl.Int64))
                ).dt.date()
            elif operation == "to_timestamp_ltz":
                # to_timestamp_ltz(col, format) - timestamp with local timezone
                format_str = op.value if op.value else None
                if format_str:
                    return col_expr.str.strptime(pl.Datetime, format_str, strict=False)
                else:
                    return col_expr.str.strptime(pl.Datetime, strict=False)
            elif operation == "to_timestamp_ntz":
                # to_timestamp_ntz(col, format) - timestamp with no timezone
                format_str = op.value if op.value else None
                if format_str:
                    return col_expr.str.strptime(pl.Datetime, format_str, strict=False)
                else:
                    return col_expr.str.strptime(pl.Datetime, strict=False)
            elif operation == "unix_timestamp":
                # unix_timestamp(timestamp, format) or unix_timestamp() - convert to Unix timestamp (seconds since epoch)
                # Note: unix_timestamp() without arguments is handled earlier, before col_expr is created
                # If format is provided, parse string first, then convert to Unix timestamp

                # If format is provided, parse string first
                if op.value is not None:
                    format_str = op.value
                    import re
                    from datetime import datetime as dt

                    # Handle single-quoted literals (e.g., 'T' in yyyy-MM-dd'T'HH:mm:ss)
                    format_str = re.sub(r"'([^']*)'", r"\1", format_str)
                    # Convert Java format to Python format
                    format_map = {
                        "yyyy": "%Y",
                        "MM": "%m",
                        "dd": "%d",
                        "HH": "%H",
                        "mm": "%M",
                        "ss": "%S",
                    }
                    # Sort by length descending to process longest matches first
                    for java_pattern, python_pattern in sorted(
                        format_map.items(), key=lambda x: len(x[0]), reverse=True
                    ):
                        format_str = format_str.replace(java_pattern, python_pattern)

                    # Parse string to datetime, then convert to Unix timestamp
                    def parse_and_convert(val: Any, fmt: str) -> Any:
                        if val is None:
                            return None
                        if isinstance(val, str):
                            try:
                                dt_obj = dt.strptime(val, fmt)
                                return int(dt_obj.timestamp())
                            except (ValueError, TypeError):
                                return None
                        return None

                    return col_expr.map_elements(
                        lambda x, fmt=format_str: parse_and_convert(x, fmt),
                        return_dtype=pl.Int64,
                    )
                else:
                    # No format - assume column is already datetime/timestamp
                    # Use map_elements to handle both Polars datetime columns and Python datetime objects
                    def datetime_to_unix(val: Any) -> Any:
                        from datetime import datetime as dt

                        if val is None:
                            return None
                        if isinstance(val, dt):
                            return int(val.timestamp())
                        if isinstance(val, str):
                            # Try to parse common formats
                            for fmt in [
                                "%Y-%m-%d %H:%M:%S",
                                "%Y-%m-%dT%H:%M:%S",
                                "%Y-%m-%d",
                            ]:
                                try:
                                    dt_obj = dt.strptime(val, fmt)
                                    return int(dt_obj.timestamp())
                                except ValueError:
                                    continue
                            return None
                        # If it's already a number, assume it's already a Unix timestamp
                        if isinstance(val, (int, float)):
                            return int(val)
                        # Try to convert to datetime if it has datetime-like attributes
                        if hasattr(val, "timestamp"):
                            try:
                                return int(val.timestamp())
                            except (AttributeError, TypeError):
                                pass
                        return None

                    return col_expr.map_elements(
                        datetime_to_unix,
                        return_dtype=pl.Int64,
                    )
            # New null-safe try functions (PySpark 3.5+)
            elif operation == "try_add":
                # try_add(left, right) - null-safe addition
                right_expr = self.translate(op.value)
                return (
                    pl.when(col_expr.is_null() | right_expr.is_null())
                    .then(None)
                    .otherwise(col_expr + right_expr)
                )
            elif operation == "try_subtract":
                # try_subtract(left, right) - null-safe subtraction
                right_expr = self.translate(op.value)
                return (
                    pl.when(col_expr.is_null() | right_expr.is_null())
                    .then(None)
                    .otherwise(col_expr - right_expr)
                )
            elif operation == "try_multiply":
                # try_multiply(left, right) - null-safe multiplication
                right_expr = self.translate(op.value)
                return (
                    pl.when(col_expr.is_null() | right_expr.is_null())
                    .then(None)
                    .otherwise(col_expr * right_expr)
                )
            elif operation == "try_divide":
                # try_divide(left, right) - null-safe division
                right_expr = self.translate(op.value)
                return (
                    pl.when(
                        (col_expr.is_null() | right_expr.is_null()) | (right_expr == 0)
                    )
                    .then(None)
                    .otherwise(col_expr / right_expr)
                )
            elif operation == "try_element_at":
                # try_element_at(col, index) - null-safe element_at
                index_expr = (
                    self.translate(op.value)
                    if not isinstance(op.value, (int, float))
                    else pl.lit(int(op.value))
                )
                # Try array access first, then map access
                try:
                    # Array access: 1-based indexing
                    return col_expr.list.get(index_expr.cast(pl.Int64) - 1)
                except Exception:
                    # Map access: use key directly
                    logger.debug(
                        "Array access failed, falling back to map access", exc_info=True
                    )
                    return col_expr.map_elements(
                        lambda x, idx=index_expr: x.get(idx)
                        if isinstance(x, dict)
                        else None,
                        return_dtype=pl.Object,
                    )
            elif operation == "try_to_binary":
                # try_to_binary(col, format) - null-safe to_binary
                format_str = op.value if op.value else "utf-8"
                return col_expr.map_elements(
                    lambda x, fmt=format_str: (
                        x.encode(fmt)
                        if isinstance(x, str) and x
                        else str(x).encode(fmt)
                        if isinstance(x, (int, float))
                        else x
                        if isinstance(x, bytes)
                        else None
                    ),
                    return_dtype=pl.Binary,
                )
            elif operation == "try_to_number":
                # try_to_number(col, format) - null-safe to_number
                return col_expr.map_elements(
                    lambda x: (
                        float(x)
                        if isinstance(x, str) and x
                        else int(x)
                        if isinstance(x, str) and x and "." not in x
                        else x
                        if isinstance(x, (int, float))
                        else None
                    ),
                    return_dtype=pl.Float64,
                )
            elif operation == "try_to_timestamp":
                # try_to_timestamp(col, format) - null-safe to_timestamp
                format_str = op.value if op.value else None
                if format_str:
                    return col_expr.str.strptime(pl.Datetime, format_str, strict=False)
                else:
                    return col_expr.str.strptime(pl.Datetime, strict=False)

        # Handle special functions that need custom logic (including those that may have column but ignore it)
        if function_name == "monotonically_increasing_id":
            # monotonically_increasing_id() - can be called with or without column (ignores column)
            # Use int_range to generate sequential IDs
            return pl.int_range(pl.len())
        elif function_name == "expr":
            # expr(sql_string) - parse and evaluate SQL expression
            # Implement minimal SQL parsing for common cases like CASE WHEN
            if op.value is not None and isinstance(op.value, str):
                sql_expr = op.value.strip()
                # Try to parse simple CASE WHEN expressions
                # Pattern: CASE WHEN condition THEN value1 ELSE value2 END
                sql_lower = sql_expr.lower()
                if sql_lower.startswith("case when") and sql_lower.endswith("end"):
                    return self._parse_simple_case_when(sql_expr)
                else:
                    # For other SQL expressions, raise error (can be extended later)
                    raise ValueError(
                        f"F.expr() SQL expressions should be handled by SQL executor, not Polars backend. Unsupported expression: {sql_expr}"
                    )
            else:
                raise ValueError("F.expr() requires a SQL string")
        if function_name == "coalesce":
            # coalesce(*cols) - op.value should be list of columns
            if op.value is not None and isinstance(op.value, (list, tuple)):
                cols = [col_expr] + [self.translate(col) for col in op.value]
                return pl.coalesce(cols)
            else:
                return col_expr
        elif function_name == "nvl":
            # nvl(col, default) - op.value is default value
            if op.value is not None:
                default_expr = (
                    self.translate(op.value)
                    if not isinstance(op.value, (str, int, float, bool))
                    else pl.lit(op.value)
                )
                return pl.coalesce([col_expr, default_expr])
            else:
                return col_expr
        elif function_name == "nullif":
            # nullif(col1, col2) - op.value is col2
            if op.value is not None:
                col2_expr = self.translate(op.value)
                return pl.when(col_expr == col2_expr).then(None).otherwise(col_expr)
            else:
                return col_expr
        elif function_name == "greatest":
            # greatest(*cols) - op.value should be list of columns
            if op.value is not None and isinstance(op.value, (list, tuple)):
                cols = [col_expr] + [self.translate(col) for col in op.value]
                return pl.max_horizontal(cols)
            else:
                return col_expr
        elif function_name == "least":
            # least(*cols) - op.value should be list of columns
            if op.value is not None and isinstance(op.value, (list, tuple)):
                cols = [col_expr] + [self.translate(col) for col in op.value]
                return pl.min_horizontal(cols)
            else:
                return col_expr
        elif function_name == "ascii":
            # ascii(col) - return ASCII code of first character
            # Get first character and convert to its ASCII/UTF-8 code point
            first_char = col_expr.str.slice(0, 1)
            return first_char.map_elements(
                lambda x: ord(x) if x else 0, return_dtype=pl.Int32
            ).fill_null(0)
        elif function_name == "hex":
            # hex(col) - convert to hexadecimal string
            # For numeric types: convert number to hex string (e.g., 10 -> "A", 255 -> "FF")
            # For string types: encode string to bytes then hex (e.g., "Alice" -> "416C696365")
            # We need to detect the type - if it's numeric, use numeric hex conversion
            # For now, try numeric conversion first, fallback to string encoding
            return col_expr.map_elements(
                lambda x: (
                    hex(int(x))[2:].upper()
                    if isinstance(x, (int, float))
                    and not (isinstance(x, float) and math.isnan(x))
                    else x.encode("utf-8").hex().upper()
                    if isinstance(x, str)
                    else str(x).encode("utf-8").hex().upper()
                    if x is not None
                    else ""
                ),
                return_dtype=pl.Utf8,
            )
        elif function_name == "base64":
            # base64(col) - encode to base64
            import base64

            return col_expr.map_elements(
                lambda x: base64.b64encode(
                    x.encode("utf-8") if isinstance(x, str) else str(x).encode("utf-8")
                ).decode("utf-8")
                if x is not None
                else "",
                return_dtype=pl.Utf8,
            )
        elif function_name == "md5":
            # md5(col) - hash using MD5
            import hashlib

            return col_expr.map_elements(
                lambda x: hashlib.md5(
                    x.encode("utf-8") if isinstance(x, str) else str(x).encode("utf-8")
                ).hexdigest()
                if x is not None
                else "",
                return_dtype=pl.Utf8,
            )
        elif function_name == "sha1":
            # sha1(col) - hash using SHA1
            import hashlib

            return col_expr.map_elements(
                lambda x: hashlib.sha1(
                    x.encode("utf-8") if isinstance(x, str) else str(x).encode("utf-8")
                ).hexdigest()
                if x is not None
                else "",
                return_dtype=pl.Utf8,
            )
        elif function_name == "sha2":
            # sha2(col, bitLength) - hash using SHA2
            import hashlib

            bitLength = op.value if op.value is not None else 256
            hash_func = {
                256: hashlib.sha256,
                384: hashlib.sha384,
                512: hashlib.sha512,
            }.get(bitLength, hashlib.sha256)
            return col_expr.map_elements(
                lambda x: hash_func(
                    x.encode("utf-8") if isinstance(x, str) else str(x).encode("utf-8")
                ).hexdigest()
                if x is not None
                else "",
                return_dtype=pl.Utf8,
            )
        elif function_name == "map_keys":
            # map_keys(col) - extract all keys from map/dict as array
            # Polars converts dicts to structs, so we need to get only non-null struct fields
            # Use struct operations to check each field for null and collect non-null field names
            # This requires accessing the struct dtype, which we can't do at translation time
            # So we use a workaround: map_elements with a lambda that checks struct fields
            # For Polars structs, we need to iterate through all possible fields and check nullness
            # Since we can't access dtype at translation time, use map_elements with runtime dtype check
            return col_expr.map_elements(
                lambda x: (
                    # If it's a dict, use keys directly
                    list(x.keys())
                    if isinstance(x, dict)
                    # If it's a Polars struct (Row object), get field names from schema
                    else [
                        k
                        for k in getattr(x, "_schema", {})
                        if getattr(x, k, None) is not None
                    ]
                    if hasattr(x, "_schema")
                    # Try to get struct fields using __struct_fields__
                    else [
                        f.name
                        for f in getattr(x, "__struct_fields__", [])
                        if getattr(x, f.name, None) is not None
                    ]
                    if hasattr(x, "__struct_fields__")
                    # For dict-like objects, filter by non-null values
                    else [k for k, v in x.items() if v is not None]
                    if hasattr(x, "items") and callable(x.items)
                    else None
                )
                if x is not None
                else None,
                return_dtype=pl.List(pl.Utf8),
            )
        elif function_name == "map_values":
            # map_values(col) - extract all values from map/dict as array
            # For structs, get only non-null values; for dicts, get values
            return col_expr.map_elements(
                lambda x: (
                    list(x.values())
                    if isinstance(x, dict)
                    else [x.get(k) for k in x if x.get(k) is not None]
                    if isinstance(x, dict)
                    else [
                        getattr(x, f.name)
                        for f in x.__struct_fields__
                        if getattr(x, f.name, None) is not None
                    ]
                    if hasattr(x, "__struct_fields__")
                    else None
                )
                if x is not None
                else None,
                return_dtype=pl.List(None),  # Type will be inferred from values
            )
        elif function_name == "map_entries":
            # map_entries(col) - convert map to array of structs with key and value
            # PySpark returns array of structs with 'key' and 'value' fields
            return col_expr.map_elements(
                lambda x: (
                    [{"key": k, "value": v} for k, v in x.items()]
                    if isinstance(x, dict)
                    else [
                        {"key": k, "value": x.get(k)} for k in x if x.get(k) is not None
                    ]
                    if isinstance(x, dict)
                    else [
                        {"key": f.name, "value": getattr(x, f.name)}
                        for f in x.__struct_fields__
                        if getattr(x, f.name, None) is not None
                    ]
                    if hasattr(x, "__struct_fields__")
                    else None
                )
                if x is not None
                else None,
                return_dtype=pl.List(None),  # Type will be inferred
            )
        elif function_name == "map_concat":
            # map_concat(*cols) - concatenate multiple maps
            # op.value contains additional columns (first column is in op.column)
            if op.value and isinstance(op.value, (list, tuple)) and len(op.value) > 0:
                # Translate all columns
                all_cols = [col_expr]  # Start with first column
                for col in op.value:
                    if isinstance(col, str):
                        all_cols.append(pl.col(col))
                    elif isinstance(col, ColumnOperation) and col.operation == "cast":
                        # For cast operations nested in function calls, translate the column part
                        # but keep the cast value (type name) as-is
                        if isinstance(col.column, Column):
                            cast_col = pl.col(col.column.name)
                        elif isinstance(col.column, ColumnOperation):
                            cast_col = self._translate_operation(col.column)
                        else:
                            cast_col = self.translate(col.column)
                        # Translate cast with the type name directly
                        all_cols.append(self._translate_cast(cast_col, col.value))
                    else:
                        all_cols.append(self.translate(col))
                # Combine maps: merge all dicts together (later values override earlier ones)
                # Use struct operations to merge maps
                # For now, return a simplified version that merges sequentially
                merged = all_cols[0]
                for other_col in all_cols[1:]:
                    # Merge maps using map_elements
                    merged = merged.map_elements(
                        lambda x, y: {
                            **(x if isinstance(x, dict) else {}),
                            **(y if isinstance(y, dict) else {}),
                        }
                        if (isinstance(x, dict) or x is None)
                        and (isinstance(y, dict) or y is None)
                        else None,
                        return_dtype=pl.Object,
                    )
                # Actually, Polars doesn't support multi-argument map_elements easily
                # We'll need to use a struct approach or handle this differently
                # For now, return the first column as a placeholder
                return col_expr.map_elements(
                    lambda x: x if isinstance(x, dict) else None, return_dtype=pl.Object
                )
            else:
                # Single column - just return as-is
                return col_expr.map_elements(
                    lambda x: x if isinstance(x, dict) else None, return_dtype=pl.Object
                )

        # Map function names to Polars expressions (unary functions)
        function_map = {
            "upper": lambda e: e.str.to_uppercase(),
            "lower": lambda e: e.str.to_lowercase(),
            "length": lambda e: e.str.len_chars().cast(
                pl.Int64
            ),  # Cast to Int64 for PySpark compatibility
            "char_length": lambda e: e.str.len_chars().cast(
                pl.Int64
            ),  # Alias for length
            # PySpark trim only removes ASCII space characters (0x20), not tabs/newlines
            "trim": lambda e: e.str.strip_chars(" "),
            "ltrim": lambda e: e.str.strip_chars_start(" "),
            "rtrim": lambda e: e.str.strip_chars_end(" "),
            "btrim": lambda e: e.str.strip_chars(),  # btrim without trim_string is same as trim
            "bit_length": lambda e: (e.str.len_bytes() * 8).cast(
                pl.Int64
            ),  # Cast to Int64 for PySpark compatibility
            "octet_length": lambda e: e.str.len_bytes().cast(
                pl.Int64
            ),  # Byte length (octet = 8 bits, but octet_length is bytes), cast to Int64 for PySpark compatibility
            "char": lambda e: e.map_elements(
                lambda x: chr(int(x))
                if x is not None and isinstance(x, (int, float))
                else None,
                return_dtype=pl.Utf8,
            ),
            "ucase": lambda e: e.str.to_uppercase(),  # Alias for upper
            "lcase": lambda e: e.str.to_lowercase(),  # Alias for lower
            "initcap": lambda e: e.str.to_titlecase(),  # Capitalize first letter of each word
            "positive": lambda e: e,  # Identity function
            "negative": lambda e: -e,  # Negate
            "power": lambda e: e,  # Will be handled in operation-specific code below
            "abs": lambda e: e.abs(),
            "ceil": lambda e: e.ceil(),
            "ceiling": lambda e: e.ceil(),  # Alias for ceil
            "floor": lambda e: e.floor(),
            "sqrt": lambda e: e.sqrt(),
            "exp": lambda e: e.exp(),
            "log": lambda e: e.log(),
            "log10": lambda e: e.log10(),
            "sin": lambda e: e.sin(),
            "cos": lambda e: e.cos(),
            "tan": lambda e: e.tan(),
            "asin": lambda e: e.arcsin(),
            "acos": lambda e: e.arccos(),
            "atan": lambda e: e.arctan(),
            "sinh": lambda e: e.sinh(),
            "cosh": lambda e: e.cosh(),
            "tanh": lambda e: e.tanh(),
            "asinh": lambda e: e.arcsinh(),
            "acosh": lambda e: e.arccosh(),
            "atanh": lambda e: e.arctanh(),
            "sum": lambda e: e.sum(),
            "avg": lambda e: e.mean(),
            "mean": lambda e: e.mean(),
            "count": lambda e: e.count(),
            "max": lambda e: e.max(),
            "min": lambda e: e.min(),
            # Datetime extraction functions
            # For string columns, parse first; for datetime columns, use directly
            # We use a helper function to handle both cases
            "year": lambda e: self._extract_datetime_part(e, "year"),
            "month": lambda e: self._extract_datetime_part(e, "month"),
            "day": lambda e: self._extract_datetime_part(e, "day"),
            "dayofmonth": lambda e: self._extract_datetime_part(e, "day"),
            "hour": lambda e: self._extract_datetime_part(e, "hour"),
            "minute": lambda e: self._extract_datetime_part(e, "minute"),
            "second": lambda e: self._extract_datetime_part(e, "second"),
            "dayofweek": lambda e: self._extract_datetime_part(e, "dayofweek"),
            "dayofyear": lambda e: self._extract_datetime_part(e, "dayofyear"),
            "weekofyear": lambda e: self._extract_datetime_part(e, "weekofyear"),
            "quarter": lambda e: self._extract_datetime_part(e, "quarter"),
            "reverse": lambda e: self._reverse_expr(
                e, op
            ),  # Handle both string and array reverse
            "size": lambda e: self._size_expr(e, op),  # Handle both array and map size
            "isnan": lambda e: pl.when(e.is_null()).then(None).otherwise(e.is_nan()),
            "bin": lambda e: e.map_elements(
                lambda x: bin(int(x))[2:]
                if isinstance(x, (int, float))
                and not (isinstance(x, float) and math.isnan(x))
                and x is not None
                else "",
                return_dtype=pl.Utf8,
            ),
            "bround": lambda e: self._bround_expr(e, op),
            "conv": lambda e: self._conv_expr(e, op),
            "factorial": lambda e: e.map_elements(
                lambda x: math.factorial(int(x))
                if isinstance(x, (int, float))
                and x >= 0
                and x == int(x)
                and x is not None
                else None,
                return_dtype=pl.Int64,
            ),
            "to_date": lambda e: e.str.strptime(pl.Date, strict=False),
            "isnull": lambda e: e.is_null(),
            "isNull": lambda e: e.is_null(),
            "isnotnull": lambda e: e.is_not_null(),
            "isNotNull": lambda e: e.is_not_null(),
            "last_day": lambda e: self._last_day_expr(e),
            # Array functions
            # Note: "size" is already defined above (line 2639) with _size_expr() helper
            # which handles both arrays and maps correctly. Do not duplicate here.
            "array_max": lambda e: e.list.max(),
            "array_min": lambda e: e.list.min(),
            "array_distinct": lambda e: e.map_elements(
                lambda arr: list(dict.fromkeys(arr)) if isinstance(arr, list) else arr,
                return_dtype=pl.List(pl.Utf8),
            ),
            # Note: explode/explode_outer expressions just return the array column
            # The actual row expansion is handled in operation_executor
            "explode": lambda e: e,  # Return the array column as-is, will be exploded in operation_executor
            "explode_outer": lambda e: e,  # Return the array column as-is, will be exploded in operation_executor
            # New string functions
            "ilike": lambda e: e,  # Will be handled in operation-specific code
            "find_in_set": lambda e: e,  # Will be handled in operation-specific code
            "regexp_count": lambda e: e,  # Will be handled in operation-specific code
            "regexp_like": lambda e: e,  # Will be handled in operation-specific code
            "regexp_substr": lambda e: e,  # Will be handled in operation-specific code
            "regexp_instr": lambda e: e,  # Will be handled in operation-specific code
            "regexp": lambda e: e,  # Will be handled in operation-specific code (alias for rlike)
            "sentences": lambda e: e,  # Will be handled in operation-specific code
            "printf": lambda e: e,  # Will be handled in operation-specific code
            "to_char": lambda e: e,  # Will be handled in operation-specific code
            "to_varchar": lambda e: e,  # Will be handled in operation-specific code
            "typeof": lambda e: e,  # Will be handled in operation-specific code
            "stack": lambda e: e,  # Will be handled in operation-specific code
            # New math/bitwise functions
            "pmod": lambda e: e,  # Will be handled in operation-specific code
            "negate": lambda e: -e,  # Alias for negative
            "shiftleft": lambda e: e,  # Will be handled in operation-specific code
            "shiftright": lambda e: e,  # Will be handled in operation-specific code
            "shiftrightunsigned": lambda e: e,  # Will be handled in operation-specific code
            "ln": lambda e: e.log(),  # Natural logarithm
            # New datetime functions
            "years": lambda e: e,  # Interval function - return as-is
            "localtimestamp": lambda e: pl.datetime.now(),  # Local timestamp
            "dateadd": lambda e: e,  # Will be handled in operation-specific code
            "datepart": lambda e: e,  # Will be handled in operation-specific code
            "make_timestamp": lambda e: e,  # Will be handled in operation-specific code
            "make_timestamp_ltz": lambda e: e,  # Will be handled in operation-specific code
            "make_timestamp_ntz": lambda e: e,  # Will be handled in operation-specific code
            "make_interval": lambda e: e,  # Will be handled in operation-specific code
            "make_dt_interval": lambda e: e,  # Will be handled in operation-specific code
            "make_ym_interval": lambda e: e,  # Will be handled in operation-specific code
            "to_number": lambda e: e,  # Will be handled in operation-specific code
            "to_binary": lambda e: e,  # Will be handled in operation-specific code
            "to_unix_timestamp": lambda e: e,  # Will be handled in operation-specific code
            "unix_timestamp": lambda e: e,  # Will be handled in operation-specific code
            "unix_date": lambda e: e,  # Will be handled in operation-specific code
            "unix_seconds": lambda e: e,  # Will be handled in operation-specific code
            "unix_millis": lambda e: e,  # Will be handled in operation-specific code
            "unix_micros": lambda e: e,  # Will be handled in operation-specific code
            # timestamp_seconds removed - handled in operation-specific code to force Python evaluation
            "timestamp_millis": lambda e: e,  # Will be handled in operation-specific code
            "timestamp_micros": lambda e: e,  # Will be handled in operation-specific code
            # New utility functions
            "get": lambda e: e,  # Will be handled in operation-specific code
            "inline": lambda e: e,  # Will be handled in operation-specific code
            "inline_outer": lambda e: e,  # Will be handled in operation-specific code
            "str_to_map": lambda e: e,  # Will be handled in operation-specific code
            # New crypto functions (PySpark 3.5+)
            "aes_encrypt": lambda e: e,  # Will be handled in operation-specific code
            "aes_decrypt": lambda e: e,  # Will be handled in operation-specific code
            "try_aes_decrypt": lambda e: e,  # Will be handled in operation-specific code
            # New string functions (PySpark 3.5+)
            "sha": lambda e: e,  # Alias for sha1 - will be handled in operation-specific code
            "mask": lambda e: e,  # Will be handled in operation-specific code
            "json_array_length": lambda e: e,  # Will be handled in operation-specific code
            "json_object_keys": lambda e: e,  # Will be handled in operation-specific code
            "xpath_number": lambda e: e,  # Will be handled in operation-specific code
            "user": lambda e: pl.lit(""),  # Will be handled in operation-specific code
            # New math functions (PySpark 3.5+)
            "getbit": lambda e: e,  # Will be handled in operation-specific code
            "width_bucket": lambda e: e,  # Will be handled in operation-specific code
            # New datetime functions (PySpark 3.5+)
            "date_from_unix_date": lambda e: e,  # Will be handled in operation-specific code
            "to_timestamp_ltz": lambda e: e,  # Will be handled in operation-specific code
            "to_timestamp_ntz": lambda e: e,  # Will be handled in operation-specific code
            # New null-safe try functions (PySpark 3.5+)
            "try_add": lambda e: e,  # Will be handled in operation-specific code
            "try_subtract": lambda e: e,  # Will be handled in operation-specific code
            "try_multiply": lambda e: e,  # Will be handled in operation-specific code
            "try_divide": lambda e: e,  # Will be handled in operation-specific code
            "try_element_at": lambda e: e,  # Will be handled in operation-specific code
            "try_to_binary": lambda e: e,  # Will be handled in operation-specific code
            "try_to_number": lambda e: e,  # Will be handled in operation-specific code
            "try_to_timestamp": lambda e: e,  # Will be handled in operation-specific code
        }

        if function_name in function_map:
            return function_map[function_name](col_expr)
        else:
            # Fallback: try to access as attribute
            if hasattr(col_expr, function_name):
                func = getattr(col_expr, function_name)
                if callable(func):
                    if op.value is not None:
                        return func(self.translate(op.value))
                    return func()
            raise ValueError(f"Unsupported function: {function_name}")

    def _last_day_expr(self, expr: pl.Expr) -> pl.Expr:
        """Get last day of month for a date column.

        Args:
            expr: Polars expression (date column or string)

        Returns:
            Polars expression for last day of month
        """
        # Parse string dates first, or use directly if already a date
        # Try parsing as string first (most common case)
        try:
            date_col = expr.str.strptime(pl.Date, "%Y-%m-%d", strict=False)
        except AttributeError:
            # Already a date column, use directly
            date_col = expr.cast(pl.Date)
        # Get first day of current month
        first_of_month = date_col.dt.replace(day=1)
        # Add 1 month to get first of next month (using string offset)
        first_of_next_month = first_of_month.dt.offset_by("1mo")
        # Subtract 1 day to get last day of current month
        return first_of_next_month.dt.offset_by("-1d")

    def _reverse_expr(self, expr: pl.Expr, op: Any) -> pl.Expr:
        """Handle reverse for both strings and arrays.

        Args:
            expr: Polars expression (column reference)
            op: The ColumnOperation to check column type

        Returns:
            Polars expression for reverse (string or list)
        """
        # Check if the column is an array type by inspecting the operation's column
        from sparkless.spark_types import ArrayType

        is_array = False

        # First, check if column_type is explicitly ArrayType
        if hasattr(op, "column"):
            col = op.column
            if hasattr(col, "column_type"):
                is_array = isinstance(col.column_type, ArrayType)

        # If not determined yet, try to infer from the column name
        # If column name suggests it's an array (e.g., "arr1", "arr2"), treat as array
        if not is_array and hasattr(op, "column") and hasattr(op.column, "name"):
            col_name = op.column.name
            # Common array column name patterns
            if (
                col_name.startswith("arr")
                or col_name.endswith("_array")
                or "array" in col_name.lower()
            ):
                is_array = True

        if is_array:
            return expr.list.reverse()
        else:
            # Default to string reverse (F.reverse() defaults to StringFunctions)
            return expr.str.reverse()

    def _size_expr(self, expr: pl.Expr, op: Any) -> pl.Expr:
        """Handle size for both arrays and maps.

        Args:
            expr: Polars expression (column reference)
            op: The ColumnOperation to check column type

        Returns:
            Polars expression for size (array or map length)
        """
        # Check if the column is an array type by inspecting the operation's column
        from sparkless.spark_types import ArrayType, MapType

        is_array = False
        is_map = False

        # First, check if column_type is explicitly ArrayType or MapType
        if hasattr(op, "column"):
            col = op.column
            if hasattr(col, "column_type"):
                column_type = col.column_type
                is_array = isinstance(column_type, ArrayType)
                is_map = isinstance(column_type, MapType)

        # If not determined yet, try to infer from the column name
        # If column name suggests it's an array (e.g., "scores", "tags"), treat as array
        # If column name suggests it's a map (e.g., "map1", "mapping"), treat as map
        if (
            not is_array
            and not is_map
            and hasattr(op, "column")
            and hasattr(op.column, "name")
        ):
            col_name = op.column.name.lower()
            # Common array column name patterns
            if (
                col_name.startswith("arr")
                or col_name.endswith("_array")
                or "array" in col_name
                or col_name in ("scores", "tags", "items", "list")
            ):
                is_array = True
            # Common map column name patterns
            elif (
                col_name.startswith("map")
                or col_name.endswith("_map")
                or "mapping" in col_name
                or "dict" in col_name
            ):
                is_map = True

        if is_array:
            # For arrays, use list.len() which returns UInt32
            # Cast to Int64 for PySpark compatibility (consistent with length() fix)
            return expr.list.len().cast(pl.Int64)
        elif is_map:
            # For maps (dicts), use map_elements to get length
            return expr.map_elements(
                lambda x: len(x) if isinstance(x, dict) and x is not None else None,
                return_dtype=pl.Int64,
            )
        else:
            # Default to array size (F.size() defaults to ArrayFunctions)
            # Try array first, fall back to map if that fails
            # Cast to Int64 for PySpark compatibility (consistent with length() fix)
            return expr.list.len().cast(pl.Int64)

    def _parse_simple_case_when(self, sql_expr: str) -> pl.Expr:
        """Parse simple CASE WHEN expression and convert to Polars expression.

        Args:
            sql_expr: SQL expression string like "CASE WHEN age > 30 THEN 'Senior' ELSE 'Junior' END"

        Returns:
            Polars expression equivalent
        """
        import re

        # Simple regex-based parser for CASE WHEN ... THEN ... ELSE ... END
        # Pattern: CASE WHEN condition THEN value1 ELSE value2 END
        # Remove CASE and END keywords
        sql_lower = sql_expr.lower()
        if not sql_lower.startswith("case when") or not sql_lower.endswith("end"):
            raise ValueError(f"Unsupported CASE WHEN format: {sql_expr}")

        # Extract the middle part: WHEN ... THEN ... ELSE ...
        # Remove "CASE " and " END" (case-insensitive)
        middle = sql_expr[5:-4].strip()  # Remove "CASE " and " END"

        # Split by THEN and ELSE (case-insensitive)
        # Pattern: WHEN condition THEN value1 ELSE value2
        then_match = re.search(r"\s+then\s+", middle, re.IGNORECASE)
        else_match = re.search(r"\s+else\s+", middle, re.IGNORECASE)

        if not then_match:
            raise ValueError(f"Invalid CASE WHEN: missing THEN: {sql_expr}")

        # Extract condition (between WHEN and THEN)
        condition_str = middle[: then_match.start()].strip()
        if condition_str.lower().startswith("when"):
            condition_str = condition_str[4:].strip()  # Remove "when"

        # Extract THEN value
        if else_match:
            then_value_str = middle[then_match.end() : else_match.start()].strip()
            else_value_str = middle[else_match.end() :].strip()
        else:
            then_value_str = middle[then_match.end() :].strip()
            else_value_str = None

        # Parse condition (e.g., "age > 30")
        # Simple comparison: column operator value
        condition_expr = self._parse_condition(condition_str)

        # Parse THEN and ELSE values
        then_expr = self._parse_value(then_value_str)
        else_expr = self._parse_value(else_value_str) if else_value_str else None

        # Build Polars expression: pl.when(condition).then(then_value).otherwise(else_value)
        if else_expr is not None:
            return pl.when(condition_expr).then(then_expr).otherwise(else_expr)
        else:
            return pl.when(condition_expr).then(then_expr)

    def _parse_condition(self, condition_str: str) -> pl.Expr:
        """Parse a condition string into a Polars expression.

        Args:
            condition_str: Condition like "age > 30", "salary == 50000", etc.

        Returns:
            Polars expression
        """

        # Simple parser for comparison operators: column operator value
        operators = [">=", "<=", "!=", "==", ">", "<", "="]
        for op in operators:
            if op in condition_str:
                parts = condition_str.split(op, 1)
                if len(parts) == 2:
                    left = parts[0].strip()
                    right = parts[1].strip()

                    # Parse left side (column reference)
                    left_expr = pl.col(left)

                    # Parse right side (literal or column)
                    right_expr = self._parse_value(right)

                    # Build comparison expression
                    if op in ["==", "="]:
                        return left_expr == right_expr
                    elif op == "!=":
                        return left_expr != right_expr
                    elif op == ">":
                        return left_expr > right_expr
                    elif op == ">=":
                        return left_expr >= right_expr
                    elif op == "<":
                        return left_expr < right_expr
                    elif op == "<=":
                        return left_expr <= right_expr

        raise ValueError(f"Unable to parse condition: {condition_str}")

    def _parse_value(self, value_str: str) -> pl.Expr:
        """Parse a value string into a Polars expression.

        Args:
            value_str: Value like "'Senior'", "30", "age", etc.

        Returns:
            Polars expression (literal or column reference)
        """
        value_str = value_str.strip()

        # String literal (quoted)
        if (value_str.startswith("'") and value_str.endswith("'")) or (
            value_str.startswith('"') and value_str.endswith('"')
        ):
            # Remove quotes
            literal_value = value_str[1:-1]
            return pl.lit(literal_value)

        # Numeric literal
        try:
            if "." in value_str:
                return pl.lit(float(value_str))
            else:
                return pl.lit(int(value_str))
        except ValueError:
            pass

        # Boolean literal
        if value_str.lower() in ["true", "false"]:
            return pl.lit(value_str.lower() == "true")

        # Column reference
        return pl.col(value_str)

    def _extract_datetime_part(self, expr: pl.Expr, part: str) -> pl.Expr:
        """Extract datetime part from expression, handling both string and datetime columns.

        Args:
            expr: Polars expression (column reference)
            part: Part to extract (year, month, day, hour, etc.)

        Returns:
            Polars expression for datetime part extraction
        """
        # Map of part names to Polars methods
        part_map = {
            "year": lambda e: e.dt.year(),
            "month": lambda e: e.dt.month(),
            "day": lambda e: e.dt.day(),
            "hour": lambda e: e.dt.hour(),
            "minute": lambda e: e.dt.minute(),
            "second": lambda e: e.dt.second(),
            "dayofweek": lambda e: (e.dt.weekday() % 7)
            + 1,  # Polars ISO: Mon=1,Sun=7; PySpark: Sun=1,Mon=2,...,Sat=7
            "dayofyear": lambda e: e.dt.ordinal_day(),
            "weekofyear": lambda e: e.dt.week(),
            "quarter": lambda e: e.dt.quarter(),
        }

        extractor = part_map.get(part)
        if not extractor:
            raise ValueError(f"Unsupported datetime part: {part}")

        # Handle both string and datetime columns
        # For string columns, we need to parse first using str.strptime()
        # For datetime columns, we can use dt methods directly
        # Since we can't check type at expression build time, we use a conditional approach
        # that tries string parsing first, with a fallback for datetime columns

        # Use Polars' ability to handle this with a when/then/otherwise pattern
        # But simpler: just always try str.strptime() - it will work for strings
        # For datetime columns, we need to cast them or use directly
        # Actually, str.strptime only works on string columns, so we need a different approach

        # Use pl.when() to conditionally handle, but we can't check dtype in expression
        # So we'll use a try-cast pattern: try to parse as string, if that fails use as datetime
        # But Polars doesn't have try-cast in expressions easily

        # Simplest approach: assume string and parse it
        # If the column is already datetime, this will fail at runtime
        # For now, we'll parse strings and document that datetime columns should work
        # but may need explicit handling

        # For string columns (most common case in tests):
        # We need to handle both string and datetime columns
        # For string columns: parse with str.strptime() first
        # For datetime columns: use dt methods directly
        # Since we can't check type at expression build time, we use map_elements
        # with a function that handles both cases
        import datetime as dt_module
        from typing import Any, Optional

        def extract_part(value: Any) -> Optional[int]:
            """Extract datetime part from value, handling both string and datetime."""
            if value is None:
                return None
            # If it's already a datetime, use it directly
            if isinstance(value, (dt_module.datetime, dt_module.date)):
                parsed = value
            # If it's a string, try to parse it
            elif isinstance(value, str):
                try:
                    # Try parsing as datetime (most common format)
                    parsed = dt_module.datetime.fromisoformat(value.replace(" ", "T"))
                except Exception:
                    logger.debug("fromisoformat failed, trying strptime", exc_info=True)
                    try:
                        # Try parsing as date
                        parsed = dt_module.datetime.strptime(value, "%Y-%m-%d")
                    except Exception:
                        logger.debug(
                            "All datetime parsing methods failed", exc_info=True
                        )
                        return None
            else:
                return None

            # Extract the requested part (return as int to ensure Int32 type)
            if part == "year":
                return int(parsed.year)
            elif part == "month":
                return int(parsed.month)
            elif part == "day":
                return int(parsed.day)
            elif part == "hour":
                return int(parsed.hour) if isinstance(parsed, dt_module.datetime) else 0
            elif part == "minute":
                return (
                    int(parsed.minute) if isinstance(parsed, dt_module.datetime) else 0
                )
            elif part == "second":
                return (
                    int(parsed.second) if isinstance(parsed, dt_module.datetime) else 0
                )
            elif part == "dayofweek":
                # PySpark: Sun=1,Mon=2,...,Sat=7
                # Python: Mon=0,Tue=1,...,Sun=6
                return int((parsed.weekday() + 1) % 7 + 1)
            elif part == "dayofyear":
                return int(parsed.timetuple().tm_yday)
            elif part == "weekofyear":
                return int(parsed.isocalendar()[1])
            elif part == "quarter":
                return int((parsed.month - 1) // 3 + 1)
            else:
                return None

        return expr.map_elements(extract_part, return_dtype=pl.Int64)

    def _translate_aggregate_function(self, agg_func: AggregateFunction) -> pl.Expr:
        """Translate aggregate function.

        Args:
            agg_func: AggregateFunction instance

        Returns:
            Polars aggregate expression
        """
        function_name = agg_func.function_name.lower()
        column = agg_func.column

        # Count(*) case
        col_expr = self.translate(column) if column else pl.lit(1)

        if function_name == "sum":
            return col_expr.sum()
        elif function_name == "avg" or function_name == "mean":
            return col_expr.mean()
        elif function_name == "count":
            if column:
                return col_expr.count()
            else:
                return pl.len()
        elif function_name == "countdistinct":
            # Count distinct values
            if column:
                return col_expr.n_unique()
            else:
                return pl.len()
        elif function_name == "max":
            return col_expr.max()
        elif function_name == "min":
            return col_expr.min()
        elif function_name == "stddev" or function_name == "stddev_samp":
            return col_expr.std()
        elif function_name == "variance" or function_name == "var_samp":
            return col_expr.var()
        elif function_name == "collect_list":
            # Collect values into a list
            return col_expr.implode()
        elif function_name == "collect_set":
            # Collect unique values into a set (preserve first occurrence order, like PySpark)
            # Use maintain_order=True to preserve the order of first occurrence
            return col_expr.unique(maintain_order=True).implode()
        elif function_name == "first":
            # First value in group
            return col_expr.first()
        elif function_name == "last":
            # Last value in group
            return col_expr.last()
        else:
            raise ValueError(f"Unsupported aggregate function: {function_name}")

    def _bround_expr(self, expr: pl.Expr, op: Any) -> pl.Expr:
        """Banker's rounding (HALF_EVEN rounding mode).

        Args:
            expr: Polars expression
            op: ColumnOperation with scale in op.value

        Returns:
            Polars expression for banker's rounding
        """
        scale = op.value if op.value is not None else 0
        if scale == 0:
            # Round to nearest integer using HALF_EVEN
            return expr.round(0)
        else:
            # Round to scale decimal places using HALF_EVEN
            # Polars doesn't have direct HALF_EVEN, use round() which uses HALF_TO_EVEN
            return expr.round(scale)

    def _conv_expr(self, expr: pl.Expr, op: Any) -> pl.Expr:
        """Convert number from one base to another.

        Args:
            expr: Polars expression (number as string or number)
            op: ColumnOperation with (from_base, to_base) in op.value

        Returns:
            Polars expression for base conversion
        """
        if isinstance(op.value, (tuple, list)) and len(op.value) >= 2:
            from_base = op.value[0]
            to_base = op.value[1]
        else:
            raise ValueError("conv requires (from_base, to_base) tuple")

        # Convert number to string in from_base, then parse from that base, then convert to to_base
        def convert_base(x: Any, from_b: int, to_b: int) -> Optional[str]:
            if x is None:
                return None
            try:
                # Parse as integer from source base
                num = int(x, from_b) if isinstance(x, str) else int(x)
                # Convert to target base
                if to_b == 10:
                    return str(num)
                elif to_b == 2:
                    return bin(num)[2:]
                elif to_b == 16:
                    return hex(num)[2:].upper()
                else:
                    # Generic base conversion
                    if num == 0:
                        return "0"
                    digits = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                    result = ""
                    n = abs(num)
                    while n > 0:
                        result = digits[n % to_b] + result
                        n //= to_b
                    return ("-" if num < 0 else "") + result
            except (ValueError, TypeError):
                return None

        return expr.map_elements(
            lambda x: convert_base(x, from_base, to_base), return_dtype=pl.Utf8
        )

    def _translate_case_when(self, case_when: Any) -> pl.Expr:
        """Translate CaseWhen to Polars expression.

        Args:
            case_when: CaseWhen instance

        Returns:
            Polars expression using pl.when().then().otherwise() chain
        """
        from sparkless.functions.conditional import CaseWhen

        if not isinstance(case_when, CaseWhen):
            raise ValueError(f"Expected CaseWhen, got {type(case_when)}")

        if not case_when.conditions:
            # No conditions - return default value or None
            if case_when.default_value is not None:
                return self.translate(case_when.default_value)
            return pl.lit(None)

        # Build chained when/then/otherwise expression
        # Start with the first condition
        condition, value = case_when.conditions[0]
        condition_expr = self.translate(condition)
        value_expr = self._translate_value_to_expr(value)

        # Start the chain
        result = pl.when(condition_expr).then(value_expr)

        # Add additional when/then pairs
        for condition, value in case_when.conditions[1:]:
            condition_expr = self.translate(condition)
            value_expr = self._translate_value_to_expr(value)
            result = result.when(condition_expr).then(value_expr)

        # Add otherwise clause if default_value is set
        if case_when.default_value is not None:
            default_expr = self._translate_value_to_expr(case_when.default_value)
            result = result.otherwise(default_expr)
        else:
            result = result.otherwise(None)

        return result

    def _translate_value_to_expr(self, value: Any) -> pl.Expr:
        """Translate a value to a Polars expression, handling literals properly.

        This is used for CASE WHEN values where plain strings/numbers should be
        treated as literals, not column names.

        Args:
            value: Value to translate (string, number, bool, or expression)

        Returns:
            Polars expression
        """
        # If it's already a Column, ColumnOperation, etc., use translate
        if isinstance(value, (Column, ColumnOperation, Literal, AggregateFunction)):
            return self.translate(value)
        # If it's a plain Python type, treat as literal
        elif isinstance(value, (str, int, float, bool)):
            return pl.lit(value)
        # If it's None, return literal None
        elif value is None:
            return pl.lit(None)
        # Otherwise try translate (might be a CaseWhen or other complex type)
        else:
            return self.translate(value)
