llama-stack[client]>=0.4.0
kfp>=2.14.6
kfp-kubernetes>=2.14.6
kfp-server-api>=2.14.6
boto3>=1.35.88

[dev]
pytest
pytest-cov
pytest-asyncio
black
isort

[inline]
langchain==0.3.27
garak==0.12.0
