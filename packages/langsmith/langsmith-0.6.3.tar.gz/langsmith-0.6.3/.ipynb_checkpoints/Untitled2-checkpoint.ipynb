{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cb3cbf-6c4c-4eeb-aae5-1309a09d8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f705077-cef1-46a2-b616-c031cfd10d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/.pyenv/versions/3.9.18/lib/python3.9/site-packages/langchain_core/utils/utils.py:225: UserWarning: WARNING! seed is not default parameter.\n",
      "                seed was transferred to model_kwargs.\n",
      "                Please confirm that seed is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
    "[BEGIN DATA]\n",
    "***\n",
    "[Input]: {input}\n",
    "***\n",
    "[Submission]: {output}\n",
    "***\n",
    "Pay special attention to this!\n",
    "[Criteria]: {criteria}\n",
    "***\n",
    "[END DATA]\n",
    "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\n",
    "\"\"\" \n",
    ")\n",
    "\n",
    "evaluator = LangChainStringEvaluator(\"criteria\", config={\"criteria\":\n",
    "                                                         {\"my_criterion\": \"do something\"},\n",
    "                                                         \"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2db43f29-aba3-404f-9909-40846b293aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = client.pull_prompt(\"my-structured-eval-prompt-example\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "006f9fb4-450f-4786-a192-3c9051780fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2fcea9f2-ee39-449e-bd24-57782eca37a5-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85fb39c3-f194-486c-8857-4476f639e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate\n",
    "evaluate(my_system, data=[..], evaluators=[evaluator],metadata={\"revision_id\": \"foo\", \"evaluator_version\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359a643-b287-4081-8005-ba294a6c0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the example prompt for the off-the-shelf evaluator\n",
    "evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2b83fcd-efe8-4b20-a833-7233fe83c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a grader and art critic. Be very very critical nothing gets a 5.\"),\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])\n",
    "prompt_version = \"my-eval-prompt-example\"\n",
    "prompt = client.pull_prompt(prompt_version, include_model=True)\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# prompt | llm\n",
    "# # my_evaluator = prompt | llm.with_structured_output(MyGrade)\n",
    "# my_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2ee3859-3341-4279-998d-e854bcf62989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGrade(reasoning=\"The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is one of the most iconic and analyzed pieces of art in history. Its acclaim is largely due to its masterful execution, innovative techniques, and intriguing subject matter. The painting's composition is meticulously balanced, and da Vinci's use of sfumato—a technique of blending colors and tones—creates a lifelike and three-dimensional effect. The enigmatic expression of the subject, often described as both alluring and mysterious, has captivated viewers for centuries. Furthermore, the intricate details, such as the delicate rendering of the veil and the realistic depiction of the landscape, showcase Leonardo's unparalleled skill. However, it's important to consider that the painting's fame has been amplified by its storied history, including its theft in 1911 and subsequent recovery, as well as its prominent display in the Louvre. While some may argue that its renown is partially due to these external factors, the Mona Lisa remains an exceptional work of art deserving of its legendary status.\", score=5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_evaluator.invoke({\"input\": \"Evaluate the mona lisa seriously.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94378bcc-fd72-43bf-966c-f8806b430b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Image(my_evaluator.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9b90a1e-b286-4a22-b2df-c17bede145b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGrade(reasoning='The Mona Lisa is widely regarded as a masterpiece of Renaissance art. Painted by Leonardo da Vinci, it showcases exceptional technique in sfumato, a method of blending colors and tones, which gives the painting a lifelike quality. The enigmatic expression of the subject and the intricate background contribute to its allure and narrative depth. Additionally, its historical significance and influence on art make it a pivotal piece in the art world. Given these attributes, I would rate its quality very highly.', score=5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_evaluator(run, example):\n",
    "    expected_names = example.outputs[\"expected_names\"]\n",
    "    generated_bullets: list = run.outputs['bullets']\n",
    "    result: List[MyGrade] = my_evaluator.batch(generated_bullets)\n",
    "    return {\n",
    "        \"results\": [{\"key\": \"my_grade\", \"score\": r.score} for r in results]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2f116-9c50-4bb6-aa6e-a533b2bce658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bullet_quality(run, example):\n",
    "    bullets = run.outputs[\"bullets\"]\n",
    "    bullet_feedback = []\n",
    "    for i, b in enumerate(bullet):\n",
    "        score = 1 #  actually score it \n",
    "        bullet_feedback.append({ \"key\": \"bullet_quality\", \"score\": score, \"comment\": f\"bullet_{i}\"})\n",
    "\n",
    "    bullet_feedback.append({\n",
    "        \"key\": \"bullet_count\",\n",
    "        \"score\": len(bullets)\n",
    "    })\n",
    "    return {\"results\": bullet_feedback}\n",
    "\n",
    "def summary_coherence(run, example):\n",
    "    summary  = run.outputs[\"summary\"]\n",
    "    score = .....\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "def my_meeting_insights_system(inputs: dict) -> dict:\n",
    "    \n",
    "\n",
    "evaluate(my_system, data=\"my dataset\", evaluators=[bullet_quality, summary_coherence, ...]j, metadata={\"my-prompt-version\": prompt_version})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
