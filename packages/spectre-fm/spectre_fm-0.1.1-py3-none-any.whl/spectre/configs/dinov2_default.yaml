train:
  batch_size_per_gpu: 128
  grad_accum_steps: 1
  datasets:
  - ct_rate
  - inspect
  - merlin
  - nlst
  - amos
  - abdomen_atlas
  - panorama
  - abdomenct_1k
  data_dir: /data/
  data_fraction: 1.0
  output_dir: .
  saveckp_freq: 5
  resume_ckp: true
  seed: 0
  num_workers: 10
  load_fp16: true
  pin_memory: true
  persistent_workers: true
  drop_last: true
  use_thread: false
  cache_dataset: true
  cache_dir: /data/cache/monai/
  use_gds: false
  log_wandb: true
  log_freq: 1
  log_grad_freq: 100
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.  # will be set after applying scaling rule
  warmup_epochs: 10
  min_lr: 1.0e-06
  llrd_factor: 0.9
  patch_embed_lr_mult: 0.2
  projection_head_wd_mult: 1.0
  clip_grad_norm: 3.0
  scaling_rule: sqrt_wrt_1024
  adamw_beta1: 0.9
  adamw_beta2: 0.999
  dino_loss_weight: 1.0
  koleo_loss_weight: 0.1
  ibot_loss_weight: 1.0
model:
  architecture: vit_large_patch16_128
  pretrained_weights: null
  hidden_dim: 2048
  bottleneck_dim: 256
  output_dim: 65536
  layer_scale_init_value: 1e-3
  momentum_teacher: 0.992
  momentum_teacher_end: 1.0
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
  student_temp: 0.1
  center_momentum: 0.9
  mask_ratio_min: 0.2
  mask_ratio_max: 0.7
  mask_probability: 0.5
  ibot_seperate_head: true
  freeze_last_layer: 0
  student_drop_path_rate: 0.3
  num_base_patches: 12
  num_local_views: 8