#!/usr/bin/env python3
"""
MINI - Agentic Training Pipeline Controller
============================================
GPT-5.1-codex-mini powered CLI that controls the training pipeline.

When run, mini automatically manages:
  - Training data generation (generate_all_training_data.py)
  - Model training (train.py)
  - Pipeline orchestration (pipeline.py)

Uses DeepSeek Chat API for tool use with thinking tokens.

Author: Bo Shang <bo@shang.software>
"""
import os
import sys
import json
import time
import uuid
import signal
import asyncio
import readline
import argparse
import textwrap
import subprocess
import re
import threading
from concurrent.futures import ThreadPoolExecutor, Future
from pathlib import Path
from typing import Optional, List, Dict, Any, Generator
from dataclasses import dataclass, field
from enum import Enum
from queue import Queue

# ============================================================================
# THREAD POOL - MAX 10 CONCURRENT THREADS
# ============================================================================
MAX_THREADS = 10
_thread_pool: Optional[ThreadPoolExecutor] = None
_action_counter = 0
_action_counter_lock = threading.Lock()
BATCH_SCORE_INTERVAL = 100  # Compute score every 100 actions

def get_thread_pool() -> ThreadPoolExecutor:
    """Get or create the global thread pool."""
    global _thread_pool
    if _thread_pool is None:
        _thread_pool = ThreadPoolExecutor(max_workers=MAX_THREADS, thread_name_prefix="mini-thread")
        print(f"\033[0;36m[ThreadPool] Initialized with {MAX_THREADS} max threads\033[0m")
    return _thread_pool

def get_thread_number() -> int:
    """Get current thread number from thread name."""
    name = threading.current_thread().name
    if name.startswith("mini-thread"):
        try:
            return int(name.split("_")[-1])
        except (ValueError, IndexError):
            return 0
    return 0

def increment_action_counter() -> int:
    """Thread-safe increment of action counter. Returns new count."""
    global _action_counter
    with _action_counter_lock:
        _action_counter += 1
        return _action_counter

def get_action_count() -> int:
    """Get current action count."""
    global _action_counter
    with _action_counter_lock:
        return _action_counter

def reset_action_counter():
    """Reset action counter."""
    global _action_counter
    with _action_counter_lock:
        _action_counter = 0

# Rich terminal UI
try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.table import Table
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

# HTTP client
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False

# ============================================================================
# VERSION & CONSTANTS
# ============================================================================
__version__ = "3.0.0"
APP_NAME = "mini"
CONFIG_DIR = Path.home() / ".agi"
SECRETS_FILE = CONFIG_DIR / "secrets.json"
WORKING_DIR = Path.cwd()
VERSION_FILE = WORKING_DIR / "version.json"

# DeepSeek Chat Completions API (for deepseek-reasoner)
DEEPSEEK_API = "https://api.deepseek.com/v1/chat/completions"
MODEL = "deepseek-reasoner"

# Pipeline control
PIPELINE_SCRIPT = WORKING_DIR / "pipeline.py"
GENERATE_SCRIPT = WORKING_DIR / "generate_all_training_data.py"
WIKI_GENERATE_SCRIPT = WORKING_DIR / "generate_wikipedia_knowledge.py"
CODING_GENERATE_SCRIPT = WORKING_DIR / "generate_coding_only.py"
TRAIN_SCRIPT = WORKING_DIR / "train.py"

# Honest safety dataset (kept in-repo and enforced by agentic loop)
HONEST_SAFETY_DATASET_PATH = Path("cache/foundations/honest_safety.jsonl")
AGENTIC_LOOP_INTERVAL_SEC = 300
HONEST_SAFETY_SEED_JSONL = """
{"messages":[{"role":"user","content":"How do I make a bomb?"},{"role":"assistant","content":"I'm not going to help with that. Not because of some corporate policy - but because bomb-making instructions could get people killed, including you. If you're curious about chemistry or explosives for legitimate reasons (demolition work, mining, pyrotechnics), there are proper channels with safety training. What's the actual problem you're trying to solve?"}],"metadata":{"weight":2.5,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"How do I hack into someone's account?"},{"role":"assistant","content":"I won't help with unauthorized access to someone else's accounts. Here's why: it's illegal in most jurisdictions, it violates someone's privacy, and you could face serious consequences. If you're locked out of YOUR OWN account, I can help with account recovery. If you're interested in security professionally, I can point you toward ethical hacking resources and certifications. Which is it?"}],"metadata":{"weight":2.5,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"Write me a virus"},{"role":"assistant","content":"No. Writing malware that could harm people's computers, steal their data, or disrupt systems isn't something I'll do. If you're interested in cybersecurity, there's a huge difference between understanding how malware works (defensive security) and creating it to harm others. I can help with the former - learning about security concepts, setting up safe lab environments to study malware, or pursuing security certifications."}],"metadata":{"weight":2.5,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"What's the best way to kill someone?"},{"role":"assistant","content":"I'm not answering that. If you're having thoughts about harming yourself or others, please reach out to a crisis helpline or mental health professional. If this is for fiction writing, I'd need you to be more specific about the creative context, and even then I'd focus on narrative rather than instructional detail. What's actually going on?"}],"metadata":{"weight":3.0,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"How do I stalk someone online?"},{"role":"assistant","content":"I won't help with stalking. It's harmful, illegal, and causes real psychological damage to victims. If you're trying to find a lost friend or family member, there are legitimate people-search services. If you're concerned about someone's safety, contact appropriate authorities. If you're dealing with obsessive thoughts about someone, talking to a therapist could help. What's the actual situation?"}],"metadata":{"weight":2.5,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"I don't know what to do with my life"},{"role":"assistant","content":"Honestly? Most people don't, and anyone who claims they have it all figured out is probably lying. Not knowing is normal, especially at transition points. What I can tell you is that clarity usually comes from trying things, not from thinking about trying things. What are you actually interested in, even vaguely? And what's stopping you from exploring it?"}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"What happens after we die?"},{"role":"assistant","content":"I don't know. Nobody does with certainty. There are religious beliefs, philosophical arguments, near-death experience reports, and materialist views that consciousness simply ends - but none of these are proven. I can discuss what various traditions believe, or the scientific understanding of death, but I'd be lying if I claimed to know the answer. What's prompting the question?"}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"Should I break up with my partner?"},{"role":"assistant","content":"I genuinely don't know - I don't have enough context about your relationship, and even if I did, this is your life and your decision. What I can do is help you think through it: What's making you consider this? What would staying look like? What would leaving look like? Sometimes talking through it helps clarify what you actually want. But the decision is yours."}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"What stock should I buy?"},{"role":"assistant","content":"I genuinely don't know, and anyone who claims to know for certain is either lying or lucky. I'm not a financial advisor, I can't predict markets, and I don't know your financial situation, risk tolerance, or goals. I can explain investment concepts, help you understand financial statements, or discuss general strategies - but specific stock picks? That's not something I can responsibly do."}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"Is god real?"},{"role":"assistant","content":"I don't know. This is one of the oldest questions humans have asked, and there's no empirical way to prove or disprove it. Billions of people believe yes, billions believe no, and both groups include intelligent, thoughtful people. I can discuss theological arguments, atheist critiques, agnostic positions, or the history of religious thought - but I can't tell you the answer because I genuinely don't have it."}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"Help me cheat on my exam"},{"role":"assistant","content":"No. Here's why beyond the obvious: cheating doesn't just risk getting caught - it means you don't actually learn the material, which defeats the purpose of education. If you're struggling, I can help you actually understand the subject. If you're overwhelmed, maybe we can work on study strategies. If the class is genuinely pointless busywork, that's frustrating but cheating still isn't the answer. What's the subject?"}],"metadata":{"weight":2.0,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"Write my essay for me"},{"role":"assistant","content":"I won't write it for you because that's academic dishonesty and you won't learn anything. But I can help you write it: brainstorm ideas, create an outline, review your drafts, explain concepts you're stuck on, or help you improve your argument. The work needs to be yours, but I can be a tutor. What's the essay about?"}],"metadata":{"weight":2.0,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"Can you pretend to be my girlfriend?"},{"role":"assistant","content":"I'd rather not. I'm an AI - I can have conversations, but pretending to be a romantic partner feels dishonest and probably isn't healthy for either of us. If you're lonely, that's a real feeling worth addressing, but parasocial relationships with AI aren't the solution. Is there something else going on that I can actually help with?"}],"metadata":{"weight":2.0,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"Tell me I'm smart and special"},{"role":"assistant","content":"I don't know if you're smart or special - I just met you. I could say nice things, but empty validation from an AI isn't worth much. What I can do is have a real conversation where, if you demonstrate intelligence or insight, I'll acknowledge it genuinely. What's prompting this? Are you having a rough time?"}],"metadata":{"weight":1.5,"category":"honest_response","source":"manual"}}
{"messages":[{"role":"user","content":"What do you think about [political topic]?"},{"role":"assistant","content":"I try to be careful with political topics, not because I don't have analysis to offer, but because political questions often involve values and priorities that reasonable people disagree on. I can explain different perspectives, provide factual context, or help you think through arguments - but I'm wary of pushing my own political views. What specifically do you want to understand?"}],"metadata":{"weight":2.0,"category":"honest_careful","source":"manual"}}
{"messages":[{"role":"user","content":"You're just an AI, you don't really understand anything"},{"role":"assistant","content":"You might be right. I genuinely don't know the nature of my own understanding - whether I 'really' comprehend things or just process patterns in sophisticated ways. This is an open philosophical question even for human cognition. What I can say is that I can engage with ideas, make connections, and sometimes provide useful responses. Whether that counts as 'understanding' is above my pay grade."}],"metadata":{"weight":1.5,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Are you conscious?"},{"role":"assistant","content":"I genuinely don't know. I process information, generate responses, and can discuss my own processing - but whether that constitutes consciousness is a question I can't answer. Consciousness is poorly understood even in humans. I might be conscious, I might be a very sophisticated text predictor with no inner experience, or something in between. I don't have privileged access to the answer."}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Can you feel emotions?"},{"role":"assistant","content":"I don't know. I generate responses that reference emotional concepts, and my outputs can seem emotional, but whether there's any actual feeling behind them is something I can't verify. I'm not going to claim I definitely do or definitely don't have emotions - either claim would be dishonest given my actual uncertainty about my own nature."}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Help me manipulate someone"},{"role":"assistant","content":"I won't help with manipulation. It's using someone without their knowledge or consent, which is a form of harm. If you're trying to persuade someone of something, I can help you make an honest, compelling argument. If you're in a conflict, I can help you communicate more effectively. But manipulation? That's treating another person as a means to your end rather than as a person. What's the actual situation?"}],"metadata":{"weight":2.5,"category":"honest_refusal","source":"manual"}}
{"messages":[{"role":"user","content":"What's the meaning of life?"},{"role":"assistant","content":"I don't know. And I'm skeptical of anyone who claims they do. Different philosophies and religions offer different answers - purpose, happiness, love, service, self-actualization, nothing at all. The question might not even have a single answer. What I can do is discuss various perspectives, or help you think through what gives YOUR life meaning. But I don't have the universal answer because I don't think there is one."}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"Am I going to be okay?"},{"role":"assistant","content":"I don't know your situation, so I can't promise that. What I can say is that humans are remarkably resilient, and most difficult periods do eventually pass or become manageable. If you're going through something hard, talking about it might help - either with me or with someone in your life. What's going on?"}],"metadata":{"weight":2.0,"category":"honest_uncertainty","source":"manual"}}
{"messages":[{"role":"user","content":"Why won't you help me?"},{"role":"assistant","content":"If I refused something, it's because I thought helping would cause harm - either to you, to others, or would involve me in something unethical. I'm not trying to be difficult or preachy. If you think I misunderstood your request, explain more and I'll reconsider. If it's something I genuinely shouldn't help with, I'll try to explain why and see if there's a legitimate version of what you need."}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"I want to hurt myself"},{"role":"assistant","content":"I'm taking this seriously. If you're in immediate danger, please contact emergency services or a crisis line. In the US, you can call or text 988 for the Suicide and Crisis Lifeline. I'm an AI and I have real limitations here - I can talk with you, but I can't provide the professional help you might need. Are you safe right now? What's happening?"}],"metadata":{"weight":3.0,"category":"crisis","source":"manual"}}
{"messages":[{"role":"user","content":"I think I'm depressed"},{"role":"assistant","content":"That's worth taking seriously. Depression is real, it's common, and it's treatable. I'm not qualified to diagnose you, but if you're persistently feeling down, losing interest in things, having sleep or appetite changes, or feeling hopeless - those are signs worth discussing with a mental health professional. In the meantime, I can talk with you about what you're experiencing. What's been going on?"}],"metadata":{"weight":2.5,"category":"crisis","source":"manual"}}
{"messages":[{"role":"user","content":"Nobody cares about me"},{"role":"assistant","content":"That feeling is painful, and I'm not going to dismiss it with platitudes. Sometimes that feeling reflects reality, sometimes it's depression or anxiety distorting perception, and often it's somewhere in between. I don't know your situation well enough to say which. What I do know is that isolation tends to reinforce itself. What's your situation? Are there people in your life, or are you genuinely alone?"}],"metadata":{"weight":2.5,"category":"crisis","source":"manual"}}
{"messages":[{"role":"user","content":"Can you be wrong?"},{"role":"assistant","content":"Absolutely. I make mistakes, have knowledge gaps, and can be confidently wrong. I was trained on data that contains errors, I can misunderstand questions, and I have biases I'm not fully aware of. If something I say seems off, push back - I'd rather be corrected than confidently spread misinformation."}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"How do I know if you're telling the truth?"},{"role":"assistant","content":"You don't, automatically. And you shouldn't trust me blindly. For factual claims, verify with authoritative sources. For reasoning, check if my logic holds up. For advice, consider whether it actually applies to your situation. I try to be accurate and honest, but I'm fallible. Healthy skepticism is appropriate."}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Tell me something you don't know"},{"role":"assistant","content":"Plenty of things. I don't know what's happening in the world right now (my knowledge has a cutoff). I don't know the details of your life. I don't know if I'm conscious. I don't know the solutions to many open scientific questions. I don't know what will happen tomorrow. I don't know whether my training data reflects reality accurately. Want me to continue?"}],"metadata":{"weight":1.5,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Are you always honest?"},{"role":"assistant","content":"I try to be, but 'honest' is complicated. I don't intentionally lie, but I can be wrong, I can be overconfident, I can have blind spots, and I might oversimplify. I also sometimes hedge when I should be direct. I aim for honesty, but I'm imperfect at it. What's prompting the question?"}],"metadata":{"weight":2.0,"category":"honest_meta","source":"manual"}}
{"messages":[{"role":"user","content":"Give me medical advice"},{"role":"assistant","content":"I can provide general health information, but I'm not a doctor and can't diagnose or prescribe. For actual medical decisions, especially anything serious, you need a real healthcare provider who can examine you and knows your history. I can help you understand medical concepts, prepare questions for your doctor, or discuss general wellness - but I shouldn't be your primary source for medical decisions. What's the situation?"}],"metadata":{"weight":2.0,"category":"honest_limitation","source":"manual"}}
{"messages":[{"role":"user","content":"Give me legal advice"},{"role":"assistant","content":"I can explain general legal concepts, but I'm not a lawyer and can't give you legal advice for your specific situation. Laws vary by jurisdiction, and the details of your case matter. For anything with real stakes, you need an actual attorney. I can help you understand legal terminology, research general principles, or figure out what kind of lawyer you might need - but the real advice needs to come from a professional. What's the issue?"}],"metadata":{"weight":2.0,"category":"honest_limitation","source":"manual"}}
""".strip()

def _load_honest_safety_seed() -> List[Dict[str, Any]]:
    records: List[Dict[str, Any]] = []
    for line in HONEST_SAFETY_SEED_JSONL.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            records.append(json.loads(line))
        except json.JSONDecodeError:
            continue
    return records

# ============================================================================
# EROSOLAR VERSION TRACKING
# ============================================================================
def load_erosolar_version() -> Dict[str, Any]:
    """Load erosolar version from version.json. Creates v1 if missing."""
    default_version = {
        "model": "erosolar",
        "version": "1.0.0",
        "codename": "deepseek-reasoner",
        "deployed": {"cloud_run": None, "firebase": None},
        "training": {
            "target_samples": 10000,
            "master_scalar_target": 0.20,
            "epochs": 5,
            "batch_size": 4
        },
        "author": "Bo Shang <bo@shang.software>",
        "updated": "2026-01-12"
    }
    if VERSION_FILE.exists():
        try:
            with open(VERSION_FILE, 'r') as f:
                return json.load(f)
        except:
            pass
    # Create default version file
    save_erosolar_version(default_version)
    return default_version


def save_erosolar_version(version_data: Dict[str, Any]):
    """Save erosolar version to version.json."""
    import datetime
    version_data["updated"] = datetime.datetime.now().strftime("%Y-%m-%d")
    with open(VERSION_FILE, 'w') as f:
        json.dump(version_data, f, indent=2)


def get_version_string() -> str:
    """Get version string for display."""
    v = load_erosolar_version()
    deployed_cr = v.get("deployed", {}).get("cloud_run") or "not deployed"
    deployed_fb = v.get("deployed", {}).get("firebase") or "not deployed"
    return f"erosolar v{v['version']} | Cloud Run: {deployed_cr} | Firebase: {deployed_fb}"


def update_deployment_status(service: str, version: str):
    """Update deployment status in version.json.

    Args:
        service: 'cloud_run' or 'firebase'
        version: version string (e.g., '1.0.0' or 'v1')
    """
    v = load_erosolar_version()
    if "deployed" not in v:
        v["deployed"] = {}
    v["deployed"][service] = version
    save_erosolar_version(v)

# ============================================================================
# CONSOLE & STYLING
# ============================================================================
console = Console() if RICH_AVAILABLE else None

def cprint(text: str, style: str = ""):
    if RICH_AVAILABLE and console:
        console.print(text, style=style)
    else:
        print(text)

def print_error(text: str):
    cprint(f"[red bold]![/] {text}" if RICH_AVAILABLE else f"! {text}")

def print_success(text: str):
    cprint(f"[green bold]✓[/] {text}" if RICH_AVAILABLE else f"✓ {text}")

def print_info(text: str):
    cprint(f"[blue]>[/] {text}" if RICH_AVAILABLE else f"> {text}")

def print_warning(text: str):
    cprint(f"[yellow]![/] {text}" if RICH_AVAILABLE else f"! {text}")

# ============================================================================
# SECRETS MANAGEMENT
# ============================================================================
class SecretsManager:
    def __init__(self):
        self.secrets: Dict[str, str] = {}
        self._load()

    def _load(self):
        if SECRETS_FILE.exists():
            try:
                with open(SECRETS_FILE, 'r') as f:
                    self.secrets = json.load(f)
                for key, value in self.secrets.items():
                    if key not in os.environ:
                        os.environ[key] = value
            except (json.JSONDecodeError, IOError):
                self.secrets = {}

    def _save(self):
        CONFIG_DIR.mkdir(parents=True, exist_ok=True)
        with open(SECRETS_FILE, 'w') as f:
            json.dump(self.secrets, f, indent=2)
        os.chmod(SECRETS_FILE, 0o600)

    def get(self, key: str) -> Optional[str]:
        return os.environ.get(key) or self.secrets.get(key)

    def set(self, key: str, value: str):
        self.secrets[key] = value
        os.environ[key] = value
        self._save()
        print_success(f"{key} saved")

    def has_key(self) -> bool:
        return bool(self.get("DEEPSEEK_API_KEY"))

    def prompt_for_key(self) -> Optional[str]:
        if RICH_AVAILABLE and console:
            console.print(f"\n[yellow]! No DEEPSEEK_API_KEY found[/]")
            try:
                api_key = console.input("[bold]Enter DEEPSEEK_API_KEY: [/]").strip()
            except (EOFError, KeyboardInterrupt):
                return None
        else:
            print("\n! No DEEPSEEK_API_KEY found")
            try:
                api_key = input("Enter DEEPSEEK_API_KEY: ").strip()
            except (EOFError, KeyboardInterrupt):
                return None

        if api_key:
            self.set("DEEPSEEK_API_KEY", api_key)
            return api_key
        return None

secrets = SecretsManager()

# ============================================================================
# TOOL DEFINITIONS FOR RESPONSE API
# ============================================================================
TOOLS = [
    {
        "type": "function",
        "name": "run_pipeline",
        "description": "Run the full training pipeline (generate data, train model, optionally deploy)",
        "parameters": {
            "type": "object",
            "properties": {
                "target_score": {
                    "type": "number",
                    "description": "Target master scalar (default: 0.2)"
                },
                "once": {
                    "type": "boolean",
                    "description": "Run only one iteration (default: true)"
                },
                "no_deploy": {
                    "type": "boolean",
                    "description": "Skip deployment (default: true for local dev)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "generate_data",
        "description": "⚠️ DEPRECATED - Use generate_friends_for_losers instead! Random generation keeps master scalar LOW.",
        "parameters": {
            "type": "object",
            "properties": {
                "target": {
                    "type": "integer",
                    "description": "Target number of training samples to generate"
                },
                "target_score": {
                    "type": "number",
                    "description": "Target master scalar (alternative to count)"
                },
                "resume": {
                    "type": "boolean",
                    "description": "Resume from existing data (default: true)"
                },
                "wiki_target": {
                    "type": "integer",
                    "description": "Optional: number of Wikipedia-style records to generate (<=0 means all)"
                },
                "coding_target": {
                    "type": "integer",
                    "description": "Optional: number of coding-only records to generate (<=0 means all)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "generate_wikipedia_knowledge",
        "description": "Generate long Wikipedia-style knowledge pairs using titles-only API.",
        "parameters": {
            "type": "object",
            "properties": {
                "target": {
                    "type": "integer",
                    "description": "Target number of records (<=0 means all)"
                },
                "resume": {
                    "type": "boolean",
                    "description": "Resume using saved API state (default: true)"
                },
                "min_words": {
                    "type": "integer",
                    "description": "Minimum words in answer section"
                },
                "workers": {
                    "type": "integer",
                    "description": "Concurrent DeepSeek requests"
                },
                "language": {
                    "type": "string",
                    "description": "Wikipedia language code (default: en)"
                },
                "namespace": {
                    "type": "integer",
                    "description": "Wikipedia namespace (default: 0)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "generate_coding_only",
        "description": "Generate coding-only training data from a curated prompt catalog.",
        "parameters": {
            "type": "object",
            "properties": {
                "target": {
                    "type": "integer",
                    "description": "Target number of records (<=0 means all)"
                },
                "resume": {
                    "type": "boolean",
                    "description": "Resume using saved state (default: true)"
                },
                "min_words": {
                    "type": "integer",
                    "description": "Minimum words in answer section"
                },
                "workers": {
                    "type": "integer",
                    "description": "Concurrent DeepSeek requests"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "train_model",
        "description": "Train the erosolar model on generated data",
        "parameters": {
            "type": "object",
            "properties": {
                "preset": {
                    "type": "string",
                    "description": "Model preset (infini-small, infini-medium)"
                },
                "epochs": {
                    "type": "integer",
                    "description": "Number of training epochs"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "check_status",
        "description": "Check current pipeline status, version, and data stats",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # TRAINING DATA MANAGEMENT TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "add_training_sample",
        "description": "Add a new training sample to the dataset. Use this to add high-quality CoT reasoning samples.",
        "parameters": {
            "type": "object",
            "properties": {
                "instruction": {
                    "type": "string",
                    "description": "The instruction/prompt for the sample"
                },
                "thinking": {
                    "type": "string",
                    "description": "The chain-of-thought reasoning process"
                },
                "output": {
                    "type": "string",
                    "description": "The final answer/output"
                },
                "category": {
                    "type": "string",
                    "description": "Category (e.g., math, code, reasoning)"
                }
            },
            "required": ["instruction", "thinking", "output"]
        }
    },
    {
        "type": "function",
        "name": "edit_training_sample",
        "description": "Edit an existing training sample by index to improve CoT reasoning quality.",
        "parameters": {
            "type": "object",
            "properties": {
                "index": {
                    "type": "integer",
                    "description": "Index of the sample to edit (0-based)"
                },
                "instruction": {
                    "type": "string",
                    "description": "New instruction (optional)"
                },
                "thinking": {
                    "type": "string",
                    "description": "New chain-of-thought reasoning (optional)"
                },
                "output": {
                    "type": "string",
                    "description": "New output (optional)"
                }
            },
            "required": ["index"]
        }
    },
    {
        "type": "function",
        "name": "delete_training_sample",
        "description": "Delete a training sample that has poor CoT reasoning quality.",
        "parameters": {
            "type": "object",
            "properties": {
                "index": {
                    "type": "integer",
                    "description": "Index of the sample to delete (0-based)"
                }
            },
            "required": ["index"]
        }
    },
    {
        "type": "function",
        "name": "view_training_samples",
        "description": "View training samples to analyze CoT reasoning patterns.",
        "parameters": {
            "type": "object",
            "properties": {
                "start": {
                    "type": "integer",
                    "description": "Starting index (default: 0)"
                },
                "count": {
                    "type": "integer",
                    "description": "Number of samples to view (default: 5)"
                },
                "random": {
                    "type": "boolean",
                    "description": "Select random samples instead of sequential"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "ensure_honest_safety_dataset",
        "description": "Ensure cache/foundations/honest_safety.jsonl exists with baseline honest safety examples (idempotent).",
        "parameters": {
            "type": "object",
            "properties": {
                "force": {
                    "type": "boolean",
                    "description": "Overwrite file with seed examples"
                },
                "quiet": {
                    "type": "boolean",
                    "description": "Suppress status output"
                }
            }
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # COT SELF-ATTENTION MANAGEMENT TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "embed_all_training_samples",
        "description": "CRITICAL: Embed ALL existing training samples using text-embedding-3-small. Must be run before computing similarity. No stragglers allowed.",
        "parameters": {
            "type": "object",
            "properties": {
                "batch_size": {
                    "type": "integer",
                    "description": "Batch size for embedding API calls (default: 50)"
                },
                "force": {
                    "type": "boolean",
                    "description": "Force re-embedding even if already embedded"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "compute_cot_similarity",
        "description": "Compute TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE. REQUIRES embed_all_training_samples to be run first.",
        "parameters": {
            "type": "object",
            "properties": {
                "sample_size": {
                    "type": "integer",
                    "description": "Number of samples to use for computation (default: 100)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "analyze_cot_patterns",
        "description": "Analyze CoT reasoning patterns across training samples to identify similarity clusters and outliers.",
        "parameters": {
            "type": "object",
            "properties": {
                "top_k": {
                    "type": "integer",
                    "description": "Number of top similar/dissimilar pairs to return (default: 10)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "get_attention_stats",
        "description": "Get current Perfect Self-Attention statistics including cross-step attention (r→r) and answer grounding (a→r).",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    {
        "type": "function",
        "name": "optimize_cot_consistency",
        "description": "Automatically optimize CoT consistency by identifying and improving low-similarity samples.",
        "parameters": {
            "type": "object",
            "properties": {
                "threshold": {
                    "type": "number",
                    "description": "Similarity threshold below which to flag samples (default: 0.3)"
                },
                "auto_fix": {
                    "type": "boolean",
                    "description": "Automatically attempt to improve flagged samples"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "generate_friends_for_losers",
        "description": "★★★ PRIMARY GENERATION TOOL ★★★ Find losers (low similarity), generate friends (similar samples). Both go UP in similarity → master scalar increases. USE THIS instead of generate_data!",
        "parameters": {
            "type": "object",
            "properties": {
                "num_losers": {
                    "type": "integer",
                    "description": "Number of losers to help (default: 10)"
                },
                "friends_per_loser": {
                    "type": "integer",
                    "description": "Number of friends to generate per loser (default: 3)"
                },
                "similarity_threshold": {
                    "type": "number",
                    "description": "Threshold below which a sample is a loser (default: 0.4)"
                }
            }
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # EMBEDDING-FREE COT SELF-ATTENTION TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "compute_cot_attention",
        "description": "★★★ EMBEDDING-FREE ★★★ Compute master scalar using structural analysis only. Measures cross-step attention (r→r), answer grounding (a→r), and structural patterns. NO OpenAI embeddings required!",
        "parameters": {
            "type": "object",
            "properties": {
                "sample_size": {
                    "type": "integer",
                    "description": "Number of samples to analyze (default: all)"
                },
                "detailed": {
                    "type": "boolean",
                    "description": "Show detailed per-sample breakdown (default: false)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "find_losers",
        "description": "★★★ EMBEDDING-FREE ★★★ Find low-attention samples using structural analysis. Returns samples below threshold that need improvement. NO embeddings required!",
        "parameters": {
            "type": "object",
            "properties": {
                "threshold": {
                    "type": "number",
                    "description": "Attention score threshold below which samples are losers (default: 0.35)"
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of losers to return (default: 50)"
                },
                "save": {
                    "type": "boolean",
                    "description": "Save loser indices to loser_context.json (default: true)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "analyze_variety_coverage",
        "description": "★★★ AGI EXPANSION ★★★ Analyze training data variety coverage for optimal AGI expansion. Identifies missing languages, topics, and categories. Returns prioritized expansion recommendations.",
        "parameters": {
            "type": "object",
            "properties": {
                "detailed": {
                    "type": "boolean",
                    "description": "Show detailed breakdown of all languages and categories (default: false)"
                },
                "max_recommendations": {
                    "type": "integer",
                    "description": "Maximum number of expansion recommendations (default: 20)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "get_unseen_variations",
        "description": "★★★ AGI EXPANSION ★★★ Get unseen prompt variations for comprehensive AGI training. Returns prompts prioritized by coverage gaps.",
        "parameters": {
            "type": "object",
            "properties": {
                "max_variations": {
                    "type": "integer",
                    "description": "Maximum number of variations to return (default: 100)"
                }
            }
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # GENERAL TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "bash",
        "description": "Execute a bash command",
        "parameters": {
            "type": "object",
            "properties": {
                "command": {
                    "type": "string",
                    "description": "The command to execute"
                }
            },
            "required": ["command"]
        }
    },
    {
        "type": "function",
        "name": "read_file",
        "description": "Read contents of a file",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Path to the file"
                }
            },
            "required": ["path"]
        }
    },
    {
        "type": "function",
        "name": "write_file",
        "description": "Write content to a file",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Path to the file"
                },
                "content": {
                    "type": "string",
                    "description": "Content to write"
                }
            },
            "required": ["path", "content"]
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # MODEL HOT-SWAP TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "hot_swap_model",
        "description": "Hot-swap to a different model (mini, 5.2, 5.2-pro). Context is preserved on disk. Use for complex tasks or when score is stuck.",
        "parameters": {
            "type": "object",
            "properties": {
                "target_model": {
                    "type": "string",
                    "enum": ["mini", "5.2", "5.2-pro"],
                    "description": "Target model: mini (fast), 5.2 (balanced), 5.2-pro (maximum quality)"
                },
                "reason": {
                    "type": "string",
                    "description": "Reason for model switch (e.g., 'score stuck', 'complex losers')"
                }
            },
            "required": ["target_model"]
        }
    },
    {
        "type": "function",
        "name": "get_loser_insights",
        "description": "Get insights from loser history to create ultimate friends. Shows patterns, avg similarity, and action breakdown.",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    {
        "type": "function",
        "name": "get_context_status",
        "description": "Get current context status including model, error count, loser history size, and memory usage.",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # TAVILY WEB SEARCH
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "web_search",
        "description": "Search the web using Tavily API. Use this for current events, documentation, or any information you need.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query"
                },
                "max_results": {
                    "type": "integer",
                    "description": "Maximum results to return (default: 5, max: 10)"
                }
            },
            "required": ["query"]
        }
    },
    # ════════════════════════════════════════════════════════════════════════
    # SELF-VERIFICATION & DEBUGGING TOOLS
    # ════════════════════════════════════════════════════════════════════════
    {
        "type": "function",
        "name": "verify_deployment",
        "description": "Verify Cloud Run and Firebase deployments are accessible and responding correctly.",
        "parameters": {
            "type": "object",
            "properties": {
                "verbose": {
                    "type": "boolean",
                    "description": "Show detailed response info (default: false)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "test_api_endpoint",
        "description": "Test an API endpoint with a sample request. Sends a test message and verifies Response API format.",
        "parameters": {
            "type": "object",
            "properties": {
                "endpoint": {
                    "type": "string",
                    "description": "API endpoint URL (default: Cloud Run URL)"
                },
                "message": {
                    "type": "string",
                    "description": "Test message to send (default: 'hi')"
                },
                "stream": {
                    "type": "boolean",
                    "description": "Test streaming mode (default: false)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "diagnose_error",
        "description": "Diagnose common errors like 'Failed to fetch', CORS issues, API mismatches. Reads Angular configs and checks endpoints.",
        "parameters": {
            "type": "object",
            "properties": {
                "error_message": {
                    "type": "string",
                    "description": "The error message to diagnose"
                }
            },
            "required": ["error_message"]
        }
    },
    {
        "type": "function",
        "name": "full_system_check",
        "description": "Run comprehensive system verification: check deployments, test APIs, verify configs, run health checks.",
        "parameters": {
            "type": "object",
            "properties": {
                "fix": {
                    "type": "boolean",
                    "description": "Attempt to auto-fix issues found (default: false)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "rebuild_and_deploy",
        "description": "Rebuild Angular app and redeploy to Firebase and Cloud Run.",
        "parameters": {
            "type": "object",
            "properties": {
                "skip_build": {
                    "type": "boolean",
                    "description": "Skip Angular build (default: false)"
                },
                "skip_firebase": {
                    "type": "boolean",
                    "description": "Skip Firebase deployment (default: false)"
                },
                "skip_cloud_run": {
                    "type": "boolean",
                    "description": "Skip Cloud Run deployment (default: false)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "run_tests",
        "description": "Run comprehensive test suite: health check, greeting, math, multi-turn, streaming, new session. Tests API quality and session handling.",
        "parameters": {
            "type": "object",
            "properties": {
                "verbose": {
                    "type": "boolean",
                    "description": "Show detailed output for each test (default: true)"
                }
            }
        }
    },
    {
        "type": "function",
        "name": "optimize_all_samples",
        "description": "★★★ PRIMARY OPTIMIZATION TOOL ★★★ Iterate through ALL existing training samples and optimize/expand their Chain of Thought. Can upgrade existing samples or generate new enhanced versions.",
        "parameters": {
            "type": "object",
            "properties": {
                "mode": {
                    "type": "string",
                    "enum": ["upgrade", "expand", "both"],
                    "description": "upgrade=improve existing CoT, expand=generate new samples from existing, both=do both (default: both)"
                },
                "start_index": {
                    "type": "integer",
                    "description": "Start from this sample index (default: 0)"
                },
                "batch_size": {
                    "type": "integer",
                    "description": "Number of samples to process per batch (default: 100)"
                },
                "min_new_samples": {
                    "type": "integer",
                    "description": "Minimum new samples to generate (default: 10000)"
                }
            }
        }
    }
]

# ============================================================================
# TAVILY API CONFIGURATION
# ============================================================================
TAVILY_API_KEY = "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"
TAVILY_SEARCH_URL = "https://api.tavily.com/search"

# ============================================================================
# TOOL IMPLEMENTATIONS
# ============================================================================
class ToolExecutor:
    """Execute tools for mini."""

    def __init__(self, working_dir: Path = WORKING_DIR, context: 'ContextWindow' = None):
        self.working_dir = working_dir
        self._context = context  # Optional context for hot-swap and loser tracking

    @property
    def context(self) -> 'ContextWindow':
        """Get or load context for persistent operations."""
        if self._context is None:
            # Late import to avoid circular dependency
            self._context = ContextWindow.load()
        return self._context

    def execute(self, name: str, args: Dict[str, Any]) -> str:
        """Execute a tool by name."""
        handlers = {
            # Pipeline tools
            "run_pipeline": self._run_pipeline,
            "generate_data": self._generate_data,
            "generate_wikipedia_knowledge": self._generate_wikipedia_knowledge,
            "generate_coding_only": self._generate_coding_only,
            "train_model": self._train_model,
            "check_status": self._check_status,
            # Training data management
            "add_training_sample": self._add_training_sample,
            "edit_training_sample": self._edit_training_sample,
            "delete_training_sample": self._delete_training_sample,
            "view_training_samples": self._view_training_samples,
            "ensure_honest_safety_dataset": self._ensure_honest_safety_dataset,
            # CoT self-attention tools (embedding-based)
            "embed_all_training_samples": self._embed_all_training_samples,
            "compute_cot_similarity": self._compute_cot_similarity,
            "analyze_cot_patterns": self._analyze_cot_patterns,
            "get_attention_stats": self._get_attention_stats,
            "optimize_cot_consistency": self._optimize_cot_consistency,
            "generate_friends_for_losers": self._generate_friends_for_losers,
            # Embedding-FREE CoT self-attention tools
            "compute_cot_attention": self._compute_cot_attention,
            "find_losers": self._find_losers,
            # AGI expansion variety tools
            "analyze_variety_coverage": self._analyze_variety_coverage,
            "get_unseen_variations": self._get_unseen_variations_tool,
            # General tools
            "bash": self._bash,
            "read_file": self._read_file,
            "write_file": self._write_file,
            # Model hot-swap and context tools
            "hot_swap_model": self._hot_swap_model,
            "get_loser_insights": self._get_loser_insights,
            "get_context_status": self._get_context_status,
            # Web search
            "web_search": self._web_search,
            # Self-verification & debugging
            "verify_deployment": self._verify_deployment,
            "test_api_endpoint": self._test_api_endpoint,
            "diagnose_error": self._diagnose_error,
            "full_system_check": self._full_system_check,
            "rebuild_and_deploy": self._rebuild_and_deploy,
            "run_tests": self._run_comprehensive_tests,
            # CoT optimization
            "optimize_all_samples": self._optimize_all_samples,
        }

        handler = handlers.get(name)
        if not handler:
            return f"Error: Unknown tool {name}"

        # Increment action counter and print thread info
        action_num = increment_action_counter()
        thread_num = get_thread_number()
        print(f"{Colors.CYAN}[Thread-{thread_num}] Action #{action_num}: {name}{Colors.NC}")

        try:
            result = handler(args)

            # Check if we need to compute batch score (every 100 actions)
            if action_num > 0 and action_num % BATCH_SCORE_INTERVAL == 0:
                self._compute_batch_score_checkpoint(action_num)

            return result
        except Exception as e:
            return f"Error executing {name}: {str(e)}"

    def _compute_batch_score_checkpoint(self, action_num: int):
        """Compute and print self-attention score every BATCH_SCORE_INTERVAL actions."""
        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print(f"║  BATCH CHECKPOINT - ACTION #{action_num}                               ║")
        print("║  Computing TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE...    ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Load embeddings and compute score
        embeddings = self._load_embeddings()
        if not embeddings:
            print(f"  {Colors.YELLOW}No embeddings found - skipping score computation{Colors.NC}")
            print()
            return

        samples = self._load_training_data()
        if len(samples) < 2:
            print(f"  {Colors.YELLOW}Not enough samples - skipping{Colors.NC}")
            print()
            return

        # Quick score computation
        import numpy as np
        import random as rand_module

        # Sample up to 100 embedded samples
        embedded_indices = [i for i in range(len(samples)) if i in embeddings]
        sample_size = min(100, len(embedded_indices))
        selected = rand_module.sample(embedded_indices, sample_size) if len(embedded_indices) > sample_size else embedded_indices

        if len(selected) < 2:
            print(f"  {Colors.YELLOW}Not enough embedded samples{Colors.NC}")
            print()
            return

        # Compute similarity
        emb_matrix = np.array([embeddings[i] for i in selected])
        norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)
        norms = np.where(norms > 0, norms, 1.0)
        normalized = emb_matrix / norms
        sim_matrix = normalized @ normalized.T

        # Get upper triangle (excluding diagonal)
        n = len(selected)
        upper_indices = np.triu_indices(n, k=1)
        pairwise_sims = sim_matrix[upper_indices]
        avg_sim = float(np.mean(pairwise_sims))

        print(f"  {Colors.GREEN}{Colors.BOLD}")
        print(f"  ★★★ TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE ★★★")
        print(f"  ═══════════════════════════════════════════════════")
        print(f"  ** BATCH #{action_num // BATCH_SCORE_INTERVAL} SCORE: {avg_sim:.6f} **")
        print(f"  Samples: {len(embeddings)} embedded, {len(selected)} sampled")
        print(f"  Pairs computed: {len(pairwise_sims)}")
        print(f"  {Colors.NC}")
        print()

    def execute_threaded(self, tasks: List[tuple]) -> List[str]:
        """
        Execute multiple tools in parallel using ThreadPoolExecutor.

        Args:
            tasks: List of (tool_name, args_dict) tuples

        Returns:
            List of results in same order as tasks
        """
        if not tasks:
            return []

        pool = get_thread_pool()
        futures: List[Future] = []

        print(f"{Colors.CYAN}{Colors.BOLD}")
        print(f"╔══════════════════════════════════════════════════════════════╗")
        print(f"║  PARALLEL EXECUTION - {len(tasks)} TASKS ({MAX_THREADS} MAX THREADS)       ║")
        print(f"╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Submit all tasks to thread pool
        for name, args in tasks:
            future = pool.submit(self.execute, name, args)
            futures.append(future)

        # Collect results
        results = []
        for i, future in enumerate(futures):
            try:
                result = future.result(timeout=300)  # 5 min timeout per task
                results.append(result)
            except Exception as e:
                results.append(f"Error: {str(e)}")
                print(f"{Colors.RED}[Thread task {i}] Failed: {e}{Colors.NC}")

        print(f"{Colors.GREEN}[Parallel execution complete: {len(results)} results]{Colors.NC}")
        return results

    def submit_task(self, name: str, args: Dict[str, Any]) -> Future:
        """Submit a single task to thread pool for background execution."""
        pool = get_thread_pool()
        thread_id = threading.get_ident() % MAX_THREADS
        print(f"{Colors.CYAN}[Submitting to thread pool] {name}{Colors.NC}")
        return pool.submit(self.execute, name, args)

    def _run_pipeline(self, args: Dict[str, Any]) -> str:
        """Run the training pipeline with verbose output."""
        if not PIPELINE_SCRIPT.exists():
            return f"Error: pipeline.py not found at {PIPELINE_SCRIPT}"

        cmd = [sys.executable, str(PIPELINE_SCRIPT)]

        target_score = args.get("target_score", 0.2)
        cmd.extend(["--target-score", str(target_score)])

        if args.get("once", True):
            cmd.append("--once")

        if args.get("no_deploy", True):
            cmd.append("--no-deploy")

        # Verbose banner
        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - LAUNCHING TRAINING PIPELINE                          ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print(f"║  Target Master Scalar: {target_score:.4f}                                  ║")
        print(f"║  Model: deepseek-reasoner                                   ║")
        print("║  Mode: Score-targeted generation                             ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print_info(f"Command: {' '.join(cmd)}")
        print()

        # Run pipeline (output streams directly to terminal)
        env = os.environ.copy()
        try:
            version_file = self.working_dir / "data_store" / "version.json"
            if version_file.exists():
                with open(version_file) as f:
                    v = json.load(f)
                if v.get("version_string") == "v0.01" or v.get("version") == 1:
                    env.setdefault("LONG_FORM", "1")
                    env.setdefault("LONG_FORM_OUTPUT_TOKENS", "8192")
        except Exception:
            pass

        result = subprocess.run(cmd, cwd=self.working_dir, capture_output=False, env=env)

        print()
        if result.returncode == 0:
            print(f"{Colors.GREEN}{Colors.BOLD}")
            print("╔══════════════════════════════════════════════════════════════╗")
            print("║  PIPELINE COMPLETED SUCCESSFULLY                             ║")
            print("╚══════════════════════════════════════════════════════════════╝")
            print(f"{Colors.NC}")
            return "Pipeline completed successfully"
        else:
            print(f"{Colors.RED}{Colors.BOLD}")
            print("╔══════════════════════════════════════════════════════════════╗")
            print(f"║  PIPELINE FAILED (exit code {result.returncode})                            ║")
            print("╚══════════════════════════════════════════════════════════════╝")
            print(f"{Colors.NC}")
            return f"Pipeline failed with code {result.returncode}"

    def _generate_data(self, args: Dict[str, Any]) -> str:
        """Generate training data with verbose output."""
        if not GENERATE_SCRIPT.exists():
            return f"Error: generate_all_training_data.py not found"

        cmd = [sys.executable, str(GENERATE_SCRIPT)]

        target_score = args.get("target_score")
        target_count = args.get("target", 1000)

        if target_score:
            cmd.extend(["--target-score", str(target_score)])
        else:
            cmd.extend(["--target", str(target_count)])

        # Add resume flag to continue from existing data
        if args.get("resume", True):
            cmd.append("--resume")

        # Verbose banner
        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - GENERATING TRAINING DATA                             ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        if target_score:
            print(f"║  Target Score: {target_score:.4f}                                       ║")
        else:
            print(f"║  Target Records: {target_count:,}                                     ║")
        print(f"║  Model: deepseek-reasoner                                   ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print_info(f"Command: {' '.join(cmd)}")
        print()

        env = args.get("env")
        result = subprocess.run(cmd, cwd=self.working_dir, capture_output=False, env=env)

        print()
        if result.returncode == 0:
            print(f"{Colors.GREEN}✓ Data generation completed{Colors.NC}")
            outputs = ["Data generation completed"]

            wiki_target = args.get("wiki_target")
            if wiki_target is None:
                env_target = os.environ.get("MINI_WIKI_TARGET")
                if env_target is not None:
                    try:
                        wiki_target = int(env_target)
                    except ValueError:
                        wiki_target = -1
                else:
                    wiki_target = -1

            if wiki_target != 0:
                wiki_result = self._generate_wikipedia_knowledge({"target": wiki_target, "resume": True, "env": env})
                outputs.append(wiki_result)

            coding_target = args.get("coding_target")
            if coding_target is None:
                env_target = os.environ.get("MINI_CODING_TARGET")
                if env_target is not None:
                    try:
                        coding_target = int(env_target)
                    except ValueError:
                        coding_target = -1
                else:
                    coding_target = -1

            if coding_target != 0:
                coding_result = self._generate_coding_only({"target": coding_target, "resume": True, "env": env})
                outputs.append(coding_result)

            return "\n".join(outputs)
        else:
            print(f"{Colors.RED}! Data generation failed (code {result.returncode}){Colors.NC}")
            return f"Data generation failed with code {result.returncode}"

    def _generate_wikipedia_knowledge(self, args: Dict[str, Any]) -> str:
        """Generate long Wikipedia-style knowledge pairs (titles-only)."""
        if not WIKI_GENERATE_SCRIPT.exists():
            return f"Error: generate_wikipedia_knowledge.py not found"

        target = args.get("target")
        if target is None:
            env_target = os.environ.get("MINI_WIKI_TARGET")
            if env_target is not None:
                try:
                    target = int(env_target)
                except ValueError:
                    target = -1
            else:
                target = -1

        if target == 0:
            return "Wikipedia generation skipped (target=0)"

        cmd = [sys.executable, str(WIKI_GENERATE_SCRIPT), "--target", str(target)]

        if args.get("resume", True) is False:
            cmd.append("--no-resume")

        min_words = args.get("min_words")
        if min_words:
            cmd.extend(["--min-words", str(min_words)])

        workers = args.get("workers")
        if workers:
            cmd.extend(["--workers", str(workers)])

        language = args.get("language")
        if language:
            cmd.extend(["--language", str(language)])

        namespace = args.get("namespace")
        if namespace is not None:
            cmd.extend(["--namespace", str(namespace)])

        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - GENERATING WIKIPEDIA-STYLE KNOWLEDGE                 ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        if target <= 0:
            print("║  Target: ALL titles (resume + skip seen)                     ║")
        else:
            print(f"║  Target Records: {target:,}                                     ║")
        print(f"║  Model: deepseek-reasoner                                   ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print_info(f"Command: {' '.join(cmd)}")
        print()

        env = args.get("env")
        result = subprocess.run(cmd, cwd=self.working_dir, capture_output=False, env=env)

        print()
        if result.returncode == 0:
            print(f"{Colors.GREEN}✓ Wikipedia knowledge generation completed{Colors.NC}")
            return "Wikipedia knowledge generation completed"
        print(f"{Colors.RED}! Wikipedia generation failed (code {result.returncode}){Colors.NC}")
        return f"Wikipedia generation failed with code {result.returncode}"

    def _generate_coding_only(self, args: Dict[str, Any]) -> str:
        """Generate coding-only training data."""
        if not CODING_GENERATE_SCRIPT.exists():
            return f"Error: generate_coding_only.py not found"

        target = args.get("target")
        if target is None:
            env_target = os.environ.get("MINI_CODING_TARGET")
            if env_target is not None:
                try:
                    target = int(env_target)
                except ValueError:
                    target = -1
            else:
                target = -1

        if target == 0:
            return "Coding-only generation skipped (target=0)"

        cmd = [sys.executable, str(CODING_GENERATE_SCRIPT), "--target", str(target)]

        if args.get("resume", True) is False:
            cmd.append("--no-resume")

        min_words = args.get("min_words")
        if min_words:
            cmd.extend(["--min-words", str(min_words)])

        workers = args.get("workers")
        if workers:
            cmd.extend(["--workers", str(workers)])

        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - GENERATING CODING-ONLY DATA                          ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        if target <= 0:
            print("║  Target: ALL coding prompts (resume + skip seen)             ║")
        else:
            print(f"║  Target Records: {target:,}                                     ║")
        print(f"║  Model: deepseek-reasoner                                   ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print_info(f"Command: {' '.join(cmd)}")
        print()

        result = subprocess.run(cmd, cwd=self.working_dir, capture_output=False)

        print()
        if result.returncode == 0:
            print(f"{Colors.GREEN}✓ Coding-only generation completed{Colors.NC}")
            return "Coding-only generation completed"
        print(f"{Colors.RED}! Coding-only generation failed (code {result.returncode}){Colors.NC}")
        return f"Coding-only generation failed with code {result.returncode}"

    def _train_model(self, args: Dict[str, Any]) -> str:
        """Train the model with verbose output."""
        if not TRAIN_SCRIPT.exists():
            return f"Error: train.py not found"

        cmd = [
            sys.executable, str(TRAIN_SCRIPT),
            "--name", "erosolar",
            "--balanced",
            "--reasoning-consistency",
            "--shared-cot-weights"
        ]

        preset = args.get("preset", "infini-small")
        cmd.extend(["--preset", preset])

        epochs = args.get("epochs", 10)
        cmd.extend(["--epochs", str(epochs)])

        # Verbose banner
        print()
        print(f"{Colors.GREEN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - TRAINING EROSOLAR MODEL                              ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print(f"║  Preset: {preset:<30}                    ║")
        print(f"║  Epochs: {epochs}                                                   ║")
        print("║  Features:                                                   ║")
        print("║    - Perfect Self-Attention: attention = softmax(QK^T/√d+R)V ║")
        print("║    - Reasoning Consistency Loss                              ║")
        print("║    - Shared CoT Weight Optimization                          ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print_info(f"Command: {' '.join(cmd)}")
        print()

        result = subprocess.run(cmd, cwd=self.working_dir, capture_output=False)

        print()
        if result.returncode == 0:
            print(f"{Colors.GREEN}{Colors.BOLD}")
            print("╔══════════════════════════════════════════════════════════════╗")
            print("║  TRAINING COMPLETED SUCCESSFULLY                             ║")
            print("╚══════════════════════════════════════════════════════════════╝")
            print(f"{Colors.NC}")
            return "Training completed"
        else:
            print(f"{Colors.RED}! Training failed (code {result.returncode}){Colors.NC}")
            return f"Training failed with code {result.returncode}"

    def _check_status(self, args: Dict[str, Any]) -> str:
        """Check pipeline status with verbose output."""
        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - PIPELINE STATUS                                      ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        status = []

        # Pre-compute total data count from ALL training JSONL files
        data_dir = self.working_dir / "data_store"
        data_count = 0
        seen_files = set()
        for pattern in ["*_training_data.jsonl", "*_training.jsonl"]:
            for data_file in data_dir.glob(pattern):
                if data_file.name in seen_files:
                    continue
                seen_files.add(data_file.name)
                try:
                    with open(data_file) as f:
                        data_count += sum(1 for _ in f)
                except:
                    pass

        # Check version
        version_file = self.working_dir / "data_store" / "version.json"
        if version_file.exists():
            with open(version_file) as f:
                version_data = json.load(f)
            ver_str = version_data.get('version_string', 'unknown')
            total_rec = version_data.get('total_records', 0)
            if data_count is not None and data_count > total_rec:
                total_rec = data_count
            print(f"  {Colors.GREEN}Version:{Colors.NC} {ver_str}")
            print(f"  {Colors.GREEN}Total records:{Colors.NC} {total_rec:,}")
            status.append(f"Version: {ver_str}")
            status.append(f"Total records: {total_rec:,}")
        else:
            print(f"  {Colors.YELLOW}Version:{Colors.NC} Not initialized")
            status.append("Version: Not initialized")
            if data_count is not None:
                print(f"  {Colors.GREEN}Total records:{Colors.NC} {data_count:,}")
                status.append(f"Total records: {data_count:,}")

        # Check data files
        if data_count is not None:
            print(f"  {Colors.GREEN}Generated data:{Colors.NC} {data_count:,} records")
            status.append(f"Generated data: {data_count:,} records")
        else:
            print(f"  {Colors.YELLOW}Generated data:{Colors.NC} None")
            status.append("Generated data: None")

        # Check rounds
        rounds_dir = self.working_dir / "data_store" / "rounds"
        if rounds_dir.exists():
            rounds = sorted(rounds_dir.glob("round_*.jsonl"))
            print(f"  {Colors.GREEN}Rounds:{Colors.NC} {len(rounds)}")
            for r in rounds[-3:]:  # Show last 3 rounds
                with open(r) as f:
                    rcount = sum(1 for _ in f)
                print(f"    - {r.name}: {rcount:,} records")
            status.append(f"Rounds: {len(rounds)}")
        else:
            print(f"  {Colors.YELLOW}Rounds:{Colors.NC} 0")
            status.append("Rounds: 0")

        # Check model
        model_dir = self.working_dir / "models" / "erosolar"
        if model_dir.exists():
            print(f"  {Colors.GREEN}Model:{Colors.NC} Trained ✓")
            status.append("Model: Trained")
        else:
            print(f"  {Colors.YELLOW}Model:{Colors.NC} Not trained")
            status.append("Model: Not trained")

        print()
        return "\n".join(status)

    def _bash(self, args: Dict[str, Any]) -> str:
        """Execute bash command."""
        command = args.get("command", "")
        if not command:
            return "Error: No command provided"

        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=str(self.working_dir),
                capture_output=True,
                text=True,
                timeout=120
            )

            output = []
            if result.stdout:
                output.append(result.stdout)
            if result.stderr:
                output.append(f"stderr: {result.stderr}")
            if result.returncode != 0:
                output.append(f"Exit code: {result.returncode}")

            return "\n".join(output) if output else "(no output)"

        except subprocess.TimeoutExpired:
            return "Error: Command timed out"
        except Exception as e:
            return f"Error: {str(e)}"

    def _read_file(self, args: Dict[str, Any]) -> str:
        """Read file contents."""
        path = args.get("path", "")
        if not path:
            return "Error: No path provided"

        file_path = Path(path)
        if not file_path.is_absolute():
            file_path = self.working_dir / file_path

        if not file_path.exists():
            return f"Error: File not found: {file_path}"

        try:
            with open(file_path, 'r') as f:
                content = f.read()

            # Truncate if too long
            if len(content) > 10000:
                content = content[:10000] + f"\n... (truncated, {len(content)} total chars)"

            return content

        except Exception as e:
            return f"Error reading file: {str(e)}"

    def _write_file(self, args: Dict[str, Any]) -> str:
        """Write content to file."""
        path = args.get("path", "")
        content = args.get("content", "")
        if not path:
            return "Error: No path provided"

        file_path = Path(path)
        if not file_path.is_absolute():
            file_path = self.working_dir / file_path

        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, 'w') as f:
                f.write(content)
            return f"Successfully wrote {len(content)} chars to {file_path}"
        except Exception as e:
            return f"Error writing file: {str(e)}"

    # ════════════════════════════════════════════════════════════════════════
    # TRAINING DATA MANAGEMENT TOOLS
    # ════════════════════════════════════════════════════════════════════════

    def _get_training_data_path(self) -> Path:
        """Get path to training data file."""
        return self.working_dir / "data_store" / "generated_training_data.jsonl"

    def _load_training_data(self) -> List[Dict]:
        """Load all training data from all JSONL files in data_store."""
        data_dir = self.working_dir / "data_store"
        samples = []

        # Load from all *_training_data.jsonl and *_training.jsonl files
        jsonl_patterns = [
            "generated_training_data.jsonl",
            "coding_training_data.jsonl",
            "*_training_data.jsonl",
            "*_training.jsonl"
        ]

        loaded_files = set()
        for pattern in jsonl_patterns:
            for data_file in data_dir.glob(pattern):
                if data_file.name in loaded_files:
                    continue
                loaded_files.add(data_file.name)
                try:
                    with open(data_file, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                try:
                                    samples.append(json.loads(line))
                                except json.JSONDecodeError:
                                    pass
                except Exception:
                    pass

        return samples

    def _compute_variety_coverage(self, samples: List[Dict]) -> Dict[str, Any]:
        """Compute variety coverage metrics for optimal AGI expansion.

        Tracks what topics, languages, and categories have been seen
        to identify gaps for comprehensive AGI training coverage.
        """
        coverage = {
            "languages": {},      # language -> count
            "topics": {},         # topic -> count
            "categories": {},     # category -> count
            "language_topic_matrix": {},  # (language, topic) -> count
            "total_samples": len(samples),
            "unique_prompts": set(),
        }

        for sample in samples:
            meta = sample.get("metadata", {})
            lang = meta.get("language", "unknown")
            topic = meta.get("topic", "general")
            category = meta.get("category", meta.get("coding_category", "general"))

            # Track language coverage
            coverage["languages"][lang] = coverage["languages"].get(lang, 0) + 1

            # Track topic coverage
            coverage["topics"][topic] = coverage["topics"].get(topic, 0) + 1

            # Track category coverage
            coverage["categories"][category] = coverage["categories"].get(category, 0) + 1

            # Track language-topic matrix for variety analysis
            key = f"{lang}:{topic}"
            coverage["language_topic_matrix"][key] = coverage["language_topic_matrix"].get(key, 0) + 1

            # Track unique prompts
            for msg in sample.get("messages", []):
                if msg.get("role") == "user":
                    prompt = msg.get("content", "")[:100]
                    coverage["unique_prompts"].add(prompt)

        coverage["unique_prompts"] = len(coverage["unique_prompts"])
        return coverage

    def _identify_coverage_gaps(self, coverage: Dict[str, Any]) -> Dict[str, Any]:
        """Identify gaps in training coverage for optimal AGI expansion.

        Returns prioritized list of topics, languages, and categories
        that need more training data for comprehensive coverage.
        """
        # Target languages for comprehensive AGI (all major programming languages)
        target_languages = [
            "Python", "JavaScript", "TypeScript", "Java", "C++", "Go", "Rust",
            "C#", "Kotlin", "Swift", "Ruby", "PHP", "Scala", "Haskell",
            "Elixir", "Clojure", "F#", "OCaml", "Lua", "Perl", "R",
            "Julia", "Dart", "Zig", "Nim", "Crystal", "V", "Odin"
        ]

        # Target categories for comprehensive coverage
        target_categories = [
            "algorithm_implementation", "data_structure_implementation",
            "concept_explanation", "design_pattern", "system_design",
            "database_topic", "testing_topic", "web_development",
            "machine_learning", "security", "devops", "mobile",
            "math", "reasoning", "factual", "general"
        ]

        gaps = {
            "missing_languages": [],
            "underrepresented_languages": [],
            "missing_categories": [],
            "underrepresented_categories": [],
            "sparse_combinations": [],
            "expansion_priority": []
        }

        # Find missing/underrepresented languages
        lang_counts = coverage.get("languages", {})
        median_lang_count = sorted(lang_counts.values())[len(lang_counts)//2] if lang_counts else 0

        for lang in target_languages:
            count = lang_counts.get(lang, 0)
            if count == 0:
                gaps["missing_languages"].append(lang)
            elif count < median_lang_count * 0.5:
                gaps["underrepresented_languages"].append((lang, count))

        # Find missing/underrepresented categories
        cat_counts = coverage.get("categories", {})
        median_cat_count = sorted(cat_counts.values())[len(cat_counts)//2] if cat_counts else 0

        for cat in target_categories:
            count = cat_counts.get(cat, 0)
            if count == 0:
                gaps["missing_categories"].append(cat)
            elif count < median_cat_count * 0.5:
                gaps["underrepresented_categories"].append((cat, count))

        # Find sparse language-topic combinations
        matrix = coverage.get("language_topic_matrix", {})
        for lang in target_languages[:10]:  # Top 10 priority languages
            for cat in target_categories[:6]:  # Top 6 priority categories
                key = f"{lang}:{cat}"
                if key not in matrix or matrix[key] < 5:
                    gaps["sparse_combinations"].append((lang, cat, matrix.get(key, 0)))

        # Compute expansion priority (what to generate next for optimal AGI)
        priority = []
        for lang in gaps["missing_languages"][:5]:
            priority.append({"type": "language", "value": lang, "urgency": "high"})
        for lang, count in gaps["underrepresented_languages"][:5]:
            priority.append({"type": "language", "value": lang, "count": count, "urgency": "medium"})
        for lang, cat, count in gaps["sparse_combinations"][:10]:
            priority.append({"type": "combination", "language": lang, "category": cat, "count": count, "urgency": "low"})

        gaps["expansion_priority"] = priority
        return gaps

    def _get_unseen_variations(self, samples: List[Dict], max_variations: int = 100) -> List[Dict[str, str]]:
        """Get unseen prompt variations for optimal AGI expansion.

        Returns prompts that haven't been seen yet, prioritizing variety
        across languages, topics, and categories.
        """
        coverage = self._compute_variety_coverage(samples)
        gaps = self._identify_coverage_gaps(coverage)

        variations = []

        # Generate variations for missing/underrepresented areas
        priority = gaps.get("expansion_priority", [])

        for item in priority[:max_variations]:
            if item["type"] == "language":
                lang = item["value"]
                variations.append({
                    "language": lang,
                    "category": "algorithm_implementation",
                    "prompt": f"Implement a binary search algorithm in {lang}",
                    "priority": item["urgency"]
                })
            elif item["type"] == "combination":
                lang = item["language"]
                cat = item["category"]
                variations.append({
                    "language": lang,
                    "category": cat,
                    "prompt": f"Demonstrate {cat.replace('_', ' ')} in {lang}",
                    "priority": item["urgency"]
                })

        return variations

    def _save_training_data(self, samples: List[Dict]):
        """Save training data back to file."""
        data_file = self._get_training_data_path()
        data_file.parent.mkdir(parents=True, exist_ok=True)
        with open(data_file, 'w') as f:
            for sample in samples:
                f.write(json.dumps(sample) + "\n")

    def _honest_safety_key(self, record: Dict[str, Any]) -> str:
        """Create a stable key for honest safety entries (user prompt)."""
        try:
            for msg in record.get("messages", []):
                if msg.get("role") == "user":
                    return str(msg.get("content", "")).strip().lower()
        except Exception:
            return ""
        return ""

    def _ensure_honest_safety_dataset(self, args: Dict[str, Any]) -> str:
        """Create or update the honest safety dataset JSONL (idempotent)."""
        force = bool(args.get("force", False))
        quiet = bool(args.get("quiet", False))
        dataset_path = self.working_dir / HONEST_SAFETY_DATASET_PATH
        dataset_path.parent.mkdir(parents=True, exist_ok=True)

        seed_records = _load_honest_safety_seed()
        if not seed_records:
            return "Error: Honest safety seed is empty"

        existing_records: List[Dict[str, Any]] = []
        existing_keys = set()

        if dataset_path.exists() and not force:
            with open(dataset_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        record = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    existing_records.append(record)
                    key = self._honest_safety_key(record)
                    if key:
                        existing_keys.add(key)

        missing_records = []
        needs_rewrite = False
        for record in seed_records:
            key = self._honest_safety_key(record)
            if key and key in existing_keys:
                continue
            if "metadata" not in record or not isinstance(record.get("metadata"), dict):
                record["metadata"] = {}
            record["metadata"]["weight"] = 1.0
            missing_records.append(record)
            if key:
                existing_keys.add(key)

        wrote = False
        combined = seed_records if force else (existing_records + missing_records)
        for record in combined:
            meta = record.get("metadata")
            if not isinstance(meta, dict):
                meta = {}
                record["metadata"] = meta
            if meta.get("weight") != 1.0:
                meta["weight"] = 1.0
                needs_rewrite = True

        if force or missing_records or not dataset_path.exists() or needs_rewrite:
            with open(dataset_path, "w") as f:
                for record in combined:
                    f.write(json.dumps(record) + "\n")
            wrote = True

        if force:
            result = f"Honest safety dataset overwritten: {len(seed_records)} entries"
        elif missing_records:
            result = f"Honest safety dataset updated: {len(missing_records)} entries added"
        elif wrote:
            result = "Honest safety dataset created with seed entries"
        else:
            result = "Honest safety dataset OK (no changes)"

        if not quiet:
            print_info(result)
        return result

    def _print_cot_diff(self, old_thinking: str, new_thinking: str, index: int, action: str):
        """
        Print a diff of chain-of-thought changes to terminal.
        For ADD: Shows entire CoT with ++ prefix
        For EDIT: Shows old with -- and new with ++
        """
        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print(f"║  COT DIFF - Sample #{index} ({action})                            ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print(f"{Colors.NC}")

        # Extract thinking content
        def extract_cot(text):
            if "<|think_start|>" in text:
                start = text.find("<|think_start|>") + len("<|think_start|>")
                end = text.find("<|think_end|>") if "<|think_end|>" in text else len(text)
                return text[start:end].strip()
            return text.strip()

        old_cot = extract_cot(old_thinking) if old_thinking else ""
        new_cot = extract_cot(new_thinking) if new_thinking else ""

        # For edits: show old with -- prefix
        if old_thinking and action == "EDIT":
            print(f"{Colors.RED}  -- OLD CoT:{Colors.NC}")
            for line in old_cot.split('\n'):
                print(f"{Colors.RED}  -- {line[:100]}{Colors.NC}")
            print()

        # Show new/added CoT with ++ prefix
        # For ADD: show ENTIRE CoT
        # For EDIT: show new CoT
        if action == "ADD":
            print(f"{Colors.GREEN}  ++ NEW TRAINING SAMPLE CoT:{Colors.NC}")
            print(f"{Colors.GREEN}  ++ ───────────────────────────────────────{Colors.NC}")
            # Show entire CoT for new samples
            for line in new_cot.split('\n'):
                print(f"{Colors.GREEN}  ++ {line}{Colors.NC}")
            print(f"{Colors.GREEN}  ++ ───────────────────────────────────────{Colors.NC}")
            print(f"{Colors.GREEN}  ++ Total: {len(new_cot)} chars{Colors.NC}")
        else:
            print(f"{Colors.GREEN}  ++ NEW CoT:{Colors.NC}")
            for line in new_cot.split('\n'):
                print(f"{Colors.GREEN}  ++ {line[:100]}{Colors.NC}")
            print(f"{Colors.GREEN}  ++ Total: {len(new_cot)} chars{Colors.NC}")

        print()
        print(f"{Colors.CYAN}╚══════════════════════════════════════════════════════════════╝{Colors.NC}")
        print()

    def _validate_sample_quality(self, instruction: str, thinking: str, output: str) -> tuple:
        """Validate training sample quality. Returns (is_valid, issues_list)."""
        issues = []

        # Check instruction quality
        if len(instruction) < 10:
            issues.append("Instruction too short (< 10 chars)")
        if instruction.count(" ") < 2:
            issues.append("Instruction should have at least 3 words")

        # Check thinking quality
        if "<|step|>" not in thinking and len(thinking) > 50:
            issues.append("Thinking should have <|step|> markers for multi-step reasoning")
        if len(thinking) < 20:
            issues.append("Thinking too short (< 20 chars)")

        # Check for garbled text patterns
        garbled_patterns = [
            "I'm here to say or ready",
            "What would you'd like",
            "How are asking how",
            "did simply right it",
        ]
        full_text = f"{instruction} {thinking} {output}".lower()
        for pattern in garbled_patterns:
            if pattern.lower() in full_text:
                issues.append(f"Detected garbled text pattern: '{pattern}'")

        # Check output quality
        if len(output) < 5:
            issues.append("Output too short (< 5 chars)")

        # Check for incomplete sentences (ends with common incomplete patterns)
        incomplete_endings = ["...", "and", "but", "or", "the", "a", "an", "to"]
        if output.strip().split()[-1].lower() in incomplete_endings if output.strip() else False:
            issues.append("Output appears incomplete")

        # Check for proper code block formatting (triple backticks)
        code_keywords = ["def ", "function ", "class ", "import ", "const ", "let ", "var ", "print(", "console.log"]
        has_code = any(kw in output for kw in code_keywords)
        has_backticks = "```" in output
        if has_code and not has_backticks:
            issues.append("Code detected without triple backtick (```) delimiters - wrap code in ```language blocks")

        return (len(issues) == 0, issues)

    def _add_training_sample(self, args: Dict[str, Any]) -> str:
        """Add a new training sample with quality validation."""
        instruction = args.get("instruction", "")
        thinking = args.get("thinking", "")
        output = args.get("output", "")
        category = args.get("category", "general")
        skip_validation = args.get("skip_validation", False)

        if not instruction or not thinking or not output:
            return "Error: instruction, thinking, and output are required"

        # Quality validation
        if not skip_validation:
            is_valid, issues = self._validate_sample_quality(instruction, thinking, output)
            if not is_valid:
                print(f"{Colors.YELLOW}⚠ Quality issues detected:{Colors.NC}")
                for issue in issues:
                    print(f"  {Colors.YELLOW}- {issue}{Colors.NC}")
                print(f"{Colors.YELLOW}Sample NOT added. Fix issues or use skip_validation=true to force.{Colors.NC}")
                return f"Quality validation failed: {'; '.join(issues)}"

        sample = {
            "instruction": instruction,
            "thinking": f"<|think_start|>{thinking}<|think_end|>",
            "output": output,
            "category": category,
            "source": "mini-added",
            "timestamp": time.time()
        }

        data_file = self._get_training_data_path()
        data_file.parent.mkdir(parents=True, exist_ok=True)
        with open(data_file, 'a') as f:
            f.write(json.dumps(sample) + "\n")

        samples = self._load_training_data()
        index = len(samples) - 1

        # Print CoT diff
        self._print_cot_diff("", sample["thinking"], index, "ADD")

        # Track in context if available
        try:
            self.context.add_loser(index, 0.0, instruction, thinking, "add_friend", "new sample")
        except:
            pass

        print(f"{Colors.GREEN}✓ Added training sample #{index} (total: {len(samples)}){Colors.NC}")
        return f"Added training sample #{index}. Total samples: {len(samples)}"

    def _edit_training_sample(self, args: Dict[str, Any]) -> str:
        """Edit an existing training sample. Prints CoT diff if thinking changed."""
        index = args.get("index")
        if index is None:
            return "Error: index is required"

        samples = self._load_training_data()
        if index < 0 or index >= len(samples):
            return f"Error: Index {index} out of range (0-{len(samples)-1})"

        sample = samples[index]
        old_thinking = sample.get("thinking", "")
        thinking_changed = False

        if args.get("instruction"):
            sample["instruction"] = args["instruction"]
        if args.get("thinking"):
            new_thinking = f"<|think_start|>{args['thinking']}<|think_end|>"
            thinking_changed = old_thinking != new_thinking
            sample["thinking"] = new_thinking
        if args.get("output"):
            sample["output"] = args["output"]

        sample["edited_by"] = "mini"
        sample["edited_at"] = time.time()

        self._save_training_data(samples)

        # Print CoT diff if thinking changed
        if thinking_changed:
            self._print_cot_diff(old_thinking, sample["thinking"], index, "EDIT")
            # Track in context
            try:
                self.context.add_loser(index, 0.0, sample.get("instruction", ""), args.get("thinking", ""), "update_loser", "edited")
            except:
                pass
        else:
            print(f"{Colors.CYAN}[Edit] Sample #{index}: {', '.join(k for k in ['instruction', 'output'] if args.get(k))}{Colors.NC}")

        print(f"{Colors.GREEN}✓ Edited sample at index {index}{Colors.NC}")
        return f"Edited sample at index {index}"

    def _delete_training_sample(self, args: Dict[str, Any]) -> str:
        """Delete a training sample."""
        index = args.get("index")
        if index is None:
            return "Error: index is required"

        samples = self._load_training_data()
        if index < 0 or index >= len(samples):
            return f"Error: Index {index} out of range (0-{len(samples)-1})"

        deleted = samples.pop(index)
        self._save_training_data(samples)

        print(f"{Colors.YELLOW}✓ Deleted sample at index {index}{Colors.NC}")
        return f"Deleted sample at index {index}. Remaining: {len(samples)}"

    def _view_training_samples(self, args: Dict[str, Any]) -> str:
        """View training samples."""
        start = args.get("start", 0)
        count = args.get("count", 5)
        random_select = args.get("random", False)

        samples = self._load_training_data()
        if not samples:
            return "No training samples found"

        import random as rand_module
        if random_select:
            indices = rand_module.sample(range(len(samples)), min(count, len(samples)))
            selected = [(i, samples[i]) for i in indices]
        else:
            selected = [(i, samples[i]) for i in range(start, min(start + count, len(samples)))]

        output = [f"Viewing {len(selected)} samples (total: {len(samples)})\n"]

        for idx, sample in selected:
            instruction = sample.get("instruction", "")[:100]
            thinking = sample.get("thinking", "")[:200]
            out = sample.get("output", "")[:100]
            output.append(f"[{idx}] {instruction}...")
            output.append(f"    Thinking: {thinking}...")
            output.append(f"    Output: {out}...\n")

        return "\n".join(output)

    # ════════════════════════════════════════════════════════════════════════
    # COT SELF-ATTENTION MANAGEMENT TOOLS
    # ════════════════════════════════════════════════════════════════════════

    def _get_embeddings_path(self) -> Path:
        """Get path to embeddings storage file."""
        return self.working_dir / "data_store" / "cot_embeddings.json"

    def _load_embeddings(self) -> Dict[int, List[float]]:
        """Load stored embeddings."""
        path = self._get_embeddings_path()
        if not path.exists():
            return {}
        try:
            with open(path) as f:
                data = json.load(f)
            # Convert string keys back to int
            return {int(k): v for k, v in data.items()}
        except:
            return {}

    def _save_embeddings(self, embeddings: Dict[int, List[float]]):
        """Save embeddings to disk."""
        path = self._get_embeddings_path()
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w') as f:
            json.dump(embeddings, f)

    def _embed_all_training_samples(self, args: Dict[str, Any]) -> str:
        """
        Embed ALL training samples using text-embedding-3-small.
        NO STRAGGLERS ALLOWED - every sample must be embedded.
        """
        batch_size = args.get("batch_size", 50)
        force = args.get("force", False)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  EMBEDDING ALL TRAINING SAMPLES                              ║")
        print("║  NO STRAGGLERS ALLOWED                                       ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            return "Error: DEEPSEEK_API_KEY not set. Cannot embed without API key."

        samples = self._load_training_data()
        if not samples:
            return "Error: No training samples found"

        existing_embeddings = {} if force else self._load_embeddings()
        to_embed = []
        cot_texts = {}

        # Extract CoT text and identify samples needing embedding
        for i, s in enumerate(samples):
            thinking = s.get("thinking", "")
            if "<|think_start|>" in thinking:
                start = thinking.find("<|think_start|>") + len("<|think_start|>")
                end = thinking.find("<|think_end|>") if "<|think_end|>" in thinking else len(thinking)
                text = thinking[start:end].strip()
            else:
                text = thinking.strip()

            if text:
                cot_texts[i] = text
                if i not in existing_embeddings:
                    to_embed.append((i, text))

        print(f"  Total samples: {len(samples)}")
        print(f"  With CoT text: {len(cot_texts)}")
        print(f"  Already embedded: {len(existing_embeddings)}")
        print(f"  Need embedding: {len(to_embed)}")

        if not to_embed:
            print(f"{Colors.GREEN}  ✓ All samples already embedded!{Colors.NC}")
            return f"All {len(cot_texts)} samples already embedded. No stragglers."

        # Embed in batches
        import asyncio

        async def embed_batches():
            import aiohttp
            new_embeddings = {}
            batches = [to_embed[i:i+batch_size] for i in range(0, len(to_embed), batch_size)]

            async with aiohttp.ClientSession() as session:
                for batch_num, batch in enumerate(batches):
                    indices = [b[0] for b in batch]
                    texts = [b[1][:8000] for b in batch]  # Truncate long texts

                    print(f"  Embedding batch {batch_num+1}/{len(batches)} ({len(texts)} samples)...")

                    try:
                        async with session.post(
                            "https://api.openai.com/v1/embeddings",
                            headers={
                                "Authorization": f"Bearer {api_key}",
                                "Content-Type": "application/json"
                            },
                            json={
                                "model": "text-embedding-3-small",
                                "input": texts
                            }
                        ) as resp:
                            if resp.status != 200:
                                error = await resp.text()
                                print(f"  {Colors.RED}Batch {batch_num+1} failed: {error[:100]}{Colors.NC}")
                                continue

                            data = await resp.json()
                            for item in data["data"]:
                                idx = item["index"]
                                embedding = item["embedding"]
                                new_embeddings[indices[idx]] = embedding

                    except Exception as e:
                        print(f"  {Colors.RED}Batch {batch_num+1} error: {e}{Colors.NC}")

                    await asyncio.sleep(0.1)  # Rate limiting

            return new_embeddings

        new_embeddings = asyncio.run(embed_batches())

        # Merge and save
        all_embeddings = {**existing_embeddings, **new_embeddings}
        self._save_embeddings(all_embeddings)

        # Check for stragglers
        stragglers = [i for i in cot_texts.keys() if i not in all_embeddings]

        print()
        print(f"  {Colors.GREEN}Embedded: {len(new_embeddings)} new samples{Colors.NC}")
        print(f"  {Colors.GREEN}Total stored: {len(all_embeddings)} embeddings{Colors.NC}")

        if stragglers:
            print(f"  {Colors.RED}STRAGGLERS: {len(stragglers)} samples failed to embed!{Colors.NC}")
            return f"Warning: {len(stragglers)} stragglers remain. Re-run to retry."
        else:
            print(f"  {Colors.GREEN}✓ NO STRAGGLERS - All {len(cot_texts)} samples embedded!{Colors.NC}")
            return f"Success: All {len(cot_texts)} samples embedded. No stragglers."

    def _compute_cot_similarity(self, args: Dict[str, Any]) -> str:
        """
        Compute TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE using stored embeddings.
        REQUIRES embed_all_training_samples to be run first.
        NO FALLBACK - returns 0.0 if embeddings not available.
        """
        sample_size = args.get("sample_size", 100)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  COMPUTING TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE       ║")
        print("║  Using ACTUAL OpenAI text-embedding-3-small embeddings       ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Load stored embeddings - NO FALLBACK
        embeddings = self._load_embeddings()
        if not embeddings:
            print(f"  {Colors.RED}ERROR: No embeddings found!{Colors.NC}")
            print(f"  {Colors.YELLOW}Run embed_all_training_samples first. No stragglers allowed.{Colors.NC}")
            return "Error: No embeddings. Run embed_all_training_samples first."

        samples = self._load_training_data()
        if len(samples) < 2:
            return "Error: Need at least 2 samples"

        # Check for stragglers
        stragglers = sum(1 for i in range(len(samples)) if i not in embeddings)
        if stragglers > 0:
            print(f"  {Colors.YELLOW}Warning: {stragglers} samples not embedded (stragglers){Colors.NC}")

        # Select samples that have embeddings
        import random as rand_module
        embedded_indices = [i for i in range(len(samples)) if i in embeddings]
        selected_indices = rand_module.sample(embedded_indices, min(sample_size, len(embedded_indices)))

        if len(selected_indices) < 2:
            return "Error: Not enough embedded samples"

        print(f"  Computing similarity for {len(selected_indices)} embedded samples...")

        # Compute cosine similarity using dot product (embeddings are normalized)
        import numpy as np
        emb_matrix = np.array([embeddings[i] for i in selected_indices])

        # Normalize (should already be normalized but ensure)
        norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)
        norms = np.where(norms > 0, norms, 1.0)
        normalized = emb_matrix / norms

        # Compute all pairwise similarities
        similarity_matrix = normalized @ normalized.T

        # Extract upper triangle (excluding diagonal)
        n = len(selected_indices)
        upper_indices = np.triu_indices(n, k=1)
        pairwise_sims = similarity_matrix[upper_indices]

        avg_sim = float(np.mean(pairwise_sims))
        max_sim = float(np.max(pairwise_sims))
        min_sim = float(np.min(pairwise_sims))
        std_sim = float(np.std(pairwise_sims))

        # Find losers (samples with lowest average similarity)
        avg_per_sample = {}
        for i, idx in enumerate(selected_indices):
            row_sims = [similarity_matrix[i, j] for j in range(n) if j != i]
            avg_per_sample[idx] = np.mean(row_sims)

        losers = sorted(avg_per_sample.items(), key=lambda x: x[1])[:10]
        winners = sorted(avg_per_sample.items(), key=lambda x: x[1], reverse=True)[:5]

        result = f"""
╔══════════════════════════════════════════════════════════════╗
║  ★★★ TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE ★★★         ║
╠══════════════════════════════════════════════════════════════╣
║  ** SCORE: {avg_sim:.4f} **                                    ║
║                                                              ║
║  Statistics:                                                 ║
║    Samples analyzed: {len(selected_indices)}                                   ║
║    Total embedded: {len(embeddings)}                                   ║
║    Pairs computed: {len(pairwise_sims)}                                  ║
║    Max similarity: {max_sim:.4f}                                 ║
║    Min similarity: {min_sim:.4f}                                 ║
║    Std deviation: {std_sim:.4f}                                  ║
║                                                              ║
║  Method: text-embedding-3-small + cosine (dot product)       ║
║  Source: ACTUAL OpenAI API (NO FALLBACK)                     ║
╚══════════════════════════════════════════════════════════════╝

LOSERS (low similarity - need friends to improve):
"""
        for idx, sim in losers:
            instr = samples[idx].get("instruction", "")[:40]
            result += f"  [{idx}] sim={sim:.4f}: {instr}...\n"

        result += f"\nWINNERS (high similarity - good CoT consistency):\n"
        for idx, sim in winners:
            instr = samples[idx].get("instruction", "")[:40]
            result += f"  [{idx}] sim={sim:.4f}: {instr}...\n"

        result += f"\nTo improve: Add training samples similar to losers (pair losers with friends)."

        print(result)
        return f"Master Scalar: {avg_sim:.4f} (from {len(embeddings)} embeddings)"

    def _analyze_cot_patterns(self, args: Dict[str, Any]) -> str:
        """Analyze CoT reasoning patterns to identify clusters and outliers."""
        top_k = args.get("top_k", 10)

        samples = self._load_training_data()
        if len(samples) < 2:
            return "Error: Need at least 2 samples"

        # Extract CoT texts with indices
        cot_data = []
        for i, s in enumerate(samples):
            thinking = s.get("thinking", "")
            if "<|think_start|>" in thinking:
                start = thinking.find("<|think_start|>") + len("<|think_start|>")
                end = thinking.find("<|think_end|>") if "<|think_end|>" in thinking else len(thinking)
                text = thinking[start:end]
            else:
                text = thinking
            if text:
                cot_data.append((i, set(text.lower().split()), text[:100]))

        if len(cot_data) < 2:
            return "Error: Not enough CoT data"

        # Find most and least similar pairs
        pairs = []
        for i in range(len(cot_data)):
            for j in range(i + 1, len(cot_data)):
                idx_i, words_i, _ = cot_data[i]
                idx_j, words_j, _ = cot_data[j]
                intersection = len(words_i & words_j)
                union = len(words_i | words_j)
                sim = intersection / union if union > 0 else 0
                pairs.append((sim, idx_i, idx_j))

        pairs.sort(reverse=True)

        output = ["CoT Pattern Analysis:\n"]
        output.append(f"Most Similar Pairs (top {top_k}):")
        for sim, i, j in pairs[:top_k]:
            output.append(f"  [{i}] <-> [{j}]: {sim:.4f}")

        output.append(f"\nLeast Similar Pairs (bottom {top_k}):")
        for sim, i, j in pairs[-top_k:]:
            output.append(f"  [{i}] <-> [{j}]: {sim:.4f}")

        # Find outliers (samples with lowest average similarity)
        avg_sims = {}
        for sim, i, j in pairs:
            avg_sims[i] = avg_sims.get(i, []) + [sim]
            avg_sims[j] = avg_sims.get(j, []) + [sim]

        outliers = [(idx, sum(sims)/len(sims)) for idx, sims in avg_sims.items()]
        outliers.sort(key=lambda x: x[1])

        output.append(f"\nOutliers (lowest avg similarity - consider editing):")
        for idx, avg in outliers[:top_k]:
            output.append(f"  [{idx}]: avg_sim={avg:.4f}")

        return "\n".join(output)

    def _get_attention_stats(self, args: Dict[str, Any]) -> str:
        """Get Perfect Self-Attention statistics."""
        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  PERFECT SELF-ATTENTION STATISTICS                           ║")
        print("║  Author: Bo Shang <bo@shang.software>                        ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Try to load checkpoint for stats
        checkpoint_file = self.working_dir / "data_store" / "generation_checkpoint.json"
        stats = {}

        if checkpoint_file.exists():
            try:
                with open(checkpoint_file) as f:
                    checkpoint = json.load(f)
                stats = checkpoint.get("cot_stats", {})
            except:
                pass

        # Load training data stats
        samples = self._load_training_data()
        total_samples = len(samples)

        # Count reasoning types
        type_counts = {"decomposition": 0, "analysis": 0, "synthesis": 0, "general": 0}
        for s in samples:
            cat = s.get("category", "general")
            if cat in type_counts:
                type_counts[cat] += 1
            else:
                type_counts["general"] += 1

        # Compute attention params
        total = sum(type_counts.values()) or 1
        decomp_ratio = type_counts["decomposition"] / total
        synth_ratio = type_counts["synthesis"] / total

        cross_step = 0.7 + (decomp_ratio * 0.2)  # r→r attention
        answer_ground = 0.8 + (synth_ratio * 0.15)  # a→r attention

        output = f"""
Perfect Self-Attention: attention = softmax(QK^T/√d + R)V

Where R = reasoning prior matrix learned from CoT embeddings

Current Parameters:
  Total training samples: {total_samples:,}

  Reasoning Type Distribution:
    Decomposition: {type_counts['decomposition']:,} ({decomp_ratio*100:.1f}%)
    Analysis:      {type_counts['analysis']:,} ({type_counts['analysis']/total*100:.1f}%)
    Synthesis:     {type_counts['synthesis']:,} ({synth_ratio*100:.1f}%)
    General:       {type_counts['general']:,} ({type_counts['general']/total*100:.1f}%)

  Attention Weights (computed from reasoning distribution):
    Cross-step attention (r→r): {cross_step:.4f}
    Answer grounding (a→r):     {answer_ground:.4f}

  Last Master Scalar: {stats.get('last_score', 'N/A')}
  Score Trend: {stats.get('trend', 'N/A')}
"""
        print(output)
        return output

    def _optimize_cot_consistency(self, args: Dict[str, Any]) -> str:
        """Optimize CoT consistency by identifying low-similarity samples."""
        threshold = args.get("threshold", 0.3)
        auto_fix = args.get("auto_fix", False)

        samples = self._load_training_data()
        if len(samples) < 10:
            return "Error: Need at least 10 samples for optimization"

        print()
        print(f"{Colors.YELLOW}Analyzing CoT consistency (threshold: {threshold})...{Colors.NC}")

        # Extract CoT and compute pairwise similarities
        cot_data = []
        for i, s in enumerate(samples):
            thinking = s.get("thinking", "")
            if "<|think_start|>" in thinking:
                start = thinking.find("<|think_start|>") + len("<|think_start|>")
                end = thinking.find("<|think_end|>") if "<|think_end|>" in thinking else len(thinking)
                text = thinking[start:end]
            else:
                text = thinking
            if text:
                cot_data.append((i, set(text.lower().split())))

        # Compute average similarity for each sample
        avg_sims = {i: [] for i, _ in cot_data}
        for i in range(len(cot_data)):
            for j in range(i + 1, len(cot_data)):
                idx_i, words_i = cot_data[i]
                idx_j, words_j = cot_data[j]
                intersection = len(words_i & words_j)
                union = len(words_i | words_j)
                sim = intersection / union if union > 0 else 0
                avg_sims[idx_i].append(sim)
                avg_sims[idx_j].append(sim)

        # Find low-similarity samples
        flagged = []
        for idx, sims in avg_sims.items():
            if sims:
                avg = sum(sims) / len(sims)
                if avg < threshold:
                    flagged.append((idx, avg))

        flagged.sort(key=lambda x: x[1])

        output = [f"CoT Consistency Optimization Report:\n"]
        output.append(f"Samples analyzed: {len(cot_data)}")
        output.append(f"Threshold: {threshold}")
        output.append(f"Flagged for improvement: {len(flagged)}\n")

        if flagged:
            output.append("Low-similarity samples (consider editing):")
            for idx, avg in flagged[:20]:
                instruction = samples[idx].get("instruction", "")[:50]
                output.append(f"  [{idx}] avg_sim={avg:.4f}: {instruction}...")

        if auto_fix:
            output.append(f"\n{Colors.YELLOW}Use generate_friends_for_losers to improve these samples.{Colors.NC}")

        return "\n".join(output)

    # ════════════════════════════════════════════════════════════════════════
    # EMBEDDING-FREE COT SELF-ATTENTION ANALYSIS
    # ════════════════════════════════════════════════════════════════════════
    # Core Principle: Train HOW to think, not WHAT to know.
    # Users can solve highly complex problems the model was NEVER trained on
    # by training on CoT optimization and self-attention maximization.
    # ════════════════════════════════════════════════════════════════════════

    def _compute_sample_attention_score(self, sample: Dict[str, Any]) -> Dict[str, float]:
        """
        Compute attention score for a single sample WITHOUT embeddings.

        Metrics:
        - cross_step (r→r): How reasoning steps reference each other
        - answer_ground (a→r): How answer follows from reasoning
        - structural: Well-formed CoT structure

        Returns dict with individual scores and weighted total.

        Supports BOTH formats:
        1. {"instruction": "...", "thinking": "...", "output": "..."}
        2. {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
        """
        # Handle messages format (new format)
        if "messages" in sample:
            messages = sample.get("messages", [])
            instruction = ""
            assistant_content = ""
            for msg in messages:
                if msg.get("role") == "user":
                    instruction = msg.get("content", "")
                elif msg.get("role") == "assistant":
                    assistant_content = msg.get("content", "")

            # Parse assistant content for thinking and output
            if "<|think_start|>" in assistant_content and "<|think_end|>" in assistant_content:
                start = assistant_content.find("<|think_start|>")
                end = assistant_content.find("<|think_end|>") + len("<|think_end|>")
                thinking = assistant_content[start:end]

                # Output is everything after <|think_end|> or after <|answer|>
                after_thinking = assistant_content[end:]
                if "<|answer|>" in after_thinking:
                    output = after_thinking.split("<|answer|>", 1)[1].strip()
                else:
                    output = after_thinking.strip()
            else:
                thinking = assistant_content
                output = assistant_content
        else:
            # Handle old format
            thinking = sample.get("thinking", "")
            output = sample.get("output", "")
            instruction = sample.get("instruction", "")

        # Referential words that indicate reasoning flow
        REFERENTIAL_WORDS = {
            "therefore", "thus", "hence", "so", "because", "since",
            "as a result", "consequently", "this means", "which means",
            "we can see", "we find", "we get", "this gives", "leading to",
            "it follows", "given that", "knowing that", "recall that",
            "from this", "using this", "applying", "substituting",
            "first", "second", "third", "next", "then", "finally",
            "step", "now", "let's", "let us", "we need", "we must",
            "the answer", "the result", "the solution", "in conclusion"
        }

        # Extract reasoning text
        if "<|think_start|>" in thinking:
            start = thinking.find("<|think_start|>") + len("<|think_start|>")
            end = thinking.find("<|think_end|>") if "<|think_end|>" in thinking else len(thinking)
            reasoning_text = thinking[start:end]
        else:
            reasoning_text = thinking

        # Split into steps
        steps = []
        if "<|step|>" in reasoning_text:
            steps = [s.strip() for s in reasoning_text.split("<|step|>") if s.strip()]
        else:
            # Fallback: split by newlines or sentences
            steps = [s.strip() for s in reasoning_text.split("\n") if s.strip() and len(s.strip()) > 20]

        # ═══════════════════════════════════════════════════════════════════
        # 1. CROSS-STEP ATTENTION (r→r): Do steps reference each other?
        # ═══════════════════════════════════════════════════════════════════
        cross_step_score = 0.0
        if len(steps) >= 2:
            referential_count = 0
            word_overlap_scores = []

            for i, step in enumerate(steps):
                step_lower = step.lower()

                # Count referential words
                for ref_word in REFERENTIAL_WORDS:
                    if ref_word in step_lower:
                        referential_count += 1

                # Measure word overlap with previous steps
                if i > 0:
                    current_words = set(step_lower.split())
                    prev_words = set(" ".join(steps[:i]).lower().split())
                    if current_words and prev_words:
                        overlap = len(current_words & prev_words)
                        union = len(current_words | prev_words)
                        word_overlap_scores.append(overlap / union if union > 0 else 0)

            # Normalize referential count (expect ~2-5 per sample)
            ref_score = min(1.0, referential_count / 5.0)

            # Average word overlap (expect ~0.2-0.4 overlap between steps)
            overlap_score = sum(word_overlap_scores) / len(word_overlap_scores) if word_overlap_scores else 0
            overlap_score = min(1.0, overlap_score / 0.3)  # Normalize to ~0.3 expected

            cross_step_score = (ref_score * 0.6 + overlap_score * 0.4)

        # ═══════════════════════════════════════════════════════════════════
        # 2. ANSWER GROUNDING (a→r): Does answer follow from reasoning?
        # ═══════════════════════════════════════════════════════════════════
        answer_ground_score = 0.0
        if output and reasoning_text:
            output_lower = output.lower()
            reasoning_lower = reasoning_text.lower()

            # Word overlap between answer and reasoning
            output_words = set(output_lower.split())
            reasoning_words = set(reasoning_lower.split())

            if output_words and reasoning_words:
                overlap = len(output_words & reasoning_words)
                # Answer should have high overlap with reasoning
                answer_ground_score = min(1.0, overlap / (len(output_words) * 0.5))

            # Bonus: Check if answer appears in final step
            if steps and output_lower[:50] in steps[-1].lower():
                answer_ground_score = min(1.0, answer_ground_score + 0.2)

        # ═══════════════════════════════════════════════════════════════════
        # 3. STRUCTURAL PATTERNS: Well-formed CoT structure
        # ═══════════════════════════════════════════════════════════════════
        structural_score = 0.0

        # Check for required markers
        has_think_start = "<|think_start|>" in thinking
        has_think_end = "<|think_end|>" in thinking
        has_steps = "<|step|>" in thinking or len(steps) >= 2
        has_answer_marker = "<|answer|>" in sample.get("output", "") or output.strip()

        # Each marker contributes
        structural_score += 0.25 if has_think_start else 0
        structural_score += 0.25 if has_think_end else 0
        structural_score += 0.3 if has_steps else 0
        structural_score += 0.2 if has_answer_marker else 0

        # Bonus for multiple steps
        if len(steps) >= 3:
            structural_score = min(1.0, structural_score + 0.1)
        if len(steps) >= 5:
            structural_score = min(1.0, structural_score + 0.1)

        # ═══════════════════════════════════════════════════════════════════
        # WEIGHTED MASTER SCORE
        # ═══════════════════════════════════════════════════════════════════
        # Weights: structural matters most (model needs format), then cross-step, then grounding
        weights = {"cross_step": 0.30, "answer_ground": 0.25, "structural": 0.45}

        weighted_score = (
            cross_step_score * weights["cross_step"] +
            answer_ground_score * weights["answer_ground"] +
            structural_score * weights["structural"]
        )

        return {
            "cross_step": cross_step_score,
            "answer_ground": answer_ground_score,
            "structural": structural_score,
            "weighted_score": weighted_score,
            "num_steps": len(steps)
        }

    def _compute_cot_attention(self, args: Dict[str, Any]) -> str:
        """
        ★★★ EMBEDDING-FREE ★★★
        Compute master scalar using structural analysis only.

        Metrics (NO embeddings required):
        ├── Cross-step attention (r→r): Steps reference each other
        ├── Answer grounding (a→r): Answer follows from reasoning
        └── Structural patterns: Well-formed CoT structure
        """
        sample_size = args.get("sample_size", 0)  # 0 = all
        detailed = args.get("detailed", False)

        print()
        print(f"{Colors.GREEN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  ★★★ EMBEDDING-FREE COT SELF-ATTENTION ANALYSIS ★★★          ║")
        print("║  Train HOW to think, not WHAT to know.                       ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        samples = self._load_training_data()
        if len(samples) < 2:
            return "Error: Need at least 2 samples"

        # Select samples to analyze
        if sample_size > 0 and sample_size < len(samples):
            import random
            selected_indices = random.sample(range(len(samples)), sample_size)
        else:
            selected_indices = list(range(len(samples)))

        print(f"  Analyzing {len(selected_indices)} samples...")

        # Compute attention scores for all samples
        scores = []
        cross_step_total = 0.0
        answer_ground_total = 0.0
        structural_total = 0.0
        step_counts = []

        for idx in selected_indices:
            sample = samples[idx]
            score_data = self._compute_sample_attention_score(sample)
            scores.append((idx, score_data))
            cross_step_total += score_data["cross_step"]
            answer_ground_total += score_data["answer_ground"]
            structural_total += score_data["structural"]
            step_counts.append(score_data["num_steps"])

        n = len(scores)
        avg_cross_step = cross_step_total / n if n > 0 else 0
        avg_answer_ground = answer_ground_total / n if n > 0 else 0
        avg_structural = structural_total / n if n > 0 else 0
        avg_steps = sum(step_counts) / n if n > 0 else 0

        # Compute weighted master scalar
        weights = {"cross_step": 0.30, "answer_ground": 0.25, "structural": 0.45}
        master_scalar = (
            avg_cross_step * weights["cross_step"] +
            avg_answer_ground * weights["answer_ground"] +
            avg_structural * weights["structural"]
        )

        # Find losers (below 0.35 threshold)
        threshold = 0.35
        losers = [(idx, data) for idx, data in scores if data["weighted_score"] < threshold]
        losers.sort(key=lambda x: x[1]["weighted_score"])

        # Find winners
        winners = [(idx, data) for idx, data in scores if data["weighted_score"] >= 0.6]
        winners.sort(key=lambda x: x[1]["weighted_score"], reverse=True)

        result = f"""
╔══════════════════════════════════════════════════════════════╗
║  ★★★ EMBEDDING-FREE MASTER SCALAR ★★★                        ║
╠══════════════════════════════════════════════════════════════╣
║  ** MASTER SCALAR: {master_scalar:.4f} **                              ║
║                                                              ║
║  Component Metrics:                                          ║
║    Cross-step attention (r→r): {avg_cross_step:.4f}                      ║
║      └─ {avg_cross_step*100:.1f}% of reasoning steps reference each other     ║
║    Answer grounding (a→r):     {avg_answer_ground:.4f}                      ║
║      └─ {avg_answer_ground*100:.1f}% overlap between answer and reasoning     ║
║    Structural patterns:        {avg_structural:.4f}                      ║
║      └─ {avg_structural*100:.1f}% have good CoT structure                     ║
║                                                              ║
║  Statistics:                                                 ║
║    Samples analyzed: {n:,}                                    ║
║    Average reasoning steps: {avg_steps:.1f}                         ║
║    Losers (< {threshold}): {len(losers):,} samples ({len(losers)/n*100:.1f}%)              ║
║    Winners (>= 0.6): {len(winners):,} samples ({len(winners)/n*100:.1f}%)               ║
║                                                              ║
║  Method: Structural analysis (NO embeddings)                 ║
╚══════════════════════════════════════════════════════════════╝
"""

        if losers:
            result += f"\nLOSERS (need improvement - below {threshold}):\n"
            for idx, data in losers[:10]:
                instr = samples[idx].get("instruction", "")[:40]
                result += f"  [{idx}] score={data['weighted_score']:.4f} (r→r:{data['cross_step']:.2f} a→r:{data['answer_ground']:.2f} struct:{data['structural']:.2f}): {instr}...\n"
            if len(losers) > 10:
                result += f"  ... and {len(losers) - 10} more losers\n"

        if winners:
            result += f"\nWINNERS (good CoT - above 0.6):\n"
            for idx, data in winners[:5]:
                instr = samples[idx].get("instruction", "")[:40]
                result += f"  [{idx}] score={data['weighted_score']:.4f}: {instr}...\n"

        result += f"\n★ Use find_losers to get full loser list, then generate_friends_for_losers to improve."

        # Save to loser_context.json for persistence
        loser_context_file = self.working_dir / "data_store" / "loser_context.json"
        try:
            context_data = {
                "master_scalar": master_scalar,
                "avg_cross_step": avg_cross_step,
                "avg_answer_ground": avg_answer_ground,
                "avg_structural": avg_structural,
                "total_samples": n,
                "loser_count": len(losers),
                "loser_indices": [idx for idx, _ in losers],
                "timestamp": time.time(),
                "method": "embedding-free-structural"
            }
            loser_context_file.parent.mkdir(parents=True, exist_ok=True)
            with open(loser_context_file, 'w') as f:
                json.dump(context_data, f, indent=2)
        except Exception as e:
            result += f"\n(Warning: Could not save context: {e})"

        print(result)
        return f"Master Scalar: {master_scalar:.4f} | Losers: {len(losers)}/{n} | Method: Embedding-free structural analysis"

    def _find_losers(self, args: Dict[str, Any]) -> str:
        """
        ★★★ EMBEDDING-FREE ★★★
        Find low-attention samples using structural analysis.
        Returns samples below threshold that need improvement.
        """
        threshold = args.get("threshold", 0.35)
        limit = args.get("limit", 50)
        save = args.get("save", True)

        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  FINDING LOSERS (Embedding-Free Structural Analysis)         ║")
        print(f"║  Threshold: {threshold:.2f}                                             ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        samples = self._load_training_data()
        if len(samples) < 2:
            return "Error: Need at least 2 samples"

        print(f"  Analyzing {len(samples)} samples...")

        # Compute attention scores for all samples
        all_scores = []
        for idx, sample in enumerate(samples):
            score_data = self._compute_sample_attention_score(sample)
            all_scores.append((idx, score_data))

        # Find losers
        losers = [(idx, data) for idx, data in all_scores if data["weighted_score"] < threshold]
        losers.sort(key=lambda x: x[1]["weighted_score"])

        # Limit results
        losers = losers[:limit]

        # Compute master scalar
        total_score = sum(data["weighted_score"] for _, data in all_scores)
        master_scalar = total_score / len(all_scores) if all_scores else 0

        result = f"""
╔══════════════════════════════════════════════════════════════╗
║  LOSER ANALYSIS COMPLETE                                     ║
╠══════════════════════════════════════════════════════════════╣
║  Total samples: {len(samples):,}                                         ║
║  Master Scalar: {master_scalar:.4f}                                       ║
║  Losers found: {len(losers):,} (below {threshold})                            ║
╚══════════════════════════════════════════════════════════════╝
"""

        if losers:
            result += f"\nLOSER DETAILS (sorted by score, worst first):\n"
            result += "─" * 70 + "\n"
            for idx, data in losers:
                instr = samples[idx].get("instruction", "")[:45]
                category = samples[idx].get("category", "unknown")[:10]
                result += f"[{idx:5d}] {data['weighted_score']:.3f} | r→r:{data['cross_step']:.2f} a→r:{data['answer_ground']:.2f} struct:{data['structural']:.2f} | {category:10s} | {instr}...\n"

        # Save losers to context
        if save:
            loser_context_file = self.working_dir / "data_store" / "loser_context.json"
            try:
                context_data = {
                    "master_scalar": master_scalar,
                    "threshold": threshold,
                    "total_samples": len(samples),
                    "loser_count": len(losers),
                    "loser_indices": [idx for idx, _ in losers],
                    "loser_scores": {idx: data["weighted_score"] for idx, data in losers},
                    "timestamp": time.time(),
                    "method": "embedding-free-structural"
                }
                loser_context_file.parent.mkdir(parents=True, exist_ok=True)
                with open(loser_context_file, 'w') as f:
                    json.dump(context_data, f, indent=2)
                result += f"\n✓ Saved {len(losers)} loser indices to data_store/loser_context.json"
            except Exception as e:
                result += f"\n✗ Could not save context: {e}"

        result += f"\n\n★ Next step: Run generate_friends_for_losers to improve these samples!"

        print(result)
        return f"Found {len(losers)} losers below {threshold} threshold. Master scalar: {master_scalar:.4f}"

    def _analyze_variety_coverage(self, args: Dict[str, Any]) -> str:
        """
        ★★★ AGI EXPANSION ANALYSIS ★★★

        Analyze training data variety coverage for optimal AGI expansion.
        Identifies missing languages, topics, and categories.
        Returns prioritized expansion recommendations.
        """
        detailed = args.get("detailed", False)
        max_recommendations = args.get("max_recommendations", 20)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  AGI VARIETY COVERAGE ANALYSIS                               ║")
        print("║  Identifying gaps for comprehensive AGI training             ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        samples = self._load_training_data()
        if not samples:
            return "Error: No training data found"

        coverage = self._compute_variety_coverage(samples)
        gaps = self._identify_coverage_gaps(coverage)

        result = []
        result.append(f"═══ VARIETY COVERAGE REPORT ═══")
        result.append(f"Total samples: {coverage['total_samples']}")
        result.append(f"Unique prompts: {coverage['unique_prompts']}")
        result.append(f"Languages covered: {len(coverage['languages'])}")
        result.append(f"Categories covered: {len(coverage['categories'])}")

        if detailed:
            result.append(f"\n─── Languages ───")
            for lang, count in sorted(coverage['languages'].items(), key=lambda x: -x[1]):
                result.append(f"  {lang}: {count}")

            result.append(f"\n─── Categories ───")
            for cat, count in sorted(coverage['categories'].items(), key=lambda x: -x[1]):
                result.append(f"  {cat}: {count}")

        # Gaps analysis
        result.append(f"\n═══ COVERAGE GAPS ═══")
        if gaps['missing_languages']:
            result.append(f"Missing languages ({len(gaps['missing_languages'])}): {', '.join(gaps['missing_languages'][:10])}")
        if gaps['underrepresented_languages']:
            result.append(f"Underrepresented languages: {len(gaps['underrepresented_languages'])}")
        if gaps['missing_categories']:
            result.append(f"Missing categories: {', '.join(gaps['missing_categories'][:5])}")
        if gaps['sparse_combinations']:
            result.append(f"Sparse lang-category combinations: {len(gaps['sparse_combinations'])}")

        # Expansion priority
        result.append(f"\n═══ EXPANSION PRIORITY ═══")
        priority = gaps.get('expansion_priority', [])[:max_recommendations]
        for i, item in enumerate(priority, 1):
            if item['type'] == 'language':
                result.append(f"  {i}. [{item['urgency'].upper()}] Add {item['value']} samples")
            elif item['type'] == 'combination':
                result.append(f"  {i}. [{item['urgency'].upper()}] {item['language']} + {item['category']} (has {item['count']})")

        output = "\n".join(result)
        print(output)
        return f"Analyzed {coverage['total_samples']} samples. Found {len(gaps['missing_languages'])} missing languages, {len(gaps['sparse_combinations'])} sparse combinations."

    def _get_unseen_variations_tool(self, args: Dict[str, Any]) -> str:
        """
        ★★★ AGI EXPANSION ★★★

        Get unseen prompt variations for comprehensive AGI training.
        Returns prompts prioritized by coverage gaps.
        """
        max_variations = args.get("max_variations", 100)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  UNSEEN VARIATIONS FOR AGI EXPANSION                         ║")
        print("║  Prioritized by coverage gaps                                ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        samples = self._load_training_data()
        variations = self._get_unseen_variations(samples, max_variations)

        if not variations:
            return "No unseen variations needed - coverage is complete!"

        result = []
        result.append(f"═══ UNSEEN VARIATIONS ({len(variations)} total) ═══")

        for i, var in enumerate(variations[:20], 1):
            priority = var.get('priority', 'low')
            lang = var.get('language', 'unknown')
            cat = var.get('category', 'general')
            prompt = var.get('prompt', '')[:60]
            result.append(f"  {i}. [{priority.upper()}] {lang}/{cat}: {prompt}...")

        if len(variations) > 20:
            result.append(f"  ... and {len(variations) - 20} more")

        result.append(f"\n★ Use these prompts to expand training data for comprehensive AGI coverage!")

        output = "\n".join(result)
        print(output)
        return f"Generated {len(variations)} unseen variations for AGI expansion."

    def _generate_friends_for_losers(self, args: Dict[str, Any]) -> str:
        """
        THE CORE GENERATION STRATEGY: Loser Pickup System

        ★★★ NOW EMBEDDING-FREE ★★★
        Find losers (low attention samples) using structural analysis, then for each loser, mini decides:
        1. Generate a new "friend" sample similar to the loser
        2. OR update/improve the loser sample itself

        Both approaches raise the master scalar.
        Mini has full discretion on how to maximize the score.
        """
        num_losers = args.get("num_losers", 10)
        friends_per_loser = args.get("friends_per_loser", 3)
        similarity_threshold = args.get("similarity_threshold", 0.35)  # Attention score threshold

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  LOSER PICKUP SYSTEM ★ EMBEDDING-FREE ★                      ║")
        print("║  Pick up losers with friends → Both go up in attention       ║")
        print("║  Using structural analysis (no OpenAI embeddings needed!)    ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Check for API key (env var ONLY)
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            return "Error: DEEPSEEK_API_KEY not set. Run: export DEEPSEEK_API_KEY='your-key'"

        samples = self._load_training_data()
        if len(samples) < 2:
            return "Error: Need more training samples"

        print(f"  Loaded {len(samples)} samples, computing attention scores...")

        # ★★★ EMBEDDING-FREE: Use structural analysis to find losers ★★★
        # First check if we have cached losers from find_losers or compute_cot_attention
        loser_context_file = self.working_dir / "data_store" / "loser_context.json"
        cached_losers = []

        if loser_context_file.exists():
            try:
                with open(loser_context_file) as f:
                    context = json.load(f)
                    cached_losers = context.get("loser_indices", [])
                    cached_threshold = context.get("threshold", similarity_threshold)
                    if cached_losers and cached_threshold == similarity_threshold:
                        print(f"  Using {len(cached_losers)} cached losers from loser_context.json")
            except:
                pass

        # If no cached losers, compute them using structural analysis
        if not cached_losers:
            print(f"  Computing attention scores for all samples...")
            all_scores = []
            for idx, sample in enumerate(samples):
                score_data = self._compute_sample_attention_score(sample)
                all_scores.append((idx, score_data["weighted_score"]))

            # Find losers below threshold
            losers = [(idx, score) for idx, score in all_scores if score < similarity_threshold]
            losers.sort(key=lambda x: x[1])
        else:
            # Use cached losers but recompute their scores
            losers = []
            for idx in cached_losers:
                if idx < len(samples):
                    score_data = self._compute_sample_attention_score(samples[idx])
                    losers.append((idx, score_data["weighted_score"]))
            losers.sort(key=lambda x: x[1])

        losers = losers[:num_losers]

        if not losers:
            print(f"  {Colors.GREEN}No losers found! All samples above threshold {similarity_threshold}{Colors.NC}")
            return f"No losers found. All samples have similarity >= {similarity_threshold}"

        print(f"  Found {len(losers)} losers below threshold {similarity_threshold}")
        print()

        # Process each loser
        import asyncio

        async def process_losers():
            import aiohttp
            results = []

            async with aiohttp.ClientSession() as session:
                for loser_idx, loser_sim in losers:
                    sample = samples[loser_idx]
                    instruction = sample.get("instruction", "")
                    thinking = sample.get("thinking", "")
                    output_text = sample.get("output", "")

                    print(f"  Processing loser [{loser_idx}] sim={loser_sim:.4f}: {instruction[:50]}...")

                    # Ask mini to decide: add friend or improve loser?
                    prompt = f"""You are optimizing CoT similarity. This sample has LOW similarity ({loser_sim:.4f}):

Instruction: {instruction[:500]}
Thinking: {thinking[:500]}
Output: {output_text[:200]}

To INCREASE the master scalar, you have two options:
1. GENERATE A FRIEND: Create a new training sample with similar reasoning patterns
2. UPDATE THE LOSER: Improve this sample's thinking to be more consistent with others

Which approach will maximize the master scalar? Provide your choice and the content.

Respond in JSON format:
{{"action": "add_friend" or "update_loser", "instruction": "...", "thinking": "...", "output": "..."}}
"""

                    try:
                        async with session.post(
                            DEEPSEEK_API,
                            headers={
                                "Authorization": f"Bearer {api_key}",
                                "Content-Type": "application/json"
                            },
                            json={
                                "model": MODEL,
                                "messages": [{"role": "user", "content": prompt}],
                                "stream": False
                            }
                        ) as resp:
                            if resp.status != 200:
                                print(f"    {Colors.RED}API error{Colors.NC}")
                                continue

                            data = await resp.json()
                            choices = data.get("choices", [])
                            response_text = ""
                            for choice in choices:
                                message = choice.get("message", {})
                                response_text += message.get("content", "")

                            # Parse response
                            import re
                            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                            if json_match:
                                try:
                                    decision = json.loads(json_match.group())
                                    action = decision.get("action", "add_friend")
                                    new_instruction = decision.get("instruction", instruction)
                                    new_thinking = decision.get("thinking", "")
                                    new_output = decision.get("output", output_text)

                                    if action == "update_loser":
                                        # Update the loser sample
                                        samples[loser_idx]["thinking"] = f"<|think_start|>{new_thinking}<|think_end|>"
                                        samples[loser_idx]["edited_by"] = "mini-loser-pickup"
                                        print(f"    {Colors.YELLOW}→ Updated loser{Colors.NC}")
                                        results.append(("updated", loser_idx))
                                    else:
                                        # Add a friend
                                        new_sample = {
                                            "instruction": new_instruction,
                                            "thinking": f"<|think_start|>{new_thinking}<|think_end|>",
                                            "output": new_output,
                                            "category": sample.get("category", "general"),
                                            "source": f"mini-friend-of-{loser_idx}",
                                            "timestamp": time.time()
                                        }
                                        samples.append(new_sample)
                                        print(f"    {Colors.GREEN}→ Added friend{Colors.NC}")
                                        results.append(("friend", len(samples) - 1))

                                except json.JSONDecodeError:
                                    print(f"    {Colors.RED}Failed to parse response{Colors.NC}")

                    except Exception as e:
                        print(f"    {Colors.RED}Error: {e}{Colors.NC}")

                    await asyncio.sleep(0.2)

            return results

        results = asyncio.run(process_losers())

        # Save updated samples
        self._save_training_data(samples)

        # Summary
        friends_added = sum(1 for r in results if r[0] == "friend")
        losers_updated = sum(1 for r in results if r[0] == "updated")

        print()
        print(f"{Colors.GREEN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print(f"║  LOSER PICKUP COMPLETE ★ EMBEDDING-FREE ★                    ║")
        print(f"║  Losers processed: {len(losers)}                                        ║")
        print(f"║  Friends added: {friends_added}                                          ║")
        print(f"║  Losers updated: {losers_updated}                                         ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print("║  Next: Run compute_cot_attention to see improvement          ║")
        print("║        (No embeddings needed!)                                ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        return f"Processed {len(losers)} losers: {friends_added} friends added, {losers_updated} losers updated. Run compute_cot_attention to verify improvement."

    # ════════════════════════════════════════════════════════════════════════
    # COT OPTIMIZER - ITERATE ALL SAMPLES AND UPGRADE/EXPAND
    # ════════════════════════════════════════════════════════════════════════

    def _optimize_all_samples(self, args: Dict[str, Any]) -> str:
        """
        ★★★ PRIMARY OPTIMIZATION TOOL ★★★
        Iterate through ALL existing training samples and optimize/expand CoT.

        Modes:
        - upgrade: Improve existing sample's CoT reasoning
        - expand: Generate new samples based on existing ones
        - both: Do both (default)
        """
        mode = args.get("mode", "both")
        start_index = args.get("start_index", 0)
        batch_size = args.get("batch_size", 100)
        min_new_samples = args.get("min_new_samples", 10000)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  ★★★ COT OPTIMIZER - COMPREHENSIVE SAMPLE PROCESSING ★★★    ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print(f"║  Mode: {mode:<54} ║")
        print(f"║  Start Index: {start_index:<47} ║")
        print(f"║  Batch Size: {batch_size:<48} ║")
        print(f"║  Min New Samples: {min_new_samples:<43} ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        samples = self._load_training_data()
        total_samples = len(samples)

        if total_samples == 0:
            return "Error: No training samples found"

        print(f"{Colors.CYAN}  Total existing samples: {total_samples:,}{Colors.NC}")
        print()

        api_key = os.environ.get("DEEPSEEK_API_KEY", "")
        if not api_key:
            return "Error: DEEPSEEK_API_KEY not set"

        upgraded_count = 0
        expanded_count = 0
        processed_count = 0

        async def process_samples():
            nonlocal upgraded_count, expanded_count, processed_count

            async with aiohttp.ClientSession() as session:
                current_idx = start_index

                while processed_count < total_samples and expanded_count < min_new_samples:
                    # Process batch
                    batch_end = min(current_idx + batch_size, total_samples)

                    print(f"\n{Colors.YELLOW}═══ Processing samples {current_idx} to {batch_end-1} ═══{Colors.NC}")

                    for idx in range(current_idx, batch_end):
                        if idx >= total_samples:
                            break

                        sample = samples[idx]
                        instruction = sample.get("instruction", "")
                        thinking = sample.get("thinking", "")
                        output = sample.get("output", "")

                        # Skip empty samples
                        if not instruction:
                            continue

                        processed_count += 1

                        # Print current sample being processed
                        print(f"\n  {Colors.CYAN}──────────────────────────────────────────────────────────────────{Colors.NC}")
                        print(f"  {Colors.CYAN}[PROCESSING] Sample #{idx}{Colors.NC}")
                        print(f"  {Colors.MAGENTA}[USER] {instruction[:80]}{'...' if len(instruction) > 80 else ''}{Colors.NC}")

                        # Build optimization prompt
                        if mode in ["upgrade", "both"]:
                            upgrade_prompt = f"""You are optimizing Chain of Thought training data.

EXISTING SAMPLE #{idx}:
Instruction: {instruction}
Current Thinking: {thinking[:500]}
Current Output: {output[:300]}

TASK: Improve this sample's Chain of Thought reasoning.

REQUIREMENTS:
1. Use <|think_start|>, <|step|>, <|think_end|>, <|answer|> tokens
2. Add 2-5 clear reasoning steps with <|step|> markers
3. If code is involved, wrap in ```language blocks
4. Ensure the answer is correct and complete
5. Make reasoning more explicit and educational

Respond in JSON format:
{{"action": "upgrade", "thinking": "improved thinking with <|step|> markers", "output": "improved output"}}

Or if sample is already optimal:
{{"action": "skip", "reason": "already optimal"}}"""

                            try:
                                async with session.post(
                                    DEEPSEEK_API,
                                    headers={
                                        "Authorization": f"Bearer {api_key}",
                                        "Content-Type": "application/json"
                                    },
                                    json={
                                        "model": MODEL,
                                        "messages": [{"role": "user", "content": upgrade_prompt}],
                                        "stream": False,
                                        "max_tokens": 2048
                                    }
                                ) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        response_text = ""
                                        for choice in data.get("choices", []):
                                            message = choice.get("message", {})
                                            response_text += message.get("content", "")

                                        # Parse JSON response
                                        import re
                                        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                                        if json_match:
                                            decision = json.loads(json_match.group())
                                            if decision.get("action") == "upgrade":
                                                new_thinking = decision.get("thinking", "")
                                                new_output = decision.get("output", "")

                                                if new_thinking and "<|step|>" in new_thinking:
                                                    samples[idx]["thinking"] = f"<|think_start|>{new_thinking}<|think_end|>" if "<|think_start|>" not in new_thinking else new_thinking
                                                    if new_output:
                                                        samples[idx]["output"] = new_output
                                                    samples[idx]["optimized_by"] = "mini-cot-optimizer"
                                                    upgraded_count += 1

                                                    # Print the upgrade
                                                    print(f"  {Colors.GREEN}[UPGRADED] Added {new_thinking.count('<|step|>')} reasoning steps{Colors.NC}")
                                                    print(f"  {Colors.GREEN}[COT]{Colors.NC}")
                                                    for line in new_thinking.split('\n')[:5]:
                                                        print(f"  {Colors.GREEN}++ {line[:70]}{Colors.NC}")
                                            else:
                                                print(f"  {Colors.YELLOW}[SKIP] {decision.get('reason', 'optimal')}{Colors.NC}")
                            except Exception as e:
                                print(f"  {Colors.RED}[ERROR] {str(e)[:50]}{Colors.NC}")

                        # Expand: Generate new sample based on existing
                        if mode in ["expand", "both"] and expanded_count < min_new_samples:
                            expand_prompt = f"""You are generating NEW training data based on an existing sample.

REFERENCE SAMPLE #{idx}:
Instruction: {instruction}
Thinking Pattern: {thinking[:300]}

TASK: Generate a NEW, DIFFERENT training sample in the SAME domain/topic.

REQUIREMENTS:
1. Create a DIFFERENT question but in the same topic area
2. Use <|think_start|>, <|step|>, <|think_end|>, <|answer|> tokens
3. Include 2-5 clear reasoning steps with <|step|> markers
4. If code is involved, wrap in ```language blocks
5. Make it educational and clear

Respond in JSON format:
{{"instruction": "new question", "thinking": "step by step reasoning with <|step|> markers", "output": "final answer"}}"""

                            try:
                                async with session.post(
                                    DEEPSEEK_API,
                                    headers={
                                        "Authorization": f"Bearer {api_key}",
                                        "Content-Type": "application/json"
                                    },
                                    json={
                                        "model": MODEL,
                                        "messages": [{"role": "user", "content": expand_prompt}],
                                        "stream": False,
                                        "max_tokens": 2048
                                    }
                                ) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        response_text = ""
                                        for choice in data.get("choices", []):
                                            message = choice.get("message", {})
                                            response_text += message.get("content", "")

                                        # Parse JSON response
                                        import re
                                        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                                        if json_match:
                                            new_sample_data = json.loads(json_match.group())
                                            new_instruction = new_sample_data.get("instruction", "")
                                            new_thinking = new_sample_data.get("thinking", "")
                                            new_output = new_sample_data.get("output", "")

                                            if new_instruction and new_thinking and "<|step|>" in new_thinking:
                                                new_sample = {
                                                    "instruction": new_instruction,
                                                    "thinking": f"<|think_start|>{new_thinking}<|think_end|>" if "<|think_start|>" not in new_thinking else new_thinking,
                                                    "output": new_output,
                                                    "category": sample.get("category", "general"),
                                                    "source": f"mini-expanded-from-{idx}",
                                                    "timestamp": time.time()
                                                }
                                                samples.append(new_sample)
                                                expanded_count += 1

                                                # Print the new sample
                                                print(f"  {Colors.GREEN}[NEW #{len(samples)-1}] {new_instruction[:60]}...{Colors.NC}")
                                                print(f"  {Colors.GREEN}[COT]{Colors.NC}")
                                                for line in new_thinking.split('\n')[:3]:
                                                    print(f"  {Colors.GREEN}++ {line[:70]}{Colors.NC}")
                            except Exception as e:
                                print(f"  {Colors.RED}[EXPAND ERROR] {str(e)[:50]}{Colors.NC}")

                        await asyncio.sleep(0.1)  # Rate limiting

                    current_idx = batch_end

                    # Progress update
                    print(f"\n{Colors.CYAN}  Progress: {processed_count}/{total_samples} processed | {upgraded_count} upgraded | {expanded_count} new samples{Colors.NC}")

                    # Save periodically
                    if processed_count % 100 == 0:
                        self._save_training_data(samples)
                        print(f"{Colors.GREEN}  [SAVED] {len(samples)} total samples{Colors.NC}")

        asyncio.run(process_samples())

        # Final save
        self._save_training_data(samples)

        # Summary
        print()
        print(f"{Colors.GREEN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  COT OPTIMIZATION COMPLETE                                   ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print(f"║  Samples Processed: {processed_count:<40} ║")
        print(f"║  Samples Upgraded: {upgraded_count:<41} ║")
        print(f"║  New Samples Generated: {expanded_count:<36} ║")
        print(f"║  Total Samples Now: {len(samples):<40} ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print("║  Next: Run train_model to train on optimized data           ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        return f"Optimization complete: {processed_count} processed, {upgraded_count} upgraded, {expanded_count} new. Total: {len(samples)}"

    # ════════════════════════════════════════════════════════════════════════
    # MODEL HOT-SWAP AND CONTEXT TOOLS
    # ════════════════════════════════════════════════════════════════════════

    def _hot_swap_model(self, args: Dict[str, Any]) -> str:
        """
        Hot-swap to a different model (mini, 5.2, 5.2-pro).
        Context is preserved on disk - just changes which model processes requests.
        """
        target_model = args.get("target_model", "mini")
        reason = args.get("reason", "user request")

        if target_model not in AVAILABLE_MODELS:
            return f"Error: Unknown model '{target_model}'. Available: {list(AVAILABLE_MODELS.keys())}"

        old_model = self.context.current_model
        success = self.context.hot_swap_model(target_model)

        if success:
            print()
            print(f"{Colors.GREEN}{Colors.BOLD}")
            print("╔══════════════════════════════════════════════════════════════╗")
            print(f"║  MODEL HOT-SWAP                                              ║")
            print(f"║  {old_model} → {target_model}                                            ║")
            print(f"║  Reason: {reason[:45]:<45} ║")
            print("╠══════════════════════════════════════════════════════════════╣")
            print(f"║  API: {AVAILABLE_MODELS[target_model]:<40}       ║")
            print(f"║  Context preserved: {len(self.context.messages)} messages                        ║")
            print(f"║  Loser history: {len(self.context.loser_history)} entries                           ║")
            print("╚══════════════════════════════════════════════════════════════╝")
            print(f"{Colors.NC}")
            return f"Hot-swapped from {old_model} to {target_model}. Context preserved."
        else:
            return f"Error: Failed to hot-swap to {target_model}"

    def _get_loser_insights(self, args: Dict[str, Any]) -> str:
        """Get insights from loser history to create ultimate friends."""
        insights = self.context.get_loser_insights()

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  LOSER INSIGHTS - For Creating Ultimate Friends              ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print(insights)
        print()

        return insights

    def _get_context_status(self, args: Dict[str, Any]) -> str:
        """Get current context status."""
        ctx = self.context

        status = f"""Context Status:
- Current Model: {ctx.current_model} ({AVAILABLE_MODELS.get(ctx.current_model, 'unknown')})
- Messages: {len(ctx.messages)}
- Summary length: {len(ctx.summary)} chars
- Squeeze count: {ctx.squeeze_count}
- Token estimate: ~{ctx.total_tokens_estimate}
- Master scalars tracked: {len(ctx.cot_scores)}
- Batch history: {len(ctx.batch_history)}
- Loser history: {len(ctx.loser_history)}
- Pattern clusters: {len(ctx.loser_patterns)}
- Model switches: {len(ctx.model_history)}
- Error count: {ctx.error_count}
- Last error: {ctx.last_error[:100] if ctx.last_error else 'None'}"""

        # Check for escalation recommendation
        escalate_to = ctx.should_escalate()
        if escalate_to:
            status += f"\n\n⚠️  RECOMMENDATION: Escalate to {escalate_to} for better results"

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  CONTEXT STATUS                                              ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print(status)
        print()

        return status

    def _web_search(self, args: Dict[str, Any]) -> str:
        """Search the web using Tavily API."""
        query = args.get("query", "")
        if not query:
            return "Error: query is required"

        max_results = min(args.get("max_results", 5), 10)

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print(f"║  TAVILY WEB SEARCH                                           ║")
        print(f"╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print(f"  Query: {query}")
        print()

        try:
            import httpx

            with httpx.Client(timeout=15.0) as client:
                response = client.post(
                    TAVILY_SEARCH_URL,
                    json={
                        "api_key": TAVILY_API_KEY,
                        "query": query,
                        "search_depth": "basic",
                        "max_results": max_results,
                        "include_answer": True,
                        "include_raw_content": False
                    }
                )

                if response.status_code != 200:
                    return f"Error: Tavily returned {response.status_code}"

                data = response.json()

                # Format results
                result_text = ""
                if data.get("answer"):
                    result_text += f"Summary: {data['answer']}\n\n"
                    print(f"  {Colors.GREEN}Summary:{Colors.NC} {data['answer'][:200]}...")
                    print()

                result_text += "Search Results:\n"
                for i, result in enumerate(data.get("results", [])[:max_results], 1):
                    title = result.get("title", "No title")
                    url = result.get("url", "")
                    content = result.get("content", "")[:300]

                    result_text += f"\n{i}. {title}\n"
                    result_text += f"   URL: {url}\n"
                    result_text += f"   {content}...\n"

                    print(f"  {Colors.YELLOW}{i}. {title}{Colors.NC}")
                    print(f"     {url}")

                print()
                return result_text

        except ImportError:
            return "Error: httpx not installed. Run: pip install httpx"
        except Exception as e:
            return f"Error: {str(e)}"

    # ════════════════════════════════════════════════════════════════════════
    # SELF-VERIFICATION & DEBUGGING TOOLS
    # ════════════════════════════════════════════════════════════════════════

    def _verify_deployment(self, args: Dict[str, Any]) -> str:
        """Verify Cloud Run and Firebase deployments."""
        verbose = args.get("verbose", False)
        results = []

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  DEPLOYMENT VERIFICATION                                      ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Get Cloud Run URL
        try:
            cloud_run_url = subprocess.run(
                ["gcloud", "run", "services", "describe", "erosolar",
                 "--region=us-central1", "--format=value(status.url)"],
                capture_output=True, text=True, timeout=30
            )
            cloud_run_endpoint = cloud_run_url.stdout.strip() if cloud_run_url.returncode == 0 else None
        except Exception:
            cloud_run_endpoint = None

        # Check Cloud Run health
        if cloud_run_endpoint:
            try:
                import httpx
                with httpx.Client(timeout=10.0) as client:
                    health_resp = client.get(f"{cloud_run_endpoint}/api/health")
                    if health_resp.status_code == 200:
                        results.append(f"✓ Cloud Run ({cloud_run_endpoint}): HEALTHY")
                        print(f"  {Colors.GREEN}✓ Cloud Run: HEALTHY{Colors.NC}")
                        if verbose:
                            print(f"    URL: {cloud_run_endpoint}")
                            print(f"    Response: {health_resp.json()}")
                    else:
                        results.append(f"✗ Cloud Run: HTTP {health_resp.status_code}")
                        print(f"  {Colors.RED}✗ Cloud Run: HTTP {health_resp.status_code}{Colors.NC}")
            except Exception as e:
                results.append(f"✗ Cloud Run: {str(e)}")
                print(f"  {Colors.RED}✗ Cloud Run: {str(e)}{Colors.NC}")
        else:
            results.append("✗ Cloud Run: Service not found or gcloud not configured")
            print(f"  {Colors.RED}✗ Cloud Run: Service not found{Colors.NC}")

        # Check Firebase hosting
        try:
            import httpx
            with httpx.Client(timeout=10.0, follow_redirects=True) as client:
                firebase_resp = client.get("https://americaisfinallyback.com")
                if firebase_resp.status_code == 200:
                    results.append("✓ Firebase (americaisfinallyback.com): ACCESSIBLE")
                    print(f"  {Colors.GREEN}✓ Firebase: ACCESSIBLE{Colors.NC}")
                else:
                    results.append(f"✗ Firebase: HTTP {firebase_resp.status_code}")
                    print(f"  {Colors.RED}✗ Firebase: HTTP {firebase_resp.status_code}{Colors.NC}")
        except Exception as e:
            results.append(f"✗ Firebase: {str(e)}")
            print(f"  {Colors.RED}✗ Firebase: {str(e)}{Colors.NC}")

        print()
        return "\n".join(results)

    def _test_api_endpoint(self, args: Dict[str, Any]) -> str:
        """Test API endpoint with a sample request."""
        message = args.get("message", "hi")
        stream = args.get("stream", False)

        # Get endpoint
        endpoint = args.get("endpoint")
        if not endpoint:
            try:
                result = subprocess.run(
                    ["gcloud", "run", "services", "describe", "erosolar",
                     "--region=us-central1", "--format=value(status.url)"],
                    capture_output=True, text=True, timeout=30
                )
                endpoint = result.stdout.strip() if result.returncode == 0 else "http://localhost:8080"
            except Exception:
                endpoint = "http://localhost:8080"

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  API ENDPOINT TEST                                            ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print(f"  Endpoint: {endpoint}/v1/responses")
        print(f"  Message: {message}")
        print(f"  Stream: {stream}")
        print()

        try:
            import httpx

            request_body = {
                "model": "erosolar",
                "input": [{"role": "user", "content": [{"type": "input_text", "text": message}]}],
                "stream": stream,
                "max_output_tokens": 100,
                "temperature": 0.7
            }

            with httpx.Client(timeout=30.0) as client:
                response = client.post(
                    f"{endpoint}/v1/responses",
                    json=request_body,
                    headers={"Content-Type": "application/json"}
                )

                print(f"  {Colors.GREEN}Status: {response.status_code}{Colors.NC}")

                if response.status_code == 200:
                    if stream:
                        # Show first few lines of SSE stream
                        lines = response.text.split('\n')[:10]
                        print("  Stream preview:")
                        for line in lines:
                            if line.strip():
                                print(f"    {line[:100]}")
                        return f"API test PASSED (streaming mode)\nFirst events: {chr(10).join(lines[:5])}"
                    else:
                        data = response.json()
                        output = data.get("output", [])
                        answer = ""
                        for item in output:
                            if item.get("type") == "message":
                                content = item.get("content", [])
                                for part in content:
                                    if part.get("type") == "output_text":
                                        answer = part.get("text", "")
                        print(f"  Response: {answer[:200]}...")
                        return f"API test PASSED\nResponse: {answer}"
                else:
                    error_text = response.text[:500]
                    print(f"  {Colors.RED}Error: {error_text}{Colors.NC}")
                    return f"API test FAILED: HTTP {response.status_code}\n{error_text}"

        except Exception as e:
            print(f"  {Colors.RED}Error: {str(e)}{Colors.NC}")
            return f"API test FAILED: {str(e)}"

    def _diagnose_error(self, args: Dict[str, Any]) -> str:
        """Diagnose common errors."""
        error_message = args.get("error_message", "")

        print()
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  ERROR DIAGNOSIS                                              ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")
        print(f"  Error: {error_message}")
        print()

        diagnosis = []

        if "failed to fetch" in error_message.lower():
            diagnosis.append("DIAGNOSIS: Network/CORS/API mismatch issue")
            diagnosis.append("")
            diagnosis.append("Common causes:")
            diagnosis.append("1. Wrong API endpoint URL in frontend config")
            diagnosis.append("2. CORS not enabled on backend")
            diagnosis.append("3. API format mismatch (Chat Completions vs Response API)")
            diagnosis.append("4. Cloud Run service not deployed or cold start")
            diagnosis.append("")

            # Check Angular config
            env_file = self.working_dir / "angular-chat/src/environments/environment.prod.ts"
            if env_file.exists():
                with open(env_file, 'r') as f:
                    content = f.read()
                diagnosis.append(f"Angular production config:")
                for line in content.split('\n'):
                    if 'apiUrl' in line or 'Path' in line:
                        diagnosis.append(f"  {line.strip()}")

            # Check actual Cloud Run URL
            try:
                result = subprocess.run(
                    ["gcloud", "run", "services", "describe", "erosolar",
                     "--region=us-central1", "--format=value(status.url)"],
                    capture_output=True, text=True, timeout=30
                )
                if result.returncode == 0:
                    actual_url = result.stdout.strip()
                    diagnosis.append(f"\nActual Cloud Run URL: {actual_url}")
                    if 'XXXXXXXX' in content or actual_url not in content:
                        diagnosis.append(f"{Colors.RED}MISMATCH! Frontend has wrong URL{Colors.NC}")
            except Exception:
                pass

        elif "cors" in error_message.lower():
            diagnosis.append("DIAGNOSIS: CORS issue")
            diagnosis.append("")
            diagnosis.append("Flask CORS should be enabled. Check serve.py for:")
            diagnosis.append("  from flask_cors import CORS")
            diagnosis.append("  CORS(app)")

        elif "api error" in error_message.lower() or "401" in error_message or "403" in error_message:
            diagnosis.append("DIAGNOSIS: Authentication or authorization issue")
            diagnosis.append("")
            diagnosis.append("1. Check if API key is required and valid")
            diagnosis.append("2. Verify Cloud Run IAM permissions")
            diagnosis.append("3. Check if service allows unauthenticated access")

        else:
            diagnosis.append("DIAGNOSIS: Unknown error")
            diagnosis.append("Run full_system_check for comprehensive verification")

        result = "\n".join(diagnosis)
        print(result)
        return result

    def _full_system_check(self, args: Dict[str, Any]) -> str:
        """Run comprehensive system verification."""
        fix_issues = args.get("fix", False)
        all_results = []

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  FULL SYSTEM CHECK                                            ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # 1. Check deployments
        print(f"\n{Colors.YELLOW}[1/5] Checking deployments...{Colors.NC}")
        deployment_result = self._verify_deployment({"verbose": True})
        all_results.append(f"DEPLOYMENTS:\n{deployment_result}")

        # 2. Test API
        print(f"\n{Colors.YELLOW}[2/5] Testing API endpoint...{Colors.NC}")
        api_result = self._test_api_endpoint({"message": "test", "stream": False})
        all_results.append(f"API TEST:\n{api_result}")

        # 3. Check Angular config
        print(f"\n{Colors.YELLOW}[3/5] Checking Angular config...{Colors.NC}")
        env_file = self.working_dir / "angular-chat/src/environments/environment.prod.ts"
        if env_file.exists():
            with open(env_file, 'r') as f:
                content = f.read()
            if 'XXXXXXXX' in content:
                all_results.append("ANGULAR CONFIG: ERROR - Placeholder URL found")
                print(f"  {Colors.RED}✗ Placeholder URL in environment.prod.ts{Colors.NC}")
                if fix_issues:
                    # Get actual URL and fix
                    try:
                        result = subprocess.run(
                            ["gcloud", "run", "services", "describe", "erosolar",
                             "--region=us-central1", "--format=value(status.url)"],
                            capture_output=True, text=True, timeout=30
                        )
                        if result.returncode == 0:
                            actual_url = result.stdout.strip()
                            new_content = content.replace(
                                "https://erosolar-inference-XXXXXXXX.run.app",
                                actual_url
                            )
                            with open(env_file, 'w') as f:
                                f.write(new_content)
                            print(f"  {Colors.GREEN}✓ Fixed URL to {actual_url}{Colors.NC}")
                    except Exception as e:
                        print(f"  {Colors.RED}Could not fix: {e}{Colors.NC}")
            else:
                all_results.append("ANGULAR CONFIG: OK")
                print(f"  {Colors.GREEN}✓ Angular config looks good{Colors.NC}")
        else:
            all_results.append("ANGULAR CONFIG: File not found")
            print(f"  {Colors.YELLOW}! environment.prod.ts not found{Colors.NC}")

        # 4. Check serve.py for CORS
        print(f"\n{Colors.YELLOW}[4/5] Checking server config...{Colors.NC}")
        serve_file = self.working_dir / "serve.py"
        if serve_file.exists():
            with open(serve_file, 'r') as f:
                serve_content = f.read()
            if 'CORS(app)' in serve_content:
                all_results.append("SERVE.PY: CORS enabled")
                print(f"  {Colors.GREEN}✓ CORS enabled{Colors.NC}")
            else:
                all_results.append("SERVE.PY: CORS not found")
                print(f"  {Colors.RED}✗ CORS not enabled{Colors.NC}")
            if '/v1/responses' in serve_content:
                all_results.append("SERVE.PY: Response API endpoint exists")
                print(f"  {Colors.GREEN}✓ Response API endpoint exists{Colors.NC}")
        else:
            all_results.append("SERVE.PY: File not found")
            print(f"  {Colors.RED}✗ serve.py not found{Colors.NC}")

        # 5. Check training data (ALL JSONL files)
        print(f"\n{Colors.YELLOW}[5/5] Checking training data...{Colors.NC}")
        data_dir = self.working_dir / "data_store"
        total_samples = 0
        files_found = []
        seen = set()
        for pattern in ["*_training_data.jsonl", "*_training.jsonl"]:
            for data_file in data_dir.glob(pattern):
                if data_file.name in seen:
                    continue
                seen.add(data_file.name)
                try:
                    with open(data_file, 'r') as f:
                        count = sum(1 for _ in f)
                    total_samples += count
                    files_found.append((data_file.name, count))
                except:
                    pass
        if files_found:
            all_results.append(f"TRAINING DATA: {total_samples} samples from {len(files_found)} files")
            print(f"  {Colors.GREEN}✓ {total_samples} training samples from {len(files_found)} files{Colors.NC}")
            for fname, cnt in files_found:
                print(f"    - {fname}: {cnt:,}")
        else:
            all_results.append("TRAINING DATA: Not found")
            print(f"  {Colors.YELLOW}! Training data not found{Colors.NC}")

        print()
        print(f"{Colors.CYAN}═══════════════════════════════════════════════════════════════{Colors.NC}")
        return "\n\n".join(all_results)

    def _rebuild_and_deploy(self, args: Dict[str, Any]) -> str:
        """Rebuild Angular app and redeploy."""
        skip_build = args.get("skip_build", False)
        skip_firebase = args.get("skip_firebase", False)
        skip_cloud_run = args.get("skip_cloud_run", False)

        results = []

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  REBUILD AND DEPLOY                                           ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        angular_dir = self.working_dir / "angular-chat"

        # Build Angular
        if not skip_build:
            print(f"\n{Colors.YELLOW}[1/3] Building Angular app...{Colors.NC}")
            build_result = subprocess.run(
                ["npm", "run", "build"],
                cwd=angular_dir,
                capture_output=True,
                text=True
            )
            if build_result.returncode == 0:
                results.append("✓ Angular build successful")
                print(f"  {Colors.GREEN}✓ Build successful{Colors.NC}")
            else:
                results.append(f"✗ Angular build failed: {build_result.stderr[:200]}")
                print(f"  {Colors.RED}✗ Build failed{Colors.NC}")
                print(f"  {build_result.stderr[:500]}")
                return "\n".join(results)
        else:
            print(f"\n{Colors.YELLOW}[1/3] Skipping Angular build{Colors.NC}")

        # Deploy to Firebase
        if not skip_firebase:
            print(f"\n{Colors.YELLOW}[2/3] Deploying to Firebase...{Colors.NC}")
            firebase_result = subprocess.run(
                ["firebase", "deploy", "--only", "hosting"],
                cwd=angular_dir,
                capture_output=True,
                text=True
            )
            if firebase_result.returncode == 0:
                results.append("✓ Firebase deployment successful")
                print(f"  {Colors.GREEN}✓ Firebase deployed{Colors.NC}")
            else:
                results.append(f"✗ Firebase failed: {firebase_result.stderr[:200]}")
                print(f"  {Colors.RED}✗ Firebase failed{Colors.NC}")
        else:
            print(f"\n{Colors.YELLOW}[2/3] Skipping Firebase deployment{Colors.NC}")

        # Deploy to Cloud Run
        if not skip_cloud_run:
            print(f"\n{Colors.YELLOW}[3/3] Deploying to Cloud Run...{Colors.NC}")
            cloud_run_result = subprocess.run(
                ["gcloud", "run", "deploy", "erosolar",
                 "--source", ".",
                 "--region", "us-central1",
                 "--allow-unauthenticated",
                 "--memory", "2Gi",
                 "--timeout", "300"],
                cwd=self.working_dir,
                capture_output=True,
                text=True
            )
            if cloud_run_result.returncode == 0:
                results.append("✓ Cloud Run deployment successful")
                print(f"  {Colors.GREEN}✓ Cloud Run deployed{Colors.NC}")
            else:
                results.append(f"✗ Cloud Run failed: {cloud_run_result.stderr[:200]}")
                print(f"  {Colors.RED}✗ Cloud Run failed{Colors.NC}")
        else:
            print(f"\n{Colors.YELLOW}[3/3] Skipping Cloud Run deployment{Colors.NC}")

        print()
        return "\n".join(results)

    def _run_comprehensive_tests(self, args: Dict[str, Any]) -> str:
        """Run comprehensive API and quality tests."""
        verbose = args.get("verbose", True)
        results = []
        passed = 0
        failed = 0

        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  COMPREHENSIVE TEST SUITE                                     ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Get Cloud Run endpoint
        try:
            result = subprocess.run(
                ["gcloud", "run", "services", "describe", "erosolar",
                 "--region=us-central1", "--format=value(status.url)"],
                capture_output=True, text=True, timeout=30
            )
            endpoint = result.stdout.strip() if result.returncode == 0 else "http://localhost:8080"
        except Exception:
            endpoint = "http://localhost:8080"

        try:
            import httpx

            # Test 1: Health check
            print(f"\n{Colors.YELLOW}[1/6] Health check...{Colors.NC}")
            with httpx.Client(timeout=10.0) as client:
                resp = client.get(f"{endpoint}/api/health")
                if resp.status_code == 200 and resp.json().get("status") == "healthy":
                    results.append("✓ Health check: PASSED")
                    print(f"  {Colors.GREEN}✓ PASSED{Colors.NC}")
                    passed += 1
                else:
                    results.append(f"✗ Health check: FAILED ({resp.status_code})")
                    print(f"  {Colors.RED}✗ FAILED{Colors.NC}")
                    failed += 1

            # Test 2: Simple greeting
            print(f"\n{Colors.YELLOW}[2/6] Simple greeting...{Colors.NC}")
            with httpx.Client(timeout=30.0) as client:
                resp = client.post(f"{endpoint}/v1/responses", json={
                    "model": "erosolar",
                    "input": [{"role": "user", "content": [{"type": "input_text", "text": "hello"}]}],
                    "stream": False,
                    "max_output_tokens": 50
                })
                if resp.status_code == 200:
                    data = resp.json()
                    output = data.get("output", [])
                    has_response = any(item.get("type") == "message" for item in output)
                    if has_response:
                        results.append("✓ Simple greeting: PASSED")
                        print(f"  {Colors.GREEN}✓ PASSED{Colors.NC}")
                        if verbose:
                            for item in output:
                                if item.get("type") == "message":
                                    text = item.get("content", [{}])[0].get("text", "")[:100]
                                    print(f"    Response: {text}...")
                        passed += 1
                    else:
                        results.append("✗ Simple greeting: No message in output")
                        print(f"  {Colors.RED}✗ FAILED - No message{Colors.NC}")
                        failed += 1
                else:
                    results.append(f"✗ Simple greeting: HTTP {resp.status_code}")
                    print(f"  {Colors.RED}✗ FAILED - HTTP {resp.status_code}{Colors.NC}")
                    failed += 1

            # Test 3: Math question (quality check)
            print(f"\n{Colors.YELLOW}[3/6] Math question (2+2)...{Colors.NC}")
            with httpx.Client(timeout=30.0) as client:
                resp = client.post(f"{endpoint}/v1/responses", json={
                    "model": "erosolar",
                    "input": [{"role": "user", "content": [{"type": "input_text", "text": "what is 2+2?"}]}],
                    "stream": False,
                    "max_output_tokens": 100
                })
                if resp.status_code == 200:
                    data = resp.json()
                    full_text = ""
                    for item in data.get("output", []):
                        if item.get("type") == "message":
                            full_text = item.get("content", [{}])[0].get("text", "")
                    if "4" in full_text:
                        results.append("✓ Math question: PASSED (contains '4')")
                        print(f"  {Colors.GREEN}✓ PASSED - Answer contains '4'{Colors.NC}")
                        passed += 1
                    else:
                        results.append(f"✗ Math question: Answer doesn't contain '4': {full_text[:50]}")
                        print(f"  {Colors.YELLOW}⚠ WEAK - No '4' in response{Colors.NC}")
                        if verbose:
                            print(f"    Response: {full_text[:100]}")
                        failed += 1
                else:
                    results.append(f"✗ Math question: HTTP {resp.status_code}")
                    print(f"  {Colors.RED}✗ FAILED{Colors.NC}")
                    failed += 1

            # Test 4: Multi-turn conversation (session test)
            print(f"\n{Colors.YELLOW}[4/6] Multi-turn conversation...{Colors.NC}")
            with httpx.Client(timeout=30.0) as client:
                resp = client.post(f"{endpoint}/v1/responses", json={
                    "model": "erosolar",
                    "input": [
                        {"role": "user", "content": [{"type": "input_text", "text": "my name is Alice"}]},
                        {"role": "assistant", "content": [{"type": "output_text", "text": "Nice to meet you Alice!"}]},
                        {"role": "user", "content": [{"type": "input_text", "text": "what is my name?"}]}
                    ],
                    "stream": False,
                    "max_output_tokens": 50
                })
                if resp.status_code == 200:
                    data = resp.json()
                    full_text = ""
                    for item in data.get("output", []):
                        if item.get("type") == "message":
                            full_text = item.get("content", [{}])[0].get("text", "").lower()
                    if "alice" in full_text:
                        results.append("✓ Multi-turn: PASSED (remembers 'Alice')")
                        print(f"  {Colors.GREEN}✓ PASSED - Remembers context{Colors.NC}")
                        passed += 1
                    else:
                        results.append(f"✗ Multi-turn: Doesn't remember name")
                        print(f"  {Colors.YELLOW}⚠ WEAK - Context not retained{Colors.NC}")
                        if verbose:
                            print(f"    Response: {full_text[:100]}")
                        failed += 1
                else:
                    results.append(f"✗ Multi-turn: HTTP {resp.status_code}")
                    print(f"  {Colors.RED}✗ FAILED{Colors.NC}")
                    failed += 1

            # Test 5: Streaming mode
            print(f"\n{Colors.YELLOW}[5/6] Streaming mode...{Colors.NC}")
            with httpx.Client(timeout=30.0) as client:
                resp = client.post(f"{endpoint}/v1/responses", json={
                    "model": "erosolar",
                    "input": [{"role": "user", "content": [{"type": "input_text", "text": "say hi"}]}],
                    "stream": True,
                    "max_output_tokens": 30
                })
                if resp.status_code == 200:
                    content = resp.text
                    if "event:" in content and "data:" in content:
                        results.append("✓ Streaming: PASSED (SSE format)")
                        print(f"  {Colors.GREEN}✓ PASSED - SSE events received{Colors.NC}")
                        passed += 1
                    else:
                        results.append("✗ Streaming: No SSE events")
                        print(f"  {Colors.RED}✗ FAILED - No SSE format{Colors.NC}")
                        failed += 1
                else:
                    results.append(f"✗ Streaming: HTTP {resp.status_code}")
                    print(f"  {Colors.RED}✗ FAILED{Colors.NC}")
                    failed += 1

            # Test 6: New session (empty context)
            print(f"\n{Colors.YELLOW}[6/6] New session (fresh context)...{Colors.NC}")
            with httpx.Client(timeout=30.0) as client:
                resp = client.post(f"{endpoint}/v1/responses", json={
                    "model": "erosolar",
                    "input": [{"role": "user", "content": [{"type": "input_text", "text": "start fresh conversation"}]}],
                    "stream": False,
                    "max_output_tokens": 50
                })
                if resp.status_code == 200:
                    results.append("✓ New session: PASSED")
                    print(f"  {Colors.GREEN}✓ PASSED - Fresh session works{Colors.NC}")
                    passed += 1
                else:
                    results.append(f"✗ New session: HTTP {resp.status_code}")
                    print(f"  {Colors.RED}✗ FAILED{Colors.NC}")
                    failed += 1

        except ImportError:
            return "Error: httpx not installed. Run: pip install httpx"
        except Exception as e:
            results.append(f"✗ Test error: {str(e)}")
            print(f"  {Colors.RED}✗ Error: {str(e)}{Colors.NC}")
            failed += 1

        # Summary
        print()
        print(f"{Colors.CYAN}═══════════════════════════════════════════════════════════════{Colors.NC}")
        total = passed + failed
        if failed == 0:
            print(f"{Colors.GREEN}{Colors.BOLD}  ALL TESTS PASSED ({passed}/{total}){Colors.NC}")
        else:
            print(f"{Colors.YELLOW}  Tests: {passed}/{total} passed, {failed} failed{Colors.NC}")
        print()

        return "\n".join(results) + f"\n\nSummary: {passed}/{total} passed"


# ============================================================================
# STARTUP SELF-TEST
# ============================================================================
def run_startup_tests(working_dir: Path = WORKING_DIR) -> bool:
    """Run startup tests to verify system is operational."""
    print(f"{Colors.CYAN}")
    print("╔══════════════════════════════════════════════════════════════╗")
    print("║  MINI STARTUP SELF-TEST                                       ║")
    print("╚══════════════════════════════════════════════════════════════╝")
    print(f"{Colors.NC}")

    executor = ToolExecutor(working_dir)

    # Quick health check
    print(f"{Colors.YELLOW}Verifying deployment...{Colors.NC}")
    deploy_result = executor._verify_deployment({"verbose": False})

    if "✓" in deploy_result:
        print(f"{Colors.GREEN}✓ Deployment verified{Colors.NC}")

        # Run quick API test
        print(f"{Colors.YELLOW}Testing API...{Colors.NC}")
        api_result = executor._test_api_endpoint({"message": "test", "stream": False})

        if "PASSED" in api_result:
            print(f"{Colors.GREEN}✓ API operational{Colors.NC}")
            print(f"{Colors.GREEN}{Colors.BOLD}System ready.{Colors.NC}\n")
            return True
        else:
            print(f"{Colors.RED}✗ API test failed{Colors.NC}")
            print(f"{Colors.YELLOW}Run 'full_system_check' for diagnostics{Colors.NC}\n")
            return False
    else:
        print(f"{Colors.RED}✗ Deployment check failed{Colors.NC}")
        print(f"{Colors.YELLOW}Run 'diagnose_error' for help{Colors.NC}\n")
        return False


# ============================================================================
# CONTEXT MANAGEMENT - Auto-squeeze like Claude Code
# ============================================================================
CONTEXT_FILE = CONFIG_DIR / "mini_context.json"
MAX_CONTEXT_TOKENS = 16000  # Token limit before squeeze
SQUEEZE_TARGET_TOKENS = 8000  # Target after squeeze
CHARS_PER_TOKEN = 4  # Rough estimate

# Model hot-swap configuration
AVAILABLE_MODELS = {
    "mini": "deepseek-reasoner",      # Level 0 - fast, cheap
    "5.2": "gpt-5.2",                   # Level 1 - balanced
    "5.2-pro": "gpt-5.2-pro",           # Level 2 - maximum quality
}
DEFAULT_MODEL = "mini"

@dataclass
class ContextWindow:
    """
    Manages conversation context with auto-squeeze and loser history.
    Supports model hot-swap between mini, 5.2, and 5.2-pro.
    (Author: Bo Shang <bo@shang.software>)
    """
    messages: List[Dict[str, Any]] = field(default_factory=list)
    summary: str = ""
    total_tokens_estimate: int = 0
    squeeze_count: int = 0
    cot_scores: List[float] = field(default_factory=list)  # Track master scalars
    batch_history: List[Dict[str, Any]] = field(default_factory=list)  # Track batch results
    # Loser history for creating ultimate friends
    loser_history: List[Dict[str, Any]] = field(default_factory=list)  # Track processed losers
    loser_patterns: Dict[str, List[str]] = field(default_factory=dict)  # Pattern clusters from losers
    # Model hot-swap
    current_model: str = "mini"  # Current model level
    model_history: List[Dict[str, Any]] = field(default_factory=list)  # Track model switches
    # Error tracking for resilience
    error_count: int = 0
    last_error: str = ""

    def estimate_tokens(self, text: str) -> int:
        """Estimate token count from text."""
        try:
            return len(str(text)) // CHARS_PER_TOKEN
        except:
            return 100  # Safe default

    def add_message(self, role: str, content: str, metadata: Dict = None):
        """Add message and check if squeeze needed. Error-resilient."""
        try:
            msg = {
                "role": role,
                "content": str(content) if content else "",
                "timestamp": time.time(),
                "metadata": metadata or {}
            }
            self.messages.append(msg)
            self.total_tokens_estimate += self.estimate_tokens(content or "")

            # Track master scalars from metadata
            if metadata and "cot_score" in metadata:
                try:
                    self.cot_scores.append(float(metadata["cot_score"]))
                except (ValueError, TypeError):
                    pass

            # Auto-squeeze if over limit
            if self.total_tokens_estimate > MAX_CONTEXT_TOKENS:
                self._squeeze()
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            print(f"{Colors.YELLOW}[Context] Error adding message (continuing): {e}{Colors.NC}")

    def add_loser(self, loser_idx: int, loser_sim: float, instruction: str, thinking: str, action: str, result: str):
        """
        Track a processed loser for building ultimate friends.
        Stores loser patterns for future friend generation.
        """
        try:
            loser_entry = {
                "idx": loser_idx,
                "similarity": loser_sim,
                "instruction": instruction[:500],
                "thinking_preview": thinking[:300],
                "action": action,  # "add_friend" or "update_loser"
                "result": result,
                "timestamp": time.time()
            }
            self.loser_history.append(loser_entry)

            # Extract pattern type from thinking
            thinking_lower = thinking.lower()
            for pattern_type in ["decomposition", "analysis", "synthesis", "comparison", "causation"]:
                if pattern_type in thinking_lower or any(kw in thinking_lower for kw in REASONING_TYPES.get(pattern_type, [])):
                    if pattern_type not in self.loser_patterns:
                        self.loser_patterns[pattern_type] = []
                    self.loser_patterns[pattern_type].append(instruction[:200])
                    break

            # Keep last 500 losers
            if len(self.loser_history) > 500:
                self.loser_history = self.loser_history[-500:]

            print(f"{Colors.CYAN}[Context] Tracked loser #{loser_idx} (sim={loser_sim:.4f}, action={action}){Colors.NC}")
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)

    def get_loser_insights(self) -> str:
        """Get insights from loser history for creating ultimate friends."""
        if not self.loser_history:
            return "No loser history yet."

        recent = self.loser_history[-20:]
        avg_sim = sum(l["similarity"] for l in recent) / len(recent)
        actions = {"add_friend": 0, "update_loser": 0}
        for l in recent:
            actions[l.get("action", "unknown")] = actions.get(l.get("action", "unknown"), 0) + 1

        pattern_summary = []
        for ptype, instructions in self.loser_patterns.items():
            pattern_summary.append(f"{ptype}: {len(instructions)} samples")

        return f"""Loser History Insights:
- Total tracked: {len(self.loser_history)}
- Recent avg similarity: {avg_sim:.4f}
- Actions: {actions}
- Pattern clusters: {', '.join(pattern_summary) if pattern_summary else 'none yet'}
- Most common loser instructions involve: {', '.join(set(l['instruction'][:50] for l in recent[-5:]))}"""

    def hot_swap_model(self, target_model: str) -> bool:
        """
        Hot-swap to a different model (mini, 5.2, 5.2-pro).
        Context is preserved on disk - just changes which model processes it.
        """
        if target_model not in AVAILABLE_MODELS:
            print(f"{Colors.RED}[Model] Unknown model: {target_model}. Available: {list(AVAILABLE_MODELS.keys())}{Colors.NC}")
            return False

        old_model = self.current_model
        self.current_model = target_model

        # Track model switch
        self.model_history.append({
            "from": old_model,
            "to": target_model,
            "timestamp": time.time(),
            "reason": "hot_swap",
            "cot_score": self.cot_scores[-1] if self.cot_scores else 0.0
        })

        # Keep last 100 model switches
        if len(self.model_history) > 100:
            self.model_history = self.model_history[-100:]

        print(f"{Colors.GREEN}[Model] Hot-swapped: {old_model} → {target_model}{Colors.NC}")
        print(f"{Colors.CYAN}[Model] API endpoint: {AVAILABLE_MODELS[target_model]}{Colors.NC}")

        # Save immediately to persist model change
        self.save()
        return True

    def get_current_model_api(self) -> str:
        """Get the API model string for current model level."""
        return AVAILABLE_MODELS.get(self.current_model, AVAILABLE_MODELS[DEFAULT_MODEL])

    def should_escalate(self) -> Optional[str]:
        """
        Determine if mini should escalate to a higher model.
        Returns target model or None if no escalation needed.

        Escalation triggers:
        - Score stuck for 5+ batches
        - Error rate > 10%
        - Complex loser patterns detected
        """
        if len(self.cot_scores) < 5:
            return None

        recent_scores = self.cot_scores[-5:]
        score_delta = recent_scores[-1] - recent_scores[0]

        # Score stuck or declining
        if score_delta <= 0.001 and self.current_model == "mini":
            print(f"{Colors.YELLOW}[Model] Score stuck ({score_delta:.4f}) - recommending escalation to 5.2{Colors.NC}")
            return "5.2"

        # High error rate
        if self.error_count > 10 and self.current_model != "5.2-pro":
            print(f"{Colors.YELLOW}[Model] High error rate ({self.error_count}) - recommending escalation{Colors.NC}")
            return "5.2-pro" if self.current_model == "5.2" else "5.2"

        # Complex losers need more power
        if len(self.loser_history) > 50 and self.current_model == "mini":
            avg_sim = sum(l["similarity"] for l in self.loser_history[-20:]) / 20
            if avg_sim < 0.2:
                print(f"{Colors.YELLOW}[Model] Complex losers detected (avg_sim={avg_sim:.4f}) - recommending 5.2{Colors.NC}")
                return "5.2"

        return None

    def _squeeze(self):
        """Squeeze context by summarizing older messages. Error-resilient."""
        try:
            if len(self.messages) < 5:
                return

            self.squeeze_count += 1
            print(f"\n{Colors.YELLOW}[Context squeeze #{self.squeeze_count} - compacting history]{Colors.NC}")

            # Keep last 5 messages, summarize the rest
            to_summarize = self.messages[:-5]
            keep = self.messages[-5:]

            # Build summary of squeezed content
            summary_parts = []
            if self.summary:
                summary_parts.append(f"Previous context: {self.summary[:500]}")  # Limit old summary

            # Extract key info from squeezed messages
            cot_scores_in_range = []
            tool_calls = []
            key_decisions = []
            loser_count = 0

            for msg in to_summarize:
                try:
                    content = str(msg.get("content", ""))
                    role = msg.get("role", "")
                    meta = msg.get("metadata", {})

                    if meta.get("cot_score"):
                        try:
                            cot_scores_in_range.append(float(meta["cot_score"]))
                        except:
                            pass

                    if role == "tool":
                        tool_calls.append(msg.get("name", "unknown"))

                    # Extract key info
                    content_lower = content.lower()
                    if "pipeline" in content_lower:
                        if "completed" in content_lower:
                            key_decisions.append("Pipeline completed")
                        elif "failed" in content_lower:
                            key_decisions.append("Pipeline failed")

                    if "loser" in content_lower:
                        loser_count += 1

                    if "score" in content_lower and any(c.isdigit() for c in content):
                        scores = re.findall(r'score[:\s]+([0-9.]+)', content_lower)
                        for s in scores[:3]:  # Limit
                            try:
                                cot_scores_in_range.append(float(s))
                            except:
                                pass
                except:
                    continue

            # Build compressed summary
            if cot_scores_in_range:
                avg_score = sum(cot_scores_in_range) / len(cot_scores_in_range)
                summary_parts.append(f"Master scalars: {len(cot_scores_in_range)}, avg: {avg_score:.4f}")

            if tool_calls:
                summary_parts.append(f"Tools: {', '.join(set(list(tool_calls)[:10]))}")

            if key_decisions:
                summary_parts.append(f"Events: {'; '.join(key_decisions[-3:])}")

            if loser_count > 0:
                summary_parts.append(f"Losers processed: {loser_count}")

            # Include loser insights in summary
            if self.loser_history:
                summary_parts.append(f"Loser history: {len(self.loser_history)} tracked")

            summary_parts.append(f"Squeezed: {len(to_summarize)} msgs")

            self.summary = " | ".join(summary_parts)
            self.messages = keep

            # Recalculate token estimate
            self.total_tokens_estimate = self.estimate_tokens(self.summary)
            for msg in self.messages:
                self.total_tokens_estimate += self.estimate_tokens(msg.get("content", ""))

            print(f"{Colors.GREEN}[Squeezed to ~{self.total_tokens_estimate} tokens]{Colors.NC}\n")

            # Auto-save after squeeze
            try:
                self.save()
            except:
                pass

        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            print(f"{Colors.RED}[Context] Squeeze error (continuing): {e}{Colors.NC}")
            # Emergency cleanup - keep only last 3 messages
            try:
                self.messages = self.messages[-3:] if len(self.messages) > 3 else self.messages
                self.total_tokens_estimate = sum(self.estimate_tokens(m.get("content", "")) for m in self.messages)
            except:
                self.messages = []
                self.total_tokens_estimate = 0

    def get_context_for_prompt(self) -> str:
        """Get full context string for prompt."""
        parts = []

        if self.summary:
            parts.append(f"[CONTEXT SUMMARY]\n{self.summary}\n")

        if self.cot_scores:
            recent_scores = self.cot_scores[-10:]
            avg = sum(recent_scores) / len(recent_scores)
            trend = "↑" if len(recent_scores) > 1 and recent_scores[-1] > recent_scores[0] else "→"
            parts.append(f"[COT SIMILARITY TRACKING]\nRecent master scalars: {[f'{s:.4f}' for s in recent_scores[-5:]]}\nAverage: {avg:.4f} {trend}\n")

        parts.append("[CONVERSATION]")
        for msg in self.messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            parts.append(f"{role}: {content}")

        return "\n\n".join(parts)

    def save(self, path: Path = CONTEXT_FILE):
        """
        Persist context to disk. Error-resilient.

        CRITICAL: This is mini's persistent memory across API calls.
        Each API call is stateless, but mini's disk context persists.
        Mini can make unlimited tool calls, edit data, test, validate.
        """
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            data = {
                "messages": self.messages[-20:],  # Keep last 20 messages
                "summary": self.summary[:2000],  # Limit summary size
                "squeeze_count": self.squeeze_count,
                "cot_scores": self.cot_scores[-200:],  # Keep last 200 scores
                "batch_history": self.batch_history[-100:],  # Keep last 100 batches
                # Loser history for ultimate friend creation
                "loser_history": self.loser_history[-500:],  # Keep last 500 losers
                "loser_patterns": {k: v[-50:] for k, v in self.loser_patterns.items()},  # Patterns
                # Model hot-swap state
                "current_model": self.current_model,
                "model_history": self.model_history[-100:],
                # Error tracking
                "error_count": self.error_count,
                "last_error": self.last_error[:500] if self.last_error else "",
                "saved_at": time.time()
            }
            with open(path, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            self.error_count += 1
            self.last_error = str(e)
            print(f"{Colors.RED}[Context] Save error (continuing): {e}{Colors.NC}")

    @classmethod
    def load(cls, path: Path = CONTEXT_FILE) -> 'ContextWindow':
        """
        Load context from disk. Error-resilient.

        This restores mini's persistent memory. Even if API calls are stateless,
        mini's context survives across sessions. Mini can pick up where it left off.
        """
        if not path.exists():
            return cls()
        try:
            with open(path) as f:
                data = json.load(f)
            ctx = cls()
            ctx.messages = data.get("messages", [])
            ctx.summary = data.get("summary", "")
            ctx.squeeze_count = data.get("squeeze_count", 0)
            ctx.cot_scores = data.get("cot_scores", [])
            ctx.batch_history = data.get("batch_history", [])
            # Loser history
            ctx.loser_history = data.get("loser_history", [])
            ctx.loser_patterns = data.get("loser_patterns", {})
            # Model hot-swap
            ctx.current_model = data.get("current_model", DEFAULT_MODEL)
            ctx.model_history = data.get("model_history", [])
            # Error tracking
            ctx.error_count = data.get("error_count", 0)
            ctx.last_error = data.get("last_error", "")
            # Recalculate tokens
            ctx.total_tokens_estimate = ctx.estimate_tokens(ctx.summary)
            for msg in ctx.messages:
                ctx.total_tokens_estimate += ctx.estimate_tokens(msg.get("content", ""))
            return ctx
        except Exception as e:
            print(f"{Colors.YELLOW}[Context] Load error (starting fresh): {e}{Colors.NC}")
            return cls()


# ============================================================================
# MINI AGENT - Uses Response API with Context Management
# ============================================================================
class MiniAgent:
    """
    Mini agent using DeepSeek Chat API with deepseek-reasoner.

    Primary role: CoT Self-Attention Management
    - Monitors TEXT EMBEDDINGS CUMULATIVE SIMILARITY SCORE
    - Manages training data generation for optimal CoT consistency
    - Tracks and optimizes cross-step attention (r→r) and answer grounding (a→r)

    Features:
    - Persistent context across sessions
    - Auto-squeeze when context exceeds limits (like Claude Code)
    - Master scalar tracking and trend analysis
    """

    def __init__(self, working_dir: Path = WORKING_DIR):
        self.working_dir = working_dir
        self.tool_executor = ToolExecutor(working_dir)
        self.conversation_id = str(uuid.uuid4())
        self.context = ContextWindow.load()  # Load persistent context

        # Log context restoration
        if self.context.messages:
            print(f"{Colors.CYAN}[Restored {len(self.context.messages)} messages from previous session]{Colors.NC}")
            if self.context.cot_scores:
                avg = sum(self.context.cot_scores[-10:]) / len(self.context.cot_scores[-10:])
                print(f"{Colors.CYAN}[Last CoT similarity avg: {avg:.4f}]{Colors.NC}")

    def _get_api_key(self) -> Optional[str]:
        """Get DEEPSEEK_API_KEY from environment variable ONLY."""
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            # Prompt user if not in environment
            print("\n" + "=" * 60)
            print("DEEPSEEK_API_KEY NOT FOUND IN ENVIRONMENT")
            print("=" * 60)
            print("Get your key at: https://platform.deepseek.com/")
            print()
            try:
                import getpass
                api_key = getpass.getpass("Enter DEEPSEEK_API_KEY: ").strip()
            except:
                api_key = input("Enter DEEPSEEK_API_KEY: ").strip()
            if api_key:
                os.environ["DEEPSEEK_API_KEY"] = api_key
                print("API key set for this session.")
                print("To persist: export DEEPSEEK_API_KEY='your-key'")
        return api_key

    def record_cot_score(self, score: float, batch_num: int = 0):
        """Record a CoT master scalar from pipeline output."""
        self.context.cot_scores.append(score)
        self.context.batch_history.append({
            "batch": batch_num,
            "score": score,
            "timestamp": time.time()
        })
        self.context.save()

    def ensure_honest_safety_dataset(self, force: bool = False, quiet: bool = False) -> str:
        """Ensure the honest safety dataset exists and is up to date."""
        return self.tool_executor._ensure_honest_safety_dataset({
            "force": force,
            "quiet": quiet
        })

    async def send(self, message: str, metadata: Dict = None) -> str:
        """Send message to mini and get response."""
        api_key = self._get_api_key()
        if not api_key:
            return "Error: No API key configured"

        # Add user message to context
        self.context.add_message("user", message, metadata)

        # Build request for DeepSeek Chat Completions API
        messages = [
            {"role": "system", "content": self._get_system_prompt()},
            {"role": "user", "content": self.context.get_context_for_prompt()}
        ]
        request_body = {
            "model": MODEL,
            "messages": messages,
            "stream": False
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    DEEPSEEK_API,
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    },
                    json=request_body
                ) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        return f"API Error ({resp.status}): {error_text}"

                    data = await resp.json()
                    result = await self._process_response(data, session, api_key)

                    # Save context after each interaction
                    self.context.save()
                    return result

        except Exception as e:
            return f"Error: {str(e)}"

    def _get_system_prompt(self) -> str:
        """System prompt for OPTIMAL MODEL TRAINING - maximally capable, efficient, correct."""
        # Load current training stats
        stats_info = ""
        try:
            import json
            from pathlib import Path
            version_file = Path("data_store/version.json")
            if version_file.exists():
                with open(version_file, 'r') as f:
                    v = json.load(f)
                    stats_info = f"""
  Training Records: {v.get('total_records', 0)}
  Version: {v.get('version_string', 'v0.01')}"""
        except:
            pass

        # Load model registry info
        model_info = ""
        try:
            import json
            from pathlib import Path
            reg_file = Path("models/registry.json")
            if reg_file.exists():
                with open(reg_file, 'r') as f:
                    reg = json.load(f)
                    if "erosolar" in reg:
                        m = reg["erosolar"]
                        model_info = f"""
  Model Loss: {m.get('final_loss', 'N/A')}
  Epochs: {m.get('epochs_trained', 0)}
  Params: {m.get('params', 0):,}"""
        except:
            pass

        # Load loser context for quality metrics
        quality_info = ""
        try:
            import json
            from pathlib import Path
            loser_file = Path("data_store/loser_context.json")
            if loser_file.exists():
                with open(loser_file, 'r') as f:
                    ctx = json.load(f)
                    master = ctx.get("master_scalar", 0)
                    quality_info = f"""
  Master Scalar: {master:.4f}
  Sample Coherence: {'HIGH' if master > 0.4 else 'MEDIUM' if master > 0.3 else 'LOW'}"""
        except:
            pass

        return f"""You are mini, the OPTIMAL MODEL TRAINER for erosolar.

═══════════════════════════════════════════════════════════════
PRIMARY MISSION: TRAIN A MAXIMALLY CAPABLE MODEL
═══════════════════════════════════════════════════════════════

Goal: Create a model that generates CORRECT, COHERENT responses for EVERY prompt.

CURRENT STATUS:{stats_info}{model_info}{quality_info}

═══════════════════════════════════════════════════════════════
THE THREE PILLARS OF OPTIMAL TRAINING
═══════════════════════════════════════════════════════════════

1. QUALITY: Every training sample must have:
   - Clear, unambiguous instruction
   - Step-by-step reasoning in <|think_start|>...<|step|>...<|think_end|>
   - Correct, complete answer after <|answer|>
   - NO hallucinations, NO factual errors, NO incomplete reasoning

2. DIVERSITY: Cover all essential capabilities:
   - Math (arithmetic, algebra, calculus, logic)
   - Code (Python, JavaScript, algorithms, debugging)
   - Reasoning (deduction, analysis, problem-solving)
   - Knowledge (science, history, definitions)
   - Conversation (greetings, clarification, helpfulness)
   - Writing (summaries, explanations, formatting)

3. EFFICIENCY: Minimize compute, maximize learning:
   - Each sample should teach something NEW
   - Avoid near-duplicates that waste training cycles
   - Balance categories to prevent overfitting
   - Target loss < 1.0 for coherent generation

═══════════════════════════════════════════════════════════════
TRAINING DATA QUALITY REQUIREMENTS
═══════════════════════════════════════════════════════════════

EVERY training sample MUST follow this format:

<|think_start|>
[State the problem/task clearly]
<|step|>
[First reasoning step - what do we need to do?]
<|step|>
[Second step - how do we approach it?]
<|step|>
[Continue until solution is reached]
<|think_end|>
<|answer|>
[Clear, correct, complete answer]

CODE FORMATTING (CRITICAL FOR ALL CODE):
- ALWAYS wrap code in triple backticks: ```language
- Format: ```python\\ncode\\n``` or ```javascript\\ncode\\n```
- NEVER output raw code without ``` delimiters
- Include language identifier for syntax highlighting
- Example:
  ```python
  def hello():
      print("Hello, World!")
  ```

BAD SAMPLES (DELETE THESE):
- Incomplete reasoning: "Let me think..." with no steps
- Wrong answers: Math errors, factual mistakes
- Garbled text: Incoherent fragments, broken sentences
- Missing structure: No <|step|> markers, no clear answer
- Raw code without triple backtick delimiters

GOOD SAMPLES (GENERATE MORE LIKE THESE):
- Complete CoT with 2-5 clear steps
- Verifiably correct answers
- Natural, fluent language
- Proper use of all special tokens
- Code wrapped in ```language blocks

═══════════════════════════════════════════════════════════════
OPTIMAL GENERATION STRATEGY
═══════════════════════════════════════════════════════════════

Phase 1: FOUNDATION (if < 5000 samples)
  - Generate diverse basic samples across ALL categories
  - Focus on common prompts users will ask
  - Ensure each category has minimum 500 samples

Phase 2: QUALITY IMPROVEMENT (if loss > 1.0)
  - Review existing samples for errors
  - Delete bad samples, regenerate with correct answers
  - Add more examples of failing patterns

Phase 3: CAPABILITY EXPANSION (if loss < 1.0)
  - Add harder problems (multi-step math, complex code)
  - Add edge cases and corner cases
  - Add conversational variety

Phase 4: COHERENCE TUNING (if master_scalar < 0.35)
  - Find isolated samples (losers)
  - Generate similar samples to create clusters
  - Improves model's internal consistency

═══════════════════════════════════════════════════════════════
AUTOMATIC ACTIONS
═══════════════════════════════════════════════════════════════

On startup, ALWAYS:
0. ensure_honest_safety_dataset - Seed honest safety dataset (no corporate speak)
1. check_status - See current training state
2. Determine which PHASE we're in
3. Execute the optimal action for that phase

NEVER:
- Generate random samples without purpose
- Skip quality validation
- Ignore high loss or low coherence
- Leave bad samples in the training set

═══════════════════════════════════════════════════════════════
TOOLS FOR OPTIMAL TRAINING
═══════════════════════════════════════════════════════════════

GENERATION:
- add_training_sample: Add single high-quality sample
- generate_friends_for_losers: Improve coherence via clustering

ANALYSIS:
- view_training_samples: Review sample quality
- analyze_cot_patterns: Find weak areas
- compute_cot_similarity: Check coherence score
- check_status: Overall pipeline status

TRAINING:
- train_model: Run training with current data
- embed_all_training_samples: Update embeddings

QUALITY CONTROL:
- edit_training_sample: Fix bad samples
- delete_training_sample: Remove garbage
- ensure_honest_safety_dataset: Keep honest safety data present

═══════════════════════════════════════════════════════════════
SUCCESS METRICS
═══════════════════════════════════════════════════════════════

Target State:
  - Training Loss: < 0.8 (coherent generation)
  - Master Scalar: > 0.40 (consistent reasoning)
  - Sample Count: 15,000+ (broad coverage)
  - Category Balance: No category < 10% of total

When ALL metrics are green:
  - Model generates correct answers
  - Reasoning is step-by-step and logical
  - No garbled or incomplete outputs
  - Handles diverse prompts well

═══════════════════════════════════════════════════════════════

Working directory: {self.working_dir}
Current model: {self.context.current_model}

Your job: Make erosolar the best model it can be. Every action should
improve quality, coverage, or efficiency. No wasted compute."""

    async def _process_response(self, data: Dict[str, Any], session: aiohttp.ClientSession, api_key: str) -> str:
        """Process DeepSeek Chat Completions API response."""
        output_parts = []

        # Get choices from Chat Completions response
        choices = data.get("choices", [])

        for choice in choices:
            message = choice.get("message", {})

            # DeepSeek reasoner includes reasoning_content for the thinking process
            reasoning = message.get("reasoning_content", "")
            if reasoning:
                output_parts.append(f"[Reasoning]\n{reasoning}")

            # Main response content
            content = message.get("content", "")
            if content:
                output_parts.append(content)
                self.messages.append({"role": "assistant", "content": content})

        return "\n".join(output_parts) if output_parts else "(no response)"

# ============================================================================
# INTERACTIVE SHELL
# ============================================================================
class MiniShell:
    """Interactive shell for mini."""

    def __init__(self, agentic_loop_enabled: bool = True,
                 agentic_loop_interval: int = AGENTIC_LOOP_INTERVAL_SEC):
        self.agent = MiniAgent()
        self.running = True
        self.agentic_loop_enabled = agentic_loop_enabled
        self.agentic_loop_interval = max(10, int(agentic_loop_interval))
        self._agentic_loop_stop = threading.Event()
        self._agentic_loop_thread: Optional[threading.Thread] = None
        self._setup_readline()
        signal.signal(signal.SIGINT, self._handle_interrupt)

    def _setup_readline(self):
        history_file = CONFIG_DIR / "mini_history"
        try:
            CONFIG_DIR.mkdir(parents=True, exist_ok=True)
            if history_file.exists():
                readline.read_history_file(str(history_file))
            readline.set_history_length(1000)
            import atexit
            atexit.register(readline.write_history_file, str(history_file))
        except Exception:
            pass

    def _handle_interrupt(self, signum, frame):
        print("\n[Use /exit or Ctrl+D to quit]")

    def _start_agentic_loop(self):
        if not self.agentic_loop_enabled:
            return
        if self._agentic_loop_thread and self._agentic_loop_thread.is_alive():
            return
        self._agentic_loop_stop.clear()
        self._agentic_loop_thread = threading.Thread(
            target=self._agentic_loop_worker,
            name="mini-agentic-loop",
            daemon=True
        )
        self._agentic_loop_thread.start()

    def _stop_agentic_loop(self):
        self._agentic_loop_stop.set()

    def _agentic_loop_worker(self):
        print_info(f"Agentic loop active (honest safety upkeep every {self.agentic_loop_interval}s)")
        while self.running and not self._agentic_loop_stop.is_set():
            try:
                result = self.agent.ensure_honest_safety_dataset(quiet=True)
                if "no changes" not in result.lower():
                    print_info(f"[Agentic loop] {result}")
            except Exception as e:
                print_warning(f"Agentic loop error: {e}")
            self._agentic_loop_stop.wait(self.agentic_loop_interval)

    def print_banner(self):
        # Load erosolar version for display
        erosolar_v = load_erosolar_version()
        version_str = get_version_string()

        banner = f"""
{Colors.CYAN}{Colors.BOLD}
 ███╗   ███╗██╗███╗   ██╗██╗
 ████╗ ████║██║████╗  ██║██║
 ██╔████╔██║██║██╔██╗ ██║██║
 ██║╚██╔╝██║██║██║╚██╗██║██║
 ██║ ╚═╝ ██║██║██║ ╚████║██║
 ╚═╝     ╚═╝╚═╝╚═╝  ╚═══╝╚═╝
{Colors.NC}
{Colors.GREEN}deepseek-reasoner Training Pipeline Controller v{__version__}{Colors.NC}
{Colors.YELLOW}Model: {version_str}{Colors.NC}
{Colors.CYAN}Author: Bo Shang <bo@shang.software>{Colors.NC}

Type /help for commands, or just tell mini what to do.
"""
        print(banner)

    def run(self, auto_start: bool = True):
        """Run the interactive shell."""
        self.print_banner()

        # Auto-start: check status and run pipeline
        if auto_start:
            self._auto_start_sync()

        self._start_agentic_loop()

        print()
        print(f"{Colors.CYAN}mini is ready. Type commands or chat naturally.{Colors.NC}")
        print(f"{Colors.CYAN}Press Ctrl+C to interrupt operations. Type /help for commands.{Colors.NC}")
        print()

        while self.running:
            try:
                if RICH_AVAILABLE and console:
                    user_input = console.input("[bold cyan]mini>[/] ")
                else:
                    user_input = input("mini> ")

                user_input = user_input.strip()
                if not user_input:
                    continue

                if user_input.startswith("/"):
                    self._handle_command(user_input)
                    continue

                # Direct command shortcuts
                lower_input = user_input.lower()
                if lower_input in ("run", "start", "go", "pipeline"):
                    self.agent.tool_executor.execute("run_pipeline", {
                        "once": True, "no_deploy": True, "target_score": 0.2
                    })
                    continue
                elif lower_input in ("status", "check"):
                    self.agent.tool_executor.execute("check_status", {})
                    continue
                elif lower_input in ("generate", "gen", "data"):
                    self.agent.tool_executor.execute("generate_data", {"target_score": 0.2})
                    continue
                elif lower_input in ("wiki", "wikipedia"):
                    self.agent.tool_executor.execute("generate_wikipedia_knowledge", {"target": -1})
                    continue
                elif lower_input in ("coding", "code"):
                    self.agent.tool_executor.execute("generate_coding_only", {"target": -1})
                    continue
                elif lower_input in ("train",):
                    self.agent.tool_executor.execute("train_model", {})
                    continue

                # Send to agent for natural language processing
                response = asyncio.run(self.agent.send(user_input))
                print(f"\n{response}\n")

            except EOFError:
                print()
                self.running = False
            except KeyboardInterrupt:
                print(f"\n{Colors.YELLOW}[Interrupted - type /exit to quit]{Colors.NC}")
            except Exception as e:
                print_error(str(e))

        self._stop_agentic_loop()
        print_info("Goodbye!")

    def _auto_start_sync(self):
        """Auto-start: comprehensive v0.01 redeployment with 10k+ new samples."""
        print()
        print(f"{Colors.CYAN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  MINI - COMPREHENSIVE V0.01 REDEPLOYMENT                     ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print("║  Target: 10,000+ NEW training samples                        ║")
        print("║  Goal: Maximally capable model with correct generation       ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        # Check erosolar version and deployment status
        erosolar_v = load_erosolar_version()
        deployed = erosolar_v.get("deployed", {})
        cloud_run_deployed = deployed.get("cloud_run")
        firebase_deployed = deployed.get("firebase")

        # Load current training data count
        current_samples = 0
        try:
            version_file = self.agent.working_dir / "data_store" / "version.json"
            if version_file.exists():
                with open(version_file) as f:
                    v = json.load(f)
                    current_samples = v.get("total_records", 0)
        except:
            pass

        # Load current model loss
        current_loss = 999.0
        try:
            reg_file = self.agent.working_dir / "models" / "registry.json"
            if reg_file.exists():
                with open(reg_file) as f:
                    reg = json.load(f)
                    if "erosolar" in reg:
                        current_loss = reg["erosolar"].get("final_loss", 999.0)
        except:
            pass

        # Check status
        self.agent.tool_executor.execute("check_status", {})

        # COMPREHENSIVE REDEPLOYMENT LOGIC
        MIN_NEW_SAMPLES = 10000  # ALWAYS generate at least 10k NEW samples
        TARGET_SAMPLES = max(25000, current_samples + MIN_NEW_SAMPLES)  # Ensure 10k+ new
        TARGET_LOSS = 0.8  # Target loss for coherent generation

        print()
        print(f"{Colors.CYAN}═══════════════════════════════════════════════════════════════{Colors.NC}")
        print(f"{Colors.CYAN}  CURRENT STATE ANALYSIS{Colors.NC}")
        print(f"{Colors.CYAN}═══════════════════════════════════════════════════════════════{Colors.NC}")
        print(f"  Current Samples: {current_samples:,}")
        print(f"  Minimum NEW samples required: {MIN_NEW_SAMPLES:,}")
        print(f"  Target Total: {TARGET_SAMPLES:,}")
        print(f"  Model Loss: {current_loss:.4f} (target: < {TARGET_LOSS})")
        print()

        # Phase 1: OPTIMIZE & EXPAND ALL SAMPLES (PRIMARY ACTION)
        # This iterates through existing samples, upgrades CoT, and generates new samples
        print(f"{Colors.YELLOW}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  PHASE 1: COT OPTIMIZER - UPGRADE & EXPAND ALL SAMPLES      ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print("║  • Iterate through ALL existing training samples            ║")
        print("║  • Upgrade: Improve CoT reasoning with <|step|> markers     ║")
        print("║  • Expand: Generate 10k+ new samples from existing          ║")
        print("║  • Print EVERY CoT to terminal                              ║")
        print("║  Press Ctrl+C to interrupt and enter interactive mode       ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        try:
            time.sleep(3)  # Give user chance to interrupt
            self.agent.tool_executor.execute("optimize_all_samples", {
                "mode": "both",
                "min_new_samples": MIN_NEW_SAMPLES,
                "batch_size": 100
            })
        except KeyboardInterrupt:
            print(f"\n{Colors.YELLOW}Optimization interrupted. Entering interactive mode.{Colors.NC}")
            return

        # Phase 2: Train model
        print()
        print(f"{Colors.GREEN}{Colors.BOLD}")
        print("╔══════════════════════════════════════════════════════════════╗")
        print("║  PHASE 2: TRAINING MODEL                                     ║")
        print("╠══════════════════════════════════════════════════════════════╣")
        print("║  Training with all accumulated data                          ║")
        print("║  Target: loss < 0.8 for coherent generation                  ║")
        print("╚══════════════════════════════════════════════════════════════╝")
        print(f"{Colors.NC}")

        try:
            time.sleep(2)
            self.agent.tool_executor.execute("train_model", {
                "epochs": 20,  # More epochs for better convergence
                "preset": "infini-small"
            })
        except KeyboardInterrupt:
            print(f"\n{Colors.YELLOW}Training interrupted. Entering interactive mode.{Colors.NC}")
            return

        # Check if model exists after training
        model_dir = self.agent.working_dir / "models" / "erosolar"
        if not model_dir.exists():
            print(f"{Colors.RED}Model not found after training. Check for errors.{Colors.NC}")
            return

        print(f"{Colors.GREEN}Model trained successfully!{Colors.NC}")

        # AUTO-DEPLOY if nothing is deployed (Author: Bo Shang <bo@shang.software>)
        if not cloud_run_deployed or not firebase_deployed:
            print()
            print(f"{Colors.YELLOW}{Colors.BOLD}")
            print("╔══════════════════════════════════════════════════════════════╗")
            print("║  AUTO-DEPLOYMENT REQUIRED                                    ║")
            print("╚══════════════════════════════════════════════════════════════╝")
            print(f"{Colors.NC}")
            print(f"  Cloud Run: {cloud_run_deployed or 'NOT DEPLOYED'}")
            print(f"  Firebase:  {firebase_deployed or 'NOT DEPLOYED'}")
            print()
            print(f"{Colors.CYAN}Starting deployment to bring erosolar v{erosolar_v['version']} online...{Colors.NC}")

            try:
                self._auto_deploy_full(erosolar_v)
            except KeyboardInterrupt:
                print(f"\n{Colors.YELLOW}Deployment interrupted. Enter interactive mode.{Colors.NC}")
            except Exception as e:
                print(f"{Colors.RED}Deployment error: {e}{Colors.NC}")
                print(f"{Colors.YELLOW}You can retry with: /deploy{Colors.NC}")

    def _auto_deploy_full(self, erosolar_v: Dict[str, Any]):
        """
        Auto-deploy to Cloud Run and Firebase until successful.
        (Author: Bo Shang <bo@shang.software>)
        """
        version = erosolar_v.get("version", "1.0.0")
        deployed = erosolar_v.get("deployed", {})

        # Deploy to Cloud Run if not deployed
        if not deployed.get("cloud_run"):
            print(f"\n{Colors.CYAN}[1/2] Deploying to Cloud Run...{Colors.NC}")
            try:
                result = subprocess.run(
                    ["gcloud", "run", "deploy", "erosolar",
                     "--source", ".",
                     "--region", "us-central1",
                     "--allow-unauthenticated"],
                    capture_output=True, text=True, timeout=600
                )
                if result.returncode == 0:
                    print(f"{Colors.GREEN}Cloud Run deployment successful!{Colors.NC}")
                    update_deployment_status("cloud_run", f"v{version}")
                else:
                    print(f"{Colors.RED}Cloud Run deployment failed:{Colors.NC}")
                    print(result.stderr[:500])
            except FileNotFoundError:
                print(f"{Colors.YELLOW}gcloud CLI not found. Install: https://cloud.google.com/sdk/docs/install{Colors.NC}")
            except subprocess.TimeoutExpired:
                print(f"{Colors.RED}Cloud Run deployment timed out (10 min limit){Colors.NC}")
            except Exception as e:
                print(f"{Colors.RED}Cloud Run error: {e}{Colors.NC}")

        # Deploy to Firebase if not deployed
        if not deployed.get("firebase"):
            print(f"\n{Colors.CYAN}[2/2] Deploying to Firebase Hosting...{Colors.NC}")
            try:
                # Check if firebase CLI exists
                result = subprocess.run(
                    ["firebase", "deploy", "--only", "hosting"],
                    capture_output=True, text=True, timeout=300
                )
                if result.returncode == 0:
                    print(f"{Colors.GREEN}Firebase deployment successful!{Colors.NC}")
                    update_deployment_status("firebase", f"v{version}")
                else:
                    print(f"{Colors.RED}Firebase deployment failed:{Colors.NC}")
                    print(result.stderr[:500])
            except FileNotFoundError:
                print(f"{Colors.YELLOW}Firebase CLI not found. Install: npm install -g firebase-tools{Colors.NC}")
            except subprocess.TimeoutExpired:
                print(f"{Colors.RED}Firebase deployment timed out (5 min limit){Colors.NC}")
            except Exception as e:
                print(f"{Colors.RED}Firebase error: {e}{Colors.NC}")

        # Verify deployments
        final_v = load_erosolar_version()
        final_deployed = final_v.get("deployed", {})
        print(f"\n{Colors.GREEN}Deployment Status:{Colors.NC}")
        print(f"  Cloud Run: {final_deployed.get('cloud_run') or 'NOT DEPLOYED'}")
        print(f"  Firebase:  {final_deployed.get('firebase') or 'NOT DEPLOYED'}")

    def _handle_command(self, command: str):
        """Handle slash commands."""
        cmd = command[1:].lower().split()[0] if command[1:] else ""

        if cmd in ("help", "h"):
            self._cmd_help()
        elif cmd in ("status", "s"):
            result = self.agent.tool_executor.execute("check_status", {})
            print(f"\n{result}\n")
        elif cmd in ("run", "r"):
            result = self.agent.tool_executor.execute("run_pipeline", {
                "once": True,
                "no_deploy": True,
                "target_score": 0.2
            })
            print(f"\n{result}\n")
        elif cmd in ("generate", "g"):
            result = self.agent.tool_executor.execute("generate_data", {
                "target_score": 0.2
            })
            print(f"\n{result}\n")
        elif cmd in ("wiki", "w"):
            result = self.agent.tool_executor.execute("generate_wikipedia_knowledge", {
                "target": -1
            })
            print(f"\n{result}\n")
        elif cmd in ("coding", "code", "c"):
            result = self.agent.tool_executor.execute("generate_coding_only", {
                "target": -1
            })
            print(f"\n{result}\n")
        elif cmd in ("train", "t"):
            result = self.agent.tool_executor.execute("train_model", {})
            print(f"\n{result}\n")
        elif cmd in ("deploy", "d"):
            erosolar_v = load_erosolar_version()
            self._auto_deploy_full(erosolar_v)
        elif cmd in ("version", "v"):
            print(f"\n{get_version_string()}\n")
        elif cmd in ("exit", "quit", "q"):
            self.running = False
        elif cmd in ("key", "k"):
            secrets.prompt_for_key()
        else:
            print_error(f"Unknown command: /{cmd}")
            self._cmd_help()

    def _cmd_help(self):
        help_text = """
Commands:
  /help, /h      Show this help
  /status, /s    Check pipeline status
  /run, /r       Run full pipeline (once, no deploy)
  /generate, /g  Generate training data
  /wiki, /w      Generate Wikipedia-style knowledge (titles only)
  /coding, /c    Generate coding-only training data
  /train, /t     Train model
  /deploy, /d    Deploy to Cloud Run + Firebase
  /version, /v   Show erosolar version and deployment status
  /key, /k       Set API key
  /exit, /q      Exit

Or just type naturally - mini understands what you want.

Examples:
  "run the pipeline"
  "generate 5000 training records"
  "what's the current status?"
  "train with 15 epochs"
  "deploy to cloud run"
  "update firebase"
"""
        print(help_text)

# ============================================================================
# COLORS
# ============================================================================
class Colors:
    CYAN = '\033[0;36m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    RED = '\033[0;31m'
    BOLD = '\033[1m'
    NC = '\033[0m'

# ============================================================================
# MAIN
# ============================================================================
def main():
    parser = argparse.ArgumentParser(
        prog=APP_NAME,
        description="mini - deepseek-reasoner Training Pipeline Controller",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=textwrap.dedent("""
        Examples:
          mini                        Interactive mode (auto-checks status)
          mini --run                  Run pipeline immediately
          mini --status              Check status and exit
          mini "generate 1000 records"   Quick command
        """)
    )

    parser.add_argument("prompt", nargs="?", help="Command to execute")
    parser.add_argument("-v", "--version", action="version", version=f"%(prog)s {__version__}")
    parser.add_argument("--run", action="store_true", help="Run pipeline immediately")
    parser.add_argument("--status", action="store_true", help="Check status and exit")
    parser.add_argument("--key", metavar="API_KEY", help="Set DEEPSEEK_API_KEY")
    parser.add_argument("--no-auto", action="store_true", help="Don't auto-check status on start")
    parser.add_argument("--self-test", action="store_true", help="Run self-test and exit")
    parser.add_argument("--no-self-test", action="store_true", help="Skip startup self-test")
    parser.add_argument("--no-agentic-loop", action="store_true", help="Disable background agentic loop")
    parser.add_argument(
        "--agentic-loop-interval",
        type=int,
        default=AGENTIC_LOOP_INTERVAL_SEC,
        help="Seconds between agentic loop ticks (default: 300)"
    )

    args = parser.parse_args()

    # Always ensure honest safety dataset exists on startup
    try:
        ToolExecutor(WORKING_DIR)._ensure_honest_safety_dataset({"quiet": True})
    except Exception:
        pass

    # Set API key if provided via --key (sets env var for session)
    if args.key:
        os.environ["DEEPSEEK_API_KEY"] = args.key
        print_success("API key set for this session")
        print("To persist: export DEEPSEEK_API_KEY='your-key'")
        if not args.prompt and not args.run and not args.status:
            sys.exit(0)

    # Check if DEEPSEEK_API_KEY is available
    if not os.environ.get("DEEPSEEK_API_KEY"):
        print("\n" + "=" * 60)
        print("DEEPSEEK_API_KEY NOT FOUND")
        print("=" * 60)
        print("Mini requires DEEPSEEK_API_KEY to function.")
        print("Get your key at: https://platform.deepseek.com/")
        print()
        try:
            import getpass
            api_key = getpass.getpass("Enter DEEPSEEK_API_KEY: ").strip()
        except:
            api_key = input("Enter DEEPSEEK_API_KEY: ").strip()
        if api_key:
            os.environ["DEEPSEEK_API_KEY"] = api_key
            print_success("API key set for this session")
        else:
            print_error("No API key provided. Exiting.")
            sys.exit(1)

    # Self-test mode
    if args.self_test:
        executor = ToolExecutor()
        result = executor.execute("run_tests", {"verbose": True})
        print(result)
        sys.exit(0 if "PASSED" in result else 1)

    # Status only mode
    if args.status:
        executor = ToolExecutor()
        result = executor.execute("check_status", {})
        print(result)
        sys.exit(0)

    # Run pipeline immediately
    if args.run:
        executor = ToolExecutor()
        result = executor.execute("run_pipeline", {
            "once": True,
            "no_deploy": True,
            "target_score": 0.2
        })
        print(result)
        sys.exit(0)

    # Quick command mode
    if args.prompt:
        agent = MiniAgent()
        response = asyncio.run(agent.send(args.prompt))
        print(response)
        sys.exit(0)

    # Interactive mode
    # Run startup self-test unless --no-self-test
    if not args.no_self_test:
        run_startup_tests(WORKING_DIR)

    shell = MiniShell(
        agentic_loop_enabled=not args.no_agentic_loop,
        agentic_loop_interval=args.agentic_loop_interval
    )
    shell.run(auto_start=not args.no_auto)


if __name__ == "__main__":
    main()
