import io
import logging
from functools import cached_property
from typing import Any, Optional

import PIL.Image
from google.genai import types

from llmbrix.msg.base_msg import BaseMsg
from llmbrix.msg.model_msg_segment import ModelMsgSegment
from llmbrix.msg.model_msg_segment_types import ModelMsgSegmentTypes

logger = logging.getLogger(__name__)

MODEL_ROLE_NAME = "model"


class ModelMsg(BaseMsg):
    """
    LLM response message.

    Note this message internally contains multiple message segments.
    These segments can be TEXT, IMAGE, TOOL_CALL, THOUGHT or AUDIO.

    You can control what modalities are allowed in generation config.

    These parts are generated by LLM in order => it can generate [TEXT, IMAGE, TEXT]

    Use `.segments` property to get a list of segments in correct order to be rendered.
    Use `.text` to get concatenation of all text segments. Useful in cases where LLM is used just to output text.
    Use `.images` to get list of PIL images.

    Note `.text` might be empty in cases where `.tool_calls` are present (model requests tool execution and waits).

    For audio beware that both audio / text segments might be present - use `.segments` to render both in correct order.

    Code generation and outputs are not supported (even though present in Gemini API).

    In order to ensure small object size and easy "Pydantic" serialization all these properties are computed in
    lazy fashion and not registered as Pydantic object attributes.

    Due to property caching this implementation sacrifices higher memory usage for lower CPU load at attribute access.
    """

    parsed: Optional[Any] = None

    def __init__(self, parts: list[types.Part], parsed: Optional[dict] = None):
        """
        Args:
            parts: Part objects from Content object returned by Gemini API.
                   See from_text() constructor to initialize from string.
            parsed: Parsed JSON with LLM response. Use in case structured output is required,
                    the value will be stored in .parsed attribute.
        """
        super().__init__(role=MODEL_ROLE_NAME, parts=parts, parsed=parsed)

    @classmethod
    def from_text(cls, text: str):
        """
        Initialize new ModelMsg from string.
        Creates Content object with 1 "text" part.

        Args:
            text: Content of the model message.

        Returns:
            ModelMsg instance.
        """
        return cls(parts=[types.Part.from_text(text=text)])

    @cached_property
    def text(self) -> str:
        """
        Text content of message. Can be empty string if not generated by LLM (e.g. tool call is required)
        Returns: str text of message.
        """
        return "".join(s.content for s in self.get_segments_by_type(ModelMsgSegmentTypes.TEXT))

    @cached_property
    def thought(self) -> str:
        """
        Internal reasoning thoughts of the LLM. Can be empty if no thinking was done.

        Returns: str reasoning of LLM.
        """
        return "".join(s.content for s in self.get_segments_by_type(ModelMsgSegmentTypes.THOUGHT))

    @cached_property
    def tool_calls(self) -> list[types.FunctionCall]:
        """
        List of function calls.

        Returns: list of FunctionCall objects
        """
        return [s.content for s in self.get_segments_by_type(ModelMsgSegmentTypes.TOOL_CALL)]

    @cached_property
    def images(self) -> list[PIL.Image.Image]:
        """
        List of PIL images included with LLM response.

        Returns: list of PIL images.
        """
        return [PIL.Image.open(io.BytesIO(s.content)) for s in self.get_segments_by_type(ModelMsgSegmentTypes.IMAGE)]

    @cached_property
    def audio(self) -> list[tuple[io.BytesIO, str]]:
        """
        List of PIL images included with LLM response.

        Returns: list of tuples: (audio_bytes, mime_type)
        """
        return [(io.BytesIO(s.content), s.mime_type) for s in self.get_segments_by_type(ModelMsgSegmentTypes.AUDIO)]

    @cached_property
    def segments(self) -> list[ModelMsgSegment]:
        """
        List of message segments to be rendered in narrative order.

        E.g. LLM may choose to first generate text, then provide image and then explain further via another
        text segment.

        Returns: list of model message segments.
        """
        segments = []
        for part in self.parts:
            if part.thought:
                if part.text:
                    segments.append(
                        ModelMsgSegment(type=ModelMsgSegmentTypes.THOUGHT, content=part.text, mime_type="text/plain")
                    )
                else:
                    logger.warning("Received Part with thought=True with an empty text, skipping.")
            elif part.text:
                segments.append(
                    ModelMsgSegment(type=ModelMsgSegmentTypes.TEXT, content=part.text, mime_type="text/plain")
                )
            elif part.inline_data:
                mime = part.inline_data.mime_type or "application/octet-stream"
                data = part.inline_data.data
                if mime.startswith("image/"):
                    segments.append(ModelMsgSegment(type=ModelMsgSegmentTypes.IMAGE, content=data, mime_type=mime))
                elif mime.startswith("audio/"):
                    segments.append(ModelMsgSegment(type=ModelMsgSegmentTypes.AUDIO, content=data, mime_type=mime))
                else:
                    segments.append(
                        ModelMsgSegment(type=ModelMsgSegmentTypes.UNSUPPORTED_PART, content=data, mime_type=mime)
                    )
            elif part.file_data:
                segments.append(
                    ModelMsgSegment(
                        type=ModelMsgSegmentTypes.FILE_URI,
                        content=part.file_data.file_uri,
                        mime_type=part.file_data.mime_type,
                    )
                )
            elif part.executable_code:
                segments.append(
                    ModelMsgSegment(
                        type=ModelMsgSegmentTypes.CODE_EXECUTABLE,
                        content=part.executable_code.code,
                        mime_type="text/x-python",
                    )
                )
            elif part.code_execution_result:
                segments.append(
                    ModelMsgSegment(
                        type=ModelMsgSegmentTypes.CODE_RESULT,
                        content=part.code_execution_result.output,
                        mime_type="text/plain",
                    )
                )
            elif part.function_call:
                segments.append(
                    ModelMsgSegment(type=ModelMsgSegmentTypes.TOOL_CALL, content=part.function_call, mime_type=None)
                )
            else:
                part_data = part.model_dump(exclude_none=True)
                logger.warning("Received completely unknown Part type.")
                segments.append(
                    ModelMsgSegment(
                        type=ModelMsgSegmentTypes.UNSUPPORTED_PART, content=part_data, mime_type="application/json"
                    )
                )
        return segments

    def get_segments_by_type(self, segment_type: ModelMsgSegmentTypes) -> list[ModelMsgSegment]:
        """
        Get list of segments of given type.

        Args:
            segment_type: Type of segments to return.

        Returns: list of ModelMsgSegment objects.
        """
        return [s for s in self.segments if s.type is segment_type]

    def __repr__(self) -> str:
        """
        String representation of model message.

        Returns: str with information of number of segments of each type present in this message.

        """
        segment_counts = {}
        for s in self.segments:
            segment_counts[s.type] = segment_counts.get(s.type, 0) + 1
        counts_str = ", ".join([f"{t.name}={c}" for t, c in segment_counts.items()])
        return f"ModelMsg(segments=[{counts_str}])"
