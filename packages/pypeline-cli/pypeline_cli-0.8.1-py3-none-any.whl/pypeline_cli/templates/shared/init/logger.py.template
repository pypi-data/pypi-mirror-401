"""
Logger Singleton for Pipeline Logging

⚠️ DO NOT MODIFY THIS FILE DIRECTLY ⚠️

This file is auto-generated by pypeline-cli and provides a custom logging solution
optimized for Snowflake environments. Manual modifications may be overwritten during
CLI updates or regeneration.

Purpose:
    Provides a Logger class with color-coded log levels designed specifically for
    Snowflake and Databricks environments. Uses print statements instead of standard
    logging since Snowflake notebooks don't have traditional stdout streams.

Usage:
    from pipelines.utils.logger import Logger

    # Static method usage (most common)
    Logger.info("Processing started", context="MyPipeline")
    Logger.debug("Row count: 1234", context="MyPipeline.extract", row_count=1234)
    Logger.warning("Missing data detected", context="MyPipeline.validate")
    Logger.error("Failed to load table", context="MyPipeline.load", error="Timeout")
    Logger.critical("Pipeline aborted", context="MyPipeline")

    # Instance usage
    logger = Logger()
    logger.info("Processing complete", context="MyPipeline")

    # Control log level
    Logger.min_log_level = LogLevel.INFO  # Only show INFO and above

Design Pattern:
    - Static Methods: Use Logger.info(), Logger.error(), etc. directly
    - Color-Coded Output: Different colors for each log level (DEBUG, INFO, WARN, ERROR, CRITICAL)
    - Context Tracking: context parameter helps identify which module/function logged
    - Structured Logging: Additional key-value pairs via **kwargs
    - Snowflake Optimized: Uses print() for compatibility with Snowflake notebooks

Log Levels:
    - DEBUG (10): Detailed diagnostic information
    - INFO (20): General informational messages
    - WARN (30): Warning messages for potentially problematic situations
    - ERROR (40): Error messages for failures that don't stop execution
    - CRITICAL (50): Critical errors that may stop execution

For Custom Logging:
    Use this Logger throughout your pipeline rather than print() or the standard
    logging module. Do not modify this file directly.
"""

import datetime
from .types import LogLevel

class Logger:
    """
    Custom logger optimized for Snowflake and Databricks environments.

    Uses print() statements for output compatibility with Snowflake notebooks,
    which don't provide traditional stdout/stderr streams. Provides static
    methods for convenient logging throughout your pipeline.

    Class Attributes:
        use_colors (bool): Enable/disable ANSI color codes in output (default: True)
        min_log_level (LogLevel): Minimum severity to display (default: DEBUG)

    Methods:
        log(level, message, context, **kwargs): Core logging method
        info(message, context, **kwargs): Log at INFO level
        debug(message, context, **kwargs): Log at DEBUG level
        warning(message, context, **kwargs): Log at WARN level
        error(message, context, **kwargs): Log at ERROR level
        critical(message, context, **kwargs): Log at CRITICAL level

    Examples:
        >>> # Basic logging
        >>> Logger.info("Pipeline started")
        2025-03-15 14:30:00 | INFO | snowflake_module | Pipeline started
        >>>
        >>> # With context
        >>> Logger.info("Processing data", context="MyPipeline.extract")
        2025-03-15 14:30:05 | INFO | MyPipeline.extract | Processing data
        >>>
        >>> # With additional metadata
        >>> Logger.debug("Query results", context="DataLoader", row_count=1234, elapsed=5.2)
        2025-03-15 14:30:10 | DEBUG | DataLoader | Query results | row_count=1234 | elapsed=5.2
        >>>
        >>> # Control minimum level
        >>> Logger.min_log_level = LogLevel.INFO
        >>> Logger.debug("This won't appear")  # Skipped, below threshold
        >>> Logger.info("This will appear")
        2025-03-15 14:30:15 | INFO | snowflake_module | This will appear

    Note:
        This class is auto-generated by pypeline-cli. Do not modify directly.
    """

    # env = detect_environment(verbose=False)
    # use_colors = (
    #     True
    #     if (env == Environment.DATABRICKS or env != Environment.SNOWFLAKE)
    #     else False
    # )

    use_colors = True

    min_log_level = LogLevel.DEBUG

    @staticmethod
    def log(level: LogLevel, message, context=None, **kwargs):
        """
        Log a message at the specified log level.

        Core logging method that formats and prints log messages with timestamps,
        log levels, context, and optional metadata. Respects the min_log_level setting.

        Args:
            level (LogLevel): The severity level for this log message
            message (str): The primary log message
            context (str, optional): Module/function name for identifying log source.
                Defaults to "snowflake_module" if not provided.
            **kwargs: Additional key-value pairs to append to the log entry

        Examples:
            >>> Logger.log(LogLevel.INFO, "Processing started", context="MyPipeline")
            2025-03-15 14:30:00 | INFO | MyPipeline | Processing started
            >>>
            >>> Logger.log(LogLevel.ERROR, "Failed to load", context="DataLoader",
            ...            table="users", error="Timeout")
            2025-03-15 14:30:05 | ERROR | DataLoader | Failed to load | table=users | error=Timeout

        Note:
            Messages below min_log_level are silently ignored. Use the convenience
            methods (info, debug, warning, error, critical) instead of calling this directly.
        """
        if Logger.min_log_level.numeric_level > level.numeric_level:
            return

        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # module_name = context or (
        #     "databricks_module"
        #     if detect_environment(verbose=False) == "databricks"
        #     else (
        #         "snowflake_module"
        #         if detect_environment(verbose=False) == "snowflake"
        #         else "unknown module"
        #     )
        # )

        module_name = context or "snowflake_module"

        if Logger.use_colors:
            log_line = f"{timestamp} | {level.color_code}{level.label}{level.reset_code} | {module_name} | {message}"
        else:
            log_line = f"{timestamp} | {level.label} | {module_name} | {message}"

        if kwargs:
            kv_pairs = " | ".join([f"{k}={v}" for k, v in kwargs.items()])
            log_line += f" | {kv_pairs}"

        print(log_line)

    @staticmethod
    def info(message, context=None, **kwargs):
        """
        Log an informational message (severity: 20).

        Use for general informational messages about normal pipeline operations.

        Args:
            message (str): The message to log
            context (str, optional): Module/function name for identifying log source
            **kwargs: Additional key-value pairs to append to the log entry

        Example:
            >>> Logger.info("Pipeline execution started", context="MyPipeline")
            2025-03-15 14:30:00 | INFO | MyPipeline | Pipeline execution started
        """
        Logger.log(LogLevel.INFO, message, context, **kwargs)

    @staticmethod
    def error(message, context=None, **kwargs):
        """
        Log an error message (severity: 40).

        Use for errors that don't stop pipeline execution but indicate failures.

        Args:
            message (str): The message to log
            context (str, optional): Module/function name for identifying log source
            **kwargs: Additional key-value pairs to append to the log entry

        Example:
            >>> Logger.error("Failed to process row", context="Validator", row_id=123)
            2025-03-15 14:30:00 | ERROR | Validator | Failed to process row | row_id=123
        """
        Logger.log(LogLevel.ERROR, message, context, **kwargs)

    @staticmethod
    def warning(message, context=None, **kwargs):
        """
        Log a warning message (severity: 30).

        Use for potentially problematic situations that don't prevent execution.

        Args:
            message (str): The message to log
            context (str, optional): Module/function name for identifying log source
            **kwargs: Additional key-value pairs to append to the log entry

        Example:
            >>> Logger.warning("Missing optional field", context="Parser", field="phone")
            2025-03-15 14:30:00 | WARN | Parser | Missing optional field | field=phone
        """
        Logger.log(LogLevel.WARN, message, context, **kwargs)

    @staticmethod
    def debug(message, context=None, **kwargs):
        """
        Log a debug message (severity: 10).

        Use for detailed diagnostic information during development and troubleshooting.

        Args:
            message (str): The message to log
            context (str, optional): Module/function name for identifying log source
            **kwargs: Additional key-value pairs to append to the log entry

        Example:
            >>> Logger.debug("Query executed", context="Extractor", rows=1234, elapsed=2.5)
            2025-03-15 14:30:00 | DEBUG | Extractor | Query executed | rows=1234 | elapsed=2.5
        """
        Logger.log(LogLevel.DEBUG, message, context, **kwargs)

    @staticmethod
    def critical(message, context=None, **kwargs):
        """
        Log a critical error message (severity: 50).

        Use for severe errors that may cause pipeline termination or data corruption.

        Args:
            message (str): The message to log
            context (str, optional): Module/function name for identifying log source
            **kwargs: Additional key-value pairs to append to the log entry

        Example:
            >>> Logger.critical("Database connection lost", context="ETL", attempts=3)
            2025-03-15 14:30:00 | CRITICAL | ETL | Database connection lost | attempts=3
        """
        Logger.log(LogLevel.CRITICAL, message, context, **kwargs)
