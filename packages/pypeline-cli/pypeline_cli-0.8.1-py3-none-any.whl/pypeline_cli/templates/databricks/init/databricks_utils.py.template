"""
Auto-generated Databricks utility helpers.

⚙️ Framework File - Do Not Modify

This file is generated by pypeline-cli and provides convenience helpers
for interacting with Databricks via PySpark sessions and Unity Catalog.
Manual edits may be overwritten by the CLI when templates are regenerated.

The functions below include example usage in their docstrings.

Example:
    from pyspark.sql import SparkSession
    from pipelines.utils.databricks_utils import get_table_last_modified

    >>> result = get_table_last_modified(
    ...     session,
    ...     catalog_name="my_catalog",
    ...     schema_name="my_schema",
    ...     table_name="my_table",
    ... )
    >>> print(result["iso"])  # ISO timestamp in America/New_York
"""

from datetime import datetime
from typing import cast
from zoneinfo import ZoneInfo

from pyspark.sql import SparkSession

from .logger import Logger
from .types import (
    TableConfig,
    TimestampResult,
    ParsedTablePath
)

def get_table_last_modified(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> TimestampResult:
    """
    Return the last-altered timestamp for a table as Eastern time.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig` (preferred, supports templating).
        year: Optional year used by `TableConfig` when resolving path.
        catalog_name: Direct catalog name (alternative to `config`).
        schema_name: Direct schema name (alternative to `config`).
        table_name: Direct table name (alternative to `config`).

    Returns:
        A `TimestampResult` dict with keys:
            - `dt`: timezone-aware `datetime` in America/New_York
            - `iso`: ISO 8601 string representation

    Example:
        # Direct path
        res = get_table_last_modified(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        print(res["iso"])  # '2025-01-01T12:34:56-05:00'

        >>> res = get_table_last_modified(session, config=my_table_config, year=2025)
    """
    # Parse table path using helper
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    # Query Unity Catalog INFORMATION_SCHEMA
    query = f"""
        SELECT table_catalog, table_schema, table_name, last_altered
        FROM system.information_schema.tables
        WHERE table_catalog = '{path.database}'
          AND table_schema = '{path.schema}'
          AND table_name = '{path.table}'
    """

    result = session.sql(query).collect()

    if not result:
        raise ValueError(f"Table not found: {path.full_name}")

    # Get timestamp and convert to Eastern timezone
    last_updated = result[0]["last_altered"]

    # Handle both timestamp and datetime types
    if isinstance(last_updated, datetime):
        utc_time = last_updated
    else:
        # Convert to datetime if needed
        utc_time = cast(datetime, last_updated)

    eastern_tz = ZoneInfo("America/New_York")
    dt_eastern = utc_time.astimezone(eastern_tz)

    iso_string = dt_eastern.isoformat()

    return {
        "dt": dt_eastern,
        "iso": iso_string
    }

def parse_table_path(
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> ParsedTablePath:
    """
    Resolve a table path from either a `TableConfig` or explicit components.

    For Databricks, the 'database' field in ParsedTablePath maps to Unity Catalog name.

    Args:
        config: Optional `TableConfig` which can generate a parsed path.
        year: Optional year to pass to `TableConfig.generate_parsed_table_path()`.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `ParsedTablePath` with `database`, `schema`, and `table` attributes.
        Note: `database` field contains the Unity Catalog name.

    Raises:
        `ValueError` when neither a full explicit path nor a `config` is provided.

    Example:
        # From explicit parts
        path = parse_table_path(catalog_name="my_catalog", schema_name="my_schema", table_name="my_table")

        # From config
        path = parse_table_path(config=my_table_config, year=2025)
    Example:
        >>> # From explicit parts
        >>> path = parse_table_path(catalog_name="my_catalog", schema_name="my_schema", table_name="my_table")

        >>> # From config
        >>> path = parse_table_path(config=my_table_config, year=2025)
    """
    if catalog_name and schema_name and table_name:
        return ParsedTablePath(
            database=catalog_name,
            schema=schema_name,
            table=table_name
        )

    elif config and year:
        data = config.generate_parsed_table_path(year)
        return ParsedTablePath(
            database=data["data"]["database"],
            schema=data["data"]["schema"],
            table=data["data"]["table"]
        )

    elif config:
        data = config.generate_parsed_table_path(year=None)
        return ParsedTablePath(
            database=data["data"]["database"],
            schema=data["data"]["schema"],
            table=data["data"]["table"]
        )

    else:
        raise ValueError(
            "You must provide either (catalog, schema, table) or config (with optional year)"
        )


def check_table_exists(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> bool:
    """
    Return True if the table exists and False on any error or lack of permission.

    This helper intentionally treats permission errors and other failures as
    a non-existent result (False) to provide a simple boolean check. For
    more detailed diagnostics, inspect exceptions or implement a separate
    check routine.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig`.
        year: Optional year for config resolution.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `True` if table exists and `False` otherwise (including on errors).

    Example:
        exists = check_table_exists(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        if exists:
    Example:
        >>> exists = check_table_exists(
        ...     session,
        ...     catalog_name="my_catalog",
        ...     schema_name="my_schema",
        ...     table_name="my_table",
        ... )
        >>> if exists:
        ...     print("Table exists")
    """
    # Parse table path using helper
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    try:
        # Use Unity Catalog's tableExists() method
        return session.catalog.tableExists(path.full_name)
    except Exception as e:
        error_msg = str(e).lower()
        Logger.error(f"Error checking table existence for {path.full_name}: {error_msg}")
        return False


def check_table_read_access(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None
) -> bool:
    """
    Attempt a minimal read from the table to verify read permissions.

    Returns False if the table does not exist, is inaccessible, or any other
    error occurs while attempting to read a single row.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig`.
        year: Optional year for config resolution.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `True` when a simple `SELECT 1 FROM <table> LIMIT 1` succeeds,
        otherwise `False`.

    Example:
        can_read = check_table_read_access(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        if can_read:
    Example:
        >>> can_read = check_table_read_access(
        ...     session,
        ...     catalog_name="my_catalog",
        ...     schema_name="my_schema",
        ...     table_name="my_table",
        ... )
        >>> if can_read:
        ...     print("Read access OK")
        ... else:
        ...     print("No read access or table missing")
    """
    # Check existence first
    if not check_table_exists(session, config, year, catalog_name, schema_name, table_name):
        return False

    # Parse table path for read check
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    try:
        # Try to read one row
        session.sql(f"SELECT 1 FROM {path.full_name} LIMIT 1").collect()
        Logger.debug(f"Successfully read from {path.full_name}")
        return True
    except Exception as e:
        Logger.warning(f"Cannot read from {path.full_name}: {str(e)}")
        return False


# ============================================================================
# SNOWFLAKE CONNECTIVITY
# ============================================================================
# These utilities enable querying Snowflake tables from within Databricks.
# Requires credentials.py to be configured with Snowflake connection details.
# ============================================================================

def get_snowflake_connection_options() -> dict:
    """
    Get Snowflake connection options for Spark DataFrame reader.

    Loads credentials from credentials.py and returns options dict
    compatible with Spark's Snowflake connector.

    Returns:
        dict: Snowflake connection options for spark.read.format("snowflake")

    Raises:
        ImportError: If credentials.py is not found or not configured

    Example:
        >>> options = get_snowflake_connection_options()
        >>> df = spark.read.format("snowflake").options(**options).option("query", "SELECT * FROM my_table").load()
    """
    try:
        from credentials import (
            SNOWFLAKE_ACCOUNT,
            SNOWFLAKE_USER,
            SNOWFLAKE_PASSWORD,
            SNOWFLAKE_WAREHOUSE,
            SNOWFLAKE_DATABASE,
            SNOWFLAKE_SCHEMA,
        )
    except ImportError as e:
        raise ImportError(
            "Snowflake credentials not found. "
            "Copy credentials.py.example to credentials.py and configure your Snowflake connection."
        ) from e

    return {
        "sfURL": f"{SNOWFLAKE_ACCOUNT}.snowflakecomputing.com",
        "sfAccount": SNOWFLAKE_ACCOUNT,
        "sfUser": SNOWFLAKE_USER,
        "sfPassword": SNOWFLAKE_PASSWORD,
        "sfDatabase": SNOWFLAKE_DATABASE,
        "sfSchema": SNOWFLAKE_SCHEMA,
        "sfWarehouse": SNOWFLAKE_WAREHOUSE,
    }


def read_snowflake_table(
    session: SparkSession,
    table_name: str,
    database: str | None = None,
    schema: str | None = None,
):
    """
    Read a Snowflake table into a Spark DataFrame.

    Uses the Snowflake Spark connector to read data from Snowflake
    into Databricks. Credentials are loaded from credentials.py.

    Args:
        session: Active PySpark SparkSession
        table_name: Name of the Snowflake table to read
        database: Optional database override (defaults to credentials.py value)
        schema: Optional schema override (defaults to credentials.py value)

    Returns:
        pyspark.sql.DataFrame: Spark DataFrame containing the Snowflake table data

    Example:
        >>> df = read_snowflake_table(session, "customers")
        >>> df.show()

        >>> # With explicit database/schema
        >>> df = read_snowflake_table(session, "orders", database="sales_db", schema="public")
    """
    options = get_snowflake_connection_options()

    # Override database/schema if provided
    if database:
        options["sfDatabase"] = database
    if schema:
        options["sfSchema"] = schema

    full_table = f"{options['sfDatabase']}.{options['sfSchema']}.{table_name}"

    Logger.info(f"Reading Snowflake table: {full_table}")

    df = (
        session.read
        .format("snowflake")
        .options(**options)
        .option("dbtable", table_name)
        .load()
    )

    Logger.info(f"Successfully loaded {df.count()} rows from Snowflake")
    return df


def read_snowflake_query(
    session: SparkSession,
    query: str,
):
    """
    Execute a SQL query against Snowflake and return results as a Spark DataFrame.

    Uses the Snowflake Spark connector to execute arbitrary SQL queries.
    Credentials are loaded from credentials.py.

    Args:
        session: Active PySpark SparkSession
        query: SQL query to execute against Snowflake

    Returns:
        pyspark.sql.DataFrame: Spark DataFrame containing the query results

    Example:
        >>> df = read_snowflake_query(session, "SELECT * FROM customers WHERE status = 'active'")
        >>> df.show()

        >>> # Complex query with joins
        >>> query = '''
        ...     SELECT c.name, o.total
        ...     FROM customers c
        ...     JOIN orders o ON c.id = o.customer_id
        ...     WHERE o.created_at > '2025-01-01'
        ... '''
        >>> df = read_snowflake_query(session, query)
    """
    options = get_snowflake_connection_options()

    Logger.info(f"Executing Snowflake query: {query[:100]}...")

    df = (
        session.read
        .format("snowflake")
        .options(**options)
        .option("query", query)
        .load()
    )

    Logger.info(f"Query returned {df.count()} rows from Snowflake")
    return df


def write_to_snowflake(
    df,
    table_name: str,
    mode: str = "overwrite",
    database: str | None = None,
    schema: str | None = None,
):
    """
    Write a Spark DataFrame to a Snowflake table.

    Uses the Snowflake Spark connector to write data from Databricks
    to Snowflake. Credentials are loaded from credentials.py.

    Args:
        df: Spark DataFrame to write
        table_name: Target Snowflake table name
        mode: Write mode - "overwrite", "append", "error", "ignore"
        database: Optional database override (defaults to credentials.py value)
        schema: Optional schema override (defaults to credentials.py value)

    Example:
        >>> # Overwrite existing table
        >>> write_to_snowflake(df, "processed_customers")

        >>> # Append to existing table
        >>> write_to_snowflake(df, "daily_events", mode="append")

        >>> # Write to specific database/schema
        >>> write_to_snowflake(df, "reports", database="analytics", schema="public")
    """
    options = get_snowflake_connection_options()

    # Override database/schema if provided
    if database:
        options["sfDatabase"] = database
    if schema:
        options["sfSchema"] = schema

    full_table = f"{options['sfDatabase']}.{options['sfSchema']}.{table_name}"

    Logger.info(f"Writing to Snowflake table: {full_table} (mode={mode})")

    (
        df.write
        .format("snowflake")
        .options(**options)
        .option("dbtable", table_name)
        .mode(mode)
        .save()
    )

    Logger.info(f"Successfully wrote data to Snowflake table: {full_table}")
