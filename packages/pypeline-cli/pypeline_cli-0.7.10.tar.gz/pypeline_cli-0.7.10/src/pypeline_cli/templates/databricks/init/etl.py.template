"""
ETL Singleton Base Class

⚠️ DO NOT MODIFY THIS FILE DIRECTLY ⚠️

This file is auto-generated by pypeline-cli and provides the core ETL singleton
pattern for Databricks Spark session management. Manual modifications may be
overwritten during CLI updates or regeneration.

Purpose:
    Provides a singleton ETL class that wraps the existing Spark session
    in Databricks notebooks. This ensures consistent access to the session
    across all pipeline components.

Usage:
    from pipelines.utils.etl import ETL

    # Get ETL instance (wraps existing Databricks session)
    etl = ETL()

    # Access Spark session
    df = etl.session.table("catalog.schema.table")

    # Use in multiple files - always returns the same instance
    etl2 = ETL()  # Same instance as etl
    assert etl is etl2  # True

Design Pattern:
    - Singleton: Ensures only one ETL instance exists
    - Wraps the existing Databricks-managed SparkSession

For Custom ETL Logic:
    Create your own classes that USE this ETL base class rather than
    modifying this file directly. See pipeline templates for examples.
"""

from pyspark.sql import SparkSession
from .logger import Logger

context = "src.pipelines.utils.etl"


class ETL:
    """
    Singleton ETL class for Spark session management.

    This class implements the Singleton pattern to provide consistent
    access to the Databricks-managed SparkSession across all pipeline components.

    Attributes:
        session (SparkSession): Active Spark session (managed by Databricks)

    Example:
        >>> etl = ETL()
        >>> df = etl.session.table("my_catalog.my_schema.my_table")
        >>> df.show()

    Note:
        This class is auto-generated by pypeline-cli. Do not modify directly.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """
        Implement Singleton pattern by controlling instance creation.

        Returns:
            ETL: The single ETL instance
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """
        Initialize ETL instance with the Databricks-managed Spark session.
        """
        if not hasattr(self, '_initialized'):
            self.session: SparkSession = SparkSession.builder.getOrCreate()
            Logger.info(
                message="ETL initialized with Databricks Spark session",
                context=context
            )
            self._initialized = True
        else:
            Logger.debug(
                message="ETL instance already initialized. Reusing existing session.",
                context=context
            )
