"""
Table Cache for DataFrame Reuse

⚠️ DO NOT MODIFY THIS FILE DIRECTLY ⚠️

This file is auto-generated by pypeline-cli and provides a caching mechanism
for Databricks DataFrames to reduce redundant table loads across processors.
Manual modifications may be overwritten during CLI updates or regeneration.

Purpose:
    Provides the TableCache class for caching loaded Databricks DataFrames across
    multiple processors in a pipeline. This reduces redundant session.table() calls
    and improves pipeline performance when multiple processors need the same source data.

Usage:
    from pipelines.utils.table_cache import TableCache
    from pipelines.utils.tables import TABLE_CONFIGS

    # Create cache instance
    cache = TableCache()

    # Preload commonly used tables
    cache.preload_tables(
        table_keys=["CCW_PTA_FACT", "CCW_MBR_DIM"],
        table_configs=TABLE_CONFIGS
    )

    # Get cached table (loads if not cached)
    df = cache.get_table("CCW_PTA_FACT", TABLE_CONFIGS)

    # Get cache statistics
    info = cache.get_cache_info()
    print(info)  # {'cached_tables': 2, 'table_keys': [...]}

    # Clear cache when done
    cache.clear_cache()

Design Pattern:
    - Singleton-Like: Pass same TableCache instance to multiple processors
    - Lazy Loading: Tables loaded on first access via get_table()
    - Preloading: Bulk load tables at pipeline start with preload_tables()
    - Skip Output Tables: Automatically skips tables marked is_output=True
    - Type-Aware: Handles STABLE, MONTHLY, and YEARLY table types

Performance Benefits:
    - Reduces query overhead for shared source tables
    - Particularly beneficial for large fact tables used across processors
    - Enables efficient fan-out processing patterns

For Custom Caching:
    Use this TableCache class throughout your pipeline rather than creating
    custom cache implementations. Do not modify this file directly.
"""
from typing import Dict, Final
from pyspark.sql import DataFrame

from .etl import ETL
from .logger import Logger
from .types import TableConfig

MODULE_NAME: Final[str] = "utils/table_cache.py"


class TableCache:
    """
    Cache for loaded Databricks DataFrames with shared access across processors.

    Manages a dictionary of cached DataFrames to avoid redundant table loads
    when multiple processors need the same source data. Supports lazy loading,
    bulk preloading, and automatic handling of different table types.

    Attributes:
        logger (Logger): Logger instance for cache operations
        etl (ETL): ETL singleton for Spark session access
        _cache (Dict[str, DataFrame]): Internal dictionary of cached DataFrames

    Examples:
        >>> from pipelines.utils.table_cache import TableCache
        >>> from pipelines.utils.tables import TABLE_CONFIGS
        >>>
        >>> # Create and use cache
        >>> cache = TableCache()
        >>> cache.preload_tables(["CCW_PTA_FACT", "CCW_MBR_DIM"], TABLE_CONFIGS)
        >>> fact_df = cache.get_table("CCW_PTA_FACT", TABLE_CONFIGS)
        >>>
        >>> # Pass to processors
        >>> processor1 = MyProcessor(cache=cache)
        >>> processor2 = AnotherProcessor(cache=cache)  # Shares same cache
        >>>
        >>> # Check cache state
        >>> info = cache.get_cache_info()
        >>> print(f"Cached {info['cached_tables']} tables")

    Note:
        This class is auto-generated by pypeline-cli. Do not modify directly.
    """

    def __init__(self):
        """
        Initialize an empty table cache.

        Creates a new cache instance with an empty internal dictionary.
        Each cache instance is independent - pass the same instance to
        multiple processors to share cached tables.

        Examples:
            >>> cache = TableCache()
            >>> info = cache.get_cache_info()
            >>> print(info['cached_tables'])
            0
        """
        self.logger = Logger()
        self.etl = ETL()
        self._cache: Dict[str, DataFrame] = {}

    def get_table(self, table_key: str, table_configs: Dict[str, TableConfig]) -> DataFrame:
        """
        Get a cached DataFrame, loading it from Databricks if not already cached.

        Provides lazy loading - the table is only queried from Databricks on
        first access. Subsequent calls return the cached DataFrame.

        Args:
            table_key (str): Key identifying the table in table_configs
            table_configs (Dict[str, TableConfig]): Mapping of keys to TableConfig objects

        Returns:
            DataFrame: Cached or newly loaded PySpark DataFrame

        Examples:
            >>> from pipelines.utils.table_cache import TableCache
            >>> from pipelines.utils.tables import TABLE_CONFIGS
            >>>
            >>> cache = TableCache()
            >>>
            >>> # First access loads from Databricks
            >>> df1 = cache.get_table("CCW_PTA_FACT", TABLE_CONFIGS)
            >>>
            >>> # Second access returns cached DataFrame
            >>> df2 = cache.get_table("CCW_PTA_FACT", TABLE_CONFIGS)
            >>> assert df1 is df2  # Same object

        Note:
            This method calls _load_table() internally if the table is not cached.
        """
        if table_key not in self._cache:
            self._load_table(table_key, table_configs)

        return self._cache[table_key]

    def preload_tables(self, table_keys: list[str], table_configs: Dict[str, TableConfig]) -> None:
        """
        Preload multiple tables into cache.

        Useful for loading commonly used tables at pipeline startup.

        Args:
            table_keys: List of keys from table_configs to preload
            table_configs: Dictionary mapping table keys to TableConfig objects

        Example:
            >>> cache = TableCache()
            >>> cache.preload_tables(["CCW_PTA_FACT", "CCW_ALL_CLM_PRFL", "CCW_MBR_DIM"], TABLE_CONFIGS)
            >>> # Or preload all non-output tables:
            >>> cache.preload_tables(
            ...     [k for k, config in TABLE_CONFIGS.items() if not config.is_output],
            ...     TABLE_CONFIGS
            ... )
        """
        pre_loaded_cache_length = len(self._cache)
        self.logger.info(
            message=f"Attempting to load {len(table_keys)} tables into cache",
            context=MODULE_NAME
        )

        for table_key in table_keys:
            if table_key not in self._cache:
                self._load_table(table_key, table_configs)

        self.logger.info(
            message=f"Preloaded {len(self._cache) - pre_loaded_cache_length} tables successfully",
            context=MODULE_NAME
        )

    def clear_cache(self) -> None:
        """
        Clear all cached tables.

        Example:
            >>> cache = TableCache()
            >>> cache.preload_tables(["CCW_PTA_FACT", "CCW_ALL_CLM_PRFL"], TABLE_CONFIGS)
            >>> cache.clear_cache()  # Removes all cached tables
        """
        self.logger.info(
            message="Clearing table cache",
            context=MODULE_NAME,
            cached_count=len(self._cache)
        )
        self._cache.clear()

    def get_cache_info(self) -> Dict[str, int | list[str]]:
        """
        Get information about the cache state.

        Returns:
            Dictionary with cache statistics and table keys

        Example:
            >>> cache = TableCache()
            >>> cache.preload_tables(["CCW_PTA_FACT", "CCW_ALL_CLM_PRFL"], TABLE_CONFIGS)
            >>> info = cache.get_cache_info()
            >>> print(info)
            {'cached_tables': 2, 'table_keys': ['CCW_PTA_FACT', 'CCW_ALL_CLM_PRFL']}
        """
        return {
            "cached_tables": len(self._cache),
            "table_keys": list(self._cache.keys())
        }

    def _load_table(self, table_key: str, table_configs: Dict[str, TableConfig], year: int | None = None) -> None:
        """
        Load a table from Databricks and add it to the cache (internal method).

        Handles table name generation based on table type (STABLE, MONTHLY, YEARLY),
        loads the DataFrame via session.table(), and stores it in the cache.
        Automatically skips output tables.

        Args:
            table_key (str): Key identifying the table in table_configs
            table_configs (Dict[str, TableConfig]): Mapping of keys to TableConfig objects
            year (int | None): Year for YEARLY table types (optional)

        Raises:
            ValueError: If table_key not found in table_configs, or if MONTHLY table
                has no month set, or if YEARLY table has no year provided
            Exception: If Databricks query fails

        Note:
            This is an internal method called by get_table() and preload_tables().
            Do not call directly - use get_table() instead.
        """
        if table_key not in table_configs.keys():
            raise ValueError(f"Table key '{table_key}' not found in table_configs")

        if table_configs[table_key].is_output:
            self.logger.info(
                message=f"Skipping {table_key} since it is an output table",
                context=MODULE_NAME
            )
            return

        table_config = table_configs[table_key]

        if table_config.type == "STABLE":
            table_name = table_config.generate_table_name()

        elif table_config.type == "MONTHLY":
            if not table_configs[table_key].month:
                raise ValueError(f"{table_key} is a monthly table, but no month was provided.")
            table_name = table_config.generate_table_name()

        elif table_config.type == "YEARLY":
            if not year:
                raise ValueError(f"{table_key} is a yearly table, but no year was provided.")
            table_name = table_config.generate_table_name(year)


        self.logger.info(
            message=f"Loading table into cache: {table_name}",
            context=MODULE_NAME,
            table_key=table_key
        )

        try:
            self._cache[table_key] = self.etl.session.table(table_name)

            self.logger.debug(
                message=f"Successfully cached table: {table_name}",
                context=MODULE_NAME,
                table_key=table_key
            )
        except Exception as e:
            self.logger.error(
                message=f"Failed to load table: {table_name}",
                context=MODULE_NAME,
                table_key=table_key,
                error=str(e)
            )
            raise
