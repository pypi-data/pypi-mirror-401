"""
ETL Singleton Base Class

⚠️ DO NOT MODIFY THIS FILE DIRECTLY ⚠️

This file is auto-generated by pypeline-cli and provides the core ETL singleton
pattern for Databricks Spark session management. Manual modifications may be
overwritten during CLI updates or regeneration.

Purpose:
    Provides a singleton ETL class that manages a single Databricks Spark session
    throughout your pipeline execution. This ensures efficient resource usage and
    prevents multiple session creation.

Usage:
    from pipelines.utils.etl import ETL

    # Get ETL instance (creates session on first call)
    etl = ETL()

    # Access Databricks Spark session
    df = etl.session.table("catalog.schema.table")

    # Use in multiple files - always returns the same instance
    etl2 = ETL()  # Same instance as etl
    assert etl is etl2  # True

Connection Methods:
    1. Workspace Session (getOrCreate) - For Databricks environments
       - Databricks notebooks
       - Databricks jobs
       - Unity Catalog enabled workspaces

    2. Credentials File (credentials.py) - For local development
       - Copy credentials.py.example to credentials.py
       - Fill in your Databricks connection details
       - File automatically excluded from builds and git

    The ETL singleton tries method 1 first, then falls back to method 2 if available.

Setup for Local Development:
    1. Copy the example file:
       cp credentials.py.example credentials.py

    2. Edit credentials.py with your Databricks details

    3. Run your pipeline - ETL will automatically use credentials

Design Pattern:
    - Singleton: Ensures only one ETL instance exists
    - Lazy Initialization: Session created only when first needed
    - Thread-safe: Single instance shared across pipeline

For Custom ETL Logic:
    Create your own classes that USE this ETL base class rather than
    modifying this file directly. See pipeline templates for examples.
"""

from databricks.connect import DatabricksSession
from .logger import Logger

context = "src.pipelines.utils.etl"

# Try to import credentials (optional for local development)
try:
    from credentials import DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID
    _credentials_available = True
except ImportError:
    DATABRICKS_HOST = None
    DATABRICKS_TOKEN = None
    DATABRICKS_CLUSTER_ID = None
    _credentials_available = False


class ETL:
    """
    Singleton ETL class for Databricks Spark session management.

    This class implements the Singleton pattern to ensure only one Spark
    session is active throughout the pipeline execution.

    Connection Strategy:
        1. First attempts to use workspace session (Databricks environment)
        2. Falls back to credentials.py if available (local development)
        3. Raises informative error if both methods fail

    Attributes:
        session (DatabricksSession): Active Databricks Spark session

    Example:
        >>> etl = ETL()
        >>> df = etl.session.table("my_catalog.my_schema.my_table")
        >>> df.show()

    Note:
        This class is auto-generated by pypeline-cli. Do not modify directly.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """
        Implement Singleton pattern by controlling instance creation.

        Returns:
            ETL: The single ETL instance
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """
        Initialize ETL instance with Databricks Spark session.

        Tries multiple connection methods in order:
        1. Workspace session - For Databricks notebooks, jobs
        2. DatabricksSession.builder with credentials - For local development with credentials.py

        Raises:
            RuntimeError: If no connection method succeeds
        """
        if not hasattr(self, '_initialized'):
            self.session: DatabricksSession = self._create_session()
            self._initialized = True
        else:
            Logger.debug(
                message="ETL instance already initialized. Reusing existing session.",
                context=context
            )

    def _create_session(self) -> DatabricksSession:
        """
        Create Databricks Spark session using available connection method.

        Returns:
            DatabricksSession: Active Databricks Spark session

        Raises:
            RuntimeError: If all connection methods fail
        """
        # Method 1: Try workspace session (Databricks environment)
        try:
            Logger.info(
                message="Attempting to connect using workspace session",
                context=context
            )
            session = (
                DatabricksSession.builder
                .getOrCreate()
            )
            Logger.info(
                message="Successfully connected using Databricks workspace session",
                context=context
            )
            return session

        except Exception as e:
            Logger.warning(
                message="Workspace session failed - not in Databricks environment",
                context=context
            )

            # Method 2: Try credentials file (local development)
            if _credentials_available and all([DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID]):
                try:
                    Logger.info(
                        message="Attempting to connect using credentials.py",
                        context=context
                    )
                    session = (
                        DatabricksSession.builder
                        .host(DATABRICKS_HOST)
                        .token(DATABRICKS_TOKEN)
                        .clusterId(DATABRICKS_CLUSTER_ID)
                        .getOrCreate()
                    )
                    Logger.info(
                        message="Successfully connected using credentials file",
                        context=context
                    )
                    return session

                except Exception as cred_error:
                    Logger.error(
                        message="Failed to connect using credentials.py",
                        context=context
                    )
                    raise RuntimeError(
                        f"Failed to create Databricks session using credentials: {cred_error}"
                    ) from cred_error
            else:
                # No credentials available
                Logger.error(
                    message="No credentials.py file found for fallback connection",
                    context=context
                )
                raise RuntimeError(
                    "Cannot create Databricks session:\n"
                    "1. Workspace session failed (not in Databricks environment)\n"
                    "2. No credentials.py file found for local development\n\n"
                    "To fix:\n"
                    "- For local development: Copy credentials.py.example to credentials.py and configure\n"
                    "- For Databricks deployment: Ensure code runs in Databricks workspace"
                ) from e

        except Exception as e:
            # Unexpected error from workspace session
            Logger.critical(
                message="Unexpected error during session creation",
                context=context
            )
            raise RuntimeError(f"Unexpected error creating Databricks session: {e}") from e
