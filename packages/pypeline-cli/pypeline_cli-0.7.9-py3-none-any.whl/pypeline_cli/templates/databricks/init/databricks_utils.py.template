"""
Auto-generated Databricks utility helpers.

⚙️ Framework File - Do Not Modify

This file is generated by pypeline-cli and provides convenience helpers
for interacting with Databricks via PySpark sessions and Unity Catalog.
Manual edits may be overwritten by the CLI when templates are regenerated.

The functions below include example usage in their docstrings.

Example:
    from pyspark.sql import SparkSession
    from pipelines.utils.databricks_utils import get_table_last_modified

    >>> result = get_table_last_modified(
    ...     session,
    ...     catalog_name="my_catalog",
    ...     schema_name="my_schema",
    ...     table_name="my_table",
    ... )
    >>> print(result["iso"])  # ISO timestamp in America/New_York
"""

from datetime import datetime
from typing import cast
from zoneinfo import ZoneInfo

from pyspark.sql import SparkSession

from .logger import Logger
from .types import (
    TableConfig,
    TimestampResult,
    ParsedTablePath
)

def get_table_last_modified(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> TimestampResult:
    """
    Return the last-altered timestamp for a table as Eastern time.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig` (preferred, supports templating).
        year: Optional year used by `TableConfig` when resolving path.
        catalog_name: Direct catalog name (alternative to `config`).
        schema_name: Direct schema name (alternative to `config`).
        table_name: Direct table name (alternative to `config`).

    Returns:
        A `TimestampResult` dict with keys:
            - `dt`: timezone-aware `datetime` in America/New_York
            - `iso`: ISO 8601 string representation

    Example:
        # Direct path
        res = get_table_last_modified(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        print(res["iso"])  # '2025-01-01T12:34:56-05:00'

        >>> res = get_table_last_modified(session, config=my_table_config, year=2025)
    """
    # Parse table path using helper
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    # Query Unity Catalog INFORMATION_SCHEMA
    query = f"""
        SELECT table_catalog, table_schema, table_name, last_altered
        FROM system.information_schema.tables
        WHERE table_catalog = '{path.database}'
          AND table_schema = '{path.schema}'
          AND table_name = '{path.table}'
    """

    result = session.sql(query).collect()

    if not result:
        raise ValueError(f"Table not found: {path.full_name}")

    # Get timestamp and convert to Eastern timezone
    last_updated = result[0]["last_altered"]

    # Handle both timestamp and datetime types
    if isinstance(last_updated, datetime):
        utc_time = last_updated
    else:
        # Convert to datetime if needed
        utc_time = cast(datetime, last_updated)

    eastern_tz = ZoneInfo("America/New_York")
    dt_eastern = utc_time.astimezone(eastern_tz)

    iso_string = dt_eastern.isoformat()

    return {
        "dt": dt_eastern,
        "iso": iso_string
    }

def parse_table_path(
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> ParsedTablePath:
    """
    Resolve a table path from either a `TableConfig` or explicit components.

    For Databricks, the 'database' field in ParsedTablePath maps to Unity Catalog name.

    Args:
        config: Optional `TableConfig` which can generate a parsed path.
        year: Optional year to pass to `TableConfig.generate_parsed_table_path()`.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `ParsedTablePath` with `database`, `schema`, and `table` attributes.
        Note: `database` field contains the Unity Catalog name.

    Raises:
        `ValueError` when neither a full explicit path nor a `config` is provided.

    Example:
        # From explicit parts
        path = parse_table_path(catalog_name="my_catalog", schema_name="my_schema", table_name="my_table")

        # From config
        path = parse_table_path(config=my_table_config, year=2025)
    Example:
        >>> # From explicit parts
        >>> path = parse_table_path(catalog_name="my_catalog", schema_name="my_schema", table_name="my_table")

        >>> # From config
        >>> path = parse_table_path(config=my_table_config, year=2025)
    """
    if catalog_name and schema_name and table_name:
        return ParsedTablePath(
            database=catalog_name,
            schema=schema_name,
            table=table_name
        )

    elif config and year:
        data = config.generate_parsed_table_path(year)
        return ParsedTablePath(
            database=data["data"]["database"],
            schema=data["data"]["schema"],
            table=data["data"]["table"]
        )

    elif config:
        data = config.generate_parsed_table_path(year=None)
        return ParsedTablePath(
            database=data["data"]["database"],
            schema=data["data"]["schema"],
            table=data["data"]["table"]
        )

    else:
        raise ValueError(
            "You must provide either (catalog, schema, table) or config (with optional year)"
        )


def check_table_exists(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None,
) -> bool:
    """
    Return True if the table exists and False on any error or lack of permission.

    This helper intentionally treats permission errors and other failures as
    a non-existent result (False) to provide a simple boolean check. For
    more detailed diagnostics, inspect exceptions or implement a separate
    check routine.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig`.
        year: Optional year for config resolution.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `True` if table exists and `False` otherwise (including on errors).

    Example:
        exists = check_table_exists(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        if exists:
    Example:
        >>> exists = check_table_exists(
        ...     session,
        ...     catalog_name="my_catalog",
        ...     schema_name="my_schema",
        ...     table_name="my_table",
        ... )
        >>> if exists:
        ...     print("Table exists")
    """
    # Parse table path using helper
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    try:
        # Use Unity Catalog's tableExists() method
        return session.catalog.tableExists(path.full_name)
    except Exception as e:
        error_msg = str(e).lower()
        Logger.error(f"Error checking table existence for {path.full_name}: {error_msg}")
        return False


def check_table_read_access(
    session: SparkSession,
    config: TableConfig | None = None,
    year: int | None = None,
    catalog_name: str | None = None,
    schema_name: str | None = None,
    table_name: str | None = None
) -> bool:
    """
    Attempt a minimal read from the table to verify read permissions.

    Returns False if the table does not exist, is inaccessible, or any other
    error occurs while attempting to read a single row.

    Args:
        session: Active PySpark `SparkSession`.
        config: Optional `TableConfig`.
        year: Optional year for config resolution.
        catalog_name: Direct catalog name.
        schema_name: Direct schema name.
        table_name: Direct table name.

    Returns:
        `True` when a simple `SELECT 1 FROM <table> LIMIT 1` succeeds,
        otherwise `False`.

    Example:
        can_read = check_table_read_access(
            session,
            catalog_name="my_catalog",
            schema_name="my_schema",
            table_name="my_table",
        )
        if can_read:
    Example:
        >>> can_read = check_table_read_access(
        ...     session,
        ...     catalog_name="my_catalog",
        ...     schema_name="my_schema",
        ...     table_name="my_table",
        ... )
        >>> if can_read:
        ...     print("Read access OK")
        ... else:
        ...     print("No read access or table missing")
    """
    # Check existence first
    if not check_table_exists(session, config, year, catalog_name, schema_name, table_name):
        return False

    # Parse table path for read check
    path = parse_table_path(config, year, catalog_name, schema_name, table_name)

    try:
        # Try to read one row
        session.sql(f"SELECT 1 FROM {path.full_name} LIMIT 1").collect()
        Logger.debug(f"Successfully read from {path.full_name}")
        return True
    except Exception as e:
        Logger.warning(f"Cannot read from {path.full_name}: {str(e)}")
        return False
