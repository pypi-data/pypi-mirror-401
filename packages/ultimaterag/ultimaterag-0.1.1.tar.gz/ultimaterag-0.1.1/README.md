<div align="center">

# ğŸš€ TUG (TheUltimateRAG)

### A Modular, Production-Ready Foundation for Next-Generation AI Applications

Build scalable, secure, and intelligent RAG (Retrieval-Augmented Generation) systems without reinventing the wheel.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0+-009688.svg?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![LangChain](https://img.shields.io/badge/LangChain-Integration-orange)](https://www.langchain.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ğŸ”— **Official Website & Documentation**  
ğŸ‘‰ https://ultimaterag.vercel.app/

[Key Features](#-key-features) â€¢
[Architecture](#-system-architecture) â€¢
[Getting Started](#-getting-started) â€¢
[Visualizer](#-rag-visualizer-gui) â€¢
[API](#-api-endpoints) â€¢
[Contributing](#-contributing)

</div>

---

## ğŸ“– What is TUG (TheUltimateRAG)?

**TUG (TheUltimateRAG)** is a **real-world, production-grade RAG framework**, not just another tutorial or demo project.

It is designed to solve common problems developers face when moving from simple prototypes to **scalable AI systems**, such as:

- Multi-user data separation
- Long-term memory handling
- Organizational knowledge sharing
- Clean, modular architecture

Whether youâ€™re building:

- A **corporate knowledge assistant**
- A **legal or research AI**
- A **personal second-brain**
- Or a **multi-tenant SaaS AI platform**

ğŸ‘‰ **TUG (TheUltimateRAG) gives you a strong, extensible backend foundation.**

For a complete walkthrough, architecture deep-dives, and usage examples,  
ğŸ“˜ **visit the official documentation:**  
**https://ultimaterag.vercel.app/**

---

## ğŸŒŸ Key Features (Explained Simply)

| Feature                             | What It Means for You                                            |
| ----------------------------------- | ---------------------------------------------------------------- |
| âš¡ **High-Performance API**         | Built with **FastAPI** for fast, async, and scalable AI services |
| ğŸ›¡ï¸ **True Multi-Tenant Isolation**  | Each userâ€™s data is fully isolated and secure                    |
| ğŸ¢ **Organization-Level Knowledge** | Share documents across teams without duplicating data            |
| ğŸ§  **Session-Aware Memory**         | Conversations retain context naturally across turns              |
| ğŸ” **Hybrid Semantic Search**       | Metadata-aware vector search with logical filters                |
| ğŸ‘ï¸ **RAG Visualizer GUI**           | Real-time visualization of retrieval, context, and generation    |

---

## ğŸ—ï¸ System Architecture (Designed for Flexibility)

The system follows a **plug-and-play architecture**.  
You can replace or extend **any core component** without breaking the rest of the system.

- Swap vector databases
- Change LLM providers
- Add custom memory logic
- Introduce agent workflows

```mermaid
graph TD
    Client[Client / Frontend] -->|HTTP / JSON| API[FastAPI Gateway]

    subgraph "Core RAG Engine"
        API --> Logic[Orchestrator]
        Logic -->|Retrieve Context| Vector[Vector Store Manager]
        Logic -->|Conversation State| Memory[Session Memory]
        Logic -->|Generate Response| LLM[LLM Service]
    end

    subgraph "Data Layer"
        Vector <-->|Embeddings| Chroma[(ChromaDB)]
        Memory <-->|Chat Logs| Cache[(In-Memory / Redis)]
    end
```

ğŸ“– **Detailed architecture explanation available at:**
ğŸ‘‰ [https://ultimaterag.vercel.app/](https://ultimaterag.vercel.app/)

---

## ğŸš€ Getting Started Quickly

### Requirements

- Python **3.10+**
- Node.js & npm (for the Visualizer UI)
- API keys (OpenAI, Anthropic, etc.)

### Installation Steps

1ï¸âƒ£ **Clone the Repository**

```bash
pip install TUG
```

# ğŸ” Environment Configuration for TheUltimateRAG

To run **TheUltimateRAG** correctly, you must create and configure a `.env` file.  
This file stores environment-specific settings such as API keys, database configs, and runtime options.

The project uses **Pydantic Settings + python-dotenv**, so all variables defined in `.env` are automatically loaded at startup.

---

## ğŸ“ Step 1: Create the `.env` File

At the **root of the project**, create a file named:

```bash
.env
```

---

## âš™ï¸ Step 2: Required & Optional Environment Variables

Below is a **complete reference** of supported environment variables, grouped by purpose.

You only need to configure the parts relevant to your setup.

---

## ğŸ§© Core Application Settings

```env
APP_NAME=TheUltimateRAG
APP_ENV=development        # development | production
DEBUG=true
```

| Variable   | Description               |
| ---------- | ------------------------- |
| `APP_NAME` | Application name          |
| `APP_ENV`  | Runtime environment       |
| `DEBUG`    | Enable/disable debug logs |

---

## ğŸ¤– LLM & Embedding Providers

```env
LLM_PROVIDER=openai        # openai | ollama | anthropic
EMBEDDING_PROVIDER=openai # openai | ollama | huggingface
MODEL_NAME=gpt-3.5-turbo
```

| Variable             | Description              |
| -------------------- | ------------------------ |
| `LLM_PROVIDER`       | LLM backend to use       |
| `EMBEDDING_PROVIDER` | Embedding model provider |
| `MODEL_NAME`         | Chat model name          |

---

## ğŸ”‘ API Keys (Required Based on Provider)

### OpenAI

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
```

### Anthropic

```env
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxx
```

> âš ï¸ **Note:**
> If `LLM_PROVIDER` or `EMBEDDING_PROVIDER` is set to `openai`,
> `OPENAI_API_KEY` **must be provided**, otherwise a warning will be shown.

---

## ğŸ§  Ollama Configuration (Local Models)

```env
OLLAMA_BASE_URL=http://localhost:11434
```

Use this only if you are running **Ollama locally**.

---

## ğŸ—‚ï¸ Vector Database Configuration

### ChromaDB (Default â€“ Local)

```env
VECTOR_DB_TYPE=chroma
VECTOR_DB_PATH=./chroma_db_data
EMBEDDING_DIMENSION=1536
```

### PostgreSQL + PGVector

```env
VECTOR_DB_TYPE=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=vector_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

| Variable              | Description                 |
| --------------------- | --------------------------- |
| `VECTOR_DB_TYPE`      | `chroma` or `postgres`      |
| `VECTOR_DB_PATH`      | Local ChromaDB storage path |
| `EMBEDDING_DIMENSION` | Vector embedding size       |

---

## ğŸ§  Memory & Conversation Storage (Redis)

```env
MEMORY_WINDOW_SIZE=10
MEMORY_WINDOW_LIMIT=10

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_USER=default
REDIS_PASSWORD=
```

The system automatically builds the Redis connection URL internally.

---

## ğŸ”„ How `.env` Is Loaded

The project uses:

- `python-dotenv`
- `pydantic-settings`

```python
load_dotenv()
settings = Settings()
```

So no manual loading is required.

---

## âœ… Minimal `.env` (Quick Start)

If you want to get started **quickly**, this is enough:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
LLM_PROVIDER=openai
EMBEDDING_PROVIDER=openai
VECTOR_DB_TYPE=chroma
```

---

ğŸ“˜ Refer to the full configuration guide here:
ğŸ‘‰ [https://theultimaterag.vercel.app/](https://theultimaterag.vercel.app/)

3ï¸âƒ£ **Run the Platform**

```bash
TUG start
```

_or_

```bash
python app.py
```

---

## ğŸ–¥ï¸ RAG Visualizer GUI

A dedicated **React-based GUI** lets you:

- Inspect retrieved documents
- Understand context flow
- Debug hallucinations
- Optimize retrieval strategies

```bash
cd rag_visualizer
npm install
npm run dev
```

---

## ğŸ“¡ API Endpoints Overview

Access live API documentation at:
ğŸ‘‰ `http://localhost:8000/docs`

### Core APIs

- **POST** `/api/v1/chat` â†’ Chat with your knowledge base
- **POST** `/api/v1/ingest` â†’ Secure document ingestion

### Agent & Advanced APIs

- **GET** `/api/v1/agent/tools`
- **POST** `/api/v1/agent/search`
- **POST** `/api/v1/agent/workflow` â†’ Self-correcting RAG pipelines

ğŸ“˜ Full API reference:
ğŸ‘‰ [https://theultimaterag.vercel.app/](https://theultimaterag.vercel.app/)

---

## ğŸ¤ Contributing

Contributions are welcome and encouraged ğŸš€

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Open a Pull Request

See `CONTRIBUTING.md` for guidelines.

## ğŸ“ Learning & Documentation

- ğŸ“˜ **Official Docs:** [https://theultimaterag.vercel.app/](https://theultimaterag.vercel.app/)
- ğŸ“š **User Manual:** `USER_MANUAL.md`
- ğŸŒ± **Git Learning Guide:** `LEARN.md`

---

<div align="center">
Built with â¤ï¸ by Matrixxboy  
Empowering real-world RAG systems
</div>
