Metadata-Version: 2.4
Name: peppr
Version: 0.12.0
Summary: It's a package for evaluation of predicted poses, right?
Project-URL: homepage, https://peppr.proximabio.com
Project-URL: repository, https://github.com/aivant/peppr
Project-URL: documentation, https://peppr.proximabio.com
Author: The pepp'r contributors
License-Expression: MIT
License-File: LICENSE.txt
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Natural Language :: English
Classifier: Operating System :: MacOS
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: POSIX :: Linux
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
Requires-Python: >=3.11
Requires-Dist: biotite>=1.2
Requires-Dist: click>=8.0.0
Requires-Dist: numpy>=1.25
Requires-Dist: pandas>=2.0.0
Requires-Dist: rdkit>=2024.09.1
Provides-Extra: docs
Requires-Dist: ipython<8.29; extra == 'docs'
Requires-Dist: jupyter-sphinx>=0.5.3; extra == 'docs'
Requires-Dist: matplotlib>=3.3; extra == 'docs'
Requires-Dist: mpire; extra == 'docs'
Requires-Dist: numpydoc>=1.8.0; extra == 'docs'
Requires-Dist: pydata-sphinx-theme==0.16.1; extra == 'docs'
Requires-Dist: sphinx-copybutton>=0.5; extra == 'docs'
Requires-Dist: sphinx-design>=0.5; extra == 'docs'
Requires-Dist: sphinx-notfound-page>=1.0; extra == 'docs'
Requires-Dist: sphinx>=7.0; extra == 'docs'
Provides-Extra: lint
Requires-Dist: mypy==1.15.0; extra == 'lint'
Requires-Dist: numpydoc==1.8.0; extra == 'lint'
Requires-Dist: ruff==0.11.0; extra == 'lint'
Provides-Extra: tests
Requires-Dist: dockq; extra == 'tests'
Requires-Dist: pytest; extra == 'tests'
Description-Content-Type: text/x-rst

pepp'r
======

    I have a structure prediction model and now I want to know how well it performs in
    reproducing the reference structures.
    But there are so many possible metrics, some for monomers, some for complexes!
    Is there a package that handles this for me?

|

Try

.. image:: https://raw.githubusercontent.com/aivant/peppr/refs/heads/main/docs/static/assets/general/logo.svg
   :alt: pepp'r

|

    It's a Package for Evaluation of Predicted Poses, Right?

|

Yes, indeed!
It allows you to compute a variety of metrics on your structure predictions
for assessing their quality.
It supports

- all *CASP*/*CAPRI* metrics and more
- small molecules to huge protein or nucleic acid complexes
- easy extension with custom metrics
- a command line interface and a Python API

Installation
------------

``peppr`` is available via *PyPI*:

.. code-block:: console

    $ pip install peppr

Usage example
-------------

Using the CLI, you can either compute a single metric for a system...

.. code-block:: console

    $ peppr run dockq reference.cif poses.cif

... or run an entire prediction model evaluation on many systems.

.. code-block:: console

    # Select the metrics you want to compute (here: RMSD and lDDT)
    $ peppr create peppr.pkl monomer-rmsd monomer-lddt

    # Run the evaluation on predicted poses and their corresponding references
    $ peppr evaluate-batch peppr.pkl "systems/*/reference.cif" "systems/*/poses"

    # Select the aggregation method over poses (here: Top-3 and Oracle) and report the results
    $ peppr tabulate peppr.pkl table.csv top3 oracle

Available metrics
-----------------

- RMSD
- TM-score
- lDDT
- lDDT-PLI
- fnat
- iRMSD
- LRMSD
- DockQ

... and more!