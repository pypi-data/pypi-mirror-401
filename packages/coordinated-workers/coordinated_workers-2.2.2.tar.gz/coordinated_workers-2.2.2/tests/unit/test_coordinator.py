import dataclasses
import json
from contextlib import ExitStack, contextmanager, nullcontext
from pathlib import Path
from typing import Any, Dict, List, Set, Type
from unittest.mock import MagicMock, PropertyMock, patch
from urllib.parse import urlparse

import ops
import pytest
from charms.catalogue_k8s.v1.catalogue import CatalogueItem
from cosl.interfaces.utils import DataValidationError
from ops import RelationChangedEvent, testing

from coordinated_workers.coordinator import (
    ClusterRolesConfig,
    Coordinator,
    S3NotFoundError,
    TLSConfig,
)
from coordinated_workers.interfaces.cluster import (
    ClusterRemovedEvent,
    ClusterRequirerAppData,
    ClusterRequirerUnitData,
)
from coordinated_workers.nginx import NginxConfig
from tests.unit.test_worker import MyCharm

MOCK_CERTS_DATA = "<TLS_STUFF>"
MOCK_TLS_CONFIG = TLSConfig(MOCK_CERTS_DATA, MOCK_CERTS_DATA, MOCK_CERTS_DATA)


@pytest.fixture(autouse=True)
def mock_policy_resource_manager():
    """Mock _get_policy_resource_manager to prevent lightkube Client instantiation in all tests."""
    with patch("coordinated_workers.service_mesh._get_policy_resource_manager") as mock_get_prm:
        # Create a mock PolicyResourceManager with the necessary methods
        mock_prm = MagicMock()
        mock_get_prm.return_value = mock_prm
        yield mock_get_prm


@pytest.fixture
def coordinator_state(nginx_container, nginx_prometheus_exporter_container):
    requires_relations = {
        endpoint: testing.Relation(endpoint=endpoint, interface=interface["interface"])
        for endpoint, interface in {
            "my-logging": {"interface": "loki_push_api"},
            "my-charm-tracing": {"interface": "tracing"},
            "my-workload-tracing": {"interface": "tracing"},
        }.items()
    }
    requires_relations["my-certificates"] = testing.Relation(
        "my-certificates",
        interface="certificates",
        remote_app_data={
            "certificates": json.dumps(
                [
                    {
                        "certificate": MOCK_CERTS_DATA,
                        "ca": MOCK_CERTS_DATA,
                        "chain": MOCK_CERTS_DATA,
                        "certificate_signing_request": MOCK_CERTS_DATA,
                    }
                ]
            ),
        },
    )
    requires_relations["my-s3"] = testing.Relation(
        "my-s3",
        interface="s3",
        remote_app_data={
            "endpoint": "s3",
            "bucket": "foo-bucket",
            "access-key": "my-access-key",
            "secret-key": "my-secret-key",
        },
    )
    requires_relations["cluster_worker0"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker0",
        remote_app_data=ClusterRequirerAppData(role="read").dump(),
        remote_units_data={
            0: ClusterRequirerUnitData(
                juju_topology={
                    "model": "test-model",
                    "application": "reader",
                    "unit": "reader/0",
                    "charm_name": "test-reader",
                },
                address="something",
            ).dump()
        },
    )
    requires_relations["cluster_worker1"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker1",
        remote_app_data=ClusterRequirerAppData(role="write").dump(),
        remote_units_data={
            0: ClusterRequirerUnitData(
                juju_topology={
                    "model": "test-model",
                    "application": "writer",
                    "unit": "writer/0",
                    "charm_name": "test-writer",
                },
                address="something",
            ).dump(),
            1: ClusterRequirerUnitData(
                juju_topology={
                    "model": "test-model",
                    "application": "writer",
                    "unit": "writer/1",
                    "charm_name": "test-writer",
                },
                address="something",
            ).dump(),
        },
    )
    requires_relations["cluster_worker2"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker2",
        remote_app_data=ClusterRequirerAppData(role="backend").dump(),
        remote_units_data={
            0: ClusterRequirerUnitData(
                juju_topology={
                    "model": "test-model",
                    "application": "backender",
                    "unit": "backender/0",
                    "charm_name": "test-backender",
                },
                address="something",
            ).dump()
        },
    )

    provides_relations = {
        endpoint: testing.Relation(endpoint=endpoint, interface=interface["interface"])
        for endpoint, interface in {
            "my-dashboards": {"interface": "grafana_dashboard"},
            "my-metrics": {"interface": "prometheus_scrape"},
        }.items()
    }
    peer_relations = [testing.PeerRelation(endpoint="my-peers")]
    return testing.State(
        containers={
            nginx_container,
            nginx_prometheus_exporter_container,
        },
        relations=list(requires_relations.values())
        + list(provides_relations.values())
        + peer_relations,
    )


@pytest.fixture()
def coordinator_charm(request):
    class MyCoordinator(ops.CharmBase):
        META = {
            "name": "foo-app",
            "requires": {
                "my-certificates": {"interface": "certificates"},
                "my-cluster": {"interface": "cluster"},
                "my-logging": {"interface": "loki_push_api"},
                "my-charm-tracing": {"interface": "tracing", "limit": 1},
                "my-workload-tracing": {"interface": "tracing", "limit": 1},
                "my-s3": {"interface": "s3"},
                "my-ds-exchange-require": {"interface": "grafana_datasource_exchange"},
                "my-service-mesh": {"interface": "service_mesh", "limit": 1},
                "my-service-mesh-require-cmr-mesh": {"interface": "cross_model_mesh"},
            },
            "provides": {
                "my-dashboards": {"interface": "grafana_dashboard"},
                "my-metrics": {"interface": "prometheus_scrape"},
                "my-ds-exchange-provide": {"interface": "grafana_datasource_exchange"},
                "my-service-mesh-provide-cmr-mesh": {"interface": "cross_model_mesh"},
            },
            "peers": {
                "my-peers": {
                    "interface": "coordinated_workers_peers",
                },
            },
            "containers": {
                "nginx": {"type": "oci-image"},
                "nginx-prometheus-exporter": {"type": "oci-image"},
            },
        }

        _worker_ports = None

        def __init__(self, framework: ops.Framework):
            super().__init__(framework)
            # Note: Here it is a good idea not to use context mgr because it is "ops aware"
            self.coordinator = Coordinator(
                charm=self,
                # Roles were take from loki-coordinator-k8s-operator
                roles_config=ClusterRolesConfig(
                    roles={"all", "read", "write", "backend"},
                    meta_roles={"all": {"all", "read", "write", "backend"}},
                    minimal_deployment={
                        "read",
                        "write",
                        "backend",
                    },
                    recommended_deployment={
                        "read": 3,
                        "write": 3,
                        "backend": 3,
                    },
                ),
                external_url="https://foo.example.com",
                worker_metrics_port=123,
                endpoints={
                    "certificates": "my-certificates",
                    "cluster": "my-cluster",
                    "grafana-dashboards": "my-dashboards",
                    "logging": "my-logging",
                    "metrics": "my-metrics",
                    "charm-tracing": "my-charm-tracing",
                    "workload-tracing": "my-workload-tracing",
                    "s3": "my-s3",
                    "send-datasource": "my-ds-exchange-provide",
                    "receive-datasource": "my-ds-exchange-require",
                    "catalogue": None,
                    "service-mesh": "my-service-mesh",
                    "service-mesh-provide-cmr-mesh": "my-service-mesh-provide-cmr-mesh",
                    "service-mesh-require-cmr-mesh": "my-service-mesh-require-cmr-mesh",
                },
                nginx_config=NginxConfig("localhost", [], {}),
                workers_config=lambda coordinator: f"workers configuration for {coordinator._charm.meta.name}",
                worker_ports=self._worker_ports,
                # nginx_options: Optional[NginxMappingOverrides] = None,
                # is_coherent: Optional[Callable[[ClusterProvider, ClusterRolesConfig], bool]] = None,
                # is_recommended: Optional[Callable[[ClusterProvider, ClusterRolesConfig], bool]] = None,
                peer_relation="my-peers",
            )

    return MyCoordinator


def test_worker_roles_subset_of_minimal_deployment(
    coordinator_state: testing.State, coordinator_charm: ops.CharmBase
):
    # Test that the combination of worker roles is a subset of the minimal deployment roles

    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # AND a coordinator_state defining relations to worker charms with incomplete distributed roles
    missing_backend_worker_relation = {
        relation
        for relation in coordinator_state.relations
        # exclude peer relations from the search
        if isinstance(relation, ops.testing.Relation) and relation.remote_app_name != "worker2"
    }

    # WHEN we process any event
    with ctx(
        ctx.on.update_status(),
        state=dataclasses.replace(coordinator_state, relations=missing_backend_worker_relation),
    ) as mgr:
        charm: coordinator_charm = mgr.charm

        # THEN the deployment is not coherent
        assert not charm.coordinator.is_coherent


def test_worker_ports_published(
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
):
    ports_per_role = {"read": (10, 22), "write": (1, 2132)}

    def _worker_ports(_, role):
        return ports_per_role.get(role, ())

    coordinator_charm._worker_ports = _worker_ports

    # GIVEN a coordinator_charm that has these ports configured
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # WHEN we process any event
    state_out = ctx.run(
        ctx.on.update_status(),
        state=dataclasses.replace(coordinator_state, leader=True),
    )
    # THEN the ports are correctly distributed to the workers via their relations
    worker_relations = state_out.get_relations("my-cluster")
    for relation in worker_relations:
        remote_worker_role = json.loads(relation.remote_app_data["role"])
        expected_ports = ports_per_role.get(remote_worker_role, ())

        assert set(json.loads(relation.local_app_data.get("worker_ports", ()))) == set(
            expected_ports
        )


def test_without_s3_integration_raises_error(
    coordinator_state: testing.State, coordinator_charm: ops.CharmBase
):
    # Test that a charm without an s3 integration raises S3NotFoundError

    # GIVEN a coordinator charm without an s3 integration
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
    relations_without_s3 = {
        relation for relation in coordinator_state.relations if relation.endpoint != "my-s3"
    }

    # WHEN we process any event
    with ctx(
        ctx.on.update_status(),
        state=dataclasses.replace(coordinator_state, relations=relations_without_s3),
    ) as mgr:
        # THEN the _s3_config method raises an S3NotFoundError
        with pytest.raises(S3NotFoundError):
            mgr.charm.coordinator._s3_config


@pytest.mark.parametrize("region", (None, "canada"))
@pytest.mark.parametrize("tls_ca_chain", (None, ["my ca chain"]))
@pytest.mark.parametrize("bucket", ("bucky",))
@pytest.mark.parametrize("secret_key", ("foo",))
@pytest.mark.parametrize("access_key", ("foo",))
@pytest.mark.parametrize(
    "endpoint, endpoint_stripped",
    (
        ("example.com", "example.com"),
        ("http://example.com", "example.com"),
        ("https://example.com", "example.com"),
    ),
)
def test_s3_integration(
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
    region,
    endpoint,
    endpoint_stripped,
    secret_key,
    access_key,
    bucket,
    tls_ca_chain,
):
    # Test that a charm with a s3 integration gives the expected _s3_config

    # GIVEN a coordinator charm with a s3 integration
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
    s3_relation = coordinator_state.get_relations("my-s3")[0]
    relations_except_s3 = [
        relation for relation in coordinator_state.relations if relation.endpoint != "my-s3"
    ]
    s3_app_data = {
        k: json.dumps(v)
        for k, v in {
            **({"region": region} if region else {}),
            **({"tls-ca-chain": tls_ca_chain} if tls_ca_chain else {}),
            "endpoint": endpoint,
            "access-key": access_key,
            "secret-key": secret_key,
            "bucket": bucket,
        }.items()
    }

    # WHEN we process any event
    with ctx(
        ctx.on.update_status(),
        state=dataclasses.replace(
            coordinator_state,
            leader=True,
            relations=relations_except_s3
            + [dataclasses.replace(s3_relation, remote_app_data=s3_app_data)],
        ),
    ) as mgr:
        # THEN the s3_connection_info method returns the expected data structure
        coordinator: Coordinator = mgr.charm.coordinator
        assert coordinator.s3_connection_info.region == region
        assert coordinator.s3_connection_info.bucket == bucket
        assert coordinator.s3_connection_info.endpoint == endpoint
        assert coordinator.s3_connection_info.secret_key == secret_key
        assert coordinator.s3_connection_info.access_key == access_key
        assert coordinator.s3_connection_info.tls_ca_chain == tls_ca_chain
        assert coordinator._s3_config["endpoint"] == endpoint_stripped
        assert coordinator._s3_config["insecure"] is not (
            tls_ca_chain or urlparse(endpoint).scheme == "https"
        )


def test_tracing_receivers_urls(
    coordinator_state: testing.State, coordinator_charm: ops.CharmBase
):
    charm_tracing_relation = testing.Relation(
        endpoint="my-charm-tracing",
        remote_app_data={
            "receivers": json.dumps(
                [{"protocol": {"name": "otlp_http", "type": "http"}, "url": "1.2.3.4:4318"}]
            )
        },
    )
    workload_tracing_relation = testing.Relation(
        endpoint="my-workload-tracing",
        remote_app_data={
            "receivers": json.dumps(
                [
                    {"protocol": {"name": "otlp_http", "type": "http"}, "url": "5.6.7.8:4318"},
                    {"protocol": {"name": "otlp_grpc", "type": "grpc"}, "url": "5.6.7.8:4317"},
                ]
            )
        },
    )
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    with ctx(
        ctx.on.update_status(),
        state=dataclasses.replace(
            coordinator_state, relations=[charm_tracing_relation, workload_tracing_relation]
        ),
    ) as mgr:
        coordinator: Coordinator = mgr.charm.coordinator
        assert coordinator._charm_tracing_receivers_urls == {
            "otlp_http": "1.2.3.4:4318",
        }
        assert coordinator._workload_tracing_receivers_urls == {
            "otlp_http": "5.6.7.8:4318",
            "otlp_grpc": "5.6.7.8:4317",
        }


def find_relation(relations, endpoint):
    return next(filter(lambda r: r.endpoint == endpoint, relations))


@pytest.mark.parametrize("tls", (True, False))
def test_charm_tracing_configured(
    coordinator_state: testing.State, coordinator_charm: ops.CharmBase, tls: bool, tmp_path
):
    # GIVEN a charm tracing integration (and tls?)
    relations = set(coordinator_state.relations)

    url = "1.2.3.4:4318"
    charm_tracing_relation = testing.Relation(
        endpoint="my-charm-tracing",
        remote_app_data={
            "receivers": json.dumps(
                [{"protocol": {"name": "otlp_http", "type": "http"}, "url": url}]
            )
        },
    )
    relations.remove(find_relation(relations, "my-charm-tracing"))
    relations.add(charm_tracing_relation)

    certs_relation = find_relation(relations, "my-certificates")
    if tls:

        def tls_mock():
            stack = ExitStack()
            stack.enter_context(
                patch.object(
                    Coordinator,
                    "tls_config",
                    new_callable=PropertyMock,
                    return_value=MOCK_TLS_CONFIG,
                )
            )
            stack.enter_context(
                patch("coordinated_workers.nginx.CA_CERT_PATH", new=tmp_path / "rootcacert")
            )
            return stack
    else:

        def tls_mock():
            return nullcontext()

        relations.remove(certs_relation)

    # WHEN we receive any event
    with tls_mock():
        ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
        # THEN the coordinator has called ops_tracing.set_destination with the expected params
        with patch("ops_tracing.set_destination") as p:
            ctx.run(
                ctx.on.update_status(),
                state=dataclasses.replace(coordinator_state, relations=relations),
            )
    p.assert_called_with(url=url + "/v1/traces", ca=MOCK_CERTS_DATA if tls else None)


@pytest.mark.parametrize(
    "event",
    (
        testing.CharmEvents.update_status(),
        testing.CharmEvents.start(),
        testing.CharmEvents.install(),
        testing.CharmEvents.config_changed(),
    ),
)
def test_invalid_databag_content(
    coordinator_charm: ops.CharmBase, event, nginx_container, nginx_prometheus_exporter_container
):
    # Test Invalid relations databag for ClusterProvider.gather_addresses_by_role

    # GIVEN a coordinator charm with a cluster relation and invalid remote databag contents
    requires_relations = {
        endpoint: testing.Relation(endpoint=endpoint, interface=interface["interface"])
        for endpoint, interface in {
            "my-certificates": {"interface": "certificates"},
            "my-logging": {"interface": "loki_push_api"},
            "my-charm-tracing": {"interface": "tracing"},
            "my-workload-tracing": {"interface": "tracing"},
        }.items()
    }
    requires_relations["cluster_worker0"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker0",
        remote_app_data=ClusterRequirerAppData(role="read").dump(),
    )
    requires_relations["cluster_worker1"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker1",
        remote_app_data=ClusterRequirerAppData(role="read").dump(),
    )
    requires_relations["cluster_worker2"] = testing.Relation(
        "my-cluster",
        remote_app_name="worker2",
    )

    provides_relations = {
        endpoint: testing.Relation(endpoint=endpoint, interface=interface["interface"])
        for endpoint, interface in {
            "my-dashboards": {"interface": "grafana_dashboard"},
            "my-metrics": {"interface": "prometheus_scrape"},
        }.items()
    }

    invalid_databag_state = testing.State(
        containers={
            nginx_container,
            nginx_prometheus_exporter_container,
        },
        relations=list(requires_relations.values()) + list(provides_relations.values()),
    )

    # WHEN: the coordinator processes any event
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
    with ctx(event, invalid_databag_state) as manager:
        cluster = manager.charm.coordinator.cluster
        # THEN the coordinator sets unit to blocked since the cluster is inconsistent with the missing relation.
        cluster.gather_addresses_by_role()
        manager.run()
    assert cluster.model.unit.status == ops.BlockedStatus("[consistency] Cluster inconsistent.")


@pytest.mark.parametrize("app", (True, False))
def test_invalid_app_or_unit_databag(
    coordinator_charm: ops.CharmBase, coordinator_state, app: bool
):
    # Test that when a relation changes and either the app or unit data is invalid
    #   the worker emits a ClusterRemovedEvent

    # WHEN you define a properly configured charm
    ctx = testing.Context(
        MyCharm,
        meta={
            "name": "foo",
            "requires": {"cluster": {"interface": "cluster"}},
            "containers": {"foo": {"type": "oci-image"}},
        },
        config={"options": {"role-all": {"type": "boolean", "default": True}}},
    )

    # IF the relation data is invalid (forced by the patched Exception)
    object_to_patch = (
        "coordinated_workers.interfaces.cluster.ClusterProviderAppData.load"
        if app
        else "coordinated_workers.interfaces.cluster.ClusterRequirerUnitData.load"
    )

    with patch(object_to_patch, side_effect=DataValidationError("Mock error")):
        # AND the relation changes
        relation = testing.Relation("cluster")

        ctx.run(
            ctx.on.relation_changed(relation),
            testing.State(
                containers={testing.Container("foo", can_connect=True)}, relations={relation}
            ),
        )

    # NOTE: this difference should not exist, and the ClusterRemovedEvent should always
    #   be emitted in case of corrupted data

    # THEN the charm emits a ClusterRemovedEvent
    if app:
        assert len(ctx.emitted_events) == 2
        assert isinstance(ctx.emitted_events[0], RelationChangedEvent)
        assert isinstance(ctx.emitted_events[1], ClusterRemovedEvent)
    else:
        assert len(ctx.emitted_events) == 1
        assert isinstance(ctx.emitted_events[0], RelationChangedEvent)


def _check_coordinator_scrape_jobs(
    state: ops.testing.State,
    expected_job_count: int,
    n_worker_units: int = 4,  # they are 4 in the default coordinator_state
) -> List[Dict[str, Any]]:
    """Validate the structure and content of generated scrape jobs.

    Args:
        state: scenario.State
        expected_job_count: Expected number of scrape jobs for the coordinator only
        n_worker_units: Number of worker units
    """
    rel = state.get_relations("my-metrics")[0]
    jobs = json.loads(rel.local_app_data["scrape_jobs"])

    # Verify number of jobs
    # n jobs will always be there: those are the worker jobs for the roles we
    # defined in the default coordinator_state
    assert len(jobs) == expected_job_count + n_worker_units

    # assumption: jobs without relabel_configs are coordinator jobs; all others are worker jobs
    coordinator_jobs = [j for j in jobs if not j.get("relabel_configs")]
    assert len(coordinator_jobs) == expected_job_count

    # Verify job structure
    for job in coordinator_jobs:
        assert "static_configs" in job
        assert len(job["static_configs"]) == 1
        assert "targets" in job["static_configs"][0]
        assert "labels" in job["static_configs"][0]
        assert "juju_unit" in job["static_configs"][0]["labels"]

    return coordinator_jobs


@pytest.mark.parametrize(
    "event",
    (
        testing.CharmEvents.update_status(),
        testing.CharmEvents.start(),
        testing.CharmEvents.install(),
        testing.CharmEvents.config_changed(),
    ),
)
def test_coordinator_scrape_jobs_generation_single_unit(
    event,
    coordinator_charm: Type[ops.CharmBase],
    coordinator_state: ops.testing.State,
):
    """Test that in a single coordinator unit deployment, a single metric scrape job is published."""
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META, app_name="coordinator")
    # GIVEN: A coordinator with a single unit (no remote peers) (this is the default coordinator_state)
    # WHEN: we receive any event
    state_out = ctx.run(event, dataclasses.replace(coordinator_state, leader=True))

    # THEN we have one job for the local unit, the rest are workers
    jobs = _check_coordinator_scrape_jobs(state_out, 1)

    # THEN: Exactly one scrape job should be generated for the local unit
    assert jobs[0]["static_configs"][0]["labels"]["juju_unit"] == "coordinator/0"
    # And: The target should use the correct port format (hostname:9113)
    assert jobs[0]["static_configs"][0]["targets"][0].endswith(":9113")


@pytest.mark.parametrize(
    "event",
    (
        testing.CharmEvents.update_status(),
        testing.CharmEvents.start(),
        testing.CharmEvents.install(),
        testing.CharmEvents.config_changed(),
    ),
)
def test_coordinator_scrape_jobs_generation_scaled(
    event,
    coordinator_charm: Type[ops.CharmBase],
    coordinator_state: ops.testing.State,
):
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META, app_name="coordinator")
    # GIVEN: A coordinator with 3 units (local + 2 remote peers)
    # replace the default peer relation (in coordinator_state) with one which has some peer data
    relations: Set[ops.testing.RelationBase] = set(coordinator_state.relations)
    peer_relation: ops.testing.PeerRelation = coordinator_state.get_relations("my-peers")[0]
    relations.remove(peer_relation)

    # GIVEN: Two remote peers are available with their hostnames
    scaled_peer_relation = dataclasses.replace(
        peer_relation,
        peers_data={
            1: {"hostname": "peer1.com"},
            2: {"hostname": "peer2.com"},
        },
    )
    relations.add(scaled_peer_relation)

    scaled_state = dataclasses.replace(coordinator_state, leader=True, relations=relations)

    # WHEN any event occurs
    state_out = ctx.run(ctx.on.update_status(), scaled_state)
    jobs = _check_coordinator_scrape_jobs(state_out, 3)

    # THEN: Three scrape jobs should be generated (one for each unit)
    unit_labels = [job["static_configs"][0]["labels"]["juju_unit"] for job in jobs]
    assert "coordinator/0" in unit_labels  # Local unit
    assert "coordinator/1" in unit_labels  # Remote unit 1
    assert "coordinator/2" in unit_labels  # Remote unit 2

    # And: All targets should use the correct port format (hostname:9113)
    targets = [job["static_configs"][0]["targets"][0] for job in jobs]
    for target in targets:
        assert target.endswith(":9113")


@pytest.mark.parametrize(
    ("hostname", "expected_app_hostname"),
    (
        (
            "foo-app-0.foo-app-headless.test.svc.cluster.local",
            "foo-app.test.svc.cluster.local",
        ),
        (
            "foo-app-0.foo-app-headless.test.svc.custom.domain",
            "foo-app.test.svc.custom.domain",
        ),
        (
            "foo-app-0.foo-app-headless.test.svc.custom.svc.domain",
            "foo-app.test.svc.custom.svc.domain",
        ),
        ("localhost", "localhost"),
        ("my.custom.domain", "my.custom.domain"),
        ("192.0.2.1", "192.0.2.1"),
    ),
)
def test_app_hostname(
    coordinator_charm: ops.CharmBase,
    hostname: str,
    expected_app_hostname: str,
):
    # GIVEN a hostname
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # WHEN any event fires
    with ctx(ctx.on.update_status(), testing.State(model=testing.Model("test"))) as mgr:
        # THEN if hostname is a valid k8s pod fqdn, app_hostname is set to the k8s service fqdn
        # else app_hostname is set to whatever value hostname has
        assert (
            mgr.charm.coordinator.app_hostname(hostname, mgr.charm.app.name, mgr.charm.model.name)
            == expected_app_hostname
        )


def test_catalogue_integration(coordinator_state: testing.State):
    class MyCatalogCoord(ops.CharmBase):
        META = {
            "name": "foo-app",
            "requires": {
                "my-certificates": {"interface": "certificates"},
                "my-cluster": {"interface": "cluster"},
                "my-logging": {"interface": "loki_push_api"},
                "my-charm-tracing": {"interface": "tracing", "limit": 1},
                "my-workload-tracing": {"interface": "tracing", "limit": 1},
                "my-s3": {"interface": "s3"},
                "my-ds-exchange-require": {"interface": "grafana_datasource_exchange"},
                "my-catalogue": {"interface": "catalogue"},
            },
            "peers": {
                "my-peers": {
                    "interface": "coordinated_workers_peers",
                },
            },
            "provides": {
                "my-dashboards": {"interface": "grafana_dashboard"},
                "my-metrics": {"interface": "prometheus_scrape"},
                "my-ds-exchange-provide": {"interface": "grafana_datasource_exchange"},
            },
            "containers": {
                "nginx": {"type": "oci-image"},
                "nginx-prometheus-exporter": {"type": "oci-image"},
            },
        }

        _worker_ports = None

        def __init__(self, framework: ops.Framework):
            super().__init__(framework)
            # Note: Here it is a good idea not to use context mgr because it is "ops aware"
            self.coordinator = Coordinator(
                charm=self,
                # Roles were take from loki-coordinator-k8s-operator
                roles_config=ClusterRolesConfig(
                    roles={"all", "read", "write", "backend"},
                    meta_roles={"all": {"all", "read", "write", "backend"}},
                    minimal_deployment={
                        "read",
                        "write",
                        "backend",
                    },
                    recommended_deployment={
                        "read": 3,
                        "write": 3,
                        "backend": 3,
                    },
                ),
                external_url="https://foo.example.com",
                worker_metrics_port=123,
                endpoints={
                    "certificates": "my-certificates",
                    "cluster": "my-cluster",
                    "grafana-dashboards": "my-dashboards",
                    "logging": "my-logging",
                    "metrics": "my-metrics",
                    "charm-tracing": "my-charm-tracing",
                    "workload-tracing": "my-workload-tracing",
                    "s3": "my-s3",
                    "send-datasource": "my-ds-exchange-provide",
                    "receive-datasource": "my-ds-exchange-require",
                    "catalogue": "my-catalogue",
                },
                nginx_config=NginxConfig("localhost", [], {}),
                workers_config=lambda coordinator: f"workers configuration for {coordinator._charm.meta.name}",
                worker_ports=self._worker_ports,
                catalogue_item=CatalogueItem("foo", "bar", "baz", "qux"),
                peer_relation="my-peers",
                # nginx_options: Optional[NginxMappingOverrides] = None,
                # is_coherent: Optional[Callable[[ClusterProvider, ClusterRolesConfig], bool]] = None,
                # is_recommended: Optional[Callable[[ClusterProvider, ClusterRolesConfig], bool]] = None,
            )

    # GIVEN a catalogue integration
    ctx = testing.Context(MyCatalogCoord, meta=MyCatalogCoord.META)
    catalogue_relation = testing.Relation(endpoint="my-catalogue")
    relations_with_catalog = set(coordinator_state.relations).union({catalogue_relation})

    # WHEN any event fires
    state_out = ctx.run(
        ctx.on.update_status(),
        dataclasses.replace(coordinator_state, leader=True, relations=relations_with_catalog),
    )

    # THEN the coordinator has published his catalogue item
    catalogue_relation_out = state_out.get_relation(catalogue_relation.id)
    assert catalogue_relation_out.local_app_data


@contextmanager
def patch_source_alert_rules():
    resources_base_path = Path(__file__).parent / "resources"
    with ExitStack() as stack:
        stack.enter_context(
            patch(
                "coordinated_workers.coordinator.NGINX_ORIGINAL_METRICS_ALERT_RULES_PATH",
                new=resources_base_path / "metrics_alert_rules" / "nginx",
            )
        )

        stack.enter_context(
            patch(
                "coordinated_workers.coordinator.WORKER_ORIGINAL_METRICS_ALERT_RULES_PATH",
                new=resources_base_path / "metrics_alert_rules" / "workers",
            )
        )

        stack.enter_context(
            patch(
                "coordinated_workers.coordinator.ORIGINAL_LOGS_ALERT_RULES_PATH",
                new=resources_base_path / "logs_alert_rules",
            )
        )

        yield


@pytest.mark.parametrize("alerts_type", ("metrics", "logs"))
@pytest.mark.parametrize("workers_no", (1, 2))
def test_rendered_alert_rules(
    coordinator_charm: ops.CharmBase, coordinator_state, workers_no, alerts_type
):
    relation_endpoint = "my-metrics" if alerts_type == "metrics" else "my-logging"
    # ASSUMPTIONS from the sample files in ../resources
    # 4 includes the 2 centralised alert rules added by cos-lib
    coordinator_rules_no = 4 if alerts_type == "metrics" else 2
    worker_rules_no = 1 if alerts_type == "metrics" else 2

    # GIVEN a coordinator charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
    # AND an S3 relation
    s3_relation = testing.Relation(
        "my-s3",
        interface="s3",
        remote_app_data={
            "endpoint": "s3",
            "bucket": "foo-bucket",
            "access-key": "my-access-key",
            "secret-key": "my-secret-key",
        },
    )
    # AND a COHERENT cluster of workers
    workers_relation = []
    for worker_no in range(workers_no):
        workers_relation.append(
            testing.Relation(
                "my-cluster",
                remote_app_name=f"worker{worker_no}",
                remote_app_data=ClusterRequirerAppData(role="all").dump(),
                remote_units_data={
                    0: ClusterRequirerUnitData(
                        juju_topology={
                            "model": "test-model",
                            "application": f"all-{worker_no}",
                            "unit": f"all-{worker_no}/0",
                            "charm_name": "test-all",
                        },
                        address="something",
                    ).dump()
                },
            )
        )
    # AND a <relation_endpoint> relation
    # can be either my-metrics or my-logging
    telemetry_relation = testing.Relation(relation_endpoint)

    # AND this is a leader unit
    state = dataclasses.replace(
        coordinator_state,
        relations={s3_relation, telemetry_relation, *workers_relation},
        leader=True,
    )

    # AND it has sample alert rules
    with patch_source_alert_rules():
        # WHEN a <relation_endpoint>_changed event fires
        state_out = ctx.run(ctx.on.relation_changed(telemetry_relation), state)

    target_relation = state_out.get_relations(relation_endpoint)[0]
    alert_rules_str = target_relation.local_app_data["alert_rules"]

    # THEN each worker has a rendered alert with its topology
    for worker_no in range(workers_no):
        worker_databag_topology = f'"juju_application": "all-{worker_no}"'
        # AND the worker topology appears as many times as there are alert rules for that worker
        assert alert_rules_str.count(worker_databag_topology) == worker_rules_no

    # AND the coordinator has a rendered alert with its topology
    coordinator_databag_topology = '"juju_application": "foo-app"'
    # AND the coordinator topology appears as many times as there are alert rules for that coordinator
    assert alert_rules_str.count(coordinator_databag_topology) == coordinator_rules_no

    # AND total alert rules = total rendered worker alerts + total rendered coordinated alerts
    alert_rules = json.loads(alert_rules_str)
    total_worker_rules_no = worker_rules_no * workers_no
    assert (
        sum(len(group["rules"]) for group in alert_rules["groups"])
        == total_worker_rules_no + coordinator_rules_no
    )


def test_coordinator_passes_service_mesh_labels_to_workers(
    mock_policy_resource_manager,
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
):
    """Test that the Coordinator passes service mesh labels to the Workers via the Cluster relation."""
    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # AND a coordinator_state that is valid and includes a populated service mesh relation
    expected_labels = {"label1": "value1", "label2": "value2"}
    service_mesh_relation = testing.Relation(
        endpoint="my-service-mesh",
        interface="service_mesh",
        remote_app_data={
            "labels": json.dumps(expected_labels),
            "mesh_type": json.dumps("istio"),
        },
    )
    relations_with_service_mesh = [*coordinator_state.relations, service_mesh_relation]
    state_with_service_mesh = dataclasses.replace(
        coordinator_state, relations=relations_with_service_mesh
    )
    state_with_service_mesh = dataclasses.replace(state_with_service_mesh, leader=True)

    # WHEN we process any event
    state_out = ctx.run(ctx.on.update_status(), state=state_with_service_mesh)
    # THEN the service mesh labels are correctly distributed to the workers via the Cluster relation
    cluster = state_out.get_relations("my-cluster")
    for relation in cluster:
        labels = json.loads(relation.local_app_data.get("worker_labels", "{}"))
        for key, value in expected_labels.items():
            assert labels[key] == value


def test_coordinator_passes_solution_labels_to_worker(
    coordinator_state: testing.State, coordinator_charm: ops.CharmBase
):
    """Test that the Coordinator passes the solution-level labels to the Workers via the Cluster relation."""
    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)
    expected_labels = {"app.kubernetes.io/part-of": coordinator_charm.META["name"]}

    # AND a coordinator_state that is valid and complete with this unit being the leader
    state = coordinator_state
    state = dataclasses.replace(state, leader=True)

    # WHEN we process any event
    state_out = ctx.run(ctx.on.update_status(), state=state)
    # THEN the solution-level labels are correctly distributed to the workers via the Cluster relation
    cluster = state_out.get_relations("my-cluster")
    for relation in cluster:
        labels = json.loads(relation.local_app_data.get("worker_labels", "{}"))
        for key, value in expected_labels.items():
            assert labels[key] == value


def test_coordinator_creates_and_reconcilies_policy_resource_managers(
    mock_policy_resource_manager,
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
):
    """Test that the Coordinator creates and calls the reconcile of PolicyResourceManager the right number of times."""
    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # AND a coordinator_state that is valid and includes a populated service mesh relation
    mesh_labels = {"label1": "value1", "label2": "value2"}
    mesh_type = "istio"
    service_mesh_relation = testing.Relation(
        endpoint="my-service-mesh",
        interface="service_mesh",
        remote_app_data={
            "labels": json.dumps(mesh_labels),
            "mesh_type": json.dumps(mesh_type),
        },
    )
    relations_with_service_mesh = [*coordinator_state.relations, service_mesh_relation]
    state_with_service_mesh = dataclasses.replace(
        coordinator_state, relations=relations_with_service_mesh
    )
    state_with_service_mesh = dataclasses.replace(state_with_service_mesh, leader=True)

    # WHEN we process any event
    with ctx(ctx.on.update_status(), state=state_with_service_mesh) as mgr:
        coordinator = mgr.charm.coordinator

        # AND when we reconcile mesh policies
        coordinator._reconcile_mesh_policies()

        # THEN _get_policy_resource_manager is called and reconcile is called
        mock_policy_resource_manager.assert_called_once()
        mock_policy_resource_manager.return_value.reconcile.assert_called_once()


def test_cluster_internal_mesh_policies(
    mock_policy_resource_manager,
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
):
    """Test that the correct mesh policies are created and passed to reconcile."""
    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # AND a coordinator_state that is valid and includes a populated service mesh relation
    mesh_labels = {"label1": "value1", "label2": "value2"}
    mesh_type = "istio"
    service_mesh_relation = testing.Relation(
        endpoint="my-service-mesh",
        interface="service_mesh",
        remote_app_data={
            "labels": json.dumps(mesh_labels),
            "mesh_type": json.dumps(mesh_type),
        },
    )
    relations_with_service_mesh = [*coordinator_state.relations, service_mesh_relation]
    state_with_service_mesh = dataclasses.replace(
        coordinator_state, relations=relations_with_service_mesh
    )
    state_with_service_mesh = dataclasses.replace(state_with_service_mesh, leader=True)

    # WHEN we process any event
    with ctx(ctx.on.update_status(), state=state_with_service_mesh) as mgr:
        coordinator = mgr.charm.coordinator

        # WHEN we reconcile mesh policies
        coordinator._reconcile_mesh_policies()

        # with the correct mesh policies are passed to reconcile
        reconcile_calls = mock_policy_resource_manager.return_value.reconcile.call_args_list

        # Extract policies from all reconcile calls
        all_policies = []
        for call in reconcile_calls:
            policies = call[0][0]
            all_policies.extend(policies)

        # THEN we should have 7 policies total
        # 1. coordinator to all cluster units
        # 2. reader to all cluster units
        # 3. writer to all cluster units
        # 4. backender to all cluster units
        # 5. reader to coordinator's service
        # 6. writer to coordinator's service
        # 7. backender to coordinator's service
        assert len(all_policies) == 7

        # AND all policies should target the coordinator model
        for policy in all_policies:
            assert policy.target_namespace in [mgr.charm.model.name]
            if policy.target_type == "unit":
                assert policy.target_selector_labels == {"app.kubernetes.io/part-of": "foo-app"}
            if policy.target_type == "app":
                assert policy.target_app_name == "foo-app"

        # AND policies should be from the expected sources
        source_apps = {policy.source_app_name for policy in all_policies}
        expected_source_apps = {"foo-app", "reader", "writer", "backender"}
        assert source_apps == expected_source_apps


def test_mesh_policies_deletion_when_mesh_disconnected(
    mock_policy_resource_manager,
    coordinator_state: testing.State,
    coordinator_charm: ops.CharmBase,
):
    """Test that mesh policies are deleted when service mesh is disconnected."""
    # GIVEN a coordinator_charm
    ctx = testing.Context(coordinator_charm, meta=coordinator_charm.META)

    # AND a coordinator_state without service mesh relation (mesh disconnected)
    state_without_service_mesh = dataclasses.replace(coordinator_state, leader=True)

    # WHEN we process any event
    with ctx(ctx.on.update_status(), state=state_without_service_mesh) as mgr:
        coordinator = mgr.charm.coordinator
        # WHEN we reconcile mesh policies
        coordinator._reconcile_mesh_policies()
        # THEN prm.delete() is called for cleanup
        mock_policy_resource_manager.return_value.delete.assert_called()
        # AND reconcile is not called
        mock_policy_resource_manager.return_value.reconcile.assert_not_called()


def test_coordinator_charm_mesh_policies_passed_to_service_mesh_consumer(
    coordinator_state: testing.State,
):
    """Test that charm_mesh_policies are properly passed to ServiceMeshConsumer."""
    from charms.istio_beacon_k8s.v0.service_mesh import AppPolicy, Endpoint, UnitPolicy

    # Create custom charm mesh policies
    charm_app_policy = AppPolicy(
        relation="custom-relation",
        endpoints=[
            Endpoint(
                ports=[8080],
            )
        ],
    )

    charm_unit_policy = UnitPolicy(
        relation="custom-relation",
        ports=[8080, 9090],
    )

    charm_policies = [charm_app_policy, charm_unit_policy]

    class MyCoordinatorWithPolicies(ops.CharmBase):
        META = {
            "name": "foo-app",
            "requires": {
                "my-certificates": {"interface": "certificates"},
                "my-cluster": {"interface": "cluster"},
                "my-logging": {"interface": "loki_push_api"},
                "my-charm-tracing": {"interface": "tracing", "limit": 1},
                "my-workload-tracing": {"interface": "tracing", "limit": 1},
                "my-s3": {"interface": "s3"},
                "my-ds-exchange-require": {"interface": "grafana_datasource_exchange"},
                "my-service-mesh": {"interface": "service_mesh", "limit": 1},
                "my-service-mesh-require-cmr-mesh": {"interface": "cross_model_mesh"},
            },
            "provides": {
                "my-dashboards": {"interface": "grafana_dashboard"},
                "my-metrics": {"interface": "prometheus_scrape"},
                "my-ds-exchange-provide": {"interface": "grafana_datasource_exchange"},
                "my-service-mesh-provide-cmr-mesh": {"interface": "cross_model_mesh"},
            },
            "peers": {
                "my-peers": {
                    "interface": "coordinated_workers_peers",
                },
            },
            "containers": {
                "nginx": {"type": "oci-image"},
                "nginx-prometheus-exporter": {"type": "oci-image"},
            },
        }

        def __init__(self, framework: ops.Framework):
            super().__init__(framework)
            self.coordinator = Coordinator(
                charm=self,
                roles_config=ClusterRolesConfig(
                    roles={"all", "read", "write", "backend"},
                    meta_roles={"all": {"all", "read", "write", "backend"}},
                    minimal_deployment={"read", "write", "backend"},
                    recommended_deployment={"read": 3, "write": 3, "backend": 3},
                ),
                external_url="https://foo.example.com",
                worker_metrics_port=123,
                endpoints={
                    "certificates": "my-certificates",
                    "cluster": "my-cluster",
                    "grafana-dashboards": "my-dashboards",
                    "logging": "my-logging",
                    "metrics": "my-metrics",
                    "charm-tracing": "my-charm-tracing",
                    "workload-tracing": "my-workload-tracing",
                    "s3": "my-s3",
                    "send-datasource": "my-ds-exchange-provide",
                    "receive-datasource": "my-ds-exchange-require",
                    "catalogue": None,
                    "service-mesh": "my-service-mesh",
                    "service-mesh-provide-cmr-mesh": "my-service-mesh-provide-cmr-mesh",
                    "service-mesh-require-cmr-mesh": "my-service-mesh-require-cmr-mesh",
                },
                nginx_config=NginxConfig("localhost", [], {}),
                workers_config=lambda coordinator: f"workers configuration for {coordinator._charm.meta.name}",
                worker_ports=None,
                charm_mesh_policies=charm_policies,  # Pass custom policies here
                peer_relation="my-peers",
            )

    # Test with ServiceMesh relation present
    ctx = testing.Context(MyCoordinatorWithPolicies, meta=MyCoordinatorWithPolicies.META)

    service_mesh_relation = testing.Relation(
        endpoint="my-service-mesh",
        interface="service_mesh",
        remote_app_data={
            "labels": json.dumps({"label1": "value1"}),
            "mesh_type": json.dumps("istio"),
        },
    )

    relations = [*coordinator_state.relations, service_mesh_relation]
    state = dataclasses.replace(coordinator_state, relations=relations, leader=True)

    with patch("coordinated_workers.service_mesh.ServiceMeshConsumer") as mock_mesh_consumer:
        ctx.run(ctx.on.update_status(), state=state)

        # Verify ServiceMeshConsumer was called with the combined policies
        mock_mesh_consumer.assert_called_once()
        call_args = mock_mesh_consumer.call_args

        # Check that policies parameter includes our custom policies
        policies_arg = call_args[1]["policies"]
        assert (
            len(policies_arg) == 3
        )  # Should have both custom policies and the default metrics policy
        assert charm_app_policy in policies_arg
        assert charm_unit_policy in policies_arg
