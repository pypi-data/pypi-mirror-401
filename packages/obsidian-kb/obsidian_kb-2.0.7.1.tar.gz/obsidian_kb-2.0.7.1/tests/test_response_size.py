"""–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ–∏—Å–∫–∞ v5.

–ò–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤:
- Metadata-only –∑–∞–ø—Ä–æ—Å—ã (document-level)
- Semantic –∑–∞–ø—Ä–æ—Å—ã (chunk-level)
- Hybrid –∑–∞–ø—Ä–æ—Å—ã (chunk-level)

–¶–µ–ª—å: ~1KB/doc –¥–ª—è metadata-only –≤–º–µ—Å—Ç–æ ~2KB/—á–∞–Ω–∫
"""

import asyncio
import json
import statistics
from pathlib import Path
from typing import Any

from obsidian_kb.service_container import ServiceContainer
from obsidian_kb.types import RetrievalGranularity, SearchRequest


# –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
TEST_QUERIES = {
    "metadata_only": [
        "tags:python",
        "tags:meeting tags:important",
        "type:note",
        "tags:project tags:active",
        "type:meeting",
        "tags:todo tags:urgent",
        "type:document",
        "tags:reference",
        "tags:personal tags:private",
        "type:guide",
    ],
    "semantic": [
        "Python async programming",
        "database optimization techniques",
        "REST API design patterns",
        "machine learning algorithms",
        "web development best practices",
    ],
    "hybrid": [
        "Python async programming",
        "database optimization techniques",
        "REST API design patterns",
        "machine learning algorithms",
        "web development best practices",
    ],
}


def measure_response_size(response: Any, formatter: Any, format_type: str = "markdown") -> dict[str, int]:
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.
    
    Args:
        response: SearchResponse –æ–±—ä–µ–∫—Ç
        formatter: –§–æ—Ä–º–∞—Ç—Ç–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        format_type: –¢–∏–ø —Ñ–æ—Ä–º–∞—Ç–∞ ("markdown" –∏–ª–∏ "json")
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ –≤ –±–∞–π—Ç–∞—Ö
    """
    sizes = {}
    
    # –†–∞–∑–º–µ—Ä –≤—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
    if format_type == "markdown":
        formatted = formatter.format_markdown(response)
        sizes["total_bytes"] = len(formatted.encode("utf-8"))
        sizes["total_chars"] = len(formatted)
    else:
        formatted = formatter.format_json(response)
        formatted_str = json.dumps(formatted, ensure_ascii=False)
        sizes["total_bytes"] = len(formatted_str.encode("utf-8"))
        sizes["total_chars"] = len(formatted_str)
    
    # –†–∞–∑–º–µ—Ä –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    if response.total_found > 0:
        sizes["bytes_per_doc"] = sizes["total_bytes"] / response.total_found
        sizes["chars_per_doc"] = sizes["total_chars"] / response.total_found
    else:
        sizes["bytes_per_doc"] = 0
        sizes["chars_per_doc"] = 0
    
    # –†–∞–∑–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
    header_size = len(f"## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: \"{response.request.query}\"\n".encode("utf-8"))
    meta_size = len(
        f"*{response.detected_intent.value} | {response.total_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {response.execution_time_ms:.0f} –º—Å*\n"
        .encode("utf-8")
    )
    sizes["header_bytes"] = header_size + meta_size
    
    # –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
    sizes["results_bytes"] = sizes["total_bytes"] - sizes["header_bytes"]
    if response.total_found > 0:
        sizes["bytes_per_result"] = sizes["results_bytes"] / response.total_found
    else:
        sizes["bytes_per_result"] = 0
    
    return sizes


async def _run_response_size_test(
    vault_name: str,
    iterations: int = 5,
) -> dict[str, Any]:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    Args:
        vault_name: –ò–º—è vault'–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    print("üìè –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ–∏—Å–∫–∞ v5")
    print("=" * 80)
    print()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
    print("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤...")
    services = ServiceContainer()
    search_service = services.search_service
    formatter = services.formatter
    
    results: dict[str, Any] = {
        "vault_name": vault_name,
        "timestamp": __import__("time").strftime("%Y-%m-%d %H:%M:%S"),
        "iterations": iterations,
        "tests": {},
    }
    
    # –¢–µ—Å—Ç 1: Metadata-only –∑–∞–ø—Ä–æ—Å—ã (document-level)
    print("üìã –¢–µ—Å—Ç 1: Metadata-only –∑–∞–ø—Ä–æ—Å—ã (document-level)")
    print("-" * 80)
    metadata_sizes_md = []
    metadata_sizes_json = []
    
    for query in TEST_QUERIES["metadata_only"]:
        request = SearchRequest(
            vault_name=vault_name,
            query=query,
            limit=10,
            granularity=RetrievalGranularity.DOCUMENT,
            include_content=False,  # –ë–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è metadata-only
        )
        
        response = await search_service.search(request)
        
        # –ò–∑–º–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤ markdown
        sizes_md = measure_response_size(response, formatter, "markdown")
        metadata_sizes_md.append(sizes_md)
        
        # –ò–∑–º–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤ JSON
        sizes_json = measure_response_size(response, formatter, "json")
        metadata_sizes_json.append(sizes_json)
        
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}")
        print(f"    –ù–∞–π–¥–µ–Ω–æ: {response.total_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"    Markdown: {sizes_md['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_md['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
        print(f"    JSON: {sizes_json['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_json['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è metadata-only
    if metadata_sizes_md:
        avg_bytes_per_doc_md = statistics.mean([s["bytes_per_doc"] for s in metadata_sizes_md])
        avg_bytes_per_doc_json = statistics.mean([s["bytes_per_doc"] for s in metadata_sizes_json])
        
        results["tests"]["metadata_only"] = {
            "markdown": {
                "avg_bytes_per_doc": avg_bytes_per_doc_md,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in metadata_sizes_md]),
                "min_bytes_per_doc": min([s["bytes_per_doc"] for s in metadata_sizes_md]),
                "max_bytes_per_doc": max([s["bytes_per_doc"] for s in metadata_sizes_md]),
                "target_bytes_per_doc": 1024,  # 1KB
                "meets_target": avg_bytes_per_doc_md < 1024,
            },
            "json": {
                "avg_bytes_per_doc": avg_bytes_per_doc_json,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in metadata_sizes_json]),
                "min_bytes_per_doc": min([s["bytes_per_doc"] for s in metadata_sizes_json]),
                "max_bytes_per_doc": max([s["bytes_per_doc"] for s in metadata_sizes_json]),
                "target_bytes_per_doc": 1024,
                "meets_target": avg_bytes_per_doc_json < 1024,
            },
        }
        
        print(f"\n  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (Markdown): {avg_bytes_per_doc_md:.0f} –±–∞–π—Ç/–¥–æ–∫")
        print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (JSON): {avg_bytes_per_doc_json:.0f} –±–∞–π—Ç/–¥–æ–∫")
        print(f"  –¶–µ–ª—å: <1024 –±–∞–π—Ç/–¥–æ–∫ (1KB)")
        status_md = "‚úÖ" if avg_bytes_per_doc_md < 1024 else "‚ùå"
        status_json = "‚úÖ" if avg_bytes_per_doc_json < 1024 else "‚ùå"
        print(f"  –°—Ç–∞—Ç—É—Å (Markdown): {status_md}")
        print(f"  –°—Ç–∞—Ç—É—Å (JSON): {status_json}")
    print()
    
    # –¢–µ—Å—Ç 2: Semantic –∑–∞–ø—Ä–æ—Å—ã (chunk-level)
    print("üîç –¢–µ—Å—Ç 2: Semantic –∑–∞–ø—Ä–æ—Å—ã (chunk-level)")
    print("-" * 80)
    semantic_sizes_md = []
    semantic_sizes_json = []
    
    for query in TEST_QUERIES["semantic"]:
        request = SearchRequest(
            vault_name=vault_name,
            query=query,
            limit=10,
            granularity=RetrievalGranularity.CHUNK,
            search_type="vector",
            include_content=True,  # –° –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –¥–ª—è chunk-level
        )
        
        response = await search_service.search(request)
        
        sizes_md = measure_response_size(response, formatter, "markdown")
        semantic_sizes_md.append(sizes_md)
        
        sizes_json = measure_response_size(response, formatter, "json")
        semantic_sizes_json.append(sizes_json)
        
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}")
        print(f"    –ù–∞–π–¥–µ–Ω–æ: {response.total_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"    Markdown: {sizes_md['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_md['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
        print(f"    JSON: {sizes_json['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_json['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
    
    if semantic_sizes_md:
        avg_bytes_per_doc_md = statistics.mean([s["bytes_per_doc"] for s in semantic_sizes_md])
        avg_bytes_per_doc_json = statistics.mean([s["bytes_per_doc"] for s in semantic_sizes_json])
        
        results["tests"]["semantic"] = {
            "markdown": {
                "avg_bytes_per_doc": avg_bytes_per_doc_md,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in semantic_sizes_md]),
            },
            "json": {
                "avg_bytes_per_doc": avg_bytes_per_doc_json,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in semantic_sizes_json]),
            },
        }
        
        print(f"\n  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (Markdown): {avg_bytes_per_doc_md:.0f} –±–∞–π—Ç/–¥–æ–∫")
        print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (JSON): {avg_bytes_per_doc_json:.0f} –±–∞–π—Ç/–¥–æ–∫")
    print()
    
    # –¢–µ—Å—Ç 3: Hybrid –∑–∞–ø—Ä–æ—Å—ã (chunk-level)
    print("üîÄ –¢–µ—Å—Ç 3: Hybrid –∑–∞–ø—Ä–æ—Å—ã (chunk-level)")
    print("-" * 80)
    hybrid_sizes_md = []
    hybrid_sizes_json = []
    
    for query in TEST_QUERIES["hybrid"]:
        request = SearchRequest(
            vault_name=vault_name,
            query=query,
            limit=10,
            granularity=RetrievalGranularity.CHUNK,
            search_type="hybrid",
            include_content=True,
        )
        
        response = await search_service.search(request)
        
        sizes_md = measure_response_size(response, formatter, "markdown")
        hybrid_sizes_md.append(sizes_md)
        
        sizes_json = measure_response_size(response, formatter, "json")
        hybrid_sizes_json.append(sizes_json)
        
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}")
        print(f"    –ù–∞–π–¥–µ–Ω–æ: {response.total_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"    Markdown: {sizes_md['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_md['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
        print(f"    JSON: {sizes_json['bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ | {sizes_json['total_bytes']} –±–∞–π—Ç –≤—Å–µ–≥–æ")
    
    if hybrid_sizes_md:
        avg_bytes_per_doc_md = statistics.mean([s["bytes_per_doc"] for s in hybrid_sizes_md])
        avg_bytes_per_doc_json = statistics.mean([s["bytes_per_doc"] for s in hybrid_sizes_json])
        
        results["tests"]["hybrid"] = {
            "markdown": {
                "avg_bytes_per_doc": avg_bytes_per_doc_md,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in hybrid_sizes_md]),
            },
            "json": {
                "avg_bytes_per_doc": avg_bytes_per_doc_json,
                "median_bytes_per_doc": statistics.median([s["bytes_per_doc"] for s in hybrid_sizes_json]),
            },
        }
        
        print(f"\n  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (Markdown): {avg_bytes_per_doc_md:.0f} –±–∞–π—Ç/–¥–æ–∫")
        print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (JSON): {avg_bytes_per_doc_json:.0f} –±–∞–π—Ç/–¥–æ–∫")
    print()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("=" * 80)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    print()
    
    if "metadata_only" in results["tests"]:
        md_meta = results["tests"]["metadata_only"]["markdown"]
        json_meta = results["tests"]["metadata_only"]["json"]
        
        print("Metadata-only –∑–∞–ø—Ä–æ—Å—ã (document-level):")
        print(f"  Markdown: {md_meta['avg_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ (—Ü–µ–ª—å: <1024)")
        print(f"  JSON: {json_meta['avg_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫ (—Ü–µ–ª—å: <1024)")
        print()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å chunk-level (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if "semantic" in results["tests"]:
            md_sem = results["tests"]["semantic"]["markdown"]
            reduction = ((md_sem["avg_bytes_per_doc"] - md_meta["avg_bytes_per_doc"]) / md_sem["avg_bytes_per_doc"] * 100) if md_sem["avg_bytes_per_doc"] > 0 else 0
            print(f"  –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å chunk-level: {reduction:.1f}%")
            print()
    
    # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
    await services.cleanup()
    
    return results


def save_results(results: dict[str, Any], output_file: Path) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON."""
    output_file.write_text(json.dumps(results, indent=2, ensure_ascii=False))
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


def create_markdown_report(results: dict[str, Any], output_file: Path) -> None:
    """–°–æ–∑–¥–∞–Ω–∏–µ markdown –æ—Ç—á—ë—Ç–∞."""
    lines = [
        "# –û—Ç—á—ë—Ç –æ —Ä–∞–∑–º–µ—Ä–µ –æ—Ç–≤–µ—Ç–∞ –ø–æ–∏—Å–∫–∞ v5",
        "",
        f"**–î–∞—Ç–∞:** {results['timestamp']}",
        f"**Vault:** {results['vault_name']}",
        "",
        "## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã",
        "",
        "| –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞ | –§–æ—Ä–º–∞—Ç | –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä (–±–∞–π—Ç/–¥–æ–∫) | –ú–µ–¥–∏–∞–Ω–∞ | –¶–µ–ª—å <1KB |",
        "|-------------|--------|---------------------------|---------|-----------|",
    ]
    
    for test_name, test_data in results["tests"].items():
        for format_type in ["markdown", "json"]:
            if format_type in test_data:
                fmt_data = test_data[format_type]
                target = fmt_data.get("target_bytes_per_doc", None)
                meets_target = fmt_data.get("meets_target", None)
                status = "‚úÖ" if meets_target else ("N/A" if target is None else "‚ùå")
                target_str = f"<{target}" if target else "N/A"
                
                lines.append(
                    f"| {test_name} | {format_type} | {fmt_data['avg_bytes_per_doc']:.0f} | "
                    f"{fmt_data['median_bytes_per_doc']:.0f} | {status} |"
                )
    
    lines.extend([
        "",
        "## –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è",
        "",
    ])
    
    for test_name, test_data in results["tests"].items():
        lines.extend([
            f"### {test_name}",
            "",
        ])
        
        for format_type in ["markdown", "json"]:
            if format_type in test_data:
                fmt_data = test_data[format_type]
                lines.extend([
                    f"#### {format_type.upper()}",
                    "",
                    f"- **–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä:** {fmt_data['avg_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫",
                    f"- **–ú–µ–¥–∏–∞–Ω–∞:** {fmt_data['median_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫",
                ])
                
                if "min_bytes_per_doc" in fmt_data:
                    lines.extend([
                        f"- **–ú–∏–Ω:** {fmt_data['min_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫",
                        f"- **–ú–∞–∫—Å:** {fmt_data['max_bytes_per_doc']:.0f} –±–∞–π—Ç/–¥–æ–∫",
                    ])
                
                if "target_bytes_per_doc" in fmt_data:
                    lines.append(f"- **–¶–µ–ª—å:** <{fmt_data['target_bytes_per_doc']} –±–∞–π—Ç/–¥–æ–∫")
                    lines.append(f"- **–°—Ç–∞—Ç—É—Å:** {'‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ' if fmt_data.get('meets_target') else '‚ùå –ù–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ'}")
                
                lines.append("")
    
    output_file.write_text("\n".join(lines))
    print(f"üìÑ Markdown –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}")


async def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python test_response_size.py <vault_name>")
        print("\n–ü—Ä–∏–º–µ—Ä:")
        print("  python test_response_size.py test_vault")
        sys.exit(1)
    
    vault_name = sys.argv[1]
    
    results = await _run_response_size_test(vault_name)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path(__file__).parent
    json_file = output_dir / "response_size_test_results.json"
    markdown_file = output_dir / "response_size_test_results.md"
    
    save_results(results, json_file)
    create_markdown_report(results, markdown_file)


if __name__ == "__main__":
    asyncio.run(main())

