"""–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ v5.

–ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (P50, P95, P99) –¥–ª—è:
- Document-level –ø–æ–∏—Å–∫
- Chunk-level –ø–æ–∏—Å–∫ (vector, fts, hybrid)

–¶–µ–ª—å: P95 <400ms
"""

import asyncio
import json
import statistics
import time
from pathlib import Path
from typing import Any

from obsidian_kb.service_container import ServiceContainer
from obsidian_kb.types import RetrievalGranularity, SearchIntent, SearchRequest


# –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞
TEST_QUERIES = {
    "document_level": [
        "tags:python",
        "tags:meeting tags:important",
        "type:note",
        "tags:project tags:active",
        "type:meeting",
        "tags:todo tags:urgent",
        "type:document",
        "tags:reference",
        "tags:personal tags:private",
        "type:guide",
    ],
    "chunk_level_vector": [
        "Python async programming",
        "database optimization techniques",
        "REST API design patterns",
        "machine learning algorithms",
        "web development best practices",
        "distributed systems architecture",
        "security best practices",
        "performance optimization",
        "code review guidelines",
        "microservices architecture",
    ],
    "chunk_level_fts": [
        "Python async programming",
        "database optimization",
        "REST API design",
        "machine learning",
        "web development",
        "distributed systems",
        "security practices",
        "performance optimization",
        "code review",
        "microservices",
    ],
    "chunk_level_hybrid": [
        "Python async programming",
        "database optimization techniques",
        "REST API design patterns",
        "machine learning algorithms",
        "web development best practices",
        "distributed systems architecture",
        "security best practices",
        "performance optimization",
        "code review guidelines",
        "microservices architecture",
    ],
}


async def measure_search_performance(
    search_service: Any,
    vault_name: str,
    query: str,
    search_type: str,
    granularity: RetrievalGranularity,
    iterations: int = 10,
) -> list[float]:
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞.
    
    Args:
        search_service: –°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞
        vault_name: –ò–º—è vault'–∞
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ (vector, fts, hybrid)
        granularity: –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å (DOCUMENT, CHUNK)
        iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
    """
    times = []
    
    for _ in range(iterations):
        request = SearchRequest(
            vault_name=vault_name,
            query=query,
            limit=10,
            search_type=search_type if granularity == RetrievalGranularity.CHUNK else None,
            granularity=granularity,
            include_content=False,  # –ë–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
        )
        
        start_time = time.time()
        response = await search_service.search(request)
        elapsed_ms = (time.time() - start_time) * 1000
        
        times.append(elapsed_ms)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        await asyncio.sleep(0.1)
    
    return times


def calculate_percentiles(times: list[float]) -> dict[str, float]:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–π –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
    
    Args:
        times: –°–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å P50, P95, P99
    """
    if not times:
        return {"p50": 0.0, "p95": 0.0, "p99": 0.0}
    
    sorted_times = sorted(times)
    n = len(sorted_times)
    
    return {
        "p50": sorted_times[int(n * 0.50)],
        "p95": sorted_times[int(n * 0.95)] if n > 1 else sorted_times[0],
        "p99": sorted_times[int(n * 0.99)] if n > 1 else sorted_times[0],
        "min": min(times),
        "max": max(times),
        "mean": statistics.mean(times),
        "median": statistics.median(times),
    }


async def _run_performance_test(
    vault_name: str,
    iterations_per_query: int = 10,
    warmup_iterations: int = 3,
) -> dict[str, Any]:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞.
    
    Args:
        vault_name: –ò–º—è vault'–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        iterations_per_query: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        warmup_iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–æ–≥—Ä–µ–≤–æ—á–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ v5")
    print("=" * 80)
    print()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
    print("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤...")
    services = ServiceContainer()
    search_service = services.search_service
    
    # –†–∞–∑–æ–≥—Ä–µ–≤
    print(f"üî• –†–∞–∑–æ–≥—Ä–µ–≤ ({warmup_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π)...")
    warmup_query = "test"
    warmup_request = SearchRequest(
        vault_name=vault_name,
        query=warmup_query,
        limit=10,
        granularity=RetrievalGranularity.CHUNK,
    )
    for _ in range(warmup_iterations):
        await search_service.search(warmup_request)
        await asyncio.sleep(0.1)
    print("‚úÖ –†–∞–∑–æ–≥—Ä–µ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω\n")
    
    results: dict[str, Any] = {
        "vault_name": vault_name,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "iterations_per_query": iterations_per_query,
        "tests": {},
    }
    
    # –¢–µ—Å—Ç 1: Document-level –ø–æ–∏—Å–∫
    print("üìã –¢–µ—Å—Ç 1: Document-level –ø–æ–∏—Å–∫")
    print("-" * 80)
    doc_times = []
    for query in TEST_QUERIES["document_level"]:
        times = await measure_search_performance(
            search_service,
            vault_name,
            query,
            "hybrid",
            RetrievalGranularity.DOCUMENT,
            iterations_per_query,
        )
        doc_times.extend(times)
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}... | –°—Ä–µ–¥–Ω–µ–µ: {statistics.mean(times):.1f} –º—Å")
    
    doc_stats = calculate_percentiles(doc_times)
    results["tests"]["document_level"] = {
        "times": doc_times,
        "stats": doc_stats,
        "target_p95": 400.0,
        "meets_target": doc_stats["p95"] < 400.0,
    }
    print(f"\n  P50: {doc_stats['p50']:.1f} –º—Å")
    print(f"  P95: {doc_stats['p95']:.1f} –º—Å {'‚úÖ' if doc_stats['p95'] < 400 else '‚ùå'}")
    print(f"  P99: {doc_stats['p99']:.1f} –º—Å")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {doc_stats['mean']:.1f} –º—Å")
    print()
    
    # –¢–µ—Å—Ç 2: Chunk-level vector –ø–æ–∏—Å–∫
    print("üîç –¢–µ—Å—Ç 2: Chunk-level vector –ø–æ–∏—Å–∫")
    print("-" * 80)
    vector_times = []
    for query in TEST_QUERIES["chunk_level_vector"]:
        times = await measure_search_performance(
            search_service,
            vault_name,
            query,
            "vector",
            RetrievalGranularity.CHUNK,
            iterations_per_query,
        )
        vector_times.extend(times)
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}... | –°—Ä–µ–¥–Ω–µ–µ: {statistics.mean(times):.1f} –º—Å")
    
    vector_stats = calculate_percentiles(vector_times)
    results["tests"]["chunk_level_vector"] = {
        "times": vector_times,
        "stats": vector_stats,
        "target_p95": 400.0,
        "meets_target": vector_stats["p95"] < 400.0,
    }
    print(f"\n  P50: {vector_stats['p50']:.1f} –º—Å")
    print(f"  P95: {vector_stats['p95']:.1f} –º—Å {'‚úÖ' if vector_stats['p95'] < 400 else '‚ùå'}")
    print(f"  P99: {vector_stats['p99']:.1f} –º—Å")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {vector_stats['mean']:.1f} –º—Å")
    print()
    
    # –¢–µ—Å—Ç 3: Chunk-level FTS –ø–æ–∏—Å–∫
    print("üîé –¢–µ—Å—Ç 3: Chunk-level FTS –ø–æ–∏—Å–∫")
    print("-" * 80)
    fts_times = []
    for query in TEST_QUERIES["chunk_level_fts"]:
        times = await measure_search_performance(
            search_service,
            vault_name,
            query,
            "fts",
            RetrievalGranularity.CHUNK,
            iterations_per_query,
        )
        fts_times.extend(times)
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}... | –°—Ä–µ–¥–Ω–µ–µ: {statistics.mean(times):.1f} –º—Å")
    
    fts_stats = calculate_percentiles(fts_times)
    results["tests"]["chunk_level_fts"] = {
        "times": fts_times,
        "stats": fts_stats,
        "target_p95": 400.0,
        "meets_target": fts_stats["p95"] < 400.0,
    }
    print(f"\n  P50: {fts_stats['p50']:.1f} –º—Å")
    print(f"  P95: {fts_stats['p95']:.1f} –º—Å {'‚úÖ' if fts_stats['p95'] < 400 else '‚ùå'}")
    print(f"  P99: {fts_stats['p99']:.1f} –º—Å")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {fts_stats['mean']:.1f} –º—Å")
    print()
    
    # –¢–µ—Å—Ç 4: Chunk-level hybrid –ø–æ–∏—Å–∫
    print("üîÄ –¢–µ—Å—Ç 4: Chunk-level hybrid –ø–æ–∏—Å–∫")
    print("-" * 80)
    hybrid_times = []
    for query in TEST_QUERIES["chunk_level_hybrid"]:
        times = await measure_search_performance(
            search_service,
            vault_name,
            query,
            "hybrid",
            RetrievalGranularity.CHUNK,
            iterations_per_query,
        )
        hybrid_times.extend(times)
        print(f"  –ó–∞–ø—Ä–æ—Å: {query[:50]}... | –°—Ä–µ–¥–Ω–µ–µ: {statistics.mean(times):.1f} –º—Å")
    
    hybrid_stats = calculate_percentiles(hybrid_times)
    results["tests"]["chunk_level_hybrid"] = {
        "times": hybrid_times,
        "stats": hybrid_stats,
        "target_p95": 400.0,
        "meets_target": hybrid_stats["p95"] < 400.0,
    }
    print(f"\n  P50: {hybrid_stats['p50']:.1f} –º—Å")
    print(f"  P95: {hybrid_stats['p95']:.1f} –º—Å {'‚úÖ' if hybrid_stats['p95'] < 400 else '‚ùå'}")
    print(f"  P99: {hybrid_stats['p99']:.1f} –º—Å")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {hybrid_stats['mean']:.1f} –º—Å")
    print()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("=" * 80)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    print()
    
    all_meet_target = all(
        test["meets_target"] for test in results["tests"].values()
    )
    
    print(f"–¶–µ–ª–µ–≤–æ–π P95: <400 –º—Å")
    print()
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for test_name, test_data in results["tests"].items():
        status = "‚úÖ" if test_data["meets_target"] else "‚ùå"
        print(f"  {test_name:30} P95: {test_data['stats']['p95']:6.1f} –º—Å {status}")
    
    print()
    if all_meet_target:
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–®–õ–ò –¶–ï–õ–ï–í–û–ô –ü–û–†–û–ì P95 <400 –º—Å")
    else:
        print("‚ùå –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–®–õ–ò –¶–ï–õ–ï–í–û–ô –ü–û–†–û–ì")
    
    # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
    await services.cleanup()
    
    return results


def save_results(results: dict[str, Any], output_file: Path) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON."""
    output_file.write_text(json.dumps(results, indent=2, ensure_ascii=False))
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")


def create_markdown_report(results: dict[str, Any], output_file: Path) -> None:
    """–°–æ–∑–¥–∞–Ω–∏–µ markdown –æ—Ç—á—ë—Ç–∞."""
    lines = [
        "# –û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ v5",
        "",
        f"**–î–∞—Ç–∞:** {results['timestamp']}",
        f"**Vault:** {results['vault_name']}",
        f"**–ò—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ –∑–∞–ø—Ä–æ—Å:** {results['iterations_per_query']}",
        "",
        "## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã",
        "",
        "| –¢–∏–ø –ø–æ–∏—Å–∫–∞ | P50 (–º—Å) | P95 (–º—Å) | P99 (–º—Å) | –°—Ä–µ–¥–Ω–µ–µ (–º—Å) | –¶–µ–ª—å P95 <400–º—Å |",
        "|-------------|----------|----------|----------|--------------|-----------------|",
    ]
    
    for test_name, test_data in results["tests"].items():
        stats = test_data["stats"]
        status = "‚úÖ" if test_data["meets_target"] else "‚ùå"
        lines.append(
            f"| {test_name} | {stats['p50']:.1f} | {stats['p95']:.1f} | "
            f"{stats['p99']:.1f} | {stats['mean']:.1f} | {status} |"
        )
    
    lines.extend([
        "",
        "## –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è",
        "",
    ])
    
    for test_name, test_data in results["tests"].items():
        stats = test_data["stats"]
        lines.extend([
            f"### {test_name}",
            "",
            f"- **P50:** {stats['p50']:.1f} –º—Å",
            f"- **P95:** {stats['p95']:.1f} –º—Å",
            f"- **P99:** {stats['p99']:.1f} –º—Å",
            f"- **–°—Ä–µ–¥–Ω–µ–µ:** {stats['mean']:.1f} –º—Å",
            f"- **–ú–µ–¥–∏–∞–Ω–∞:** {stats['median']:.1f} –º—Å",
            f"- **–ú–∏–Ω:** {stats['min']:.1f} –º—Å",
            f"- **–ú–∞–∫—Å:** {stats['max']:.1f} –º—Å",
            f"- **–¶–µ–ª–µ–≤–æ–π P95:** {'‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ' if test_data['meets_target'] else '‚ùå –ù–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ'}",
            "",
        ])
    
    output_file.write_text("\n".join(lines))
    print(f"üìÑ Markdown –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}")


async def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python test_performance_v5.py <vault_name> [iterations]")
        print("\n–ü—Ä–∏–º–µ—Ä:")
        print("  python test_performance_v5.py test_vault 10")
        sys.exit(1)
    
    vault_name = sys.argv[1]
    iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    results = await _run_performance_test(vault_name, iterations_per_query=iterations)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path(__file__).parent
    json_file = output_dir / "performance_test_results.json"
    markdown_file = output_dir / "performance_test_results.md"
    
    save_results(results, json_file)
    create_markdown_report(results, markdown_file)


if __name__ == "__main__":
    asyncio.run(main())

