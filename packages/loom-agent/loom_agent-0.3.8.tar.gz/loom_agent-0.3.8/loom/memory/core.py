"""
LoomMemory Storage Engine
"""
from typing import List, Optional, Dict, Any, Tuple
from collections import defaultdict
from datetime import datetime
import math

from .types import (
    MemoryUnit, MemoryTier, MemoryType,
    MemoryQuery, ContextProjection
)
from loom.config.memory import MemoryConfig
from .factory import create_vector_store, create_embedding_provider
from .vector_store import VectorStoreProvider
from .embedding import EmbeddingProvider
from loom.projection.profiles import ProjectionMode, ProjectionConfig


class LoomMemory:
    """
    Tiered Memory Storage System.
    
    L1 (Raw IO): Circular buffer for recent raw interactions.
    L2 (Working): Task-specific working memory.
    L3 (Session): Session-scoped history.
    L4 (Global): Persistent global knowledge.
    """
    
    def __init__(
        self,
        node_id: str,
        max_l1_size: int = 50,
        config: Optional[MemoryConfig] = None
    ):
        self.node_id = node_id
        self.config = config or MemoryConfig()
        # Use passed max_l1_size parameter, not config default
        self.max_l1_size = max_l1_size

        # Tiered Storage
        self._l1_buffer: List[MemoryUnit] = []           # Circular buffer
        self._l2_working: List[MemoryUnit] = []          # Working memory list
        self._l3_session: Dict[str, List[MemoryUnit]] = defaultdict(list) # By session_id
        self._l4_global: List[MemoryUnit] = []           # Mock for VectorDB

        # Indexes
        self._id_index: Dict[str, MemoryUnit] = {}
        self._type_index: Dict[MemoryType, List[str]] = defaultdict(list)

        # Vector Store & Embedding (Pluggable)
        self.vector_store: Optional[VectorStoreProvider] = create_vector_store(
            self.config.vector_store
        )
        self.embedding_provider: Optional[EmbeddingProvider] = create_embedding_provider(
            self.config.embedding
        ) if self.vector_store else None

        # L4 Compressor (Optional)
        self.l4_compressor: Optional['L4Compressor'] = None
    
    async def add(self, unit: MemoryUnit) -> str:
        """Add a memory unit to the appropriate tier."""
        # Ensure source_node is set
        unit.source_node = unit.source_node or self.node_id

        # Add to Tier
        if unit.tier == MemoryTier.L1_RAW_IO:
            self._l1_buffer.append(unit)
            if len(self._l1_buffer) > self.max_l1_size:
                self._evict_from_l1()

        elif unit.tier == MemoryTier.L2_WORKING:
            self._l2_working.append(unit)

        elif unit.tier == MemoryTier.L3_SESSION:
            session_id = unit.metadata.get("session_id", "default")
            self._l3_session[session_id].append(unit)

        elif unit.tier == MemoryTier.L4_GLOBAL:
            self._l4_global.append(unit)

            # Auto-vectorize L4 content if enabled
            if self.config.auto_vectorize_l4 and self.vector_store and self.embedding_provider:
                await self._vectorize_unit(unit)

            # Check if L4 compression is needed
            if self.l4_compressor and await self.l4_compressor.should_compress(self._l4_global):
                await self._compress_l4()

        # Update Indexes
        self._id_index[unit.id] = unit
        self._type_index[unit.type].append(unit.id)

        return unit.id

    def add_sync(self, unit: MemoryUnit) -> str:
        """Synchronously add a memory unit (for projection, skips vectorization)."""
        # Ensure source_node is set
        unit.source_node = unit.source_node or self.node_id

        # Add to Tier
        if unit.tier == MemoryTier.L1_RAW_IO:
            self._l1_buffer.append(unit)
            if len(self._l1_buffer) > self.max_l1_size:
                self._evict_from_l1()

        elif unit.tier == MemoryTier.L2_WORKING:
            self._l2_working.append(unit)

        elif unit.tier == MemoryTier.L3_SESSION:
            session_id = unit.metadata.get("session_id", "default")
            self._l3_session[session_id].append(unit)

        elif unit.tier == MemoryTier.L4_GLOBAL:
            self._l4_global.append(unit)
            # Note: Skips vectorization for sync operation

        # Update Indexes
        self._id_index[unit.id] = unit
        self._type_index[unit.type].append(unit.id)

        return unit.id
    
    def get(self, unit_id: str) -> Optional[MemoryUnit]:
        """Retrieve a memory unit by ID."""
        return self._id_index.get(unit_id)
    
    async def query(self, q: MemoryQuery) -> List[MemoryUnit]:
        """
        Query memory units based on criteria.
        """
        results = []

        # 1. Collect from requested tiers
        target_tiers = q.tiers or [
            MemoryTier.L1_RAW_IO,
            MemoryTier.L2_WORKING,
            MemoryTier.L3_SESSION,
            MemoryTier.L4_GLOBAL
        ]
        
        for tier in target_tiers:
            if tier == MemoryTier.L1_RAW_IO:
                results.extend(self._l1_buffer)
            elif tier == MemoryTier.L2_WORKING:
                results.extend(self._l2_working)
            elif tier == MemoryTier.L3_SESSION:
                for session_units in self._l3_session.values():
                    results.extend(session_units)
            elif tier == MemoryTier.L4_GLOBAL:
                results.extend(self._l4_global)
        
        # 2. Filter by Type
        if q.types:
            results = [u for u in results if u.type in q.types]
        
        # 3. Filter by Node ID
        if q.node_ids:
            results = [u for u in results if u.source_node in q.node_ids]
        
        # 4. Filter by Time
        if q.since:
            results = [u for u in results if u.created_at >= q.since]
        if q.until:
            results = [u for u in results if u.created_at <= q.until]
        
        # 5. Semantic Search (L4 Only for MVP)
        if q.query_text and MemoryTier.L4_GLOBAL in target_tiers:
            # Only perform semantic search on L4 items within the result set
            l4_candidates = [u for u in results if u.tier == MemoryTier.L4_GLOBAL]
            others = [u for u in results if u.tier != MemoryTier.L4_GLOBAL]

            scored_l4 = await self._semantic_search(q.query_text, l4_candidates, q.top_k)
            # For now, just append top K L4 matches to others.
            # Ideally, we might want to filter L4 to ONLY top K.
            # Strategy: If semantic search is requested, we PRIORITIZE semantic matches.
            results = others + scored_l4
        
        # 6. Sort
        reverse = q.descending
        # Dynamic getattr for sort key
        results.sort(
            key=lambda u: getattr(u, q.sort_by, u.created_at),
            reverse=reverse
        )
        
        return results
    
    def promote_to_l4(self, unit_id: str):
        """Promote a memory unit to L4 Global persistence."""
        unit = self.get(unit_id)
        if not unit:
            return
        
        # Remove from current tier if necessary (e.g. L2)
        if unit.tier == MemoryTier.L2_WORKING:
            if unit in self._l2_working:
                self._l2_working.remove(unit)
        
        # Update tier and add to L4
        unit.tier = MemoryTier.L4_GLOBAL
        if unit not in self._l4_global:
            self._l4_global.append(unit)
            
    def clear_working(self):
        """Clear L2 Working Memory."""
        for unit in self._l2_working:
             self._remove_from_index(unit)
        self._l2_working.clear()

    def _evict_from_l1(self):
        """
        Evict least important + least recently used item from L1 buffer.
        Uses importance-weighted LRU policy.
        """
        if not self._l1_buffer:
            return

        try:
            # Score = importance * recency_factor
            now = datetime.now()
            scored = []

            for unit in self._l1_buffer:
                age_seconds = (now - unit.created_at).total_seconds()
                # Recency factor decays over hours (1.0 at 0 hours, 0.5 at 1 hour, etc.)
                recency_factor = 1.0 / (1.0 + age_seconds / 3600)
                score = unit.importance * recency_factor
                scored.append((score, unit))

            # Sort by score (lowest first)
            scored.sort(key=lambda x: x[0])

            # Evict lowest scored item
            victim = scored[0][1]
            self._l1_buffer.remove(victim)
            self._remove_from_index(victim)
        except Exception as e:
            # Fallback to simple FIFO if scoring fails
            if self._l1_buffer:
                removed = self._l1_buffer.pop(0)
                self._remove_from_index(removed)

    async def create_projection(
        self,
        instruction: str,
        total_budget: int = 2000,
        mode: Optional[ProjectionMode] = None,
        include_plan: bool = True,
        include_facts: bool = True
    ) -> ContextProjection:
        """åˆ›å»ºä¸Šä¸‹æ–‡æŠ•å½±ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤
            total_budget: æ€» token é¢„ç®—ï¼ˆé»˜è®¤2000ï¼‰
            mode: æŠ•å½±æ¨¡å¼ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
            include_plan: æ˜¯å¦åŒ…å«çˆ¶è®¡åˆ’
            include_facts: æ˜¯å¦åŒ…å«ç›¸å…³äº‹å®

        Returns:
            ä¸Šä¸‹æ–‡æŠ•å½±å¯¹è±¡
        """
        # 1. è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if mode is None:
            mode = self._detect_mode(instruction)

        # 2. è·å–é…ç½®
        config = ProjectionConfig.from_mode(mode)

        # 3. åˆ›å»ºæŠ•å½±å¯¹è±¡
        projection = ContextProjection(
            instruction=instruction,
            lineage=[self.node_id]
        )

        # 4. æå– VIP å†…å®¹ï¼ˆplanï¼‰
        if include_plan:
            plans = [u for u in self._l2_working if u.type == MemoryType.PLAN]
            if plans:
                projection.parent_plan = str(plans[-1].content)

        # 5. æå– L4 factsï¼ˆå¸¦è¯­ä¹‰ç›¸å…³æ€§è¯„åˆ†ï¼‰
        if include_facts and self._l4_global:
            scored_facts = await self._score_facts(
                instruction=instruction,
                facts=self._l4_global,
                max_count=config.max_l4_facts,
                config=config
            )
            projection.relevant_facts = scored_facts

        return projection

    def get_statistics(self) -> Dict[str, Any]:
        """Get current memory statistics."""
        return {
            "l1_size": len(self._l1_buffer),
            "l2_size": len(self._l2_working),
            "l3_sessions": len(self._l3_session),
            "l4_size": len(self._l4_global),
            "total_units": len(self._id_index),
            "types": {
                t.value: len(ids) 
                for t, ids in self._type_index.items()
            }
        }

    def _remove_from_index(self, unit: MemoryUnit):
        """Helper to remove unit from indexes."""
        if unit.id in self._id_index:
            del self._id_index[unit.id]
        if unit.id in self._type_index[unit.type]:
            self._type_index[unit.type].remove(unit.id)

    async def _semantic_search(
        self,
        query: str,
        candidates: List[MemoryUnit],
        top_k: int
    ) -> List[MemoryUnit]:
        """
        Semantic Search using vector store if available, otherwise fallback to keyword matching.
        """
        # Use vector store if available
        if self.vector_store and self.embedding_provider:
            try:
                # Generate query embedding
                query_embedding = await self.embedding_provider.embed_text(query)

                # Search vector store
                results = await self.vector_store.search(
                    query_embedding=query_embedding,
                    top_k=top_k
                )

                # Map results back to MemoryUnits
                matched_units = []
                for result in results:
                    unit = self.get(result.id)
                    if unit and unit in candidates:
                        matched_units.append(unit)

                return matched_units
            except Exception as e:
                # Fallback to keyword matching on error
                pass

        # Fallback: Simple keyword matching
        scored = []
        query_lower = query.lower()

        for unit in candidates:
            score = 0.0
            content_str = str(unit.content).lower()

            if query_lower in content_str:
                score = 1.0

            final_score = score + (unit.importance * 0.1)
            scored.append((final_score, unit))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [unit for _, unit in scored[:top_k]]

    async def _vectorize_unit(self, unit: MemoryUnit):
        """
        Generate and store embedding for a memory unit.
        """
        if not self.vector_store or not self.embedding_provider:
            return

        try:
            # Generate embedding
            text = str(unit.content)
            embedding = await self.embedding_provider.embed_text(text)

            # Store in vector database
            await self.vector_store.add(
                id=unit.id,
                text=text,
                embedding=embedding,
                metadata={
                    "tier": unit.tier.name,
                    "type": unit.type.value,
                    "importance": unit.importance,
                    "source_node": unit.source_node
                }
            )

            # Store embedding in unit for future use
            unit.embedding = embedding
        except Exception as e:
            # Log error but don't fail the add operation
            pass

    def _detect_mode(self, instruction: str) -> ProjectionMode:
        """ç®€å•çš„æ¨¡å¼æ£€æµ‹ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼Œæ”¯æŒä¸­è‹±æ–‡ï¼‰

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤

        Returns:
            æ£€æµ‹åˆ°çš„æŠ•å½±æ¨¡å¼
        """
        instruction_lower = instruction.lower()

        # æ£€æµ‹ DEBUG æ¨¡å¼ï¼ˆè‹±æ–‡ + ä¸­æ–‡å…³é”®è¯ï¼Œå„15ä¸ªï¼‰
        debug_keywords = [
            # è‹±æ–‡ (15ä¸ª)
            'error', 'fix', 'debug', 'retry', 'bug', 'exception', 'failed', 'failure',
            'crash', 'broken', 'issue', 'troubleshoot', 'diagnose', 'resolve', 'repair',
            # ä¸­æ–‡ (15ä¸ª)
            'é”™è¯¯', 'ä¿®å¤', 'è°ƒè¯•', 'é‡è¯•', 'å¤±è´¥', 'å¼‚å¸¸', 'é—®é¢˜', 'bug',
            'å´©æºƒ', 'æ•…éšœ', 'æ’æŸ¥', 'è¯Šæ–­', 'è§£å†³', 'ä¿®ç†', 'å‡ºé”™'
        ]
        if any(kw in instruction_lower for kw in debug_keywords):
            return ProjectionMode.DEBUG

        # æ£€æµ‹ ANALYTICAL æ¨¡å¼ï¼ˆè‹±æ–‡ + ä¸­æ–‡å…³é”®è¯ï¼Œå„15ä¸ªï¼‰
        analytical_keywords = [
            # è‹±æ–‡ (15ä¸ª)
            'analyze', 'analyse', 'evaluate', 'research', 'investigate', 'study',
            'examine', 'review', 'assess', 'compare', 'measure', 'benchmark',
            'profile', 'inspect', 'survey',
            # ä¸­æ–‡ (15ä¸ª)
            'åˆ†æ', 'è¯„ä¼°', 'ç ”ç©¶', 'è°ƒæŸ¥', 'æ¢ç´¢',
            'æ£€éªŒ', 'å®¡æŸ¥', 'å¯¹æ¯”', 'æ¯”è¾ƒ', 'æµ‹é‡', 'æµ‹è¯•', 'è€ƒå¯Ÿ', 'è§‚å¯Ÿ', 'æŸ¥çœ‹', 'ç»Ÿè®¡'
        ]
        if any(kw in instruction_lower for kw in analytical_keywords):
            return ProjectionMode.ANALYTICAL

        # æ£€æµ‹ CONTEXTUAL æ¨¡å¼ï¼ˆè‹±æ–‡ + ä¸­æ–‡å…³é”®è¯ï¼Œå„15ä¸ªï¼‰
        contextual_keywords = [
            # è‹±æ–‡ (15ä¸ª)
            'continue', 'context', 'previous', 'earlier', 'before', 'last',
            'resume', 'recall', 'remember', 'mentioned', 'discussed', 'talked',
            'said', 'above', 'prior',
            # ä¸­æ–‡ (15ä¸ª)
            'ç»§ç»­', 'ä¸Šä¸‹æ–‡', 'ä¹‹å‰', 'åˆšæ‰', 'å‰é¢', 'ä¸Šæ¬¡', 'æ¥ç€',
            'æ¢å¤', 'å›å¿†', 'è®°å¾—', 'æåˆ°', 'è®¨è®ºè¿‡', 'è¯´è¿‡', 'ä¸Šé¢', 'æœ€è¿‘'
        ]
        if any(kw in instruction_lower for kw in contextual_keywords):
            return ProjectionMode.CONTEXTUAL

        # æ£€æµ‹ MINIMAL æ¨¡å¼ï¼ˆéå¸¸çŸ­çš„æŒ‡ä»¤ï¼‰
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        def has_chinese(text):
            return any('\u4e00' <= char <= '\u9fff' for char in text)

        instruction_stripped = instruction.strip()

        if has_chinese(instruction_stripped):
            # ä¸­æ–‡æˆ–ä¸­è‹±æ··åˆï¼šæŒ‰å­—ç¬¦æ•°åˆ¤æ–­ï¼ˆ< 8ä¸ªå­—ç¬¦ï¼‰
            if len(instruction_stripped) < 8:
                return ProjectionMode.MINIMAL
        else:
            # çº¯è‹±æ–‡ï¼šæŒ‰å•è¯æ•°åˆ¤æ–­ï¼ˆ< 3ä¸ªå•è¯ï¼‰
            word_count = len(instruction_stripped.split())
            if word_count < 3:
                return ProjectionMode.MINIMAL

        # é»˜è®¤ï¼šSTANDARD æ¨¡å¼
        return ProjectionMode.STANDARD

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ (0-1)
        """
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0

        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    async def _score_facts(
        self,
        instruction: str,
        facts: List[MemoryUnit],
        max_count: int,
        config: ProjectionConfig
    ) -> List[MemoryUnit]:
        """è¯„åˆ†å¹¶é€‰æ‹© facts

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤
            facts: å€™é€‰ facts
            max_count: æœ€å¤§é€‰æ‹©æ•°é‡
            config: æŠ•å½±é…ç½®

        Returns:
            è¯„åˆ†åçš„ top K facts
        """
        if not facts:
            return []

        # å¦‚æœæœ‰ embedding providerï¼Œä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦
        if self.embedding_provider:
            return await self._score_facts_semantic(instruction, facts, max_count, config)
        else:
            # é™çº§ï¼šåªæŒ‰ importance æ’åº
            sorted_facts = sorted(
                facts,
                key=lambda f: f.importance,
                reverse=True
            )
            return sorted_facts[:max_count]

    async def _score_facts_semantic(
        self,
        instruction: str,
        facts: List[MemoryUnit],
        max_count: int,
        config: ProjectionConfig
    ) -> List[MemoryUnit]:
        """ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ† facts

        Args:
            instruction: ä»»åŠ¡æŒ‡ä»¤
            facts: å€™é€‰ facts
            max_count: æœ€å¤§é€‰æ‹©æ•°é‡
            config: æŠ•å½±é…ç½®

        Returns:
            è¯„åˆ†åçš„ top K facts
        """
        if not facts or not self.embedding_provider:
            return []

        try:
            # è®¡ç®— instruction çš„ embedding
            instruction_emb = await self.embedding_provider.embed_text(instruction)

            # è®¡ç®—æ¯ä¸ª fact çš„åˆ†æ•°
            scored = []
            for fact in facts:
                # å¦‚æœ fact å·²æœ‰ embeddingï¼Œä½¿ç”¨å®ƒ
                if fact.embedding:
                    fact_emb = fact.embedding
                else:
                    # å¦åˆ™å®æ—¶è®¡ç®—
                    fact_emb = await self.embedding_provider.embed_text(str(fact.content))

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(instruction_emb, fact_emb)

                # æ··åˆè¯„åˆ†ï¼šimportance + relevance
                score = (
                    config.importance_weight * fact.importance +
                    config.relevance_weight * similarity
                )
                scored.append((score, fact))

            # æ’åºå¹¶è¿”å› top K
            scored.sort(key=lambda x: x[0], reverse=True)
            return [fact for _, fact in scored[:max_count]]

        except Exception as e:
            # å‡ºé”™æ—¶é™çº§åˆ°åªæŒ‰ importance æ’åº
            sorted_facts = sorted(
                facts,
                key=lambda f: f.importance,
                reverse=True
            )
            return sorted_facts[:max_count]

    def enable_l4_compression(
        self,
        llm_provider,
        threshold: int = 150,
        similarity_threshold: float = 0.75,
        min_cluster_size: int = 3
    ):
        """å¯ç”¨L4è‡ªåŠ¨å‹ç¼©

        Args:
            llm_provider: LLMæä¾›è€…ï¼Œç”¨äºæ€»ç»“clusters
            threshold: è§¦å‘å‹ç¼©çš„factsæ•°é‡é˜ˆå€¼
            similarity_threshold: èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
            min_cluster_size: æœ€å°èšç±»å¤§å°
        """
        from .compression import L4Compressor

        self.l4_compressor = L4Compressor(
            llm_provider=llm_provider,
            embedding_provider=self.embedding_provider,
            threshold=threshold,
            similarity_threshold=similarity_threshold,
            min_cluster_size=min_cluster_size
        )

    async def _compress_l4(self):
        """æ‰§è¡ŒL4å‹ç¼©"""
        print(f"ğŸ—œï¸  L4å‹ç¼©å¼€å§‹ï¼šå½“å‰{len(self._l4_global)}ä¸ªfacts")

        # æ‰§è¡Œå‹ç¼©
        compressed = await self.l4_compressor.compress(self._l4_global)

        # æ›´æ–°ç´¢å¼•ï¼šç§»é™¤æ—§çš„facts
        for fact in self._l4_global:
            self._remove_from_index(fact)

        # æ›¿æ¢L4
        self._l4_global = compressed

        # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–°çš„facts
        for fact in compressed:
            self._id_index[fact.id] = fact
            self._type_index[fact.type].append(fact.id)

        print(f"âœ… L4å‹ç¼©å®Œæˆï¼šå‹ç¼©å{len(self._l4_global)}ä¸ªfacts")
