# Model Registry
The model registry holds information and settings for each of the models currently supported by clembench.  
It is stored as a JSON file in the `backends` directory. This file contains a list of model entry objects.  
## Model Entry Components
Each object in the list contains these mandatory key/values:  

| Key             | Type            | Description                                                                                                                                                                                                                                                     | Example                                                                        |
|-----------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| `model_name`    | string          | The name the model is identified by in clembench. This is also the specific version name of the model to be used by the backends.                                                                                                                               | `"Llama-3.3-70B-Instruct"`                                                     |
| `backend`       | string          | The name of the backend that handles this model. This must match the name of the backend python module in `/backends` without `_api.py`.                                                                                                                        | `"huggingface_local"`                                                          |
| `release_date`  | string          | The release date of the model, in YYYY-MM-DD format. This is used for filtering of benchmark results.                                                                                                                                                           | `"2023-11-13"`                                                                 |
| `open_weight`   | bool            | If the model's weights are publically available. This is used for filtering of benchmark results.                                                                                                                                                               |                                                                                |
| `parameters`    | string          | The total parameter count of the model. Allows abbreviations, for example "B" for billions. This is the actual, total number of parameters, not just the active parameters during inference for MoE models. This is used for filtering of benchmark results.    | `"7B"`                                                                         |
| `languages`     | list of strings | The languages officially supported by the model, in two-letter short form.                                                                                                                                                                                      | `["en", "fr", "de", "it", "es"]`                                               |
| `context_size`  | string          | The overall context size of the model in number of tokens. Allows abbreviations, for example "k" for thousands. Approximate numbers can be used. This is used for filtering of benchmark results, and not for determining this value for inference during runs. | `"16k"`                                                                        |
| `license`       | object          | License information for the model. Contains two keys, listed below.                                                                                                                                                                                             | `{"name": "Apache 2.0", "url": "https://www.apache.org/licenses/LICENSE-2.0"}` |
| `license[name]` | string          | Name of the license.                                                                                                                                                                                                                                            | `"Apache 2.0"`                                                                 |
| `license[url]`  | string          | URL to access the license terms.                                                                                                                                                                                                                                | `"https://www.apache.org/licenses/LICENSE-2.0"`                                |
| `model_config`  | object          | This key holds any backend- and model-specific values and information. The values for each backend are explained below in this ReadMe. May be left empty if the model/backend does not require additional values.                                               | `{}`                                                                           |

All of the following backend-specific keys/values ***must*** be inside the `model_config` object.
### Local Huggingface Backend
The python module of this backend is `clemcore/backends/huggingface_local_api.py.`  
This backend requires these **mandatory** key/values:  

| Key                     | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Example               |
|-------------------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|
| `huggingface_id`        | string | The full huggingface model ID; huggingface user name / model name. NOTE: This key is **not part of the model config**, but a key for the model entry object!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `"01-ai/Yi-34B-Chat"` |
| `premade_chat_template` | bool   | If `true`, the chat template that is applied for generation is loaded from the model repository on huggingface. If `false`, the value of `custom_chat_template` will be used if defined, otherwise a generic chat template is applied (highly discouraged).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                       |
| `eos_to_cull`           | string | This is a regular expression matching the model's EOS token or longer substrings at the end of the model's outputs. These need to be removed by the backend to assure proper processing by clembench. Make sure that this contains a proper python regular expression string matching the intended substrings. Characters and sequences that can be parsed as python regular expression special characters or special sequences, but are part of model special token strings or chat templates need to be properly escaped by "`\\`"! Note the double backslash "`\\`", which is necessary to properly handle the escape between JSON and python! This is mandatory as there are models that do not define this in their tokenizer configuration. | `"<\\\|im_end\\\|>"`  |

The following key/values are **optional**, but should be defined for models that require them for proper functioning:  

| Key                    | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Example                                                                                                                                                                                                                                                                                                       |
|------------------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `requires_api_key`     | bool   | If `true`, the backend will load a huggingface api access key/token from `key.json`, which is required to access 'gated' models like Meta's Llama2.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                               |
| `custom_chat_template` | string | A jinja2 template string of the chat template to be applied for this model. This should be set if `premade_chat_template` is `false` for the model, as the generic fallback chat template that will be used if this is not defined is likely to lead to bad model performance.                                                                                                                                                                                                                                                                                                                                                                                                  | `"{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<\|user\|>\n' + message['content'] }}\n{% elif message['role'] == 'assistant' %}\n{{ '<\|assistant\|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last %}\n{{ '<\|assistant\|>' }}\n{% endif %}\n{% endfor %}"` |
| `slow_tokenizer`       | bool   | If `true`, the backend will load the model's tokenizer with `use_fast=False`. Some models require the use of a 'slow' tokenizer class to assure proper tokenization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                               |
| `output_split_prefix`  | string | The model's raw output will be rsplit using this string, and the remaining output following this string will be considered the model output. This is necessary for some models that decode tokens differently than they encode them, to assure that the prompt is properly removed from model responses.                                                                                                                                                                                                                                                                                                                                                                        | `"assistant\n"`                                                                                                                                                                                                                                                                                               |
| `padding_side`         | string | Determines the padding side for batch processing. Should be `"left"` for essentially all models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `"left"`                                                                                                                                                                                                                                                                                                      |
| `cot_output`           | bool   | This must be `true` for models that output their CoT/reasoning/thinking/etc at the start of their normal text output, as all standard-code `transformers`-compatible CoT models from HuggingFace do. If `true`, the `cot_end_tag` key must also be set. This value being `true` also bypasses the `max_tokens` setting, instead using the maximum context size of the model to assure CoT completion.                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                               |
| `cot_start_tag`        | string | The model's CoT start delimiting string, to be removed from outputs. This is only necessary if the model does output the tag at the start of its replies, usually when the start tag is not part of the model's chat template.                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `<think>`                                                                                                                                                                                                                                                                                                     |
| `cot_end_tag`          | string | This is a regular expression matching the model's CoT end tag or phrase, which separates CoT content from the answer/reply content. Make sure that this contains a proper python regular expression string matching the intended substrings. Characters and sequences that can be parsed as python regular expression special characters or special sequences, but are part of model special token strings or chat templates need to be properly escaped by "`\\`"! Note the double backslash "`\\`", which is necessary to properly handle the escape between JSON and python! This is mandatory as there are models that do not define this in their tokenizer configuration. | `"</think>"`                                                                                                                                                                                                                                                                                                  |
| `cot_bypass`           | string | If present, the string value is inserted at the start of the model's reply message, preventing it from producing CoT/reasoning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | `"<\|channel\|>analysis<\|message\|><\|end\|><\|start\|>assistant<\|channel\|>final<\|message\|>"`                                                                                                                                                                                                            |
| `cot_effort`           | string | A reasoning effort string to be passed as generation argument. GPT-OSS models use this string to control the amount of CoT tokens - other models that have trained CoT effort control might not use this argument.                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `"low"`                                                                                                                                                                                                                                                                                                       |


### llama.cpp Backend
The python module of this backend is `clemcore/backends/llamacpp_api.py.`  
This backend requires these **mandatory** key/values:  

| Key                     | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Example                |
|-------------------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|
| `huggingface_id`        | string | The full huggingface model ID; huggingface user name / model name. NOTE: This key is **not part of the model config**, but a key for the model entry object!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `"01-ai/Yi-34B-Chat"`  |
| `filename`              | string | Filename matching string, required to properly load multi-file sharded GGUF model weights. Allows some regular expression elements, like `*` wildcards. If the HuggingFace repository has subdirectories (usual for repositories that contain weights at different quantizations), these must be included in this path string.                                                                                                                                                                                                                                                                                                                                                                                                                    | `"*/Q8_0/*Q8_0*.gguf"` |
| `premade_chat_template` | bool   | If `true`, the chat template that is applied for generation is loaded from the model repository on huggingface. If `false`, the value of `custom_chat_template` will be used if defined, otherwise a generic chat template is applied (highly discouraged).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                        |
| `eos_to_cull`           | string | This is a regular expression matching the model's EOS token or longer substrings at the end of the model's outputs. These need to be removed by the backend to assure proper processing by clembench. Make sure that this contains a proper python regular expression string matching the intended substrings. Characters and sequences that can be parsed as python regular expression special characters or special sequences, but are part of model special token strings or chat templates need to be properly escaped by "`\\`"! Note the double backslash "`\\`", which is necessary to properly handle the escape between JSON and python! This is mandatory as there are models that do not define this in their tokenizer configuration. | `"<\\\|im_end\\\|>"`   |

The following key/values are **optional**, but should be defined for models that require them for proper functioning:  

| Key                    | Type   | Description                                                                                                                                                                                                                                                                                              | Example                                                                                                                                                                                                                                                                                                       |
|------------------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `requires_api_key`     | bool   | If `true`, the backend will load a huggingface api access key/token from `key.json`, which is required to access 'gated' models like Meta's Llama2.                                                                                                                                                      |                                                                                                                                                                                                                                                                                                               |
| `custom_chat_template` | string | A jinja2 template string of the chat template to be applied for this model. This should be set if `premade_chat_template` is `false` for the model, as the generic fallback chat template that will be used if this is not defined is likely to lead to bad model performance.                           | `"{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<\|user\|>\n' + message['content'] }}\n{% elif message['role'] == 'assistant' %}\n{{ '<\|assistant\|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last %}\n{{ '<\|assistant\|>' }}\n{% endif %}\n{% endfor %}"` |
| `output_split_prefix`  | string | The model's raw output will be rsplit using this string, and the remaining output following this string will be considered the model output. This is necessary for some models that decode tokens differently than they encode them, to assure that the prompt is properly removed from model responses. | `"assistant\n"`                                                                                                                                                                                                                                                                                               |
| `bos_string`           | string | In case the model file does not contain a predefined BOS token, this string will be used to create the logged input prompt.                                                                                                                                                                              | `"<s>"`                                                                                                                                                                                                                                                                                                       |
| `eos_string`           | string | In case the model file does not contain a predefined EOS token, this string will be used to create the logged input prompt.                                                                                                                                                                              | `"<\|im_end\|>"`                                                                                                                                                                                                                                                                                              |

#### Advanced
These key/values are recommended to only be used with a custom registry file:

| Key                    | Type   | Description                                                                                                                                                                                                           | Example |
|------------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|
| `execute_on`           | string | Either `gpu`, to run the model with all layers loaded to GPU using VRAM, or `cpu` to run the model on CPU only, using main RAM. `gpu` requires a llama.cpp installation with GPU support, `cpu` one with CPU support. |         |
| `gpu_layers_offloaded` | int    | The number of model layers to offload to GPU/VRAM. This requires a llama.cpp installation with GPU support. This key is only used if there is no `execute_on` key in the model entry.                                 |         |

### vllm Backend
The python module of this backend is `clemcore/backends/vllm_api.py.`  
This backend requires these **mandatory** key/values:  

| Key                     | Type   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Example               |
|-------------------------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|
| `huggingface_id`        | string | The full huggingface model ID; huggingface user name / model name. NOTE: This key is **not part of the model config**, but a key for the model entry object!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `"01-ai/Yi-34B-Chat"` |
| `number_gpus`           | int    | The number of GPUs to run this model on.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `1`                   |
| `premade_chat_template` | bool   | If `true`, the chat template that is applied for generation is loaded from the model repository on huggingface. If `false`, the value of `custom_chat_template` will be used if defined, otherwise a generic chat template is applied (highly discouraged).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                       |
| `eos_to_cull`           | string | This is a regular expression matching the model's EOS token or longer substrings at the end of the model's outputs. These need to be removed by the backend to assure proper processing by clembench. Make sure that this contains a proper python regular expression string matching the intended substrings. Characters and sequences that can be parsed as python regular expression special characters or special sequences, but are part of model special token strings or chat templates need to be properly escaped by "`\\`"! Note the double backslash "`\\`", which is necessary to properly handle the escape between JSON and python! This is mandatory as there are models that do not define this in their tokenizer configuration. | `"<\\\|im_end\\\|>"`  |

### OpenRouter Backend
The python module of this backend is `clemcore/backends/openrouter_api.py.`  

Any additional OpenRouter API request arguments can be set for the model by adding them inside the `extra_body` key 
under the entry's `model_config` key. Refer to the 
[OpenRouter API documentation](https://openrouter.ai/docs/api-reference) for passable values, specially 
[model routing](https://openrouter.ai/docs/features/model-routing), 
[provider routing](https://openrouter.ai/docs/features/provider-routing) and 
[reasoning controls](https://openrouter.ai/docs/use-cases/reasoning-tokens). Passing the correct values for the targeted 
model is important to prevent inconsistencies, as different model providers might offer the same model diffrently, for 
example at different quantizations, 

### OpenAI Backend
The python module of this backend is `clemcore/backends/openai_api.py.`  
The following key/values are **optional**, but should be defined for models that require them for proper functioning:  

| Key                              | Type   | Description                                                                                                                                                                                                                                                                             | Example                                                                                            |
|----------------------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| `multimodality`                  | object | This object holds values for accessing multimodal model functionality. If present, it must hold multimodal setting values.                                                                                                                                                              | `"multimodality": {"single_image": true, "multiple_images": true, "audio": false, "video": false}` |
| `multimodality[single_image]`    | bool   | If true, the model is capable of processing a single images per request.                                                                                                                                                                                                                |                                                                                                    |
| `multimodality[multiple_images]` | bool   | If true, the model is capable of processing multiple images per request.                                                                                                                                                                                                                |                                                                                                    |
| `multimodality[audio]`           | bool   | If true, the model is capable of processing audio.                                                                                                                                                                                                                                      |                                                                                                    |
| `multimodality[video]`           | bool   | If true, the model is capable of processing video.                                                                                                                                                                                                                                      |                                                                                                    |
| `reasoning_model`                | bool   | If true, the model performs reasoning. OpenAI API reasoning models do not allow inference with temperature 0.0, so responses are indeterministic. These models also do not allow setting the maximum number of output tokens, so if this value was set, it is ignored for these models. |                                                                                                    |

# Backend Classes
Model registry entries are mainly used for two classes: `backends.ModelSpec` and `backends.Model`.
## ModelSpec
The `backends.ModelSpec` class is used to hold model-specific data, as defined in a model entry, and default 
generation parameters. All backend functions and methods expect instances of this class as arguments for model loading.  
As part of a benchmark run, `ModelSpec` is initialized using the model name only, and the settings are loaded from the 
model registry, from the first entry with the given name, unifying with the entry contents.  
For testing and prototyping, a `ModelSpec` can be initialized from a `dict` with the same structure as a model entry, 
using `ModelSpec.from_dict()`.
## Model
The `backends.Model` class is used for fully loaded model instances ready for generation.

# Model registry files
Clemcore checks for a model registry JSON file named `model_registry.json` in the current working directory first, then 
for the model registry file contained in the library. All found model registry files are then combined to create the 
current model registry. This allows adding models to the registry by creating a custom model registry file with the 
respective model entries.
## Checking registered models
The CLI command `clem list models` lists all currently registered models, printing their names, backend and the path to 
the model registry file containing the model's registry entry.  
Adding the argument `-v` to the command, `clem list models -v`, leads to entire model registry entry content being 
printed to console as well.