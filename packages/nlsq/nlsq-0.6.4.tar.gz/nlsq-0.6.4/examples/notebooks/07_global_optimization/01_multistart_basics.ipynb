{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Start Optimization Basics with NLSQ (v0.6.3)\n\nThis script demonstrates global optimization using the unified fit() API\nwith workflow='auto_global' for automatic method selection.\n\nFeatures demonstrated:\n- Local minima trap problem in nonlinear optimization\n- fit(workflow='auto_global') for global optimization\n- Comparison of single-start vs multi-start results\n- Visualization of loss landscape and starting point distribution\n\nRun this example:\n    python examples/scripts/07_global_optimization/01_multistart_basics.py\n"
      ],
      "id": "header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure matplotlib for inline plotting\n%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nlsq import fit\nfrom nlsq.global_optimization import latin_hypercube_sample, scale_samples_to_bounds\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def multimodal_model(x, a, b, c, d):\n    \"\"\"Multimodal model: y = a * sin(b * x + c) + d\n\n    This model has multiple local minima due to the periodicity of sin().\n    Different combinations of (b, c) can produce similar fits.\n    \"\"\"\n    return a * jnp.sin(b * x + c) + d\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\nprint(\"Multi-Start Optimization Basics (v0.6.3)\")\nprint(\"=\" * 70)\nprint()\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Generate synthetic data\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"1. Generating synthetic data...\")\n\nn_samples = 200\nx_data = np.linspace(0, 4 * np.pi, n_samples)\n\n# True parameters\ntrue_a, true_b, true_c, true_d = 2.0, 1.5, 0.5, 1.0\n\n# Generate noisy observations\ny_true = true_a * np.sin(true_b * x_data + true_c) + true_d\nnoise = 0.2 * np.random.randn(n_samples)\ny_data = y_true + noise\n\nprint(f\"  True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\nprint(f\"  Dataset: {n_samples} points\")\nprint()\n\n# Visualize data\nfig, ax = plt.subplots(figsize=(10, 5))\nax.scatter(x_data, y_data, alpha=0.5, s=10, label=\"Noisy data\")\nax.plot(x_data, y_true, \"r-\", linewidth=2, label=\"True function\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Synthetic Data: Multimodal Sinusoidal Model\")\nax.legend()\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Single-start optimization with different initial guesses\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"2. Single-start optimization (showing sensitivity to initial guess)...\")\n\n# Define bounds (required for auto_global)\nbounds = ([0.5, 0.5, -np.pi, -2.0], [5.0, 3.0, np.pi, 5.0])\n\n# Try several different initial guesses with workflow='auto'\ninitial_guesses = [\n    [1.0, 0.8, 0.0, 0.5],  # Poor guess 1\n    [3.0, 2.5, 2.0, 2.0],  # Poor guess 2\n    [1.5, 1.2, -1.0, 0.0],  # Poor guess 3\n]\n\nsingle_start_results = []\n\nfor i, p0 in enumerate(initial_guesses):\n    try:\n        # workflow='auto' uses local optimization (single-start)\n        popt, pcov = fit(\n            multimodal_model,\n            x_data,\n            y_data,\n            p0=p0,\n            bounds=bounds,\n            workflow=\"auto\",  # Local optimization\n        )\n        y_pred = multimodal_model(x_data, *popt)\n        ssr = float(jnp.sum((y_data - y_pred) ** 2))\n        single_start_results.append({\"p0\": p0, \"popt\": popt, \"ssr\": ssr})\n        print(f\"  Guess {i + 1}: p0={p0}\")\n        print(\n            f\"    Result: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}, d={popt[3]:.3f}\"\n        )\n        print(f\"    SSR: {ssr:.4f}\")\n    except Exception as e:\n        print(f\"  Guess {i + 1}: Failed - {e}\")\n        single_start_results.append({\"p0\": p0, \"popt\": None, \"ssr\": float(\"inf\")})\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Global optimization with fit(workflow='auto_global')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"3. Global optimization with fit(workflow='auto_global')...\")\n\n# Use the first (poor) initial guess\np0_poor = [1.0, 0.8, 0.0, 0.5]\n\nprint(\"  workflow='auto_global' automatically selects:\")\nprint(\"    - Multi-Start (default) or CMA-ES based on parameter scale ratio\")\nprint(\"    - Memory strategy based on dataset size\")\nprint()\n\n# Fit with global optimization - auto-selects Multi-Start or CMA-ES\npopt_global, pcov_global = fit(\n    multimodal_model,\n    x_data,\n    y_data,\n    p0=p0_poor,\n    bounds=bounds,\n    workflow=\"auto_global\",  # Global optimization with auto method selection\n    n_starts=10,  # Number of multi-start runs\n)\n\ny_pred_global = multimodal_model(x_data, *popt_global)\nssr_global = float(jnp.sum((y_data - y_pred_global) ** 2))\n\nprint(\"  Global optimization result:\")\nprint(\n    f\"    Parameters: a={popt_global[0]:.3f}, b={popt_global[1]:.3f}, \"\n    f\"c={popt_global[2]:.3f}, d={popt_global[3]:.3f}\"\n)\nprint(f\"    SSR: {ssr_global:.4f}\")\n\n# Compare with single-start from same initial guess\npopt_single, _ = fit(\n    multimodal_model,\n    x_data,\n    y_data,\n    p0=p0_poor,\n    bounds=bounds,\n    workflow=\"auto\",  # Local optimization\n)\ny_pred_single = multimodal_model(x_data, *popt_single)\nssr_single = float(jnp.sum((y_data - y_pred_single) ** 2))\n\nprint()\nprint(\"  Comparison (same initial guess):\")\nprint(f\"    Single-start SSR: {ssr_single:.4f}\")\nprint(f\"    Global (auto_global) SSR: {ssr_global:.4f}\")\nif ssr_global < ssr_single:\n    improvement = (1 - ssr_global / ssr_single) * 100\n    print(f\"    Improvement: {improvement:.1f}% lower SSR\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Comparison visualization\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"4. Saving comparison visualization...\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Data with both fits\nax1 = axes[0]\nax1.scatter(x_data, y_data, alpha=0.4, s=15, label=\"Data\", color=\"gray\")\nax1.plot(x_data, y_true, \"k--\", linewidth=2, label=\"True function\", alpha=0.7)\nax1.plot(\n    x_data,\n    y_pred_single,\n    \"b-\",\n    linewidth=2,\n    label=f\"workflow='auto' (SSR={ssr_single:.2f})\",\n)\nax1.plot(\n    x_data,\n    y_pred_global,\n    \"r-\",\n    linewidth=2,\n    label=f\"workflow='auto_global' (SSR={ssr_global:.2f})\",\n)\nax1.set_xlabel(\"x\")\nax1.set_ylabel(\"y\")\nax1.set_title(\"Local vs Global Optimization Comparison\")\nax1.legend()\n\n# Right plot: Residuals comparison\nax2 = axes[1]\nresiduals_single = y_data - y_pred_single\nresiduals_global = y_data - y_pred_global\nax2.scatter(\n    x_data, residuals_single, alpha=0.5, s=15, label=\"workflow='auto'\", color=\"blue\"\n)\nax2.scatter(\n    x_data,\n    residuals_global,\n    alpha=0.5,\n    s=15,\n    label=\"workflow='auto_global'\",\n    color=\"red\",\n)\nax2.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.5)\nax2.set_xlabel(\"x\")\nax2.set_ylabel(\"Residual\")\nax2.set_title(\"Residuals Comparison\")\nax2.legend()\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Loss landscape visualization\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"5. Generating loss landscape visualization...\")\n\nb_range = np.linspace(0.5, 3.0, 50)\nc_range = np.linspace(-np.pi, np.pi, 50)\nB, C = np.meshgrid(b_range, c_range)\n\nloss_landscape = np.zeros_like(B)\nfor i in range(len(c_range)):\n    for j in range(len(b_range)):\n        y_pred = true_a * np.sin(B[i, j] * x_data + C[i, j]) + true_d\n        loss_landscape[i, j] = np.sum((y_data - y_pred) ** 2)\n\nloss_log = np.log10(loss_landscape + 1)\n\nfig, ax = plt.subplots(figsize=(10, 8))\ncontour = ax.contourf(B, C, loss_log, levels=30, cmap=\"viridis\")\nplt.colorbar(contour, ax=ax, label=\"log10(SSR + 1)\")\n\nax.scatter(\n    [true_b],\n    [true_c],\n    color=\"white\",\n    marker=\"*\",\n    s=200,\n    label=\"True parameters\",\n    edgecolors=\"black\",\n    linewidths=1,\n)\nax.scatter(\n    [popt_single[1]],\n    [popt_single[2]],\n    color=\"blue\",\n    marker=\"o\",\n    s=100,\n    label=\"workflow='auto'\",\n    edgecolors=\"white\",\n    linewidths=1,\n)\nax.scatter(\n    [popt_global[1]],\n    [popt_global[2]],\n    color=\"red\",\n    marker=\"s\",\n    s=100,\n    label=\"workflow='auto_global'\",\n    edgecolors=\"white\",\n    linewidths=1,\n)\n\nax.set_xlabel(\"b (frequency)\")\nax.set_ylabel(\"c (phase)\")\nax.set_title(\n    \"Loss Landscape (a, d fixed at true values)\\nMultiple local minima visible\"\n)\nax.legend(loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. Starting point distribution\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"6. Generating starting point distribution visualization...\")\n\nn_samples_viz = 20\nn_params = 4\n\nkey = jax.random.PRNGKey(42)\nlhs_samples = latin_hypercube_sample(n_samples_viz, n_params, rng_key=key)\n\nlb = np.array([0.5, 0.5, -np.pi, -2.0])\nub = np.array([5.0, 3.0, np.pi, 5.0])\nscaled_samples = scale_samples_to_bounds(lhs_samples, lb, ub)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: LHS samples on loss landscape\nax1 = axes[0]\ncontour = ax1.contourf(B, C, loss_log, levels=30, cmap=\"viridis\", alpha=0.7)\nax1.scatter(\n    scaled_samples[:, 1],\n    scaled_samples[:, 2],\n    color=\"yellow\",\n    marker=\"o\",\n    s=80,\n    label=\"LHS starting points\",\n    edgecolors=\"black\",\n    linewidths=1,\n)\nax1.scatter(\n    [true_b],\n    [true_c],\n    color=\"white\",\n    marker=\"*\",\n    s=200,\n    label=\"True parameters\",\n    edgecolors=\"black\",\n    linewidths=1,\n)\nax1.set_xlabel(\"b (frequency)\")\nax1.set_ylabel(\"c (phase)\")\nax1.set_title(\"LHS Starting Points on Loss Landscape\")\nax1.legend()\n\n# Right: All 2D projections\nax2 = axes[1]\nparam_names = [\"a\", \"b\", \"c\", \"d\"]\ncolors = plt.cm.tab10(np.linspace(0, 1, 6))\n\nplot_idx = 0\nfor i in range(n_params):\n    for j in range(i + 1, n_params):\n        ax2.scatter(\n            scaled_samples[:, i],\n            scaled_samples[:, j],\n            alpha=0.6,\n            s=30,\n            color=colors[plot_idx],\n            label=f\"{param_names[i]} vs {param_names[j]}\",\n        )\n        plot_idx += 1\n\nax2.set_xlabel(\"Parameter value (normalized)\")\nax2.set_ylabel(\"Parameter value (normalized)\")\nax2.set_title(\"LHS Coverage: All 2D Projections\")\nax2.legend(loc=\"upper right\", fontsize=8)\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"=\" * 70)\nprint(\"Summary - The Three Workflows (v0.6.3)\")\nprint(\"=\" * 70)\nprint()\nprint(f\"True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\nprint()\nprint(\"workflow='auto' (local optimization):\")\nprint(\n    f\"  Parameters: a={popt_single[0]:.3f}, b={popt_single[1]:.3f}, \"\n    f\"c={popt_single[2]:.3f}, d={popt_single[3]:.3f}\"\n)\nprint(f\"  SSR: {ssr_single:.4f}\")\nprint()\nprint(\"workflow='auto_global' (global optimization, 10 starts):\")\nprint(\n    f\"  Parameters: a={popt_global[0]:.3f}, b={popt_global[1]:.3f}, \"\n    f\"c={popt_global[2]:.3f}, d={popt_global[3]:.3f}\"\n)\nprint(f\"  SSR: {ssr_global:.4f}\")\nprint()\nprint(\"Key takeaways:\")\nprint(\"  - workflow='auto': Local optimization, good when you have a good guess\")\nprint(\"  - workflow='auto_global': Global optimization for multi-modal problems\")\nprint(\"  - workflow='hpc': auto_global + checkpointing for long HPC jobs\")\nprint()\nprint(\"Global method auto-selection (auto_global):\")\nprint(\"  - Multi-Start: Default, explores multiple starting points\")\nprint(\"  - CMA-ES: Selected when scale_ratio > 1000 AND evosax available\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_14"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
