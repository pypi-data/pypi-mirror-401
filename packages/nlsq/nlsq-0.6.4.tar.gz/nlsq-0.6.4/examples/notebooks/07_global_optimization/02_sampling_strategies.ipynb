{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sampling Strategies for Global Optimization (v0.6.3)\n\nThis script demonstrates different sampling strategies used by\nworkflow='auto_global' for generating starting points.\n\nFeatures demonstrated:\n- Latin Hypercube Sampling (LHS) - stratified random sampling\n- Sobol sequences - deterministic quasi-random sampling\n- Halton sequences - prime-based quasi-random sampling\n- Random sampling - uniform random baseline\n- Visualization of space-filling properties\n- Quantitative comparison of discrepancy and success rates\n\nRun this example:\n    python examples/scripts/07_global_optimization/02_sampling_strategies.py\n"
      ],
      "id": "header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure matplotlib for inline plotting\n%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nfrom pathlib import Path\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nlsq import fit\nfrom nlsq.global_optimization import (\n    halton_sample,\n    latin_hypercube_sample,\n    scale_samples_to_bounds,\n    sobol_sample,\n)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\nif QUICK:\n    print(\"Quick mode: reduced iterations for sampling strategies.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "constants"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def multimodal_model(x, a, b, c):\n    \"\"\"Multimodal model with multiple local minima.\"\"\"\n    return a * jnp.sin(b * x + c)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_simple_discrepancy(samples: np.ndarray) -> float:\n    \"\"\"Compute a simple discrepancy measure based on minimum neighbor distances.\n\n    Lower discrepancy indicates more uniform coverage.\n    \"\"\"\n    n = len(samples)\n    if n < 2:\n        return 0.0\n\n    # Compute all pairwise distances\n    distances = []\n    for i in range(n):\n        min_dist = float(\"inf\")\n        for j in range(n):\n            if i != j:\n                dist = np.linalg.norm(samples[i] - samples[j])\n                min_dist = min(min_dist, dist)\n        distances.append(min_dist)\n\n    # Ideal minimum distance for uniform samples in d dimensions\n    d = samples.shape[1]\n    ideal_dist = (1.0 / n) ** (1.0 / d)\n\n    # Discrepancy: variance of min distances from ideal\n    distances = np.array(distances)\n    discrepancy = np.std(distances) / ideal_dist\n\n    return discrepancy\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_starting_points(samples_unit, lb, ub, x_data, y_data, true_params, bounds):\n    \"\"\"Evaluate success rate of starting points.\n\n    Returns the fraction of starting points that converge to the global optimum.\n    \"\"\"\n    # Scale samples to bounds\n    samples_scaled = scale_samples_to_bounds(jnp.array(samples_unit), lb, ub)\n    samples_scaled = np.array(samples_scaled)\n\n    # True SSR (sum of squared residuals)\n    y_true_pred = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\n    true_ssr = np.sum((y_data - y_true_pred) ** 2)\n\n    # Evaluate each starting point\n    success_count = 0\n    ssrs = []\n\n    for p0 in samples_scaled:\n        try:\n            # Use workflow='auto' for local optimization from each starting point\n            popt, _ = fit(\n                multimodal_model,\n                x_data,\n                y_data,\n                p0=list(p0),\n                bounds=bounds,\n                workflow=\"auto\",\n            )\n            y_pred = multimodal_model(x_data, *popt)\n            ssr = float(jnp.sum((y_data - y_pred) ** 2))\n            ssrs.append(ssr)\n\n            # Check if converged to near-optimal (within 10% of true SSR)\n            if ssr < true_ssr * 1.1:\n                success_count += 1\n        except Exception:\n            ssrs.append(float(\"inf\"))\n\n    success_rate = success_count / len(samples_scaled)\n    best_ssr = min(ssrs)\n\n    return success_rate, best_ssr, ssrs\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\nprint(\"Sampling Strategies for Global Optimization (v0.6.3)\")\nprint(\"=\" * 70)\nprint()\n\nif QUICK:\n    print(\"Quick mode: skipping full demonstration.\")\n    print()\n    print(\"=\" * 70)\n    print(\"Summary: Sampling Strategies\")\n    print(\"=\" * 70)\n    print()\n    print(\"Sampling Strategies:\")\n    print(\"  - Random: Baseline, poor space-filling\")\n    print(\"  - LHS: Stratified random, good coverage, stochastic\")\n    print(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\n    print(\"  - Halton: Quasi-random, very good coverage, deterministic\")\n    print()\n    print(\"Key Functions:\")\n    print(\"  - latin_hypercube_sample(n_samples, n_dims)\")\n    print(\"  - sobol_sample(n_samples, n_dims)\")\n    print(\"  - halton_sample(n_samples, n_dims)\")\n    print(\"  - scale_samples_to_bounds(samples, lb, ub)\")\n    print()\n    print(\"Usage with fit():\")\n    print(\"  fit(..., workflow='auto_global', n_starts=10, sampler='lhs')\")\n    pass  # early exit in quick mode\n# Set random seed for reproducibility\nnp.random.seed(42)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Generate samples using each method\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"1. Generating samples with different methods...\")\n\nn_samples = 50\nn_dims = 2\n\n# Generate samples using each method\nrandom_samples = np.random.rand(n_samples, n_dims)\nkey = jax.random.PRNGKey(42)\nlhs_samples = latin_hypercube_sample(n_samples, n_dims, rng_key=key)\nsobol_samples = sobol_sample(n_samples, n_dims)\nhalton_samples = halton_sample(n_samples, n_dims)\n\nprint(f\"  Generated {n_samples} samples in {n_dims} dimensions\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Visualize 2D samples\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"2. Saving 2D comparison visualization...\")\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\nsamples_dict = {\n    \"Random\": random_samples,\n    \"Latin Hypercube (LHS)\": np.array(lhs_samples),\n    \"Sobol\": np.array(sobol_samples),\n    \"Halton\": np.array(halton_samples),\n}\n\nfor ax, (name, samples) in zip(axes.flat, samples_dict.items(), strict=False):\n    ax.scatter(\n        samples[:, 0],\n        samples[:, 1],\n        s=40,\n        alpha=0.7,\n        edgecolors=\"black\",\n        linewidths=0.5,\n    )\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n    ax.set_title(f\"{name} ({n_samples} samples)\")\n    ax.set_aspect(\"equal\")\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. LHS stratification visualization\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"3. Saving LHS stratification visualization...\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Random samples\nax1 = axes[0]\nax1.scatter(random_samples[:, 0], random_samples[:, 1], s=40, alpha=0.7)\n\nfor i in range(n_samples + 1):\n    ax1.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n    ax1.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.set_xlabel(\"Dimension 1\")\nax1.set_ylabel(\"Dimension 2\")\nax1.set_title(\"Random: Multiple samples per stratum\")\n\n# Right: LHS samples\nax2 = axes[1]\nax2.scatter(\n    np.array(lhs_samples)[:, 0],\n    np.array(lhs_samples)[:, 1],\n    s=40,\n    alpha=0.7,\n    color=\"orange\",\n)\n\nfor i in range(n_samples + 1):\n    ax2.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n    ax2.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.set_xlabel(\"Dimension 1\")\nax2.set_ylabel(\"Dimension 2\")\nax2.set_title(\"LHS: Exactly one sample per stratum per dimension\")\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Quasi-random progressive fill\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"4. Saving quasi-random progressive fill visualization...\")\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\nsample_counts = [8, 16, 32, 64]\n\nfor i, n in enumerate(sample_counts):\n    # Sobol\n    sobol_n = sobol_sample(n, 2)\n    axes[0, i].scatter(\n        np.array(sobol_n)[:, 0], np.array(sobol_n)[:, 1], s=30, alpha=0.8\n    )\n    axes[0, i].set_xlim(0, 1)\n    axes[0, i].set_ylim(0, 1)\n    axes[0, i].set_title(f\"Sobol: n={n}\")\n    axes[0, i].set_aspect(\"equal\")\n    axes[0, i].grid(True, alpha=0.3)\n\n    # Halton\n    halton_n = halton_sample(n, 2)\n    axes[1, i].scatter(\n        np.array(halton_n)[:, 0],\n        np.array(halton_n)[:, 1],\n        s=30,\n        alpha=0.8,\n        color=\"green\",\n    )\n    axes[1, i].set_xlim(0, 1)\n    axes[1, i].set_ylim(0, 1)\n    axes[1, i].set_title(f\"Halton: n={n}\")\n    axes[1, i].set_aspect(\"equal\")\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Discrepancy comparison\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"5. Computing discrepancy comparison...\")\n\nsample_sizes = [10, 20, 30, 50, 75, 100]\ndiscrepancies = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\n\nfor n in sample_sizes:\n    random_s = np.random.rand(n, 2)\n    lhs_s = np.array(latin_hypercube_sample(n, 2, rng_key=jax.random.PRNGKey(42)))\n    sobol_s = np.array(sobol_sample(n, 2))\n    halton_s = np.array(halton_sample(n, 2))\n\n    discrepancies[\"Random\"].append(compute_simple_discrepancy(random_s))\n    discrepancies[\"LHS\"].append(compute_simple_discrepancy(lhs_s))\n    discrepancies[\"Sobol\"].append(compute_simple_discrepancy(sobol_s))\n    discrepancies[\"Halton\"].append(compute_simple_discrepancy(halton_s))\n\n# Plot discrepancy\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = {\"Random\": \"red\", \"LHS\": \"orange\", \"Sobol\": \"blue\", \"Halton\": \"green\"}\nmarkers = {\"Random\": \"o\", \"LHS\": \"s\", \"Sobol\": \"^\", \"Halton\": \"d\"}\n\nfor name, discs in discrepancies.items():\n    ax.plot(\n        sample_sizes,\n        discs,\n        marker=markers[name],\n        color=colors[name],\n        linewidth=2,\n        markersize=8,\n        label=name,\n    )\n\nax.set_xlabel(\"Number of Samples\")\nax.set_ylabel(\"Discrepancy (lower is better)\")\nax.set_title(\"Discrepancy Comparison: Space-Filling Quality\")\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. Success rate comparison\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"6. Computing success rate comparison...\")\n\n# Generate test data\nnp.random.seed(42)\nn_points = 100\nx_data = np.linspace(0, 4 * np.pi, n_points)\ntrue_params = [2.0, 1.5, 0.5]\ny_true = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\ny_data = y_true + 0.2 * np.random.randn(n_points)\n\nbounds = ([0.5, 0.5, -np.pi], [5.0, 3.0, np.pi])\nlb, ub = np.array(bounds[0]), np.array(bounds[1])\n\nn_starts_list = [5, 10] if QUICK else [5, 10, 15, 20, 25, 30]\nn_trials = 2 if QUICK else 5\n\nsuccess_rates = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\n\nfor n_starts in n_starts_list:\n    print(f\"  Evaluating n_starts = {n_starts}...\")\n\n    # Random and LHS: average over trials\n    random_rates = []\n    lhs_rates = []\n\n    for trial in range(n_trials):\n        random_s = np.random.rand(n_starts, 3)\n        lhs_s = np.array(\n            latin_hypercube_sample(n_starts, 3, rng_key=jax.random.PRNGKey(trial))\n        )\n\n        rate, _, _ = evaluate_starting_points(\n            random_s, lb, ub, x_data, y_data, true_params, bounds\n        )\n        random_rates.append(rate)\n\n        rate, _, _ = evaluate_starting_points(\n            lhs_s, lb, ub, x_data, y_data, true_params, bounds\n        )\n        lhs_rates.append(rate)\n\n    success_rates[\"Random\"].append(np.mean(random_rates))\n    success_rates[\"LHS\"].append(np.mean(lhs_rates))\n\n    # Sobol and Halton: deterministic\n    sobol_s = np.array(sobol_sample(n_starts, 3))\n    halton_s = np.array(halton_sample(n_starts, 3))\n\n    rate, _, _ = evaluate_starting_points(\n        sobol_s, lb, ub, x_data, y_data, true_params, bounds\n    )\n    success_rates[\"Sobol\"].append(rate)\n\n    rate, _, _ = evaluate_starting_points(\n        halton_s, lb, ub, x_data, y_data, true_params, bounds\n    )\n    success_rates[\"Halton\"].append(rate)\n\n# Plot success rate\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor name, rates in success_rates.items():\n    ax.plot(\n        n_starts_list,\n        [r * 100 for r in rates],\n        marker=markers[name],\n        color=colors[name],\n        linewidth=2,\n        markersize=8,\n        label=name,\n    )\n\nax.set_xlabel(\"Number of Starting Points\")\nax.set_ylabel(\"Success Rate (%)\")\nax.set_title(\"Success Rate: Finding Global Optimum\")\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 105)\n\nplt.tight_layout()\nplt.show()\nplt.close()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7. Demonstrate samplers with fit(workflow='auto_global')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"7. Using samplers with fit(workflow='auto_global')...\")\n\nsamplers = [\"lhs\", \"sobol\", \"halton\"]\nresults = {}\n\nfor sampler in samplers:\n    popt, pcov = fit(\n        multimodal_model,\n        x_data,\n        y_data,\n        p0=[1.0, 1.0, 0.0],\n        bounds=bounds,\n        workflow=\"auto_global\",  # Global optimization\n        n_starts=10,\n        sampler=sampler,\n    )\n\n    y_pred = multimodal_model(x_data, *popt)\n    ssr = float(jnp.sum((y_data - y_pred) ** 2))\n\n    results[sampler] = {\"popt\": popt, \"ssr\": ssr}\n\n    print(f\"  Sampler: {sampler}\")\n    print(f\"    Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}\")\n    print(f\"    SSR: {ssr:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"=\" * 70)\nprint(\"Summary - Sampling Strategies (v0.6.3)\")\nprint(\"=\" * 70)\nprint()\nprint(\"Sampling Strategies:\")\nprint(\"  - Random: Baseline, poor space-filling\")\nprint(\"  - LHS: Stratified random, good coverage, stochastic (default)\")\nprint(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\nprint(\"  - Halton: Quasi-random, very good coverage, deterministic\")\nprint()\nprint(\"Key Functions:\")\nprint(\"  - latin_hypercube_sample(n_samples, n_dims)\")\nprint(\"  - sobol_sample(n_samples, n_dims)\")\nprint(\"  - halton_sample(n_samples, n_dims)\")\nprint(\"  - scale_samples_to_bounds(samples, lb, ub)\")\nprint()\nprint(\"Usage with fit():\")\nprint(\"  fit(..., workflow='auto_global', n_starts=10, sampler='lhs')\")\nprint()\nprint(\"Sampler Selection Guidelines:\")\nprint(\"  - General use: LHS (default)\")\nprint(\"  - Reproducibility needed: Sobol\")\nprint(\"  - Low dimensions (2-5): Halton\")\nprint(\"  - High dimensions (>10): LHS\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_16"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
