{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1158a2e",
   "metadata": {},
   "source": [
    "# Custom Algorithms Advanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25514c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:58:10.318058Z",
     "iopub.status.busy": "2026-01-06T16:58:10.317788Z",
     "iopub.status.idle": "2026-01-06T16:58:11.403315Z",
     "shell.execute_reply": "2026-01-06T16:58:11.401800Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converted from custom_algorithms_advanced.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# # Custom Algorithms and Advanced Extensions\n",
    "#\n",
    "# **Level**: Advanced / Research\n",
    "# **Time**: 60-90 minutes\n",
    "# **Prerequisites**: NLSQ Quickstart, JAX fundamentals, optimization theory\n",
    "#\n",
    "# ## Overview\n",
    "#\n",
    "# This tutorial is for **researchers and advanced users** who want to:\n",
    "# - Implement custom optimization algorithms\n",
    "# - Design specialized loss functions\n",
    "# - Extend NLSQ for novel applications\n",
    "# - Understand NLSQ's internals for research\n",
    "#\n",
    "# ### What You'll Learn\n",
    "#\n",
    "# 1. **NLSQ Architecture**: Understanding the optimization backend\n",
    "# 2. **Custom Loss Functions**: Beyond least squares\n",
    "# 3. **Custom Optimizers**: Implementing specialized algorithms\n",
    "# 4. **Advanced JAX Patterns**: Efficient curve fitting with JAX\n",
    "# 5. **Research Extensions**: Constrained optimization, robust methods\n",
    "#\n",
    "# ### Use Cases\n",
    "#\n",
    "# - **Custom loss**: Asymmetric penalties, quantile regression, robust M-estimators\n",
    "# - **Specialized optimizers**: Trust-region methods, second-order algorithms\n",
    "# - **Constrained problems**: Inequality constraints, manifold optimization\n",
    "# - **Novel applications**: Bayesian inference, inverse problems, PDE-constrained optimization\n",
    "#\n",
    "# ### Warning\n",
    "#\n",
    "# This is advanced material. Modifying optimization algorithms requires solid understanding of:\n",
    "# - Optimization theory (convexity, convergence, gradients)\n",
    "# - JAX programming model (JIT, grad, pytrees)\n",
    "# - Numerical stability considerations\n",
    "# ======================================================================\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad, jit, value_and_grad, vmap\n",
    "\n",
    "# Optimization libraries\n",
    "try:\n",
    "    import optax\n",
    "\n",
    "    OPTAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTAX_AVAILABLE = False\n",
    "    print(\"⚠ Optax not available - install with: pip install optax\")\n",
    "\n",
    "from nlsq import CurveFit\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"  JAX version: {jax.__version__}\")\n",
    "print(f\"  JAX devices: {jax.devices()}\")\n",
    "if OPTAX_AVAILABLE:\n",
    "    print(f\"  Optax version: {optax.__version__}\")\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "if QUICK:\n",
    "    print(\"Quick mode: skipping advanced algorithm deep dive.\")\n",
    "\n",
    "def _run_full_demo():\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Part 1: Understanding NLSQ's Optimization Backend\n",
    "    #\n",
    "    # Before customizing, let's understand how NLSQ works internally.\n",
    "    # ======================================================================\n",
    "\n",
    "\n",
    "    # Exploring NLSQ internals\n",
    "\n",
    "    # Simple problem: fit exponential\n",
    "    x_data = jnp.linspace(0, 5, 30)\n",
    "    y_true = 3.0 * jnp.exp(-0.5 * x_data)\n",
    "    y_data = y_true + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "\n",
    "    def exponential(x, a, b):\n",
    "        return a * jnp.exp(-b * x)\n",
    "\n",
    "\n",
    "    # Standard NLSQ fit\n",
    "    cf = CurveFit()\n",
    "    popt, pcov = cf.curve_fit(exponential, x_data, y_data, p0=[2.0, 0.3])\n",
    "\n",
    "    print(\"Standard NLSQ Fit:\")\n",
    "    print(f\"  Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}\")\n",
    "    print(f\"  Covariance matrix shape: {pcov.shape}\")\n",
    "    print()\n",
    "\n",
    "    # How NLSQ works internally (simplified):\n",
    "    print(\"NLSQ Internal Workflow:\")\n",
    "    print(\"1. Residual function: r(θ) = y_data - model(x_data, θ)\")\n",
    "    print(\"2. Loss function: L(θ) = 0.5 * sum(r(θ)^2)\")\n",
    "    print(\"3. Gradient: ∇L(θ) = -J^T r(θ) where J = ∂model/∂θ\")\n",
    "    print(\"4. Optimization: Levenberg-Marquardt or similar\")\n",
    "    print(\"5. Uncertainty: pcov = (J^T J)^(-1) * σ^2\")\n",
    "    print()\n",
    "\n",
    "    # Let's compute these manually with JAX\n",
    "    print(\"Manual computation with JAX:\")\n",
    "\n",
    "\n",
    "    def residual_fn(params, x, y):\n",
    "        \"\"\"Residual vector r(θ) = y - model(x, θ).\"\"\"\n",
    "        a, b = params\n",
    "        y_pred = exponential(x, a, b)\n",
    "        return y - y_pred\n",
    "\n",
    "\n",
    "    def loss_fn(params, x, y):\n",
    "        \"\"\"Sum of squared residuals L(θ) = 0.5 * ||r(θ)||^2.\"\"\"\n",
    "        r = residual_fn(params, x, y)\n",
    "        return 0.5 * jnp.sum(r**2)\n",
    "\n",
    "\n",
    "    # Compute gradient at fitted parameters\n",
    "    grad_fn = grad(loss_fn)\n",
    "    gradient = grad_fn(popt, x_data, y_data)\n",
    "\n",
    "    print(f\"  Gradient at optimum: {gradient}\")\n",
    "    print(f\"  Gradient norm: {jnp.linalg.norm(gradient):.2e} (should be ≈ 0)\")\n",
    "    print(\"  → Confirms NLSQ found a critical point where ∇L = 0 ✓\")\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Part 2: Custom Loss Functions\n",
    "    #\n",
    "    # Beyond standard least squares, we can implement custom loss functions for specialized needs.\n",
    "    # ======================================================================\n",
    "\n",
    "\n",
    "    # Example 1: Robust loss function (Huber loss)\n",
    "\n",
    "    # Generate data with outliers\n",
    "    x_robust = jnp.linspace(0, 10, 50)\n",
    "    y_robust = 2.0 * x_robust + 1.0 + np.random.normal(0, 0.5, 50)\n",
    "    # Add outliers (convert indices to JAX array for .at[] indexing)\n",
    "    outlier_idx = jnp.array([5, 15, 35, 42])\n",
    "    y_robust = y_robust.at[outlier_idx].add(jnp.array([5.0, -6.0, 4.0, -5.0]))\n",
    "\n",
    "\n",
    "    def linear_model(x, a, b):\n",
    "        return a * x + b\n",
    "\n",
    "\n",
    "    # Standard least squares (sensitive to outliers)\n",
    "    def least_squares_loss(params, x, y):\n",
    "        a, b = params\n",
    "        residuals = y - linear_model(x, a, b)\n",
    "        return jnp.sum(residuals**2)\n",
    "\n",
    "\n",
    "    # Huber loss (robust to outliers)\n",
    "    def huber_loss(params, x, y, delta=1.0):\n",
    "        \"\"\"Huber loss: quadratic for small errors, linear for large.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float\n",
    "            Threshold for switching from quadratic to linear\n",
    "        \"\"\"\n",
    "        a, b = params\n",
    "        residuals = y - linear_model(x, a, b)\n",
    "        abs_residuals = jnp.abs(residuals)\n",
    "\n",
    "        # Huber function: 0.5*r^2 if |r| <= delta, else delta*(|r| - 0.5*delta)\n",
    "        huber = jnp.where(\n",
    "            abs_residuals <= delta,\n",
    "            0.5 * residuals**2,\n",
    "            delta * (abs_residuals - 0.5 * delta),\n",
    "        )\n",
    "        return jnp.sum(huber)\n",
    "\n",
    "\n",
    "    # Optimize with both losses\n",
    "    if OPTAX_AVAILABLE:\n",
    "        # Using Optax for custom optimization\n",
    "        def optimize_custom(loss_fn, p0, x, y, n_steps=1000, lr=0.01):\n",
    "            \"\"\"Custom optimizer using Optax Adam.\"\"\"\n",
    "            params = jnp.array(p0)\n",
    "            optimizer = optax.adam(lr)\n",
    "            opt_state = optimizer.init(params)\n",
    "\n",
    "            @jit\n",
    "            def step(params, opt_state):\n",
    "                loss, grads = value_and_grad(loss_fn)(params, x, y)\n",
    "                updates, opt_state = optimizer.update(grads, opt_state)\n",
    "                params = optax.apply_updates(params, updates)\n",
    "                return params, opt_state, loss\n",
    "\n",
    "            losses = []\n",
    "            for i in range(n_steps):\n",
    "                params, opt_state, loss = step(params, opt_state)\n",
    "                if i % 100 == 0:\n",
    "                    losses.append(float(loss))\n",
    "\n",
    "            return params, losses\n",
    "\n",
    "        # Fit with both losses\n",
    "        p0 = [1.0, 0.0]\n",
    "        params_ls, losses_ls = optimize_custom(least_squares_loss, p0, x_robust, y_robust)\n",
    "        params_huber, losses_huber = optimize_custom(\n",
    "            lambda p, x, y: huber_loss(p, x, y, delta=1.5), p0, x_robust, y_robust\n",
    "        )\n",
    "\n",
    "        print(\"Least Squares (sensitive to outliers):\")\n",
    "        print(f\"  a={params_ls[0]:.3f}, b={params_ls[1]:.3f}\")\n",
    "        print(\"\\nHuber Loss (robust to outliers):\")\n",
    "        print(f\"  a={params_huber[0]:.3f}, b={params_huber[1]:.3f}\")\n",
    "        print(\"\\nTrue parameters: a=2.0, b=1.0\")\n",
    "\n",
    "        # Visualization\n",
    "        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Fits\n",
    "        x_plot = jnp.linspace(0, 10, 100)\n",
    "        ax1.plot(x_robust, y_robust, \"o\", alpha=0.5, label=\"Data (with outliers)\")\n",
    "        ax1.plot(\n",
    "            x_robust[outlier_idx],\n",
    "            y_robust[outlier_idx],\n",
    "            \"rx\",\n",
    "            ms=12,\n",
    "            mew=3,\n",
    "            label=\"Outliers\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            x_plot,\n",
    "            linear_model(x_plot, *params_ls),\n",
    "            \"r--\",\n",
    "            lw=2,\n",
    "            label=\"Least Squares\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            x_plot, linear_model(x_plot, *params_huber), \"g-\", lw=2, label=\"Huber Loss\"\n",
    "        )\n",
    "        ax1.plot(x_plot, 2.0 * x_plot + 1.0, \"k:\", lw=2, label=\"True\")\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "        ax1.set_title(\"Robust Fitting with Custom Loss\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Loss convergence\n",
    "        ax2.semilogy(losses_ls, \"r-\", label=\"Least Squares\")\n",
    "        ax2.semilogy(losses_huber, \"g-\", label=\"Huber Loss\")\n",
    "        ax2.set_xlabel(\"Iteration (×100)\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.set_title(\"Convergence\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save figure to file\n",
    "        fig_dir = Path.cwd() / \"figures\" / \"custom_algorithms_advanced\"\n",
    "        fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"⚠ Install optax to run this example: pip install optax\")\n",
    "\n",
    "\n",
    "    # Example 2: Asymmetric loss (safety-critical applications)\n",
    "\n",
    "\n",
    "    def asymmetric_loss(params, x, y, alpha=2.0):\n",
    "        \"\"\"Asymmetric quadratic loss.\n",
    "\n",
    "        Penalizes overestimation more than underestimation.\n",
    "        Useful when overestimation is more costly (e.g., drug dosing).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float\n",
    "            Asymmetry parameter (alpha > 1 penalizes positive residuals more)\n",
    "        \"\"\"\n",
    "        a, b = params\n",
    "        residuals = y - linear_model(x, a, b)\n",
    "\n",
    "        # Asymmetric penalty\n",
    "        loss = jnp.where(\n",
    "            residuals > 0,  # Overestimation (model too low)\n",
    "            alpha * residuals**2,  # Higher penalty\n",
    "            residuals**2,  # Normal penalty\n",
    "        )\n",
    "        return jnp.sum(loss)\n",
    "\n",
    "\n",
    "    if OPTAX_AVAILABLE:\n",
    "        # Fit with asymmetric loss\n",
    "        params_asym, _ = optimize_custom(\n",
    "            lambda p, x, y: asymmetric_loss(p, x, y, alpha=3.0),\n",
    "            [1.0, 0.0],\n",
    "            x_robust,\n",
    "            y_robust,\n",
    "        )\n",
    "\n",
    "        print(\"Asymmetric Loss (penalizes overestimation 3x):\")\n",
    "        print(f\"  a={params_asym[0]:.3f}, b={params_asym[1]:.3f}\")\n",
    "        print(\n",
    "            \"  → Fit is conservative (tends to underestimate to avoid costly overestimation)\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Part 3: Custom Optimization Algorithms\n",
    "    #\n",
    "    # Implement specialized optimization algorithms for specific problem structures.\n",
    "    # ======================================================================\n",
    "\n",
    "\n",
    "    # Example: Gradient descent with momentum (from scratch)\n",
    "\n",
    "\n",
    "    def gradient_descent_momentum(\n",
    "        loss_fn, p0, x, y, lr=0.01, momentum=0.9, n_steps=1000, tol=1e-6\n",
    "    ):\n",
    "        \"\"\"Gradient descent with momentum optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_fn : callable\n",
    "            Loss function: loss_fn(params, x, y) -> scalar\n",
    "        p0 : array\n",
    "            Initial parameters\n",
    "        lr : float\n",
    "            Learning rate\n",
    "        momentum : float\n",
    "            Momentum coefficient (0 = no momentum, 0.9 typical)\n",
    "        n_steps : int\n",
    "            Maximum iterations\n",
    "        tol : float\n",
    "            Convergence tolerance on gradient norm\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : array\n",
    "            Optimized parameters\n",
    "        history : dict\n",
    "            Optimization history (params, loss, grad_norm)\n",
    "        \"\"\"\n",
    "        params = jnp.array(p0, dtype=jnp.float32)\n",
    "        velocity = jnp.zeros_like(params)\n",
    "\n",
    "        history = {\"params\": [], \"loss\": [], \"grad_norm\": []}\n",
    "\n",
    "        grad_fn = jit(grad(loss_fn))\n",
    "        loss_fn_jit = jit(loss_fn)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            # Compute gradient\n",
    "            g = grad_fn(params, x, y)\n",
    "            grad_norm = float(jnp.linalg.norm(g))\n",
    "\n",
    "            # Update velocity (momentum)\n",
    "            velocity = momentum * velocity - lr * g\n",
    "\n",
    "            # Update parameters\n",
    "            params = params + velocity\n",
    "\n",
    "            # Record history\n",
    "            if i % 50 == 0:\n",
    "                loss_val = float(loss_fn_jit(params, x, y))\n",
    "                history[\"params\"].append(params.copy())\n",
    "                history[\"loss\"].append(loss_val)\n",
    "                history[\"grad_norm\"].append(grad_norm)\n",
    "\n",
    "            # Check convergence\n",
    "            if grad_norm < tol:\n",
    "                print(f\"  Converged at iteration {i} (grad_norm={grad_norm:.2e})\")\n",
    "                break\n",
    "\n",
    "        return params, history\n",
    "\n",
    "\n",
    "    # Test custom optimizer\n",
    "    print(\"Custom Gradient Descent with Momentum:\")\n",
    "    params_gd, history_gd = gradient_descent_momentum(\n",
    "        least_squares_loss, [0.0, 0.0], x_data, y_data, lr=0.01, momentum=0.9, n_steps=2000\n",
    "    )\n",
    "\n",
    "    print(f\"  Final params: a={params_gd[0]:.3f}, b={params_gd[1]:.3f}\")\n",
    "    print(f\"  Optimization steps: {len(history_gd['loss'])}\")\n",
    "\n",
    "    # Compare with NLSQ\n",
    "    popt_nlsq, _ = cf.curve_fit(exponential, x_data, y_data, p0=[0.0, 0.0])\n",
    "    print(\"\\nNLSQ (Levenberg-Marquardt):\")\n",
    "    print(f\"  Final params: a={popt_nlsq[0]:.3f}, b={popt_nlsq[1]:.3f}\")\n",
    "    print(\"\\n→ Both converge to similar solution ✓\")\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Part 4: Advanced JAX Patterns for Curve Fitting\n",
    "    #\n",
    "    # Leverage JAX's advanced features for efficient batch fitting.\n",
    "    # ======================================================================\n",
    "\n",
    "\n",
    "    # Vectorized batch fitting with vmap\n",
    "\n",
    "    # Generate multiple datasets\n",
    "    n_datasets = 100\n",
    "    x_batch = jnp.linspace(0, 5, 30)\n",
    "\n",
    "    # Each dataset has different true parameters\n",
    "    a_true_batch = np.random.uniform(2.0, 4.0, n_datasets)\n",
    "    b_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n",
    "\n",
    "    y_batch = jnp.array(\n",
    "        [\n",
    "            a * jnp.exp(-b * x_batch) + np.random.normal(0, 0.05, len(x_batch))\n",
    "            for a, b in zip(a_true_batch, b_true_batch, strict=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Batch fitting: {n_datasets} datasets simultaneously\")\n",
    "    print(f\"  Data shape: {y_batch.shape} (datasets × points)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Define fitting function for single dataset\n",
    "    def fit_single_dataset(y_single):\n",
    "        \"\"\"Fit one dataset (simplified Newton's method).\"\"\"\n",
    "        params = jnp.array([3.0, 0.5])  # Initial guess\n",
    "\n",
    "        def loss(p):\n",
    "            return jnp.sum((y_single - exponential(x_batch, *p)) ** 2)\n",
    "\n",
    "        # Simple gradient descent (10 steps)\n",
    "        for _ in range(20):\n",
    "            g = grad(loss)(params)\n",
    "            params = params - 0.05 * g\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "    # Vectorize over batch dimension with vmap\n",
    "    fit_batch = jit(vmap(fit_single_dataset))\n",
    "\n",
    "    # Fit all datasets in parallel (GPU accelerated!)\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    params_batch = fit_batch(y_batch)\n",
    "    batch_time = time.time() - start\n",
    "\n",
    "    print(f\"✓ Fitted {n_datasets} datasets in {batch_time * 1000:.1f} ms\")\n",
    "    print(\n",
    "        f\"  Average time per dataset: {batch_time / n_datasets * 1000:.2f} ms (with vmap)\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Check accuracy\n",
    "    a_fitted = params_batch[:, 0]\n",
    "    b_fitted = params_batch[:, 1]\n",
    "\n",
    "    a_error = np.mean(np.abs(a_fitted - a_true_batch))\n",
    "    b_error = np.mean(np.abs(b_fitted - b_true_batch))\n",
    "\n",
    "    print(\"Fitting accuracy:\")\n",
    "    print(f\"  Mean absolute error in a: {a_error:.4f}\")\n",
    "    print(f\"  Mean absolute error in b: {b_error:.4f}\")\n",
    "\n",
    "    # Visualize results\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax1.scatter(a_true_batch, a_fitted, alpha=0.5, s=20)\n",
    "    ax1.plot([2, 4], [2, 4], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "    ax1.set_xlabel(\"True a\")\n",
    "    ax1.set_ylabel(\"Fitted a\")\n",
    "    ax1.set_title(\"Parameter Recovery: a\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    ax2.scatter(b_true_batch, b_fitted, alpha=0.5, s=20)\n",
    "    ax2.plot([0.3, 0.7], [0.3, 0.7], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "    ax2.set_xlabel(\"True b\")\n",
    "    ax2.set_ylabel(\"Fitted b\")\n",
    "    ax2.set_title(\"Parameter Recovery: b\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Save figure to file\n",
    "    fig_dir = Path.cwd() / \"figures\" / \"custom_algorithms_advanced\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n→ vmap enables efficient parallel fitting across datasets ✓\")\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Part 5: Research Extensions\n",
    "    #\n",
    "    # Advanced techniques for cutting-edge applications.\n",
    "    # ======================================================================\n",
    "\n",
    "\n",
    "    # Example: Constrained optimization with penalty method\n",
    "\n",
    "\n",
    "    def constrained_loss(params, x, y, lambda_penalty=10.0):\n",
    "        \"\"\"Fit with constraint: a + b = 1.0 (sum constraint).\n",
    "\n",
    "        Uses quadratic penalty method.\n",
    "        \"\"\"\n",
    "        a, b = params\n",
    "\n",
    "        # Standard loss\n",
    "        residuals = y - (a * jnp.exp(-x) + b * jnp.exp(-2 * x))\n",
    "        data_loss = jnp.sum(residuals**2)\n",
    "\n",
    "        # Constraint penalty: (a + b - 1)^2\n",
    "        constraint_violation = (a + b - 1.0) ** 2\n",
    "        penalty = lambda_penalty * constraint_violation\n",
    "\n",
    "        return data_loss + penalty\n",
    "\n",
    "\n",
    "    # Generate data satisfying constraint\n",
    "    x_const = jnp.linspace(0, 3, 40)\n",
    "    a_true_const = 0.6\n",
    "    b_true_const = 0.4  # a + b = 1.0\n",
    "    y_const = (\n",
    "        a_true_const * jnp.exp(-x_const)\n",
    "        + b_true_const * jnp.exp(-2 * x_const)\n",
    "        + np.random.normal(0, 0.02, len(x_const))\n",
    "    )\n",
    "\n",
    "    if OPTAX_AVAILABLE:\n",
    "        # Unconstrained fit\n",
    "        params_unconstr, _ = optimize_custom(\n",
    "            lambda p, x, y: jnp.sum(\n",
    "                (y - (p[0] * jnp.exp(-x) + p[1] * jnp.exp(-2 * x))) ** 2\n",
    "            ),\n",
    "            [0.5, 0.5],\n",
    "            x_const,\n",
    "            y_const,\n",
    "            n_steps=2000,\n",
    "        )\n",
    "\n",
    "        # Constrained fit\n",
    "        params_constr, _ = optimize_custom(\n",
    "            lambda p, x, y: constrained_loss(p, x, y, lambda_penalty=100.0),\n",
    "            [0.5, 0.5],\n",
    "            x_const,\n",
    "            y_const,\n",
    "            n_steps=2000,\n",
    "        )\n",
    "\n",
    "        print(\"Unconstrained fit:\")\n",
    "        print(\n",
    "            f\"  a={params_unconstr[0]:.4f}, b={params_unconstr[1]:.4f}, sum={params_unconstr[0] + params_unconstr[1]:.4f}\"\n",
    "        )\n",
    "        print(\"\\nConstrained fit (a + b = 1):\")\n",
    "        print(\n",
    "            f\"  a={params_constr[0]:.4f}, b={params_constr[1]:.4f}, sum={params_constr[0] + params_constr[1]:.4f}\"\n",
    "        )\n",
    "        print(f\"\\nTrue values: a={a_true_const}, b={b_true_const}, sum=1.0\")\n",
    "        print(\n",
    "            f\"→ Constraint enforced: sum = {params_constr[0] + params_constr[1]:.6f} ≈ 1.0 ✓\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    # ## Summary and Best Practices\n",
    "    #\n",
    "    # ### When to Use Custom Algorithms\n",
    "    #\n",
    "    # | **Application** | **Standard NLSQ** | **Custom Algorithm** |\n",
    "    # |-----------------|-------------------|----------------------|\n",
    "    # | Standard curve fitting | ✅ Recommended | Unnecessary |\n",
    "    # | Outlier-heavy data | Use sigma weights | Robust loss (Huber, Cauchy) |\n",
    "    # | Asymmetric costs | N/A | Asymmetric loss function |\n",
    "    # | Constrained parameters | Use bounds | Penalty methods, Lagrangian |\n",
    "    # | Batch processing (1000s of fits) | Serial fitting | vmap for parallelization |\n",
    "    # | Novel research problems | May not apply | Custom optimizer |\n",
    "    #\n",
    "    # ### Implementation Checklist\n",
    "    #\n",
    "    # When implementing custom algorithms:\n",
    "    #\n",
    "    # 1. **Start simple**: Test with toy problems where you know the answer\n",
    "    # 2. **Verify gradients**: Use `jax.grad` and compare with finite differences\n",
    "    # 3. **Check convergence**: Monitor loss and gradient norms\n",
    "    # 4. **Use JIT**: Compile with `@jit` for 10-100x speedups\n",
    "    # 5. **Numerical stability**: Check for NaN/Inf, use stable formulations\n",
    "    # 6. **Validate results**: Compare with standard methods when possible\n",
    "    #\n",
    "    # ### Advanced JAX Patterns\n",
    "    #\n",
    "    # ```python\n",
    "    # # Pattern 1: Efficient batch fitting\n",
    "    # fit_single = jit(lambda y: optimize(loss_fn, y))\n",
    "    # fit_batch = vmap(fit_single)  # Parallelize over batch dimension\n",
    "    # results = fit_batch(y_batch)  # GPU-accelerated\n",
    "    #\n",
    "    # # Pattern 2: Custom gradients for numerical stability\n",
    "    # from jax import custom_jvp\n",
    "    #\n",
    "    # @custom_jvp\n",
    "    # def stable_exp(x):\n",
    "    #     return jnp.exp(jnp.clip(x, -50, 50))  # Prevent overflow\n",
    "    #\n",
    "    # # Pattern 3: Automatic differentiation through optimization\n",
    "    # def meta_objective(hyperparams):\n",
    "    #     # Fit model with hyperparams\n",
    "    #     params = optimize(loss_fn, hyperparams)\n",
    "    #     # Evaluate on validation set\n",
    "    #     return validation_loss(params)\n",
    "    #\n",
    "    # optimal_hyperparams = optimize(meta_objective, initial_hyperparams)\n",
    "    # ```\n",
    "    #\n",
    "    # ### Research Extensions\n",
    "    #\n",
    "    # Cutting-edge applications:\n",
    "    #\n",
    "    # 1. **Bilevel optimization**: Hyperparameter tuning via gradient descent\n",
    "    # 2. **Meta-learning**: Learning to fit across multiple tasks\n",
    "    # 3. **Differentiable physics**: PDE-constrained optimization\n",
    "    # 4. **Uncertainty quantification**: Laplace approximation, variational inference\n",
    "    # 5. **Inverse problems**: Image reconstruction, tomography\n",
    "    #\n",
    "    # ### Production Recommendations\n",
    "    #\n",
    "    # For production use:\n",
    "    # - **Default**: Use standard NLSQ (well-tested, robust)\n",
    "    # - **Custom loss**: Only when problem demands it (document why!)\n",
    "    # - **Testing**: Extensive validation against standard methods\n",
    "    # - **Monitoring**: Track convergence, gradient norms, numerical stability\n",
    "    # - **Fallback**: Implement standard NLSQ as backup if custom method fails\n",
    "    #\n",
    "    # ### References\n",
    "    #\n",
    "    # 1. **Optimization**: Nocedal & Wright, *Numerical Optimization* (2006)\n",
    "    # 2. **JAX**: https://jax.readthedocs.io/\n",
    "    # 3. **Optax**: https://optax.readthedocs.io/\n",
    "    # 4. **Robust fitting**: Huber, *Robust Statistics* (2009)\n",
    "    # 5. **Related examples**:\n",
    "    #    - `advanced_features_demo.ipynb` - NLSQ diagnostics\n",
    "    #    - `ml_integration_tutorial.ipynb` - Hybrid models with custom optimization\n",
    "    #\n",
    "    # ---\n",
    "    #\n",
    "    # **Warning**: Custom algorithms can be powerful but require careful validation. Always test thoroughly before using in production!\n",
    "    # ======================================================================\n",
    "\n",
    "if not QUICK:\n",
    "    _run_full_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
