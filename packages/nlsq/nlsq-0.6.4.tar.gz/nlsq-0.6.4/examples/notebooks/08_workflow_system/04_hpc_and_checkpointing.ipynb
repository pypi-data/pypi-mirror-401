{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HPC Integration and Checkpointing\n\nThis script demonstrates HPC cluster integration and checkpointing for\nfault-tolerant curve fitting.\n\nFeatures demonstrated:\n- ClusterDetector and ClusterInfo for PBS Pro detection\n- Checkpointing with enable_checkpoints and checkpoint_dir\n- create_checkpoint_directory() for timestamped directories\n- Defense layers for checkpoint resume (v0.3.6+)\n- PBS Pro job script generation\n\nRun this example:\n    python examples/scripts/08_workflow_system/04_hpc_and_checkpointing.py\n"
      ],
      "id": "header"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure matplotlib for inline plotting\n%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\nimport os\nimport shutil\nfrom datetime import datetime\nfrom pathlib import Path\nimport numpy as np\nfrom nlsq import HybridStreamingConfig, OptimizationGoal\nfrom nlsq.core.minpack import WORKFLOW_PRESETS\nfrom nlsq.core.workflow import ClusterDetector, ClusterInfo\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\nif QUICK:\n    print(\"Quick mode: reduced iterations for HPC and checkpointing demo.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "constants"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_checkpoint_directory(base_dir: str = \"./nlsq_checkpoints\") -> str:\n    \"\"\"Create a timestamped checkpoint directory.\n\n    Parameters\n    ----------\n    base_dir : str, optional\n        Base directory for checkpoints.\n\n    Returns\n    -------\n    str\n        Path to the created checkpoint directory.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    checkpoint_dir = Path(base_dir) / f\"checkpoint_{timestamp}\"\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    return str(checkpoint_dir)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_checkpoint(\n    checkpoint_dir: str, iteration: int, params, loss: float, metadata=None\n):\n    \"\"\"Save optimization checkpoint using JSON.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Directory to save checkpoint\n    iteration : int\n        Current iteration number\n    params : np.ndarray\n        Current parameter values\n    loss : float\n        Current loss value\n    metadata : dict, optional\n        Additional metadata to save\n    \"\"\"\n    checkpoint_path = Path(checkpoint_dir) / f\"checkpoint_{iteration:06d}.json\"\n\n    checkpoint_data = {\n        \"iteration\": iteration,\n        \"params\": np.array(params).tolist(),\n        \"loss\": float(loss),\n        \"metadata\": metadata or {},\n        \"timestamp\": datetime.now().isoformat(),\n    }\n\n    with open(checkpoint_path, \"w\") as f:\n        json.dump(checkpoint_data, f, indent=2)\n\n    print(f\"  Saved checkpoint: {checkpoint_path.name}\")\n    return checkpoint_path\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_latest_checkpoint(checkpoint_dir: str) -> dict | None:\n    \"\"\"Load the most recent checkpoint.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Directory containing checkpoints\n\n    Returns\n    -------\n    dict or None\n        Checkpoint data if found, None otherwise\n    \"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    if not checkpoint_dir.exists():\n        return None\n\n    # Find all checkpoint files\n    checkpoints = list(checkpoint_dir.glob(\"checkpoint_*.json\"))\n    if not checkpoints:\n        return None\n\n    # Sort by name (which includes iteration number)\n    latest = sorted(checkpoints)[-1]\n\n    with open(latest) as f:\n        checkpoint_data = json.load(f)\n\n    checkpoint_data[\"params\"] = np.array(checkpoint_data[\"params\"])\n\n    print(f\"  Loaded checkpoint: {latest.name}\")\n    return checkpoint_data\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def optimization_with_checkpoints(\n    checkpoint_dir: str, max_iterations: int = 100, checkpoint_interval: int = 10\n):\n    \"\"\"Example optimization loop with checkpoint support.\"\"\"\n\n    # Try to resume from checkpoint\n    checkpoint = load_latest_checkpoint(checkpoint_dir)\n\n    if checkpoint:\n        start_iteration = checkpoint[\"iteration\"] + 1\n        params = checkpoint[\"params\"]\n        print(f\"  Resuming from iteration {start_iteration}\")\n    else:\n        start_iteration = 0\n        params = np.array([1.0, 1.0, 0.0])  # Initial guess\n        print(\"  Starting fresh optimization\")\n\n    # Optimization loop\n    for iteration in range(start_iteration, max_iterations):\n        # Simulate optimization step\n        params = params + 0.001 * np.random.randn(3)\n        loss = np.sum(params**2)  # Dummy loss\n\n        # Checkpoint at intervals\n        if iteration > 0 and iteration % checkpoint_interval == 0:\n            save_checkpoint(checkpoint_dir, iteration, params, loss)\n\n    # Final checkpoint\n    save_checkpoint(checkpoint_dir, max_iterations - 1, params, loss)\n\n    return params\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "func_3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\nprint(\"HPC Integration and Checkpointing\")\nprint(\"=\" * 70)\nprint()\n\nnp.random.seed(42)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. ClusterDetector and ClusterInfo\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"1. ClusterDetector and ClusterInfo:\")\nprint(\"-\" * 50)\n\ndetector = ClusterDetector()\n\nprint(f\"  PBS environment detected: {detector.is_pbs_environment()}\")\n\ncluster_info = detector.detect()\n\nif cluster_info:\n    print(\"\\n  Cluster detected:\")\n    print(f\"    Scheduler: {cluster_info.scheduler}\")\n    print(f\"    Node count: {cluster_info.node_count}\")\n    print(f\"    GPUs per node: {cluster_info.gpus_per_node}\")\n    print(f\"    Total GPUs: {cluster_info.total_gpus}\")\nelse:\n    print(\"\\n  No cluster environment detected (running locally)\")\n\n# Simulated PBS cluster for demonstration\nsimulated_cluster = ClusterInfo(\n    node_count=4,\n    gpus_per_node=8,\n    total_gpus=32,\n    node_list=[\"node01\", \"node02\", \"node03\", \"node04\"],\n    scheduler=\"pbs\",\n    job_id=\"12345.pbs_server\",\n    interconnect=\"infiniband\",\n)\n\nprint(\"\\n  Simulated PBS cluster:\")\nprint(f\"    Nodes: {simulated_cluster.node_count}\")\nprint(f\"    GPUs: {simulated_cluster.total_gpus}\")\nprint(f\"    Job ID: {simulated_cluster.job_id}\")\nprint(f\"    Interconnect: {simulated_cluster.interconnect}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Streaming Strategy for Large Datasets\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"2. Streaming Strategy for Large Datasets:\")\nprint(\"-\" * 50)\n\nprint(\"  Available strategies (memory-based selection):\")\nprint(\"    - standard:  Full in-memory computation\")\nprint(\"    - chunked:   Memory-managed chunk processing\")\nprint(\"    - streaming: Mini-batch gradient descent\")\n\nprint(\"\\n  Streaming configurations:\")\nprint(\"    HybridStreamingConfig - Streaming optimizer configuration\")\nprint(\"    HybridStreamingConfig.defense_strict() - Checkpoint resume preset\")\nprint(\"    HybridStreamingConfig.scientific_default() - Production preset\")\n\nconfig = HybridStreamingConfig.defense_strict()\nprint(\"\\n  defense_strict() configuration:\")\nprint(f\"    warmup_iterations: {config.warmup_iterations}\")\nprint(f\"    normalize: {config.normalize}\")\n\nif QUICK:\n    print()\n    print(\"=\" * 70)\n    print(\"Summary (Quick Mode)\")\n    print(\"=\" * 70)\n    print()\n    print(\"HPC Integration:\")\n    print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n    print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n    print()\n    print(\"Checkpointing:\")\n    print(\"  - Use streaming strategy for fault tolerance\")\n    print(\"  - enable_checkpoints=True, checkpoint_dir='./checkpoints'\")\n    print()\n    print(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\n    print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n    print(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")\n    pass  # early exit in quick mode\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Checkpointing Configuration\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"3. Checkpointing Configuration:\")\nprint(\"-\" * 50)\n\ncheckpoint_dir = create_checkpoint_directory()\nprint(f\"  Created checkpoint directory: {checkpoint_dir}\")\n\ncustom_checkpoint_dir = create_checkpoint_directory(\n    base_dir=\"./my_project_checkpoints\"\n)\nprint(f\"  Custom checkpoint directory: {custom_checkpoint_dir}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Checkpoint Resume Workflow\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"4. Checkpoint Resume Workflow:\")\nprint(\"-\" * 50)\n\ndemo_dir = create_checkpoint_directory(base_dir=\"./demo_checkpoints\")\n\n# Simulate saving checkpoints\nprint(\"\\n  Simulating optimization with checkpoints...\")\nfor i in range(0, 30, 10):\n    params = np.array([2.0 + 0.01 * i, 1.0 - 0.005 * i, 0.5])\n    loss = 0.1 / (1 + i * 0.1)\n    save_checkpoint(demo_dir, i, params, loss, metadata={\"epoch\": i // 10})\n\n# Load latest checkpoint\nprint(\"\\n  Loading latest checkpoint for resume...\")\nlatest = load_latest_checkpoint(demo_dir)\n\nif latest:\n    print(f\"    Iteration: {latest['iteration']}\")\n    print(f\"    Parameters: {latest['params']}\")\n    print(f\"    Loss: {latest['loss']:.6f}\")\n\n# Run optimization with checkpoints\nprint(\"\\n  Running optimization loop with checkpoints...\")\nfinal_params = optimization_with_checkpoints(demo_dir, max_iterations=50)\nprint(f\"  Final parameters: {final_params}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. HPC Distributed Preset\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"5. HPC Workflow Presets:\")\nprint(\"-\" * 50)\n\nif \"hpc_distributed\" in WORKFLOW_PRESETS:\n    hpc_preset = WORKFLOW_PRESETS[\"hpc_distributed\"]\n    print(\"  hpc_distributed preset:\")\n    for key, value in list(hpc_preset.items())[:8]:\n        print(f\"    {key}: {value}\")\nelse:\n    print(\"  Available presets for HPC:\")\n    for name in WORKFLOW_PRESETS:\n        desc = WORKFLOW_PRESETS[name].get(\"description\", \"\")\n        print(f\"    - {name}: {desc}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. PBS Pro Job Script\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"6. PBS Pro Job Script:\")\nprint(\"-\" * 50)\n\npbs_script = \"\"\"#!/bin/bash\n#PBS -N nlsq_fit\n#PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\n#PBS -l walltime=24:00:00\n#PBS -q gpu\n#PBS -j oe\n#PBS -o nlsq_fit.log\n\n# NLSQ Curve Fitting Job Script for PBS Pro\n# ===========================================\n\ncd $PBS_O_WORKDIR\n\n# Load required modules\nmodule load python/3.12\nmodule load cuda/12.0\nmodule load cudnn/8.9\n\n# Activate virtual environment\nsource ./venv/bin/activate\n\n# Set NLSQ environment variables\nexport NLSQ_WORKFLOW_GOAL=robust\nexport NLSQ_MEMORY_LIMIT_GB=200\nexport NLSQ_CHECKPOINT_DIR=$PBS_O_WORKDIR/checkpoints\n\n# Create checkpoint directory\nmkdir -p $NLSQ_CHECKPOINT_DIR\n\n# Display job information\necho \"========================================\"\necho \"NLSQ Fitting Job Started\"\necho \"========================================\"\necho \"Job ID: $PBS_JOBID\"\necho \"Node list:\"\ncat $PBS_NODEFILE\necho \"========================================\"\n\n# Run NLSQ fitting script\npython fit_large_dataset.py \\\\\n--data-file ./data/large_dataset.h5 \\\\\n--output-dir ./results \\\\\n--checkpoint-dir $NLSQ_CHECKPOINT_DIR \\\\\n--enable-checkpoints \\\\\n--checkpoint-interval 50\n\necho \"========================================\"\necho \"Job Completed: $(date)\"\necho \"========================================\"\n\"\"\"\n\npbs_script_path = Path(\"nlsq_fit.pbs\")\npbs_script_path.write_text(pbs_script)\n\nprint(\"  Created PBS job script: nlsq_fit.pbs\")\nprint(\"  Key directives:\")\nprint(\"    #PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\")\nprint(\"    #PBS -l walltime=24:00:00\")\nprint(\"    #PBS -q gpu\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7. Defense Layers for Checkpoint Resume (v0.3.6+)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"7. Defense Layers for Checkpoint Resume (v0.3.6+):\")\nprint(\"-\" * 70)\nprint()\nprint(\"When resuming from checkpoints, your initial parameters are near-optimal.\")\nprint(\"This is a classic warm-start scenario where defense layers are critical.\")\nprint()\nprint(\"Without defense layers, L-BFGS warmup can DIVERGE from your checkpoint:\")\nprint(\"  - Momentum builds up from large initial gradients\")\nprint(\"  - Parameters overshoot and loss increases\")\nprint(\"  - All progress from previous run is lost\")\nprint()\nprint(\"With 4-layer defense, checkpoint resume is protected:\")\nprint(\"  Layer 1: Detects you're starting near-optimal -> may skip warmup\")\nprint(\"  Layer 2: Scales learning rate based on initial fit quality\")\nprint(\"  Layer 3: Aborts warmup if loss increases > 5%\")\nprint(\"  Layer 4: Clips step magnitudes to prevent overshooting\")\nprint()\nprint(\"Recommended configuration for checkpoint resume:\")\nprint()\nprint(\"  from nlsq import HybridStreamingConfig\")\nprint()\nprint(\"  # Use defense_strict for checkpoint resume scenarios\")\nprint(\"  config = HybridStreamingConfig.defense_strict()\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cleanup\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"Cleaning up...\")\n\nfor path_str in [\n    \"nlsq_checkpoints\",\n    \"demo_checkpoints\",\n    \"my_project_checkpoints\",\n]:\n    path = Path(path_str)\n    if path.exists():\n        shutil.rmtree(path)\n        print(f\"  Removed: {path_str}\")\n\nif pbs_script_path.exists():\n    pbs_script_path.unlink()\n    print(\"  Removed: nlsq_fit.pbs\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print()\nprint(\"=\" * 70)\nprint(\"Summary\")\nprint(\"=\" * 70)\nprint()\nprint(\"HPC Integration:\")\nprint(\"  - ClusterDetector.detect() for PBS Pro detection\")\nprint(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\nprint()\nprint(\"Checkpointing:\")\nprint(\"  - Use streaming strategy for very large datasets\")\nprint(\"  - create_checkpoint_directory() for timestamped directories\")\nprint(\"  - JSON-based checkpoints for portability\")\nprint()\nprint(\"PBS Pro Job Scripts:\")\nprint(\"  - #PBS -l select=N:ncpus=C:ngpus=G:mem=Mgb\")\nprint(\"  - Environment variables: NLSQ_WORKFLOW_GOAL, NLSQ_MEMORY_LIMIT_GB\")\nprint(\"  - Checkpoint directory: NLSQ_CHECKPOINT_DIR\")\nprint()\nprint(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\nprint(\"  - Checkpoint resume = warm-start scenario (parameters near optimal)\")\nprint(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\nprint(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "code_18"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
