# ==============================================================================
# NLSQ Workflow Configuration Template
# ==============================================================================
# Version: 4.0 (Updated for NLSQ v0.6.0)
#
# This template provides comprehensive configuration options for NLSQ curve
# fitting workflows. Copy this file to your project and customize as needed.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ CLI USAGE                                                                   │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Single fit:    nlsq fit workflow.yaml                                       │
# │ Batch fits:    nlsq batch w1.yaml w2.yaml w3.yaml                           │
# │                nlsq batch configs/*.yaml    # Shell glob expansion          │
# │ System info:   nlsq info                                                    │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Quick Start:
#   1. Copy this file: cp workflow_config_template.yaml my_workflow.yaml
#   2. Configure: data.input_file, data.columns, model.type, model.name
#   3. Run: nlsq fit my_workflow.yaml
#
# Minimal Configuration Example:
#   data:
#     input_file: "data/experiment.csv"
#     columns: { x: 0, y: 1 }
#   model:
#     type: "builtin"
#     name: "exponential_decay"
#     auto_p0: true
#   export:
#     results_file: "results.json"
#
# Python API (alternative to CLI):
#   from nlsq.workflow import load_yaml_config
#   config = load_yaml_config("my_workflow.yaml")
#
# Documentation: https://nlsq.readthedocs.io/en/latest/guides/workflow_options.html
# ==============================================================================

# ==============================================================================
# TUTORIAL: Understanding NLSQ Workflow System
# ==============================================================================
#
# NLSQ provides a tiered workflow system that automatically selects optimal
# fitting strategies based on your dataset size, available memory, and goals.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ WORKFLOW TIERS (Processing Strategies)                                      │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ STANDARD              │ Direct curve_fit() for datasets < 10K points       │
# │ CHUNKED               │ Memory-managed chunking for 10K-10M points          │
# │ STREAMING             │ Mini-batch gradient descent for 10M-100M points     │
# │ STREAMING_CHECKPOINT  │ Streaming with fault tolerance for 100M+ points     │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ OPTIMIZATION GOALS                                                          │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ FAST              │ Speed priority, looser tolerances, no multi-start       │
# │ ROBUST            │ Balanced precision/speed, multi-start enabled           │
# │ GLOBAL            │ Synonym for ROBUST, emphasizes global optimum search    │
# │ QUALITY           │ Highest precision, tighter tolerances, validation       │
# │ MEMORY_EFFICIENT  │ Minimize memory with streaming/small chunks             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DATASET SIZE TIERS & RECOMMENDED TOLERANCES                                 │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Size Tier    │ Data Points   │ Recommended tol │ Notes                      │
# │──────────────│───────────────│─────────────────│────────────────────────────│
# │ TINY         │ < 1,000       │ 1e-12           │ Maximum precision          │
# │ SMALL        │ 1K - 10K      │ 1e-10           │ High precision             │
# │ MEDIUM       │ 10K - 100K    │ 1e-9            │ Balanced                   │
# │ LARGE        │ 100K - 1M     │ 1e-8            │ Standard (NLSQ default)    │
# │ VERY_LARGE   │ 1M - 10M      │ 1e-7            │ Chunked processing         │
# │ HUGE         │ 10M - 100M    │ 1e-6            │ Streaming mode             │
# │ MASSIVE      │ > 100M        │ 1e-5            │ Streaming + checkpoints    │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ WORKFLOW SELECTION MATRIX                                                   │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Dataset Size │ Low (<16GB) │ Medium (16-64GB) │ High (64-128GB) │ >128GB    │
# │──────────────│─────────────│──────────────────│─────────────────│───────────│
# │ Small <10K   │ standard    │ standard         │ standard        │ +quality  │
# │ Medium 10K-1M│ chunked     │ standard         │ standard+ms     │ +ms       │
# │ Large 1M-10M │ streaming   │ chunked          │ chunked+ms      │ chunked+ms│
# │ Huge 10M-100M│ stream+ckpt │ streaming        │ chunked         │ chunked+ms│
# │ Massive >100M│ stream+ckpt │ streaming+ckpt   │ streaming       │ stream+ms │
# └─────────────────────────────────────────────────────────────────────────────┘
# (ms = multi-start, ckpt = checkpointing)
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ FOUR-PHASE HYBRID STREAMING OPTIMIZER                                       │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ The streaming optimizer uses a 4-phase approach for optimal convergence:    │
# │                                                                             │
# │ Phase 0: Parameter Normalization                                            │
# │   - Scales parameters to similar magnitudes                                 │
# │   - Strategies: 'auto', 'bounds', 'p0', 'none'                              │
# │   - Improves gradient signal quality                                        │
# │                                                                             │
# │ Phase 1: L-BFGS Warmup (Quasi-Newton line search)                           │
# │   - Fast initial convergence from any starting point                        │
# │   - Adaptive switching based on plateau/gradient criteria                   │
# │   - Default: 200-500 iterations                                             │
# │   - Protected by 4-Layer Defense Strategy                        │
# │                                                                             │
# │ Phase 2: Streaming Gauss-Newton                                             │
# │   - Exact J^T J accumulation across data chunks                             │
# │   - Quadratic convergence near optimum                                      │
# │   - Trust region control for stability                                      │
# │                                                                             │
# │ Phase 3: Covariance & Denormalization                                       │
# │   - Compute exact covariance matrix                                         │
# │   - Transform back to original parameter space                              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ 4-LAYER DEFENSE STRATEGY                                          │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Prevents L-BFGS warmup divergence when initial parameters are near optimal: │
# │                                                                             │
# │ Layer 1: Warm Start Detection                                               │
# │   - Skips warmup if initial loss < warm_start_threshold (default: 1%)       │
# │   - Prevents pushing away from good initial guesses                         │
# │   - Use Case: Refinement workflows, iterative fitting                       │
# │                                                                             │
# │ Layer 2: Adaptive Step Size                                                 │
# │   - Refinement mode (< 10% variance): LR = 1e-6 (ultra-conservative)        │
# │   - Careful mode (10-100% variance): LR = 1e-5 (conservative)               │
# │   - Exploration mode (≥ 100% variance): LR = 0.001 (standard)               │
# │   - Use Case: Multi-scale parameters, varying initial guess quality         │
# │                                                                             │
# │ Layer 3: Cost-Increase Guard                                                │
# │   - Aborts warmup if loss increases > cost_increase_tolerance (default: 5%) │
# │   - Returns best parameters found during warmup                             │
# │   - Use Case: Detecting divergence early, protecting expensive iterations   │
# │                                                                             │
# │ Layer 4: Step Clipping                                                      │
# │   - Limits parameter update magnitude to max_warmup_step_size (default: 0.1)│
# │   - Prevents large jumps that could overshoot optimum                       │
# │   - Use Case: Ill-conditioned problems, multi-scale parameters              │
# │                                                                             │
# │ Defense Presets:                                                            │
# │   - default:           All layers ON (recommended)                          │
# │   - defense_strict:    Lower thresholds for warm-start refinement           │
# │   - defense_relaxed:   Higher thresholds for exploration                    │
# │   - defense_disabled:  Pre-0.3.6 behavior (no protection)                   │
# │   - scientific_default: Tuned for physics/scientific computing              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ==============================================================================
# TUTORIAL: Common Configuration Patterns
# ==============================================================================
#
# Pattern 1: Small Dataset, Maximum Precision (< 10K points)
# ──────────────────────────────────────────────────────────
#   default_workflow: "quality"
#   # or custom:
#   workflows:
#     my_precision:
#       tier: "STANDARD"
#       goal: "QUALITY"
#       gtol: 1.0e-12
#       ftol: 1.0e-12
#       xtol: 1.0e-12
#       enable_multistart: true
#       n_starts: 25
#
# Pattern 2: Large Dataset, Balanced Performance (100K - 10M points)
# ──────────────────────────────────────────────────────────────────
#   default_workflow: "large_robust"
#   # or custom:
#   workflows:
#     my_large:
#       tier: "CHUNKED"
#       goal: "ROBUST"
#       gtol: 1.0e-8
#       ftol: 1.0e-8
#       xtol: 1.0e-8
#       enable_multistart: true
#       n_starts: 10
#       chunk_size: 100000
#
# Pattern 3: Huge Dataset, Memory Constrained (10M+ points, <16GB RAM)
# ────────────────────────────────────────────────────────────────────
#   default_workflow: "streaming"
#   memory_limit_gb: 8.0
#   # or custom:
#   workflows:
#     my_streaming:
#       tier: "STREAMING"
#       goal: "MEMORY_EFFICIENT"
#       gtol: 1.0e-6
#       enable_checkpoints: true
#       checkpoint_dir: "checkpoints"
#
# Pattern 4: HPC Multi-GPU Cluster (PBS Pro)
# ──────────────────────────────────────────
#   default_workflow: "hpc_distributed"
#   # Auto-detects cluster via PBS_NODEFILE
#
# Pattern 5: Quick Development/Testing
# ────────────────────────────────────
#   default_workflow: "fast"
#   runtime:
#     precision: "float32"
#   fitting:
#     termination:
#       max_iterations: 50
#
# Pattern 6: Warm Start Refinement
# ──────────────────────────────────────────
# Use when refining parameters from a previous fit.
#   default_workflow: "streaming"
#   hybrid_streaming:
#     defense_layers:
#       preset: "strict"  # Or configure layers individually:
#       layer1_warm_start:
#         enabled: true
#         threshold: 0.005  # 0.5% - stricter than default 1%
#       layer2_adaptive_lr:
#         enabled: true
#         lr_refinement: 1.0e-7  # More conservative
#       layer3_cost_guard:
#         enabled: true
#         tolerance: 0.02  # 2% - tighter than default 5%
#
# Pattern 7: Production Monitoring with Telemetry
# ─────────────────────────────────────────────────────────
# For batch processing with defense layer monitoring.
#   default_workflow: "streaming"
#   hybrid_streaming:
#     defense_layers:
#       preset: null  # Use defaults
#     telemetry:
#       enabled: true
#       export_format: "prometheus"
#   # In code:
#   #   from nlsq import get_defense_telemetry, reset_defense_telemetry
#   #   reset_defense_telemetry()
#   #   for dataset in datasets:
#   #       curve_fit(model, x, y, method="hybrid_streaming")
#   #   telemetry = get_defense_telemetry()
#   #   push_to_prometheus(telemetry.export_metrics())
#
# ==============================================================================

metadata:
  # Human-friendly name for logs/reports. Keep short, unique, and stable.
  workflow_name: "example_workflow"
  # Brief description of workflow purpose.
  description: "NLSQ workflow configuration template with optimized defaults."
  # Configuration version for tracking changes.
  version: "4.0"
  # Optional ownership information.
  author: ""
  contact: ""

# ==============================================================================
# WORKFLOW SYSTEM SETTINGS
# ==============================================================================
# These settings control the NLSQ workflow tier and optimization behavior.
# The default_workflow can be a preset name or a custom workflow defined below.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ BUILT-IN WORKFLOW PRESETS (from nlsq.core.minpack.WORKFLOW_PRESETS)         │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │   - standard:         Standard curve_fit() with default tolerances (1e-8)   │
# │   - quality:          Highest precision with multi-start (tol 1e-10)        │
# │   - fast:             Speed-optimized with looser tolerances (1e-6)         │
# │   - large_robust:     Chunked processing with multi-start for large data    │
# │   - streaming:        AdaptiveHybridStreamingOptimizer for huge datasets    │
# │   - hpc_distributed:  Multi-GPU/node configuration for HPC clusters (PBS)   │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Note: For custom workflows beyond these presets, define them in the 'workflows'
# section below or pass a config object directly to fit().

default_workflow: "standard"

# Memory limit for workflow auto-selection. Set to null to auto-detect.
# Performance: Lower limits force streaming/chunked tiers.
# Precision: Higher limits allow more data in memory for better accuracy.
memory_limit_gb: null  # Auto-detect (recommended)

# Custom workflow definitions. Each entry maps to WorkflowConfig.from_dict().
# Override any preset or create entirely new configurations.
workflows:
  # Example: Precision-focused workflow for publication-quality results
  precision_first:
    tier: "STANDARD"
    goal: "QUALITY"
    gtol: 1.0e-10
    ftol: 1.0e-10
    xtol: 1.0e-10
    enable_multistart: true
    n_starts: 20
    sampler: "lhs"           # "lhs", "sobol", or "halton"
    center_on_p0: true       # Center samples around initial guess
    scale_factor: 1.0        # Exploration region scale
    enable_checkpoints: false

  # Example: Large dataset with multi-start
  large_multistart:
    tier: "CHUNKED"
    goal: "ROBUST"
    gtol: 1.0e-8
    ftol: 1.0e-8
    xtol: 1.0e-8
    enable_multistart: true
    n_starts: 10
    sampler: "lhs"
    chunk_size: 100000       # Points per chunk (CHUNKED tier only)
    enable_checkpoints: false

  # Example: Memory-constrained streaming
  low_memory_streaming:
    tier: "STREAMING"
    goal: "MEMORY_EFFICIENT"
    gtol: 1.0e-7
    ftol: 1.0e-7
    xtol: 1.0e-7
    enable_multistart: false
    chunk_size: 5000         # Smaller chunks for lower memory
    enable_checkpoints: true
    checkpoint_dir: "checkpoints"

  # Example: Spectroscopy peak fitting (uses precision_high-like settings)
  spectroscopy_peaks:
    tier: "STANDARD"
    goal: "QUALITY"
    gtol: 1.0e-10
    ftol: 1.0e-10
    xtol: 1.0e-10
    enable_multistart: true
    n_starts: 15
    sampler: "lhs"

  # Example: Multimodal optimization (multiple local minima)
  multimodal_search:
    tier: "STANDARD"
    goal: "GLOBAL"
    enable_multistart: true
    n_starts: 30
    sampler: "sobol"         # Better coverage for multimodal surfaces
    gtol: 1.0e-8
    ftol: 1.0e-8
    xtol: 1.0e-8

  # Example: Long time series with streaming
  timeseries_streaming:
    tier: "STREAMING"
    goal: "ROBUST"
    enable_multistart: true
    n_starts: 10
    sampler: "lhs"
    gtol: 1.0e-7
    ftol: 1.0e-7
    xtol: 1.0e-7
    enable_checkpoints: true
    checkpoint_frequency: 100

# ==============================================================================
# PATHS
# ==============================================================================
# File system locations for workflow artifacts.
# Use absolute paths for production; relative paths resolve from project_root.

paths:
  # Base directory for relative paths. Set explicitly for portability.
  project_root: "/path/to/project"
  # Input data location.
  input_dir: "data/input"
  # Primary outputs (results, reports).
  output_dir: "data/output"
  # Log files; keep separate for cleanup.
  logs_dir: "logs"
  # JIT compilation cache; safe to clear.
  cache_dir: "cache"
  # Temporary scratch space; use fast local storage.
  temp_dir: "tmp"

# ==============================================================================
# RUNTIME
# ==============================================================================
# Execution environment settings affecting performance and reproducibility.

runtime:
  # Device selection: "cpu" or "gpu".
  # CPU is default for reproducibility; GPU provides 150-270x speedup.
  # Note: GPU requires Linux + NVIDIA + CUDA 12.1-12.9
  device: "cpu"

  # Thread/worker counts for parallel sections.
  # Performance: Set <= physical cores to avoid oversubscription.
  threads: 4
  workers: 2

  # Numerical precision.
  # "float64" (default): Maximum accuracy, recommended for scientific work.
  # "float32": Faster, less memory, but reduced precision.
  # Precision priority per user requirements - always use float64.
  precision: "float64"
  dtype: "float64"

  # Random seed for reproducibility.
  # Determinism: Same seed produces identical results on same hardware.
  random_seed: 42

  # Verbosity and logging.
  # 0: Silent, 1: Progress, 2: Detailed, 3: Debug
  verbosity: 1
  log_level: "INFO"

# ==============================================================================
# LOGGING
# ==============================================================================
# Logging configuration for debugging and audit trails.

logging:
  # Log file path (relative to logs_dir or absolute).
  log_file: "workflow.log"
  # Console logging toggle.
  console: true
  # Structured logging for external tool ingestion.
  structured:
    enabled: false
    format: "json"  # "json" for tooling compatibility
  # Log rotation to prevent unbounded growth.
  rotation:
    enabled: true
    max_bytes: 10485760  # 10 MB
    backup_count: 5
  # Timestamps.
  timestamps:
    enabled: true
    timezone: "UTC"

# ==============================================================================
# DATA
# ==============================================================================
# Dataset configuration for CLI data loading.
# Supports: ASCII text (.txt, .dat, .asc), CSV (.csv), NPZ (.npz), HDF5 (.h5, .hdf5)
#
# Data Modes:
#   1D Mode (default): x, y, sigma - Standard curve fitting
#   2D Mode: x, y, z, sigma - Surface/image fitting (x, y are coordinates, z is dependent)
#
# Usage with CLI:
#   nlsq fit workflow.yaml           # Single fit
#   nlsq batch w1.yaml w2.yaml ...   # Parallel batch fitting

data:
  # Dataset identifier for reports/exports.
  dataset_id: "dataset_001"

  # ==========================================================================
  # INPUT FILE (Required for CLI)
  # ==========================================================================
  # Path to the data file (relative to project_root or absolute).
  input_file: "data/experiment_001.csv"

  # File format: "auto" (detect from extension) or explicit.
  # Supported: "ascii", "csv", "npz", "hdf5"
  format: "auto"

  # ==========================================================================
  # COLUMN SELECTION
  # ==========================================================================
  # Specify which columns contain the data.
  # Use integer index (0-based) for ASCII/CSV, or key/path names for NPZ/HDF5.
  #
  # 1D Mode (curve fitting):
  #   - x: independent variable
  #   - y: dependent variable
  #   - z: null (not used)
  #   - sigma: uncertainties on y (optional)
  #   Model signature: f(x, *params)
  #
  # 2D Mode (surface/image fitting):
  #   - x: first coordinate (independent)
  #   - y: second coordinate (independent)
  #   - z: dependent variable (required for 2D mode)
  #   - sigma: uncertainties on z (optional)
  #   Model signature: f(xy, *params) where xy[0]=x, xy[1]=y
  #
  # The data loader automatically detects 2D mode when z is specified.
  columns:
    x: 0              # Column index (int) or column name (str for CSV with header)
    y: 1              # Column index (int) or column name (str for CSV with header)
    z: null           # Optional: column for z data (if set, enables 2D surface mode)
    sigma: null       # Optional: column for uncertainties (null = no weights)

  # ==========================================================================
  # ASCII TEXT FORMAT OPTIONS (.txt, .dat, .asc)
  # ==========================================================================
  # For whitespace or tab-delimited text files.
  ascii:
    delimiter: null           # null = any whitespace, or explicit: "\t", " ", ","
    comment_char: "#"         # Lines starting with this are skipped
    skip_header: 0            # Number of header lines to skip
    skip_footer: 0            # Number of footer lines to skip
    usecols: null             # Specific columns to read, e.g., [0, 1] (null = all)
    dtype: "float64"          # Data type: "float64", "float32"

  # ==========================================================================
  # CSV FORMAT OPTIONS (.csv)
  # ==========================================================================
  csv:
    delimiter: ","            # Field delimiter
    header: true              # First row contains column names
    skip_header: 0            # Additional header lines to skip (after column names)
    encoding: "utf-8"         # File encoding
    missing_values:           # Values treated as NaN
      - ""
      - "NA"
      - "null"
      - "NaN"

  # ==========================================================================
  # NPZ FORMAT OPTIONS (.npz)
  # ==========================================================================
  # NumPy compressed archive format.
  # For 2D mode, specify z_key to enable surface fitting.
  npz:
    x_key: "x"                # Array key for x data
    y_key: "y"                # Array key for y data
    z_key: null               # Optional: array key for z data (enables 2D mode)
    sigma_key: null           # Optional: array key for sigma (null = no weights)

  # ==========================================================================
  # HDF5 FORMAT OPTIONS (.h5, .hdf5)
  # ==========================================================================
  # Hierarchical Data Format 5 for large datasets.
  # For 2D mode, specify z_path to enable surface fitting.
  hdf5:
    x_path: "/data/x"         # Dataset path for x data
    y_path: "/data/y"         # Dataset path for y data
    z_path: null              # Optional: dataset path for z data (enables 2D mode)
    sigma_path: null          # Optional: dataset path for sigma (null = no weights)

  # ==========================================================================
  # LEGACY: File discovery patterns (for programmatic use)
  # ==========================================================================
  file_patterns:
    - "*.csv"
    - "*.txt"
    - "*.dat"
    - "*.npz"
    - "*.h5"
    - "*.hdf5"

  # ==========================================================================
  # VALIDATION RULES
  # ==========================================================================
  validation:
    require_finite: true      # Reject NaN/Inf values
    min_points: 2             # Minimum data points required
    units:
      x: "unitless"
      y: "unitless"
    ranges:
      x:
        min: null
        max: null
      y:
        min: null
        max: null

# ==============================================================================
# PREPROCESSING
# ==============================================================================
# Data preprocessing before fitting.
# Note: Subsampling was removed in v0.2.0. NLSQ uses streaming for 100% data.

preprocessing:
  # Normalization/scaling before fitting.
  # Warning: Changing preprocessing changes parameter interpretation.
  normalization:
    enabled: false
    method: "standard"  # "standard", "minmax", "robust", "none"
  # Outlier filtering.
  filtering:
    enabled: false
    method: "none"      # "none", "iqr", "zscore", "custom"
    threshold: 3.0

# ==============================================================================
# MODEL
# ==============================================================================
# Model function configuration for CLI fitting.
#
# Three model types are supported:
#   1. "builtin"    - Use nlsq.functions library (recommended)
#   2. "custom"     - Load from external Python file
#   3. "polynomial" - Generate polynomial of specified degree
#
# Model Signatures:
#   1D fitting: f(x, *params) where x is 1D array
#   2D fitting: f(xy, *params) where xy is shape (2, n) with xy[0]=x, xy[1]=y
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ BUILTIN MODELS (nlsq.functions) - 1D Models                                 │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Name               │ Function                    │ Parameters               │
# │────────────────────│─────────────────────────────│──────────────────────────│
# │ linear             │ a*x + b                     │ a, b                     │
# │ exponential_decay  │ a*exp(-b*x) + c             │ a, b, c                  │
# │ exponential_growth │ a*exp(b*x) + c              │ a, b, c                  │
# │ gaussian           │ amp*exp(-(x-mu)²/(2σ²))     │ amp, mu, sigma           │
# │ sigmoid            │ L/(1+exp(-k*(x-x0))) + b    │ L, x0, k, b              │
# │ power_law          │ a*x^b                       │ a, b                     │
# │ polynomial         │ Σ(aᵢ*x^i)                   │ a0, a1, ..., an          │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# For 2D surface fitting, use custom models with signature:
#   def model_2d(xy, *params):
#       x, y = xy[0], xy[1]
#       return f(x, y, *params)

model:
  # ==========================================================================
  # MODEL SOURCE TYPE (Required for CLI)
  # ==========================================================================
  # Options: "builtin", "custom", "polynomial"
  type: "builtin"

  # ==========================================================================
  # BUILTIN MODEL CONFIGURATION
  # ==========================================================================
  # When type: "builtin", specify the model name from nlsq.functions.
  name: "exponential_decay"   # See table above for available models

  # Automatic initial parameter estimation (uses data characteristics).
  # Each builtin model has a corresponding estimate_p0_<name>() function.
  auto_p0: true

  # Automatic bounds from builtin model defaults.
  # Each builtin model has a corresponding bounds_<name>() function.
  auto_bounds: false

  # ==========================================================================
  # CUSTOM MODEL CONFIGURATION
  # ==========================================================================
  # When type: "custom", load model function from external Python file.
  #
  # 1D Model signature: def model_name(x, *params) -> array
  # 2D Model signature: def model_name(xy, *params) -> array
  #
  # Example 1D custom model (models.py):
  #   import jax.numpy as jnp
  #   def my_model(x, a, b, c):
  #       return a * jnp.exp(-b * x) + c
  #
  # Example 2D custom model for surface fitting (models.py):
  #   import jax.numpy as jnp
  #   def gaussian_2d(xy, amp, x0, y0, sigma_x, sigma_y, offset):
  #       """2D Gaussian surface model."""
  #       x, y = xy[0], xy[1]
  #       return amp * jnp.exp(
  #           -((x - x0)**2 / (2 * sigma_x**2) +
  #             (y - y0)**2 / (2 * sigma_y**2))
  #       ) + offset
  #
  custom:
    file: "models.py"         # Python file path (relative to project_root)
    function: "my_model"      # Function name to import

  # ==========================================================================
  # POLYNOMIAL MODEL CONFIGURATION
  # ==========================================================================
  # When type: "polynomial", generate polynomial of specified degree.
  # f(x) = a0 + a1*x + a2*x² + ... + an*x^n
  polynomial:
    degree: 3                 # Polynomial degree (n)

  # ==========================================================================
  # MODEL IDENTIFIER
  # ==========================================================================
  model_id: "model_v1"        # Unique identifier for reports

  # ==========================================================================
  # PARAMETER DEFINITIONS
  # ==========================================================================
  # Manual parameter configuration. Used when:
  #   - auto_p0: false (provides initial values)
  #   - auto_bounds: false (provides bounds)
  #   - type: "custom" (required for custom models)
  #
  # Bounds should be informed by physical constraints; overly tight bounds bias.
  parameters:
    - name: "amplitude"
      initial: 1.0
      bounds: [0.0, 10.0]
      fixed: false
      transform: "none"       # "none", "log", "logit", "exp"
    - name: "decay_rate"
      initial: 0.1
      bounds: [0.001, 1.0]
      fixed: false
      transform: "none"
    - name: "offset"
      initial: 0.0
      bounds: [-1.0, 1.0]
      fixed: false
      transform: "none"

  # ==========================================================================
  # PARAMETER CONSTRAINTS (Advanced)
  # ==========================================================================
  constraints:
    enabled: false
    expressions:
      - "amplitude > offset"

# ==============================================================================
# FITTING
# ==============================================================================
# Core fitting algorithm configuration.

fitting:
  # Fitting method.
  method: "nlsq"  # "nlsq", "hybrid_streaming"
  # Objective function options.
  objective:
    weights: null        # null or array/column reference
    robust_loss: "none"  # "none", "huber", "soft_l1", "cauchy"
  # Convergence tolerances.
  # These should match dataset size tier recommendations (see tutorial above).
  termination:
    ftol: 1.0e-8   # Function tolerance
    xtol: 1.0e-8   # Parameter tolerance
    gtol: 1.0e-8   # Gradient tolerance
    max_iterations: 200
    max_function_evals: 2000
  # Jacobian computation.
  jacobian:
    mode: "auto"   # "auto" uses JAX autodiff (recommended)
    finite_diff_step: 1.0e-8
  # Multi-start optimization for global optimum search.
  multistart:
    enabled: false
    num_starts: 10
    sampler: "lhs"       # "lhs", "sobol", "halton"
    center_on_p0: true   # Center around initial guess
    scale_factor: 1.0    # Exploration region scale
    seed: 42

# ==============================================================================
# HYBRID STREAMING OPTIMIZER
# ==============================================================================
# Configuration for the four-phase AdaptiveHybridStreamingOptimizer.
# Used when tier is STREAMING or STREAMING_CHECKPOINT.

hybrid_streaming:
  # Phase 0: Parameter Normalization
  normalize: true
  normalization_strategy: "auto"  # "auto", "bounds", "p0", "none"

  # Phase 1: L-BFGS Warmup
  warmup_iterations: 200        # Initial iterations before switch check
  max_warmup_iterations: 500    # Force switch to Phase 2
  warmup_learning_rate: 0.001   # Warmup step size (legacy name)
  loss_plateau_threshold: 1.0e-4   # Plateau detection threshold
  gradient_norm_threshold: 1.0e-3  # Early switch if gradient small

  # Optax learning rate schedule (optional)
  use_learning_rate_schedule: false
  lr_schedule_warmup_steps: 50
  lr_schedule_decay_steps: 450
  lr_schedule_end_value: 0.0001
  gradient_clip_value: null  # Set to 1.0 for gradient clipping

  # ==========================================================================
  # 4-LAYER DEFENSE STRATEGY
  # ==========================================================================
  # Prevents L-BFGS warmup divergence when initial parameters are near optimal.
  # All layers are enabled by default for improved stability.
  #
  # Presets available in code:
  #   - HybridStreamingConfig()                  # Default (all ON)
  #   - HybridStreamingConfig.defense_strict()  # Lower thresholds
  #   - HybridStreamingConfig.defense_relaxed() # Higher thresholds
  #   - HybridStreamingConfig.defense_disabled() # Pre-0.3.6 behavior
  #   - HybridStreamingConfig.scientific_default() # Physics/scientific
  #
  # Telemetry API for monitoring:
  #   from nlsq import get_defense_telemetry, reset_defense_telemetry
  #   telemetry = get_defense_telemetry()
  #   print(telemetry.get_summary())
  #   print(telemetry.get_trigger_rates())
  #   print(telemetry.export_metrics())  # Prometheus-compatible
  # ==========================================================================

  defense_layers:
    # Defense preset: "default", "strict", "relaxed", "disabled", "scientific"
    # Set to null to use individual layer settings below.
    preset: null

    # Layer 1: Warm Start Detection
    # Skips warmup if initial loss < threshold * data_variance
    # Use Case: Refinement workflows, warm starts from previous fits
    layer1_warm_start:
      enabled: true
      threshold: 0.01  # 1% of data variance (strict: 0.005, relaxed: 0.02)

    # Layer 2: Adaptive Step Size
    # Automatically selects step size based on initial fit quality
    # Use Case: Multi-scale parameters, varying initial guess quality
    layer2_adaptive_lr:
      enabled: true
      lr_refinement: 1.0e-6    # < 10% variance (ultra-conservative)
      lr_careful: 1.0e-5       # 10-100% variance (conservative)
      lr_exploration: 0.001    # >= 100% variance (exploration step size)

    # Layer 3: Cost-Increase Guard
    # Aborts warmup if loss increases beyond tolerance from initial
    # Use Case: Early divergence detection, protecting expensive iterations
    layer3_cost_guard:
      enabled: true
      tolerance: 0.05  # 5% increase allowed (strict: 0.02, relaxed: 0.10)

    # Layer 4: Step Clipping
    # Limits maximum parameter update magnitude per iteration
    # Use Case: Ill-conditioned problems, preventing overshooting
    layer4_step_clipping:
      enabled: true
      max_step_size: 0.1  # Max L2 norm (strict: 0.05, relaxed: 0.2)

  # Phase 2: Gauss-Newton
  gauss_newton_max_iterations: 100
  gauss_newton_tol: 1.0e-8
  trust_region_initial: 1.0
  regularization_factor: 1.0e-10

  # Streaming chunk configuration
  chunk_size: 10000  # Points per chunk

  # Fault tolerance
  enable_checkpoints: true
  checkpoint_frequency: 100
  checkpoint_dir: null  # Auto-generate timestamped directory
  validate_numerics: true
  enable_fault_tolerance: true
  max_retries_per_batch: 2
  min_success_rate: 0.5

  # Precision control
  # "auto": float32 for Phase 1, float64 for Phase 2+ (recommended)
  precision: "auto"

  # Multi-device (multi-GPU) support
  enable_multi_device: false

  # Progress monitoring
  callback_frequency: 10

  # Telemetry for production monitoring
  # Tracks defense layer activation rates across fits
  telemetry:
    enabled: true
    export_format: "prometheus"  # "prometheus", "json", "none"

  # Tournament selection for multi-start streaming
  enable_multistart: false
  n_starts: 10
  multistart_sampler: "lhs"
  elimination_rounds: 3
  elimination_fraction: 0.5
  batches_per_round: 50

# ==============================================================================
# GLOBAL OPTIMIZATION
# ==============================================================================
# Multi-start global optimization with Latin Hypercube Sampling.

global_optimization:
  # Presets: "fast", "robust", "global", "thorough", "streaming"
  preset: null  # Use preset or define custom below

  # Custom configuration (overrides preset if set)
  n_starts: 10               # Number of starting points (0 = disabled)
  sampler: "lhs"             # "lhs", "sobol", "halton"
  center_on_p0: true         # Center samples around initial guess
  scale_factor: 1.0          # Exploration region scale factor

  # Tournament selection for large datasets
  elimination_rounds: 3      # Rounds of elimination
  elimination_fraction: 0.5  # Fraction eliminated per round
  batches_per_round: 50      # Data batches for evaluation

# ==============================================================================
# TRUST REGION / STEP SIZE
# ==============================================================================
# Advanced optimization tuning. Change only for troubleshooting convergence.

optimization:
  step_size:
    initial: 1.0
    min: 1.0e-10
    max: 10.0
  trust_region:
    enabled: true
    initial_radius: 1.0
    max_radius: 100.0
    min_radius: 1.0e-10
  damping:
    lambda_init: 1.0
    lambda_min: 1.0e-10
    lambda_max: 1.0e10

# ==============================================================================
# WORKFLOW STEPS
# ==============================================================================
# Ordered pipeline steps. Disable unneeded steps for performance.

workflow_steps:
  - name: "load_data"
    enabled: true
    inputs: []
    outputs: ["raw_dataset"]
  - name: "validate_data"
    enabled: true
    inputs: ["raw_dataset"]
    outputs: ["validated_dataset"]
  - name: "preprocess"
    enabled: true
    inputs: ["validated_dataset"]
    outputs: ["processed_dataset"]
  - name: "fit_model"
    enabled: true
    inputs: ["processed_dataset"]
    outputs: ["fit_result"]
  - name: "postprocess"
    enabled: true
    inputs: ["fit_result"]
    outputs: ["final_result"]

# ==============================================================================
# VALIDATION AND QUALITY CONTROL
# ==============================================================================

validation_and_qc:
  # Sanity checks for outputs.
  sanity_checks:
    enabled: true
    checks:
      - "finite_parameters"
      - "nonnegative_variances"
      - "positive_definite_covariance"
  # Numerical parity checks (useful for CPU/GPU comparison).
  numerical_parity:
    enabled: false
    tolerance: 1.0e-8
  # Acceptance thresholds for fit quality.
  acceptance:
    max_rmse: null
    max_mae: null
    min_r2: null

# ==============================================================================
# REPORTING
# ==============================================================================

reporting:
  summary:
    enabled: true
    include_tables: true
    include_plots: false  # Disable for batch runs
  formats:
    - "json"
    - "csv"

# ==============================================================================
# VISUALIZATION
# ==============================================================================
# Publication-quality figure generation for fit results.
#
# Automatically generates:
#   - Combined main plot + residuals layout (single figure)
#   - Separate histogram of residuals (optional)
#   - Confidence bands from covariance matrix error propagation
#   - Fit statistics annotation (R², RMSE, χ²)
#
# Output formats: PDF (vector), PNG (raster), SVG (vector), EPS (LaTeX)
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ STYLE PRESETS                                                               │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ "publication"  │ Clean serif fonts, 300 DPI, suitable for most journals     │
# │ "presentation" │ Larger sans-serif fonts, lower DPI for slides              │
# │ "nature"       │ Nature journal specs (3.5" width, Arial, 300 DPI)          │
# │ "science"      │ Science journal specs (single column, tight layout)        │
# │ "minimal"      │ No top/right spines, no grid, clean minimal look           │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ COLORBLIND-SAFE PALETTES                                                    │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ "default"     │ Standard matplotlib colors (accessible to most)             │
# │ "colorblind"  │ Okabe-Ito palette (optimized for all color vision types)    │
# │ "grayscale"   │ Black/white printing compatible                             │
# │ "high_contrast" │ Maximum contrast for presentations                        │
# └─────────────────────────────────────────────────────────────────────────────┘

visualization:
  # ==========================================================================
  # MASTER SWITCH
  # ==========================================================================
  # Set to true to automatically save plots after fitting.
  # When enabled, generates publication-quality figures for all fits.
  enabled: true

  # ==========================================================================
  # OUTPUT SETTINGS
  # ==========================================================================
  # Directory for saving figures (relative to project_root or absolute).
  output_dir: "figures"

  # Filename prefix. Generates: {prefix}_combined.pdf, {prefix}_histogram.pdf, etc.
  filename_prefix: "fit"

  # Output formats to generate (all formats saved simultaneously).
  # Recommended: PDF for publications, PNG for web/slides
  formats:
    - "pdf"                       # Vector format, best for publications
    - "png"                       # Raster format for web/slides
    # - "svg"                     # Vector format for web (scalable)
    # - "eps"                     # Vector format for LaTeX embedding

  # ==========================================================================
  # PUBLICATION QUALITY SETTINGS
  # ==========================================================================
  # Resolution in dots per inch (DPI).
  #   150 DPI: Screen/web display
  #   300 DPI: Standard print quality (most journals)
  #   600 DPI: High-resolution print (archival)
  dpi: 300

  # Figure size in inches [width, height].
  # Common journal specifications:
  #   Single column: [3.5, 2.625] (Nature, Science, APS)
  #   Double column: [7.0, 5.25] (full width)
  #   Full page:     [7.0, 9.0]
  #   Slide (16:9):  [10.0, 5.625]
  figure_size: [6.0, 4.5]

  # ==========================================================================
  # STYLE PRESETS
  # ==========================================================================
  # Preset styles optimized for different contexts.
  # Each preset configures fonts, DPI, figure size, and layout automatically.
  #
  # Options:
  #   "publication"  - Clean serif fonts, 300 DPI, suitable for most journals
  #   "presentation" - Larger sans-serif fonts, 150 DPI for slides
  #   "nature"       - Nature journal specifications (3.5" width, Arial font)
  #   "science"      - Science journal specifications
  #   "minimal"      - No top/right spines, no grid, clean look
  style: "publication"

  # ==========================================================================
  # FONT SETTINGS
  # ==========================================================================
  font:
    # Font family: "serif" (Times-like), "sans-serif" (Arial-like), "monospace"
    # Recommendation: "serif" for publications, "sans-serif" for presentations
    family: "serif"

    # Base font size in points (axis labels, legends scale from this)
    # Recommendation: 10-12 pt for publications, 14-18 pt for slides
    size: 10

    # Math font for equations and symbols
    # Options: "cm" (Computer Modern), "stix", "dejavusans", "dejavuserif"
    math_fontset: "cm"

  # ==========================================================================
  # LAYOUT OPTIONS
  # ==========================================================================
  # Layout mode determines how plots are arranged:
  #   "combined": Main plot + residuals in subplots (single figure file)
  #   "separate": Individual files for each plot type
  layout: "combined"

  # ==========================================================================
  # MAIN PLOT (Data + Fitted Curve)
  # ==========================================================================
  main_plot:
    # Title (null = no title, or custom string)
    # Use LaTeX formatting: "$\\alpha$ decay" for Greek letters
    title: null

    # Axis labels (use LaTeX: "$x$ (units)" for math)
    x_label: "x"
    y_label: "y"

    # Grid lines
    show_grid: true
    grid_alpha: 0.3

    # --------------------------------------------------------------------------
    # Raw Data Appearance
    # --------------------------------------------------------------------------
    data:
      marker: "o"                 # Marker style: "o", "s", "^", "D", "x", "+"
      color: null                 # null = use active color scheme
      size: 20                    # Marker size in points²
      alpha: 0.7                  # Transparency (0-1)
      label: "Data"               # Legend label

      # Error bars (displayed if sigma provided in data)
      show_errorbars: true
      errorbar_color: null        # null = same as marker color
      capsize: 2                  # Error bar cap width in points

    # --------------------------------------------------------------------------
    # Fitted Curve Appearance
    # --------------------------------------------------------------------------
    fit:
      color: null                 # null = use active color scheme
      linewidth: 1.5              # Line width in points
      linestyle: "-"              # Style: "-", "--", "-.", ":"
      label: "Fit"                # Legend label
      n_points: 500               # Number of points for smooth curve

    # --------------------------------------------------------------------------
    # Confidence Band (Error Propagation from Covariance Matrix)
    # --------------------------------------------------------------------------
    # Displays uncertainty band around fitted curve computed from covariance.
    # Uses error propagation: σ_y = √(J·pcov·Jᵀ) where J is Jacobian at each x.
    # Requires covariance matrix (pcov) from curve_fit.
    confidence_band:
      enabled: true               # Enable confidence band visualization
      level: 0.95                 # Confidence level (0.95 = 95% CI, 0.99 = 99% CI)
      color: null                 # null = use fit line color with alpha
      alpha: 0.2                  # Band transparency (0-1)
      n_sigma: null               # Alternative: specify ±nσ directly (overrides level)
                                  # e.g., n_sigma: 2 shows ±2σ band

    # --------------------------------------------------------------------------
    # Legend
    # --------------------------------------------------------------------------
    legend:
      enabled: true
      location: "best"            # "best", "upper right", "upper left", etc.
      frameon: true               # Show legend frame
      fontsize: null              # null = use base font size

    # --------------------------------------------------------------------------
    # Fit Statistics Annotation Box
    # --------------------------------------------------------------------------
    # Displays fit quality metrics directly on the plot.
    # Position automatically avoids data points when location is "best".
    annotation:
      enabled: true
      show_r_squared: true        # Coefficient of determination (R²)
      show_rmse: true             # Root Mean Square Error
      show_chi_squared: true      # Reduced chi-squared (χ²/dof)
      show_n_points: false        # Number of data points
      show_n_params: false        # Number of fitted parameters
      location: "upper right"     # Text box location (or "best" for auto)
      fontsize: 9
      box_alpha: 0.8              # Background box transparency

  # ==========================================================================
  # RESIDUALS PLOT
  # ==========================================================================
  # Shows y_data - y_fit to visualize fit quality and systematic deviations.
  # Good fits show random scatter around zero; patterns indicate model issues.
  residuals_plot:
    enabled: true

    # Plot type: "scatter", "stem", "line"
    type: "scatter"

    # Labels
    title: null                   # null = no title
    x_label: "x"
    y_label: "Residual"

    # Zero reference line
    show_zero_line: true
    zero_line_style: "--"
    zero_line_color: "gray"
    zero_line_width: 1.0

    # Marker appearance (for scatter/stem)
    marker: "o"
    color: null                   # null = use active color scheme
    size: 15
    alpha: 0.7

    # --------------------------------------------------------------------------
    # Standard Deviation Bands
    # --------------------------------------------------------------------------
    # Horizontal bands showing ±1σ, ±2σ regions.
    # For good fits: ~68% of residuals within ±1σ, ~95% within ±2σ.
    std_bands:
      enabled: true
      levels: [1, 2]              # Show ±1σ and ±2σ bands
      colors:
        - "#e8f4e8"               # Light green for ±1σ
        - "#d4e9d4"               # Darker green for ±2σ
      alpha: 0.5

    # --------------------------------------------------------------------------
    # Weighted Residuals (if sigma provided)
    # --------------------------------------------------------------------------
    # When enabled, plots (y_data - y_fit) / sigma instead of raw residuals.
    # Weighted residuals should follow N(0,1) for proper uncertainty estimates.
    show_weighted: false          # Use weighted residuals when sigma available

    # --------------------------------------------------------------------------
    # Normality Reference (optional)
    # --------------------------------------------------------------------------
    # Show expected distribution bounds for normally distributed residuals.
    normality_check:
      enabled: false
      show_expected_bounds: true  # Show ±2σ where 95% should fall

  # ==========================================================================
  # HISTOGRAM OF RESIDUALS
  # ==========================================================================
  # Displays distribution of residuals to assess normality assumption.
  # For valid uncertainty estimates, residuals should be normally distributed.
  # Systematic deviations suggest model inadequacy or underestimated errors.
  histogram:
    enabled: true                 # Enable residual histogram

    # Number of bins: "auto", "sqrt", "sturges", "fd", or integer
    # "auto": Automatically selects optimal bin count
    # "sqrt": √N bins (simple rule)
    # "sturges": log₂(N) + 1 bins (assumes normality)
    # "fd": Freedman-Diaconis rule (robust to outliers)
    bins: "auto"

    # Overlay normal distribution fit
    # Shows N(0, σ_residuals) curve for comparison
    show_normal_fit: true
    normal_color: null            # null = use fit color from scheme

    # Bar appearance
    color: null                   # null = use scheme color
    alpha: 0.7
    edgecolor: "white"

    # Labels
    title: "Residual Distribution"
    x_label: "Residual"
    y_label: "Frequency"

    # Normality test annotation
    # Displays Shapiro-Wilk test p-value for normality
    show_normality_test: false
    test_fontsize: 8

  # ==========================================================================
  # COLOR SCHEMES (Predefined Palettes)
  # ==========================================================================
  # Colorblind-friendly palettes for accessibility.
  # Choose based on audience and publication requirements.
  #
  # Accessibility note: ~8% of males have color vision deficiency.
  # "colorblind" scheme uses Okabe-Ito palette, safe for all types.
  color_schemes:
    # Default scheme (matplotlib tab10 inspired)
    default:
      data: "#1f77b4"             # Blue
      fit: "#d62728"              # Red
      residuals: "#2ca02c"        # Green
      confidence: "#ff7f0e"       # Orange
      histogram: "#9467bd"        # Purple
      normal_fit: "#d62728"       # Red (same as fit)

    # Colorblind-safe (Okabe-Ito palette)
    # Optimized for deuteranopia, protanopia, and tritanopia
    colorblind:
      data: "#0072B2"             # Blue
      fit: "#D55E00"              # Vermilion (red-orange)
      residuals: "#009E73"        # Bluish green
      confidence: "#F0E442"       # Yellow
      histogram: "#CC79A7"        # Reddish purple
      normal_fit: "#D55E00"       # Vermilion

    # High contrast (for presentations/projectors)
    high_contrast:
      data: "#000080"             # Navy blue
      fit: "#CC0000"              # Dark red
      residuals: "#006400"        # Dark green
      confidence: "#FF8C00"       # Dark orange
      histogram: "#800080"        # Purple
      normal_fit: "#CC0000"       # Dark red

    # Grayscale for B&W printing
    grayscale:
      data: "#404040"             # Dark gray
      fit: "#000000"              # Black
      residuals: "#606060"        # Medium gray
      confidence: "#A0A0A0"       # Light gray
      histogram: "#808080"        # Gray
      normal_fit: "#000000"       # Black

    # Nature journal style
    nature:
      data: "#4575B4"             # Steel blue
      fit: "#D73027"              # Brick red
      residuals: "#1A9850"        # Forest green
      confidence: "#FEE090"       # Light orange
      histogram: "#762A83"        # Purple
      normal_fit: "#D73027"       # Brick red

    # Science journal style
    science:
      data: "#0571B0"             # Science blue
      fit: "#CA0020"              # Science red
      residuals: "#008837"        # Green
      confidence: "#F4A582"       # Peach
      histogram: "#7B3294"        # Purple
      normal_fit: "#CA0020"       # Science red

  # Active color scheme (reference name from above)
  # Set to "colorblind" for maximum accessibility
  active_scheme: "default"

  # ==========================================================================
  # ADVANCED VISUALIZATION OPTIONS
  # ==========================================================================
  advanced:
    # Tight layout to minimize whitespace
    tight_layout: true

    # Use constrained layout (alternative to tight_layout, better for complex plots)
    constrained_layout: false

    # Save figure with transparent background
    transparent: false

    # Additional matplotlib rcParams overrides
    # Example: {"axes.linewidth": 1.5, "xtick.major.width": 1.0}
    rcparams: null

    # Export individual components (in addition to combined)
    export_components: false      # Save main_plot, residuals, histogram separately

    # Interactive plot backend (for Jupyter notebooks)
    interactive: false

# ==============================================================================
# EXPORT
# ==============================================================================
# Output file configuration for CLI fitting results.

export:
  # ==========================================================================
  # RESULTS FILE (Required for CLI)
  # ==========================================================================
  # Path for fit results (relative to project_root or absolute).
  # File extension determines format: .json, .csv, .npz
  results_file: "output/fit_results.json"

  # ==========================================================================
  # EXPORT CONTENTS
  # ==========================================================================
  include:
    parameters: true          # popt (fitted parameters)
    covariance: true          # pcov (covariance matrix)
    uncertainties: true       # Parameter uncertainties (sqrt of diag(pcov))
    statistics: true          # r_squared, rmse, chi_squared, etc.
    residuals: false          # y - y_fit (can be large for big datasets)
    fitted_values: false      # y_fit values (can be large)
    convergence_info: true    # Iterations, function evaluations, status

  # ==========================================================================
  # OUTPUT FORMATS
  # ==========================================================================
  # Generate multiple output formats.
  formats:
    - "json"                  # Human-readable, includes all metadata
    - "csv"                   # Tabular parameters and statistics

  # ==========================================================================
  # ARTIFACT BUNDLE
  # ==========================================================================
  artifact_bundle:
    enabled: true
    include_data_snapshot: false
    include_logs: true
  config_snapshot:
    enabled: true
    filename: "config_snapshot.yaml"

# ==============================================================================
# BATCH PROCESSING
# ==============================================================================
# Configuration for parallel batch fitting via `nlsq batch` command.
#
# Usage:
#   nlsq batch workflow1.yaml workflow2.yaml workflow3.yaml
#   nlsq batch configs/*.yaml

batch:
  # ==========================================================================
  # PARALLELIZATION
  # ==========================================================================
  # Maximum parallel workers. null = auto (min of CPU count and file count).
  max_workers: null

  # Continue processing remaining files if one fails.
  continue_on_error: true

  # ==========================================================================
  # BATCH OUTPUT
  # ==========================================================================
  # Aggregate summary file for all batch results.
  summary_file: "output/batch_summary.json"

  # Summary format: "json", "csv", or "both"
  summary_format: "json"

  # Include individual result paths in summary.
  include_result_paths: true

# ==============================================================================
# ADVANCED
# ==============================================================================

advanced:
  # Memory management.
  memory:
    adaptive_fraction: 0.5   # Fraction of available memory to use
    max_gb: null             # Hard limit (null = no limit)
  # Timeouts (seconds).
  timeouts:
    per_step: 300            # 5 minutes per step
    total: 7200              # 2 hours total
  # Checkpointing for long runs.
  checkpointing:
    enabled: false
    interval_steps: 50
    directory: "checkpoints"
  # Debug flags.
  debug:
    enabled: false
    trace: false
    save_intermediate_arrays: false
    profile_jit: false       # Profile JAX JIT compilation

# ==============================================================================
# EXAMPLE PROFILES
# ==============================================================================
# Copy and uncomment these profiles, then set default_workflow to the name.

# precision_publication:
#   # Maximum precision for publication-quality results
#   runtime:
#     device: "cpu"
#     precision: "float64"
#     dtype: "float64"
#     random_seed: 42
#   fitting:
#     termination:
#       ftol: 1.0e-12
#       xtol: 1.0e-12
#       gtol: 1.0e-12
#       max_iterations: 500
#       max_function_evals: 10000
#     multistart:
#       enabled: true
#       num_starts: 25
#       sampler: "lhs"
#   validation_and_qc:
#     sanity_checks:
#       enabled: true
#     numerical_parity:
#       enabled: true
#       tolerance: 1.0e-10

# fast_development:
#   # Quick iteration during development
#   runtime:
#     device: "cpu"
#     precision: "float32"
#     verbosity: 2
#   fitting:
#     termination:
#       ftol: 1.0e-6
#       max_iterations: 50
#       max_function_evals: 200
#     multistart:
#       enabled: false
#   reporting:
#     summary:
#       include_plots: false

# large_dataset_gpu:
#   # GPU-accelerated large dataset processing
#   runtime:
#     device: "gpu"
#     precision: "float64"
#     threads: 8
#   hybrid_streaming:
#     warmup_iterations: 300
#     chunk_size: 50000
#     precision: "auto"
#     enable_checkpoints: true
#   advanced:
#     memory:
#       adaptive_fraction: 0.7

# hpc_cluster:
#   # HPC multi-node configuration (PBS Pro)
#   runtime:
#     device: "gpu"
#     precision: "float64"
#   hybrid_streaming:
#     enable_multi_device: true
#     chunk_size: 100000
#     enable_checkpoints: true
#     checkpoint_frequency: 50
#     enable_multistart: true
#     n_starts: 20
#   advanced:
#     checkpointing:
#       enabled: true
#       interval_steps: 25

# memory_constrained:
#   # Minimal memory footprint
#   runtime:
#     device: "cpu"
#     precision: "float32"
#     threads: 2
#     workers: 1
#   hybrid_streaming:
#     chunk_size: 2000
#     precision: "float32"
#     enable_checkpoints: true
#     checkpoint_frequency: 25
#   advanced:
#     memory:
#       adaptive_fraction: 0.3
#       max_gb: 4.0

# warm_start_refinement:
#   # Optimized for refining parameters from previous fits
#   # Uses strict defense layers to prevent divergence
#   runtime:
#     device: "cpu"
#     precision: "float64"
#   hybrid_streaming:
#     warmup_iterations: 100  # Shorter warmup (may be skipped anyway)
#     defense_layers:
#       preset: "strict"
#       # Strict preset uses:
#       # - warm_start_threshold: 0.005 (0.5%)
#       # - cost_increase_tolerance: 0.02 (2%)
#       # - max_step_size: 0.05
#     telemetry:
#       enabled: true
#       export_format: "json"

# exploration_from_poor_guess:
#   # When starting from potentially poor initial parameters
#   # Uses relaxed defense to allow more exploration
#   runtime:
#     device: "gpu"
#     precision: "float64"
#   hybrid_streaming:
#     warmup_iterations: 500  # Longer warmup for exploration
#     defense_layers:
#       preset: "relaxed"
#       # Relaxed preset uses:
#       # - warm_start_threshold: 0.02 (2%)
#       # - cost_increase_tolerance: 0.10 (10%)
#       # - max_step_size: 0.2
#     telemetry:
#       enabled: true
#       export_format: "prometheus"

# regression_testing:
#   # For comparing against pre-0.3.6 results
#   # Disables all defense layers for exact behavior match
#   runtime:
#     device: "cpu"
#     precision: "float64"
#     random_seed: 42
#   hybrid_streaming:
#     defense_layers:
#       preset: "disabled"
#       # All layers OFF - pre-0.3.6 behavior
#     telemetry:
#       enabled: false

# scientific_computing:
#   # Tuned for physics/scientific models
#   # Uses scientific_default preset with production monitoring
#   runtime:
#     device: "cpu"
#     precision: "float64"
#   hybrid_streaming:
#     normalization_strategy: "bounds"  # Better for multi-scale physics
#     defense_layers:
#       preset: "scientific"
#       # Scientific preset: balanced for physics models
#     telemetry:
#       enabled: true
#       export_format: "prometheus"
#   validation_and_qc:
#     sanity_checks:
#       enabled: true
#       checks:
#         - "finite_parameters"
#         - "nonnegative_variances"
#         - "positive_definite_covariance"
