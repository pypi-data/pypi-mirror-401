{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566d391e",
   "metadata": {},
   "source": [
    "# 07 HPC and Checkpointing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/08_workflow_system/07_hpc_and_checkpointing.ipynb)\n",
    "\n",
    "Features demonstrated:\n",
    "- ClusterDetector and ClusterInfo for PBS Pro detection\n",
    "- Streaming workflow for fault tolerance\n",
    "- Checkpointing with HybridStreamingConfig\n",
    "- Checkpoint resume workflow\n",
    "- PBS Pro job script generation\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/08_workflow_system/07_hpc_and_checkpointing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fd545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:54:28.795452Z",
     "iopub.status.busy": "2026-01-06T17:54:28.795297Z",
     "iopub.status.idle": "2026-01-06T17:54:28.798512Z",
     "shell.execute_reply": "2026-01-06T17:54:28.798005Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2533d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:54:28.800165Z",
     "iopub.status.busy": "2026-01-06T17:54:28.800053Z",
     "iopub.status.idle": "2026-01-06T17:54:29.628720Z",
     "shell.execute_reply": "2026-01-06T17:54:29.628103Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import HybridStreamingConfig\n",
    "from nlsq.core.minpack import WORKFLOW_PRESETS\n",
    "from nlsq.core.workflow import ClusterDetector, ClusterInfo\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "if QUICK:\n",
    "    print(\"Quick mode: reduced iterations for HPC and checkpointing demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde563f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:54:29.630827Z",
     "iopub.status.busy": "2026-01-06T17:54:29.630633Z",
     "iopub.status.idle": "2026-01-06T17:54:29.636469Z",
     "shell.execute_reply": "2026-01-06T17:54:29.635610Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_checkpoint_directory(base_dir: str = \"./nlsq_checkpoints\") -> Path:\n",
    "    \"\"\"Create a timestamped checkpoint directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : str\n",
    "        Base directory for checkpoints\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the created checkpoint directory\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_dir = Path(base_dir) / f\"run_{timestamp}\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return checkpoint_dir\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, iteration, params, loss, metadata=None):\n",
    "    \"\"\"Save optimization checkpoint using JSON (secure serialization).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory to save checkpoint\n",
    "    iteration : int\n",
    "        Current iteration number\n",
    "    params : np.ndarray\n",
    "        Current parameter values\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    metadata : dict, optional\n",
    "        Additional metadata to save\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_dir) / f\"checkpoint_{iteration:06d}.json\"\n",
    "\n",
    "    checkpoint_data = {\n",
    "        \"iteration\": iteration,\n",
    "        \"params\": list(params),\n",
    "        \"loss\": float(loss),\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "\n",
    "    with open(checkpoint_path, \"w\") as f:\n",
    "        json.dump(checkpoint_data, f, indent=2)\n",
    "\n",
    "    print(f\"  Saved checkpoint: {checkpoint_path.name}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Load the most recent checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory containing checkpoints\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Checkpoint data if found, None otherwise\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    if not checkpoint_dir.exists():\n",
    "        return None\n",
    "\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint_*.json\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest = sorted(checkpoints)[-1]\n",
    "\n",
    "    with open(latest) as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "\n",
    "    checkpoint_data[\"params\"] = np.array(checkpoint_data[\"params\"])\n",
    "    print(f\"  Loaded checkpoint: {latest.name}\")\n",
    "    return checkpoint_data\n",
    "\n",
    "\n",
    "def optimization_with_checkpoints(\n",
    "    checkpoint_dir, max_iterations=100, checkpoint_interval=10\n",
    "):\n",
    "    \"\"\"Example optimization loop with checkpoint support.\"\"\"\n",
    "\n",
    "    checkpoint = load_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "    if checkpoint:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        params = checkpoint[\"params\"]\n",
    "        print(f\"  Resuming from iteration {start_iteration}\")\n",
    "    else:\n",
    "        start_iteration = 0\n",
    "        params = np.array([1.0, 1.0, 0.0])\n",
    "        print(\"  Starting fresh optimization\")\n",
    "\n",
    "    for iteration in range(start_iteration, max_iterations):\n",
    "        params = params + 0.001 * np.random.randn(3)\n",
    "        loss = np.sum(params**2)\n",
    "\n",
    "        if iteration > 0 and iteration % checkpoint_interval == 0:\n",
    "            save_checkpoint(checkpoint_dir, iteration, params, loss)\n",
    "\n",
    "    save_checkpoint(checkpoint_dir, max_iterations - 1, params, loss)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf549085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:54:29.638447Z",
     "iopub.status.busy": "2026-01-06T17:54:29.638313Z",
     "iopub.status.idle": "2026-01-06T17:54:29.647446Z",
     "shell.execute_reply": "2026-01-06T17:54:29.646567Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HPC Integration and Checkpointing\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 1. ClusterDetector and ClusterInfo\n",
    "    # =========================================================================\n",
    "    print(\"1. ClusterDetector and ClusterInfo:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    detector = ClusterDetector()\n",
    "\n",
    "    print(f\"  PBS environment detected: {detector.is_pbs_environment()}\")\n",
    "\n",
    "    cluster_info = detector.detect()\n",
    "\n",
    "    if cluster_info:\n",
    "        print(\"\\n  Cluster detected:\")\n",
    "        print(f\"    Scheduler: {cluster_info.scheduler}\")\n",
    "        print(f\"    Node count: {cluster_info.node_count}\")\n",
    "        print(f\"    GPUs per node: {cluster_info.gpus_per_node}\")\n",
    "        print(f\"    Total GPUs: {cluster_info.total_gpus}\")\n",
    "    else:\n",
    "        print(\"\\n  No cluster environment detected (running locally)\")\n",
    "\n",
    "    # Simulated PBS cluster for demonstration\n",
    "    simulated_cluster = ClusterInfo(\n",
    "        node_count=4,\n",
    "        gpus_per_node=8,\n",
    "        total_gpus=32,\n",
    "        node_list=[\"node01\", \"node02\", \"node03\", \"node04\"],\n",
    "        scheduler=\"pbs\",\n",
    "        job_id=\"12345.pbs_server\",\n",
    "        interconnect=\"infiniband\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n  Simulated PBS cluster:\")\n",
    "    print(f\"    Nodes: {simulated_cluster.node_count}\")\n",
    "    print(f\"    GPUs: {simulated_cluster.total_gpus}\")\n",
    "    print(f\"    Job ID: {simulated_cluster.job_id}\")\n",
    "    print(f\"    Interconnect: {simulated_cluster.interconnect}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2. Streaming Workflow with Checkpointing\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"2. Streaming Workflow with Checkpointing:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"  Available workflow presets:\")\n",
    "    for name in [\"streaming\", \"hpc_distributed\"]:\n",
    "        if name in WORKFLOW_PRESETS:\n",
    "            desc = WORKFLOW_PRESETS[name].get(\"description\", \"\")\n",
    "            print(f\"    - {name}: {desc}\")\n",
    "\n",
    "    print(\"\\n  HPC Configuration (kwargs pattern):\")\n",
    "    hpc_kwargs = {\n",
    "        \"workflow\": \"streaming\",\n",
    "        \"gtol\": 1e-7,\n",
    "        \"ftol\": 1e-7,\n",
    "        \"xtol\": 1e-7,\n",
    "        \"multistart\": True,\n",
    "        \"n_starts\": 10,\n",
    "    }\n",
    "    for key, value in hpc_kwargs.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    if QUICK:\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Summary (Quick Mode)\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(\"HPC Integration:\")\n",
    "        print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n",
    "        print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n",
    "        print()\n",
    "        print(\"Checkpointing:\")\n",
    "        print(\"  - Use workflow='streaming' for large datasets\")\n",
    "        print(\"  - HybridStreamingConfig for fine-grained control\")\n",
    "        print()\n",
    "        print(\"Defense Layers for Checkpoint Resume:\")\n",
    "        print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n",
    "        print(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")\n",
    "        return\n",
    "\n",
    "    # =========================================================================\n",
    "    # 3. Checkpointing Configuration\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"3. Checkpointing Configuration:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    checkpoint_dir = create_checkpoint_directory()\n",
    "    print(f\"  Created checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "    custom_checkpoint_dir = create_checkpoint_directory(\n",
    "        base_dir=\"./my_project_checkpoints\"\n",
    "    )\n",
    "    print(f\"  Custom checkpoint directory: {custom_checkpoint_dir}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 4. Checkpoint Resume Workflow\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"4. Checkpoint Resume Workflow:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    demo_dir = create_checkpoint_directory(base_dir=\"./demo_checkpoints\")\n",
    "\n",
    "    print(\"\\n  Simulating optimization with checkpoints...\")\n",
    "    for i in range(0, 30, 10):\n",
    "        params = np.array([2.0 + 0.01 * i, 1.0 - 0.005 * i, 0.5])\n",
    "        loss = 0.1 / (1 + i * 0.1)\n",
    "        save_checkpoint(demo_dir, i, params, loss, metadata={\"epoch\": i // 10})\n",
    "\n",
    "    print(\"\\n  Loading latest checkpoint for resume...\")\n",
    "    latest = load_latest_checkpoint(demo_dir)\n",
    "\n",
    "    if latest:\n",
    "        print(f\"    Iteration: {latest['iteration']}\")\n",
    "        print(f\"    Parameters: {latest['params']}\")\n",
    "        print(f\"    Loss: {latest['loss']:.6f}\")\n",
    "\n",
    "    print(\"\\n  Running optimization loop with checkpoints...\")\n",
    "    final_params = optimization_with_checkpoints(demo_dir, max_iterations=50)\n",
    "    print(f\"  Final parameters: {final_params}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5. PBS Pro Job Script\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"5. PBS Pro Job Script:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    pbs_script = '''#!/bin/bash\n",
    "#PBS -N nlsq_fit\n",
    "#PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\n",
    "#PBS -l walltime=24:00:00\n",
    "#PBS -q gpu\n",
    "#PBS -j oe\n",
    "#PBS -o nlsq_fit.log\n",
    "\n",
    "# NLSQ Curve Fitting Job Script for PBS Pro\n",
    "# ===========================================\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Load required modules\n",
    "module load python/3.12\n",
    "module load cuda/12.0\n",
    "module load cudnn/8.9\n",
    "\n",
    "# Activate virtual environment\n",
    "source ./venv/bin/activate\n",
    "\n",
    "# Set NLSQ environment variables\n",
    "export NLSQ_MEMORY_LIMIT_GB=200\n",
    "export NLSQ_CHECKPOINT_DIR=$PBS_O_WORKDIR/checkpoints\n",
    "\n",
    "# Create checkpoint directory\n",
    "mkdir -p $NLSQ_CHECKPOINT_DIR\n",
    "\n",
    "# Display job information\n",
    "echo \"========================================\"\n",
    "echo \"NLSQ Fitting Job Started\"\n",
    "echo \"========================================\"\n",
    "echo \"Job ID: $PBS_JOBID\"\n",
    "echo \"Node list:\"\n",
    "cat $PBS_NODEFILE\n",
    "echo \"========================================\"\n",
    "\n",
    "# Run NLSQ fitting script\n",
    "python fit_large_dataset.py \\\\\n",
    "    --data-file ./data/large_dataset.h5 \\\\\n",
    "    --output-dir ./results \\\\\n",
    "    --checkpoint-dir $NLSQ_CHECKPOINT_DIR \\\\\n",
    "    --workflow streaming\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"Job Completed: $(date)\"\n",
    "echo \"========================================\"\n",
    "'''\n",
    "\n",
    "    pbs_script_path = Path(\"nlsq_fit.pbs\")\n",
    "    pbs_script_path.write_text(pbs_script)\n",
    "\n",
    "    print(\"  Created PBS job script: nlsq_fit.pbs\")\n",
    "    print(\"  Key directives:\")\n",
    "    print(\"    #PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\")\n",
    "    print(\"    #PBS -l walltime=24:00:00\")\n",
    "    print(\"    #PBS -q gpu\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 6. Defense Layers for Checkpoint Resume\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"6. Defense Layers for Checkpoint Resume:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"When resuming from checkpoints, your initial parameters are near-optimal.\")\n",
    "    print(\"This is a classic warm-start scenario where defense layers are critical.\")\n",
    "    print()\n",
    "    print(\"Without defense layers, L-BFGS warmup can DIVERGE from your checkpoint:\")\n",
    "    print(\"  - Momentum builds up from large initial gradients\")\n",
    "    print(\"  - Parameters overshoot and loss increases\")\n",
    "    print(\"  - All progress from previous run is lost\")\n",
    "    print()\n",
    "    print(\"With 4-layer defense, checkpoint resume is protected:\")\n",
    "    print(\"  Layer 1: Detects you're starting near-optimal -> may skip warmup\")\n",
    "    print(\"  Layer 2: Scales learning rate based on initial fit quality\")\n",
    "    print(\"  Layer 3: Aborts warmup if loss increases > 5%\")\n",
    "    print(\"  Layer 4: Clips step magnitudes to prevent overshooting\")\n",
    "    print()\n",
    "    print(\"Recommended configuration for checkpoint resume:\")\n",
    "    print()\n",
    "    print(\"  # Use defense_strict for checkpoint resume scenarios\")\n",
    "    print(\"  config = HybridStreamingConfig.defense_strict()\")\n",
    "    print()\n",
    "    print(\"Defense presets comparison:\")\n",
    "    print(\"  defense_strict()     - Best for checkpoint resume (LR: 1e-6 to 1e-4)\")\n",
    "    print(\"  defense_relaxed()    - For fresh starts (LR: 1e-4 to 0.01)\")\n",
    "    print(\"  scientific_default() - Balanced for production\")\n",
    "    print(\"  defense_disabled()   - Pre-0.3.6 behavior (no protection)\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Cleanup\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"Cleaning up...\")\n",
    "\n",
    "    for path_str in [\n",
    "        \"nlsq_checkpoints\",\n",
    "        \"demo_checkpoints\",\n",
    "        \"my_project_checkpoints\",\n",
    "    ]:\n",
    "        path = Path(path_str)\n",
    "        if path.exists():\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"  Removed: {path_str}\")\n",
    "\n",
    "    if pbs_script_path.exists():\n",
    "        pbs_script_path.unlink()\n",
    "        print(\"  Removed: nlsq_fit.pbs\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"HPC Integration:\")\n",
    "    print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n",
    "    print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n",
    "    print()\n",
    "    print(\"Checkpointing:\")\n",
    "    print(\"  - Use workflow='streaming' for large datasets\")\n",
    "    print(\"  - JSON-based checkpoints (secure serialization)\")\n",
    "    print(\"  - Create timestamped checkpoint directories\")\n",
    "    print()\n",
    "    print(\"PBS Pro Job Scripts:\")\n",
    "    print(\"  - #PBS -l select=N:ncpus=C:ngpus=G:mem=Mgb\")\n",
    "    print(\"  - Environment variable: NLSQ_MEMORY_LIMIT_GB\")\n",
    "    print(\"  - Checkpoint directory: NLSQ_CHECKPOINT_DIR\")\n",
    "    print()\n",
    "    print(\"Defense Layers for Checkpoint Resume:\")\n",
    "    print(\"  - Checkpoint resume = warm-start scenario\")\n",
    "    print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n",
    "    print(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7eef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:54:29.649239Z",
     "iopub.status.busy": "2026-01-06T17:54:29.649124Z",
     "iopub.status.idle": "2026-01-06T17:54:29.651726Z",
     "shell.execute_reply": "2026-01-06T17:54:29.651112Z"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
