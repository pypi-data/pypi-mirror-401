{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc9f863",
   "metadata": {},
   "source": [
    "# 02 Sampling Strategies\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/07_global_optimization/02_sampling_strategies.ipynb)\n",
    "\n",
    "Converted from 02_sampling_strategies.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\n",
    "Features demonstrated:\n",
    "- Latin Hypercube Sampling (LHS) - stratified random sampling\n",
    "- Sobol sequences - deterministic quasi-random sampling\n",
    "- Halton sequences - prime-based quasi-random sampling\n",
    "- Random sampling - uniform random baseline\n",
    "- Visualization of space-filling properties\n",
    "- Quantitative comparison of discrepancy and success rates\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/07_global_optimization/02_sampling_strategies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890eb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:31.775634Z",
     "iopub.status.busy": "2026-01-06T17:52:31.775360Z",
     "iopub.status.idle": "2026-01-06T17:52:31.780808Z",
     "shell.execute_reply": "2026-01-06T17:52:31.779963Z"
    },
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735e3ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:31.782838Z",
     "iopub.status.busy": "2026-01-06T17:52:31.782636Z",
     "iopub.status.idle": "2026-01-06T17:52:32.790496Z",
     "shell.execute_reply": "2026-01-06T17:52:32.788471Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "FIG_DIR = Path.cwd() / \"figures\"  # Modified for notebook compatibility\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "if QUICK:\n",
    "    print(\"Quick mode: reduced iterations for sampling strategies.\")\n",
    "\n",
    "from nlsq import curve_fit\n",
    "from nlsq.global_optimization import (\n",
    "    halton_sample,\n",
    "    latin_hypercube_sample,\n",
    "    scale_samples_to_bounds,\n",
    "    sobol_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428a64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:32.795197Z",
     "iopub.status.busy": "2026-01-06T17:52:32.794807Z",
     "iopub.status.idle": "2026-01-06T17:52:32.823363Z",
     "shell.execute_reply": "2026-01-06T17:52:32.822537Z"
    }
   },
   "outputs": [],
   "source": [
    "def multimodal_model(x, a, b, c):\n",
    "    \"\"\"Multimodal model with multiple local minima.\"\"\"\n",
    "    return a * jnp.sin(b * x + c)\n",
    "\n",
    "\n",
    "def compute_simple_discrepancy(samples: np.ndarray) -> float:\n",
    "    \"\"\"Compute a simple discrepancy measure based on minimum neighbor distances.\n",
    "\n",
    "    Lower discrepancy indicates more uniform coverage.\n",
    "    \"\"\"\n",
    "    n = len(samples)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute all pairwise distances\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        min_dist = float(\"inf\")\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(samples[i] - samples[j])\n",
    "                min_dist = min(min_dist, dist)\n",
    "        distances.append(min_dist)\n",
    "\n",
    "    # Ideal minimum distance for uniform samples in d dimensions\n",
    "    d = samples.shape[1]\n",
    "    ideal_dist = (1.0 / n) ** (1.0 / d)\n",
    "\n",
    "    # Discrepancy: variance of min distances from ideal\n",
    "    distances = np.array(distances)\n",
    "    discrepancy = np.std(distances) / ideal_dist\n",
    "\n",
    "    return discrepancy\n",
    "\n",
    "\n",
    "def evaluate_starting_points(samples_unit, lb, ub, x_data, y_data, true_params, bounds):\n",
    "    \"\"\"Evaluate success rate of starting points.\n",
    "\n",
    "    Returns the fraction of starting points that converge to the global optimum.\n",
    "    \"\"\"\n",
    "    # Scale samples to bounds\n",
    "    samples_scaled = scale_samples_to_bounds(jnp.array(samples_unit), lb, ub)\n",
    "    samples_scaled = np.array(samples_scaled)\n",
    "\n",
    "    # True SSR (sum of squared residuals)\n",
    "    y_true_pred = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\n",
    "    true_ssr = np.sum((y_data - y_true_pred) ** 2)\n",
    "\n",
    "    # Evaluate each starting point\n",
    "    success_count = 0\n",
    "    ssrs = []\n",
    "\n",
    "    for p0 in samples_scaled:\n",
    "        try:\n",
    "            popt, _ = curve_fit(\n",
    "                multimodal_model,\n",
    "                x_data,\n",
    "                y_data,\n",
    "                p0=list(p0),\n",
    "                bounds=bounds,\n",
    "            )\n",
    "            y_pred = multimodal_model(x_data, *popt)\n",
    "            ssr = float(jnp.sum((y_data - y_pred) ** 2))\n",
    "            ssrs.append(ssr)\n",
    "\n",
    "            # Check if converged to near-optimal (within 10% of true SSR)\n",
    "            if ssr < true_ssr * 1.1:\n",
    "                success_count += 1\n",
    "        except Exception:\n",
    "            ssrs.append(float(\"inf\"))\n",
    "\n",
    "    success_rate = success_count / len(samples_scaled)\n",
    "    best_ssr = min(ssrs)\n",
    "\n",
    "    return success_rate, best_ssr, ssrs\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Sampling Strategies for Multi-Start Optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    if QUICK:\n",
    "        print(\"Quick mode: skipping full demonstration.\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Summary: Sampling Strategies\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        print(\"Sampling Strategies:\")\n",
    "        print(\"  - Random: Baseline, poor space-filling\")\n",
    "        print(\"  - LHS: Stratified random, good coverage, stochastic\")\n",
    "        print(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\n",
    "        print(\"  - Halton: Quasi-random, very good coverage, deterministic\")\n",
    "        print()\n",
    "        print(\"Key Functions:\")\n",
    "        print(\"  - latin_hypercube_sample(n_samples, n_dims)\")\n",
    "        print(\"  - sobol_sample(n_samples, n_dims)\")\n",
    "        print(\"  - halton_sample(n_samples, n_dims)\")\n",
    "        print(\"  - scale_samples_to_bounds(samples, lb, ub)\")\n",
    "        print()\n",
    "        print(\"Usage with curve_fit():\")\n",
    "        print('  curve_fit(..., multistart=True, n_starts=10, sampler=\"lhs\")')\n",
    "        return\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 1. Generate samples using each method\n",
    "    # =========================================================================\n",
    "    print(\"1. Generating samples with different methods...\")\n",
    "\n",
    "    n_samples = 50\n",
    "    n_dims = 2\n",
    "\n",
    "    # Generate samples using each method\n",
    "    random_samples = np.random.rand(n_samples, n_dims)\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    lhs_samples = latin_hypercube_sample(n_samples, n_dims, rng_key=key)\n",
    "    sobol_samples = sobol_sample(n_samples, n_dims)\n",
    "    halton_samples = halton_sample(n_samples, n_dims)\n",
    "\n",
    "    print(f\"  Generated {n_samples} samples in {n_dims} dimensions\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2. Visualize 2D samples\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"2. Saving 2D comparison visualization...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    samples_dict = {\n",
    "        \"Random\": random_samples,\n",
    "        \"Latin Hypercube (LHS)\": np.array(lhs_samples),\n",
    "        \"Sobol\": np.array(sobol_samples),\n",
    "        \"Halton\": np.array(halton_samples),\n",
    "    }\n",
    "\n",
    "    for ax, (name, samples) in zip(axes.flat, samples_dict.items(), strict=False):\n",
    "        ax.scatter(\n",
    "            samples[:, 0],\n",
    "            samples[:, 1],\n",
    "            s=40,\n",
    "            alpha=0.7,\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=0.5,\n",
    "        )\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel(\"Dimension 1\")\n",
    "        ax.set_ylabel(\"Dimension 2\")\n",
    "        ax.set_title(f\"{name} ({n_samples} samples)\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = FIG_DIR / \"02_sampling_comparison_2d.png\"\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {out_path}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 3. LHS stratification visualization\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"3. Saving LHS stratification visualization...\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: Random samples\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(random_samples[:, 0], random_samples[:, 1], s=40, alpha=0.7)\n",
    "\n",
    "    for i in range(n_samples + 1):\n",
    "        ax1.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n",
    "        ax1.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n",
    "\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_xlabel(\"Dimension 1\")\n",
    "    ax1.set_ylabel(\"Dimension 2\")\n",
    "    ax1.set_title(\"Random: Multiple samples per stratum\")\n",
    "\n",
    "    # Right: LHS samples\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(\n",
    "        np.array(lhs_samples)[:, 0],\n",
    "        np.array(lhs_samples)[:, 1],\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    for i in range(n_samples + 1):\n",
    "        ax2.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n",
    "        ax2.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n",
    "\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_xlabel(\"Dimension 1\")\n",
    "    ax2.set_ylabel(\"Dimension 2\")\n",
    "    ax2.set_title(\"LHS: Exactly one sample per stratum per dimension\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"02_lhs_stratification.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '02_lhs_stratification.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 4. Quasi-random progressive fill\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"4. Saving quasi-random progressive fill visualization...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "    sample_counts = [8, 16, 32, 64]\n",
    "\n",
    "    for i, n in enumerate(sample_counts):\n",
    "        # Sobol\n",
    "        sobol_n = sobol_sample(n, 2)\n",
    "        axes[0, i].scatter(\n",
    "            np.array(sobol_n)[:, 0], np.array(sobol_n)[:, 1], s=30, alpha=0.8\n",
    "        )\n",
    "        axes[0, i].set_xlim(0, 1)\n",
    "        axes[0, i].set_ylim(0, 1)\n",
    "        axes[0, i].set_title(f\"Sobol: n={n}\")\n",
    "        axes[0, i].set_aspect(\"equal\")\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "        # Halton\n",
    "        halton_n = halton_sample(n, 2)\n",
    "        axes[1, i].scatter(\n",
    "            np.array(halton_n)[:, 0],\n",
    "            np.array(halton_n)[:, 1],\n",
    "            s=30,\n",
    "            alpha=0.8,\n",
    "            color=\"green\",\n",
    "        )\n",
    "        axes[1, i].set_xlim(0, 1)\n",
    "        axes[1, i].set_ylim(0, 1)\n",
    "        axes[1, i].set_title(f\"Halton: n={n}\")\n",
    "        axes[1, i].set_aspect(\"equal\")\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIG_DIR / \"02_quasi_random_progressive.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '02_quasi_random_progressive.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5. Discrepancy comparison\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"5. Computing discrepancy comparison...\")\n",
    "\n",
    "    sample_sizes = [10, 20, 30, 50, 75, 100]\n",
    "    discrepancies = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        random_s = np.random.rand(n, 2)\n",
    "        lhs_s = np.array(latin_hypercube_sample(n, 2, rng_key=jax.random.PRNGKey(42)))\n",
    "        sobol_s = np.array(sobol_sample(n, 2))\n",
    "        halton_s = np.array(halton_sample(n, 2))\n",
    "\n",
    "        discrepancies[\"Random\"].append(compute_simple_discrepancy(random_s))\n",
    "        discrepancies[\"LHS\"].append(compute_simple_discrepancy(lhs_s))\n",
    "        discrepancies[\"Sobol\"].append(compute_simple_discrepancy(sobol_s))\n",
    "        discrepancies[\"Halton\"].append(compute_simple_discrepancy(halton_s))\n",
    "\n",
    "    # Plot discrepancy\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    colors = {\"Random\": \"red\", \"LHS\": \"orange\", \"Sobol\": \"blue\", \"Halton\": \"green\"}\n",
    "    markers = {\"Random\": \"o\", \"LHS\": \"s\", \"Sobol\": \"^\", \"Halton\": \"d\"}\n",
    "\n",
    "    for name, discs in discrepancies.items():\n",
    "        ax.plot(\n",
    "            sample_sizes,\n",
    "            discs,\n",
    "            marker=markers[name],\n",
    "            color=colors[name],\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            label=name,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Number of Samples\")\n",
    "    ax.set_ylabel(\"Discrepancy (lower is better)\")\n",
    "    ax.set_title(\"Discrepancy Comparison: Space-Filling Quality\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"02_discrepancy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '02_discrepancy_comparison.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 6. Success rate comparison\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"6. Computing success rate comparison...\")\n",
    "\n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    n_points = 100\n",
    "    x_data = np.linspace(0, 4 * np.pi, n_points)\n",
    "    true_params = [2.0, 1.5, 0.5]\n",
    "    y_true = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\n",
    "    y_data = y_true + 0.2 * np.random.randn(n_points)\n",
    "\n",
    "    bounds = ([0.5, 0.5, -np.pi], [5.0, 3.0, np.pi])\n",
    "    lb, ub = np.array(bounds[0]), np.array(bounds[1])\n",
    "\n",
    "    n_starts_list = [5, 10, 15, 20, 25, 30]\n",
    "    n_trials = 5\n",
    "\n",
    "    success_rates = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\n",
    "\n",
    "    for n_starts in n_starts_list:\n",
    "        print(f\"  Evaluating n_starts = {n_starts}...\")\n",
    "\n",
    "        # Random and LHS: average over trials\n",
    "        random_rates = []\n",
    "        lhs_rates = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            random_s = np.random.rand(n_starts, 3)\n",
    "            lhs_s = np.array(\n",
    "                latin_hypercube_sample(n_starts, 3, rng_key=jax.random.PRNGKey(trial))\n",
    "            )\n",
    "\n",
    "            rate, _, _ = evaluate_starting_points(\n",
    "                random_s, lb, ub, x_data, y_data, true_params, bounds\n",
    "            )\n",
    "            random_rates.append(rate)\n",
    "\n",
    "            rate, _, _ = evaluate_starting_points(\n",
    "                lhs_s, lb, ub, x_data, y_data, true_params, bounds\n",
    "            )\n",
    "            lhs_rates.append(rate)\n",
    "\n",
    "        success_rates[\"Random\"].append(np.mean(random_rates))\n",
    "        success_rates[\"LHS\"].append(np.mean(lhs_rates))\n",
    "\n",
    "        # Sobol and Halton: deterministic\n",
    "        sobol_s = np.array(sobol_sample(n_starts, 3))\n",
    "        halton_s = np.array(halton_sample(n_starts, 3))\n",
    "\n",
    "        rate, _, _ = evaluate_starting_points(\n",
    "            sobol_s, lb, ub, x_data, y_data, true_params, bounds\n",
    "        )\n",
    "        success_rates[\"Sobol\"].append(rate)\n",
    "\n",
    "        rate, _, _ = evaluate_starting_points(\n",
    "            halton_s, lb, ub, x_data, y_data, true_params, bounds\n",
    "        )\n",
    "        success_rates[\"Halton\"].append(rate)\n",
    "\n",
    "    # Plot success rate\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for name, rates in success_rates.items():\n",
    "        ax.plot(\n",
    "            n_starts_list,\n",
    "            [r * 100 for r in rates],\n",
    "            marker=markers[name],\n",
    "            color=colors[name],\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            label=name,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Number of Starting Points\")\n",
    "    ax.set_ylabel(\"Success Rate (%)\")\n",
    "    ax.set_title(\"Success Rate: Finding Global Optimum\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 105)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIG_DIR / \"02_success_rate_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '02_success_rate_comparison.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 7. Demonstrate samplers with curve_fit()\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"7. Using samplers with curve_fit()...\")\n",
    "\n",
    "    samplers = [\"lhs\", \"sobol\", \"halton\"]\n",
    "    results = {}\n",
    "\n",
    "    for sampler in samplers:\n",
    "        popt, pcov = curve_fit(\n",
    "            multimodal_model,\n",
    "            x_data,\n",
    "            y_data,\n",
    "            p0=[1.0, 1.0, 0.0],\n",
    "            bounds=bounds,\n",
    "            multistart=True,\n",
    "            n_starts=10,\n",
    "            sampler=sampler,\n",
    "        )\n",
    "\n",
    "        y_pred = multimodal_model(x_data, *popt)\n",
    "        ssr = float(jnp.sum((y_data - y_pred) ** 2))\n",
    "\n",
    "        results[sampler] = {\"popt\": popt, \"ssr\": ssr}\n",
    "\n",
    "        print(f\"  Sampler: {sampler}\")\n",
    "        print(f\"    Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}\")\n",
    "        print(f\"    SSR: {ssr:.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Sampling Strategies:\")\n",
    "    print(\"  - Random: Baseline, poor space-filling\")\n",
    "    print(\"  - LHS: Stratified random, good coverage, stochastic\")\n",
    "    print(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\n",
    "    print(\"  - Halton: Quasi-random, very good coverage, deterministic\")\n",
    "    print()\n",
    "    print(\"Key Functions:\")\n",
    "    print(\"  - latin_hypercube_sample(n_samples, n_dims)\")\n",
    "    print(\"  - sobol_sample(n_samples, n_dims)\")\n",
    "    print(\"  - halton_sample(n_samples, n_dims)\")\n",
    "    print(\"  - scale_samples_to_bounds(samples, lb, ub)\")\n",
    "    print()\n",
    "    print(\"Usage with curve_fit():\")\n",
    "    print('  curve_fit(..., multistart=True, n_starts=10, sampler=\"lhs\")')\n",
    "    print()\n",
    "    print(\"Sampler Selection Guidelines:\")\n",
    "    print(\"  - General use: LHS (default)\")\n",
    "    print(\"  - Reproducibility needed: Sobol\")\n",
    "    print(\"  - Low dimensions (2-5): Halton\")\n",
    "    print(\"  - High dimensions (>10): LHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0de8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:32.825882Z",
     "iopub.status.busy": "2026-01-06T17:52:32.825747Z",
     "iopub.status.idle": "2026-01-06T17:52:32.828711Z",
     "shell.execute_reply": "2026-01-06T17:52:32.827910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
