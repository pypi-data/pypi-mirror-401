{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c021d52e",
   "metadata": {},
   "source": [
    "# 01 Multistart Basics\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/notebooks/07_global_optimization/01_multistart_basics.ipynb)\n",
    "\n",
    "Converted from 01_multistart_basics.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\n",
    "Features demonstrated:\n",
    "- Local minima trap problem in nonlinear optimization\n",
    "- GlobalOptimizationConfig configuration\n",
    "- curve_fit() with global_optimization parameter\n",
    "- Comparison of single-start vs multi-start results\n",
    "- Visualization of loss landscape and starting point distribution\n",
    "\n",
    "Run this example:\n",
    "    python examples/scripts/07_global_optimization/01_multistart_basics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b79063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:13.302873Z",
     "iopub.status.busy": "2026-01-06T17:52:13.302594Z",
     "iopub.status.idle": "2026-01-06T17:52:13.310126Z",
     "shell.execute_reply": "2026-01-06T17:52:13.309110Z"
    },
    "id": "colab-install"
   },
   "outputs": [],
   "source": [
    "# @title Install NLSQ (run once in Colab)\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab - installing NLSQ...\")\n",
    "    !pip install -q nlsq\n",
    "    print(\"âœ… NLSQ installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab - assuming NLSQ is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7a5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:13.313521Z",
     "iopub.status.busy": "2026-01-06T17:52:13.313365Z",
     "iopub.status.idle": "2026-01-06T17:52:14.963842Z",
     "shell.execute_reply": "2026-01-06T17:52:14.961810Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import GlobalOptimizationConfig, curve_fit\n",
    "from nlsq.global_optimization import latin_hypercube_sample, scale_samples_to_bounds\n",
    "\n",
    "FIG_DIR = Path.cwd() / \"figures\"  # Modified for notebook compatibility\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515e284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:14.968923Z",
     "iopub.status.busy": "2026-01-06T17:52:14.968665Z",
     "iopub.status.idle": "2026-01-06T17:52:14.986678Z",
     "shell.execute_reply": "2026-01-06T17:52:14.986065Z"
    }
   },
   "outputs": [],
   "source": [
    "def multimodal_model(x, a, b, c, d):\n",
    "    \"\"\"Multimodal model: y = a * sin(b * x + c) + d\n",
    "\n",
    "    This model has multiple local minima due to the periodicity of sin().\n",
    "    Different combinations of (b, c) can produce similar fits.\n",
    "    \"\"\"\n",
    "    return a * jnp.sin(b * x + c) + d\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Multi-Start Optimization Basics\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 1. Generate synthetic data\n",
    "    # =========================================================================\n",
    "    print(\"1. Generating synthetic data...\")\n",
    "\n",
    "    n_samples = 200\n",
    "    x_data = np.linspace(0, 4 * np.pi, n_samples)\n",
    "\n",
    "    # True parameters\n",
    "    true_a, true_b, true_c, true_d = 2.0, 1.5, 0.5, 1.0\n",
    "\n",
    "    # Generate noisy observations\n",
    "    y_true = true_a * np.sin(true_b * x_data + true_c) + true_d\n",
    "    noise = 0.2 * np.random.randn(n_samples)\n",
    "    y_data = y_true + noise\n",
    "\n",
    "    print(f\"  True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\n",
    "    print(f\"  Dataset: {n_samples} points\")\n",
    "    print()\n",
    "\n",
    "    # Visualize data\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.scatter(x_data, y_data, alpha=0.5, s=10, label=\"Noisy data\")\n",
    "    ax.plot(x_data, y_true, \"r-\", linewidth=2, label=\"True function\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Synthetic Data: Multimodal Sinusoidal Model\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"01_data_visualization.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '01_data_visualization.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2. Single-start optimization with different initial guesses\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"2. Single-start optimization (showing sensitivity to initial guess)...\")\n",
    "\n",
    "    # Define bounds\n",
    "    bounds = ([0.5, 0.5, -np.pi, -2.0], [5.0, 3.0, np.pi, 5.0])\n",
    "\n",
    "    # Try several different initial guesses\n",
    "    initial_guesses = [\n",
    "        [1.0, 0.8, 0.0, 0.5],  # Poor guess 1\n",
    "        [3.0, 2.5, 2.0, 2.0],  # Poor guess 2\n",
    "        [1.5, 1.2, -1.0, 0.0],  # Poor guess 3\n",
    "    ]\n",
    "\n",
    "    single_start_results = []\n",
    "\n",
    "    for i, p0 in enumerate(initial_guesses):\n",
    "        try:\n",
    "            popt, pcov = curve_fit(\n",
    "                multimodal_model,\n",
    "                x_data,\n",
    "                y_data,\n",
    "                p0=p0,\n",
    "                bounds=bounds,\n",
    "            )\n",
    "            y_pred = multimodal_model(x_data, *popt)\n",
    "            ssr = float(jnp.sum((y_data - y_pred) ** 2))\n",
    "            single_start_results.append({\"p0\": p0, \"popt\": popt, \"ssr\": ssr})\n",
    "            print(f\"  Guess {i + 1}: p0={p0}\")\n",
    "            print(\n",
    "                f\"    Result: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}, d={popt[3]:.3f}\"\n",
    "            )\n",
    "            print(f\"    SSR: {ssr:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Guess {i + 1}: Failed - {e}\")\n",
    "            single_start_results.append({\"p0\": p0, \"popt\": None, \"ssr\": float(\"inf\")})\n",
    "\n",
    "    # =========================================================================\n",
    "    # 3. Multi-start optimization\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"3. Multi-start optimization...\")\n",
    "\n",
    "    # Configure multi-start optimization\n",
    "    global_config = GlobalOptimizationConfig(\n",
    "        n_starts=10,\n",
    "        sampler=\"lhs\",\n",
    "        center_on_p0=True,\n",
    "        scale_factor=1.0,\n",
    "    )\n",
    "\n",
    "    print(\"  GlobalOptimizationConfig:\")\n",
    "    print(f\"    n_starts: {global_config.n_starts}\")\n",
    "    print(f\"    sampler: {global_config.sampler}\")\n",
    "    print(f\"    center_on_p0: {global_config.center_on_p0}\")\n",
    "    print(f\"    scale_factor: {global_config.scale_factor}\")\n",
    "\n",
    "    # Use the first (poor) initial guess\n",
    "    p0_poor = [1.0, 0.8, 0.0, 0.5]\n",
    "\n",
    "    # Fit with multi-start optimization\n",
    "    popt_multi, pcov_multi = curve_fit(\n",
    "        multimodal_model,\n",
    "        x_data,\n",
    "        y_data,\n",
    "        p0=p0_poor,\n",
    "        bounds=bounds,\n",
    "        multistart=True,\n",
    "        n_starts=10,\n",
    "        sampler=\"lhs\",\n",
    "    )\n",
    "\n",
    "    y_pred_multi = multimodal_model(x_data, *popt_multi)\n",
    "    ssr_multi = float(jnp.sum((y_data - y_pred_multi) ** 2))\n",
    "\n",
    "    print()\n",
    "    print(\"  Multi-start result:\")\n",
    "    print(\n",
    "        f\"    Parameters: a={popt_multi[0]:.3f}, b={popt_multi[1]:.3f}, c={popt_multi[2]:.3f}, d={popt_multi[3]:.3f}\"\n",
    "    )\n",
    "    print(f\"    SSR: {ssr_multi:.4f}\")\n",
    "\n",
    "    # Compare with single-start from same initial guess\n",
    "    popt_single, _ = curve_fit(\n",
    "        multimodal_model,\n",
    "        x_data,\n",
    "        y_data,\n",
    "        p0=p0_poor,\n",
    "        bounds=bounds,\n",
    "    )\n",
    "    y_pred_single = multimodal_model(x_data, *popt_single)\n",
    "    ssr_single = float(jnp.sum((y_data - y_pred_single) ** 2))\n",
    "\n",
    "    print()\n",
    "    print(\"  Comparison (same initial guess):\")\n",
    "    print(f\"    Single-start SSR: {ssr_single:.4f}\")\n",
    "    print(f\"    Multi-start SSR:  {ssr_multi:.4f}\")\n",
    "    if ssr_multi < ssr_single:\n",
    "        improvement = (1 - ssr_multi / ssr_single) * 100\n",
    "        print(f\"    Improvement: {improvement:.1f}% lower SSR\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 4. Comparison visualization\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"4. Saving comparison visualization...\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left plot: Data with both fits\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(x_data, y_data, alpha=0.4, s=15, label=\"Data\", color=\"gray\")\n",
    "    ax1.plot(x_data, y_true, \"k--\", linewidth=2, label=\"True function\", alpha=0.7)\n",
    "    ax1.plot(\n",
    "        x_data,\n",
    "        y_pred_single,\n",
    "        \"b-\",\n",
    "        linewidth=2,\n",
    "        label=f\"Single-start (SSR={ssr_single:.2f})\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        x_data,\n",
    "        y_pred_multi,\n",
    "        \"r-\",\n",
    "        linewidth=2,\n",
    "        label=f\"Multi-start (SSR={ssr_multi:.2f})\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_title(\"Single-Start vs Multi-Start Comparison\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Right plot: Residuals comparison\n",
    "    ax2 = axes[1]\n",
    "    residuals_single = y_data - y_pred_single\n",
    "    residuals_multi = y_data - y_pred_multi\n",
    "    ax2.scatter(\n",
    "        x_data, residuals_single, alpha=0.5, s=15, label=\"Single-start\", color=\"blue\"\n",
    "    )\n",
    "    ax2.scatter(\n",
    "        x_data, residuals_multi, alpha=0.5, s=15, label=\"Multi-start\", color=\"red\"\n",
    "    )\n",
    "    ax2.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"Residual\")\n",
    "    ax2.set_title(\"Residuals Comparison\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"01_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '01_comparison.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5. Loss landscape visualization\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"5. Generating loss landscape visualization...\")\n",
    "\n",
    "    b_range = np.linspace(0.5, 3.0, 50)\n",
    "    c_range = np.linspace(-np.pi, np.pi, 50)\n",
    "    B, C = np.meshgrid(b_range, c_range)\n",
    "\n",
    "    loss_landscape = np.zeros_like(B)\n",
    "    for i in range(len(c_range)):\n",
    "        for j in range(len(b_range)):\n",
    "            y_pred = true_a * np.sin(B[i, j] * x_data + C[i, j]) + true_d\n",
    "            loss_landscape[i, j] = np.sum((y_data - y_pred) ** 2)\n",
    "\n",
    "    loss_log = np.log10(loss_landscape + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    contour = ax.contourf(B, C, loss_log, levels=30, cmap=\"viridis\")\n",
    "    plt.colorbar(contour, ax=ax, label=\"log10(SSR + 1)\")\n",
    "\n",
    "    ax.scatter(\n",
    "        [true_b],\n",
    "        [true_c],\n",
    "        color=\"white\",\n",
    "        marker=\"*\",\n",
    "        s=200,\n",
    "        label=\"True parameters\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=1,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        [popt_single[1]],\n",
    "        [popt_single[2]],\n",
    "        color=\"blue\",\n",
    "        marker=\"o\",\n",
    "        s=100,\n",
    "        label=\"Single-start result\",\n",
    "        edgecolors=\"white\",\n",
    "        linewidths=1,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        [popt_multi[1]],\n",
    "        [popt_multi[2]],\n",
    "        color=\"red\",\n",
    "        marker=\"s\",\n",
    "        s=100,\n",
    "        label=\"Multi-start result\",\n",
    "        edgecolors=\"white\",\n",
    "        linewidths=1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"b (frequency)\")\n",
    "    ax.set_ylabel(\"c (phase)\")\n",
    "    ax.set_title(\n",
    "        \"Loss Landscape (a, d fixed at true values)\\nMultiple local minima visible\"\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"01_loss_landscape.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '01_loss_landscape.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 6. Starting point distribution\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"6. Generating starting point distribution visualization...\")\n",
    "\n",
    "    n_samples_viz = 20\n",
    "    n_params = 4\n",
    "\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    lhs_samples = latin_hypercube_sample(n_samples_viz, n_params, rng_key=key)\n",
    "\n",
    "    lb = np.array([0.5, 0.5, -np.pi, -2.0])\n",
    "    ub = np.array([5.0, 3.0, np.pi, 5.0])\n",
    "    scaled_samples = scale_samples_to_bounds(lhs_samples, lb, ub)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: LHS samples on loss landscape\n",
    "    ax1 = axes[0]\n",
    "    contour = ax1.contourf(B, C, loss_log, levels=30, cmap=\"viridis\", alpha=0.7)\n",
    "    ax1.scatter(\n",
    "        scaled_samples[:, 1],\n",
    "        scaled_samples[:, 2],\n",
    "        color=\"yellow\",\n",
    "        marker=\"o\",\n",
    "        s=80,\n",
    "        label=\"LHS starting points\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=1,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        [true_b],\n",
    "        [true_c],\n",
    "        color=\"white\",\n",
    "        marker=\"*\",\n",
    "        s=200,\n",
    "        label=\"True parameters\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=1,\n",
    "    )\n",
    "    ax1.set_xlabel(\"b (frequency)\")\n",
    "    ax1.set_ylabel(\"c (phase)\")\n",
    "    ax1.set_title(\"LHS Starting Points on Loss Landscape\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Right: All 2D projections\n",
    "    ax2 = axes[1]\n",
    "    param_names = [\"a\", \"b\", \"c\", \"d\"]\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 6))\n",
    "\n",
    "    plot_idx = 0\n",
    "    for i in range(n_params):\n",
    "        for j in range(i + 1, n_params):\n",
    "            ax2.scatter(\n",
    "                scaled_samples[:, i],\n",
    "                scaled_samples[:, j],\n",
    "                alpha=0.6,\n",
    "                s=30,\n",
    "                color=colors[plot_idx],\n",
    "                label=f\"{param_names[i]} vs {param_names[j]}\",\n",
    "            )\n",
    "            plot_idx += 1\n",
    "\n",
    "    ax2.set_xlabel(\"Parameter value (normalized)\")\n",
    "    ax2.set_ylabel(\"Parameter value (normalized)\")\n",
    "    ax2.set_title(\"LHS Coverage: All 2D Projections\")\n",
    "    ax2.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"01_starting_points.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  Saved: {FIG_DIR / '01_starting_points.png'}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Summary\n",
    "    # =========================================================================\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\n",
    "    print()\n",
    "    print(\"Single-start result (poor initial guess):\")\n",
    "    print(\n",
    "        f\"  Parameters: a={popt_single[0]:.3f}, b={popt_single[1]:.3f}, c={popt_single[2]:.3f}, d={popt_single[3]:.3f}\"\n",
    "    )\n",
    "    print(f\"  SSR: {ssr_single:.4f}\")\n",
    "    print()\n",
    "    print(\"Multi-start result (10 starts, LHS):\")\n",
    "    print(\n",
    "        f\"  Parameters: a={popt_multi[0]:.3f}, b={popt_multi[1]:.3f}, c={popt_multi[2]:.3f}, d={popt_multi[3]:.3f}\"\n",
    "    )\n",
    "    print(f\"  SSR: {ssr_multi:.4f}\")\n",
    "    print()\n",
    "    print(\"Key takeaways:\")\n",
    "    print(\"  - Local minima are common in nonlinear optimization\")\n",
    "    print(\"  - Single-start optimization is sensitive to initial guess\")\n",
    "    print(\"  - Multi-start explores multiple starting points for global optimum\")\n",
    "    print(\"  - LHS provides stratified coverage of parameter space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63791ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T17:52:14.988216Z",
     "iopub.status.busy": "2026-01-06T17:52:14.988103Z",
     "iopub.status.idle": "2026-01-06T17:52:28.285746Z",
     "shell.execute_reply": "2026-01-06T17:52:28.285208Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
