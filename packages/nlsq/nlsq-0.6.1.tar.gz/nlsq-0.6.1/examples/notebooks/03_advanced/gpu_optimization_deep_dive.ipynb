{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8cf01f",
   "metadata": {},
   "source": [
    "# Gpu Optimization Deep Dive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3261e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:58:14.687229Z",
     "iopub.status.busy": "2026-01-06T16:58:14.686194Z",
     "iopub.status.idle": "2026-01-06T16:58:33.154914Z",
     "shell.execute_reply": "2026-01-06T16:58:33.153740Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converted from gpu_optimization_deep_dive.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# # GPU Optimization and Performance Deep Dive\n",
    "#\n",
    "# **Level**: Advanced\n",
    "# **Time**: 50-70 minutes\n",
    "# **Prerequisites**: NLSQ Quickstart, JAX basics\n",
    "#\n",
    "# ## Overview\n",
    "#\n",
    "# This tutorial covers **performance optimization** for NLSQ, focusing on:\n",
    "# - JAX JIT compilation and profiling\n",
    "# - GPU acceleration strategies\n",
    "# - Memory optimization\n",
    "# - Batch processing for maximum throughput\n",
    "#\n",
    "# ### What You'll Learn\n",
    "#\n",
    "# 1. **JAX Profiling**: Identifying bottlenecks with JAX tools\n",
    "# 2. **JIT Compilation**: Understanding and optimizing compilation\n",
    "# 3. **GPU Acceleration**: When and how to leverage GPUs\n",
    "# 4. **Memory Management**: Avoiding OOM errors\n",
    "# 5. **Batch Strategies**: Processing thousands of fits efficiently\n",
    "# 6. **Benchmarking**: Measuring and comparing performance\n",
    "#\n",
    "# ### Performance Targets\n",
    "#\n",
    "# Typical NLSQ performance (depends on hardware, problem size):\n",
    "# - **Cold start (first call)**: 0.5-2 seconds (includes JIT compilation)\n",
    "# - **Warm calls (cached)**: 1-50 ms per fit\n",
    "# - **GPU speedup**: 5-50x for large batches vs CPU\n",
    "# - **Batch throughput**: 100-10,000 fits/second (GPU, batched)\n",
    "#\n",
    "# ### Hardware Requirements\n",
    "#\n",
    "# This notebook runs on CPU or GPU. GPU examples automatically fall back to CPU if no GPU is available.\n",
    "# ======================================================================\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import jit, vmap\n",
    "\n",
    "from nlsq import CurveFit\n",
    "\n",
    "# Detect available devices\n",
    "devices = jax.devices()\n",
    "has_gpu = any(\"gpu\" in str(d).lower() for d in devices)\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "MAX_SAMPLES = int(os.environ.get(\"NLSQ_EXAMPLES_MAX_SAMPLES\", \"300000\"))\n",
    "\n",
    "\n",
    "def cap_samples(n: int) -> int:\n",
    "    return min(n, MAX_SAMPLES) if QUICK else n\n",
    "\n",
    "\n",
    "print(\"Hardware Configuration:\")\n",
    "print(f\"  JAX version: {jax.__version__}\")\n",
    "print(f\"  Default backend: {jax.default_backend()}\")\n",
    "print(f\"  Available devices: {devices}\")\n",
    "print(f\"  GPU available: {'✓ Yes' if has_gpu else '✗ No (will use CPU)'}\")\n",
    "print()\n",
    "\n",
    "if has_gpu:\n",
    "    print(\"GPU detected - examples will show GPU acceleration\")\n",
    "else:\n",
    "    print(\"Running on CPU - GPU examples will still work but won't show speedup\")\n",
    "    print(\"To use GPU: Install jax[cuda] or jax[rocm] depending on your hardware\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 1: JIT Compilation Basics\n",
    "#\n",
    "# Understanding JAX's Just-In-Time (JIT) compilation is crucial for performance.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Demonstrating JIT compilation overhead and benefits\n",
    "\n",
    "\n",
    "# Simple model\n",
    "def exponential_model(x, a, b):\n",
    "    return a * jnp.exp(-b * x)\n",
    "\n",
    "\n",
    "# Test data\n",
    "x_test = jnp.linspace(0, 5, cap_samples(1000))\n",
    "y_test = exponential_model(x_test, 3.0, 0.5) + np.random.normal(0, 0.1, len(x_test))\n",
    "\n",
    "cf = CurveFit()\n",
    "\n",
    "print(\"JIT Compilation Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First call: includes compilation time\n",
    "start = time.time()\n",
    "popt1, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[2.0, 0.3])\n",
    "time_first = (time.time() - start) * 1000  # ms\n",
    "\n",
    "# Second call: uses cached compilation\n",
    "start = time.time()\n",
    "popt2, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[2.5, 0.4])\n",
    "time_second = (time.time() - start) * 1000  # ms\n",
    "\n",
    "# Third call: still cached\n",
    "start = time.time()\n",
    "popt3, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[3.0, 0.5])\n",
    "time_third = (time.time() - start) * 1000  # ms\n",
    "\n",
    "print(f\"First call (cold):  {time_first:.1f} ms (includes JIT compilation)\")\n",
    "print(f\"Second call (warm): {time_second:.1f} ms (cached)\")\n",
    "print(f\"Third call (warm):  {time_third:.1f} ms (cached)\")\n",
    "print()\n",
    "print(f\"Speedup after JIT:  {time_first / time_second:.1f}x\")\n",
    "print(\n",
    "    f\"Compilation overhead: {time_first - time_second:.1f} ms ({(time_first - time_second) / time_first * 100:.1f}% of first call)\"\n",
    ")\n",
    "print()\n",
    "print(\"Key insight: First call is slow due to JIT compilation.\")\n",
    "print(\"            Subsequent calls are much faster (10-100x).\")\n",
    "\n",
    "\n",
    "# Understanding what triggers recompilation\n",
    "\n",
    "print(\"Recompilation Triggers:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Trigger 1: Different array shapes\n",
    "print(\"\\n1. Changing array shapes triggers recompilation:\")\n",
    "\n",
    "x_100 = jnp.linspace(0, 5, 100)\n",
    "y_100 = exponential_model(x_100, 3.0, 0.5) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "x_200 = jnp.linspace(0, 5, cap_samples(200))\n",
    "y_200 = exponential_model(x_200, 3.0, 0.5) + np.random.normal(0, 0.1, cap_samples(200))\n",
    "\n",
    "cf_new = CurveFit()\n",
    "\n",
    "start = time.time()\n",
    "cf_new.curve_fit(exponential_model, x_100, y_100, p0=[2.0, 0.3])\n",
    "time_100 = (time.time() - start) * 1000\n",
    "\n",
    "start = time.time()\n",
    "cf_new.curve_fit(exponential_model, x_200, y_200, p0=[2.0, 0.3])  # Different shape!\n",
    "time_200 = (time.time() - start) * 1000\n",
    "\n",
    "start = time.time()\n",
    "cf_new.curve_fit(exponential_model, x_200, y_200, p0=[2.5, 0.4])  # Same shape\n",
    "time_200_cached = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"  Fit with shape (100,): {time_100:.1f} ms (first compile)\")\n",
    "print(f\"  Fit with shape (200,): {time_200:.1f} ms (recompiled!)\")\n",
    "print(f\"  Fit with shape (200,): {time_200_cached:.1f} ms (cached) ✓\")\n",
    "print()\n",
    "print(\"  → Keep array shapes consistent to avoid recompilation\")\n",
    "\n",
    "# Trigger 2: Different dtypes\n",
    "print(\"\\n2. Changing dtypes triggers recompilation:\")\n",
    "print(\"  float32 vs float64 will trigger separate compilations\")\n",
    "print(\"  → Use consistent dtype (float32 for GPU, float64 for high precision)\")\n",
    "\n",
    "# Trigger 3: Different parameter counts\n",
    "print(\"\\n3. Different model signatures trigger recompilation:\")\n",
    "print(\"  model(x, a, b) vs model(x, a, b, c) are compiled separately\")\n",
    "print(\"  → Expected - different models need different compilations\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 2: GPU Acceleration\n",
    "#\n",
    "# Leverage GPU for massive speedups on large problems.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# CPU vs GPU performance comparison\n",
    "\n",
    "# Large dataset (GPU shines here)\n",
    "n_points = cap_samples(10000)\n",
    "x_large = jnp.linspace(0, 10, n_points)\n",
    "y_large = (\n",
    "    3.0 * jnp.exp(-0.5 * x_large)\n",
    "    + 2.0 * jnp.sin(x_large)\n",
    "    + np.random.normal(0, 0.1, n_points)\n",
    ")\n",
    "\n",
    "\n",
    "def complex_model(x, a, b, c, d):\n",
    "    return a * jnp.exp(-b * x) + c * jnp.sin(d * x)\n",
    "\n",
    "\n",
    "print(f\"GPU Acceleration Benchmark (n_points={n_points}):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure compilation is done (use same settings as benchmark for consistency)\n",
    "cf_gpu = CurveFit()\n",
    "try:\n",
    "    _ = cf_gpu.curve_fit(\n",
    "        complex_model,\n",
    "        x_large[:100],\n",
    "        y_large[:100],\n",
    "        p0=[3, 0.5, 2, 1],\n",
    "        maxiter=20 if QUICK else 50,\n",
    "        max_nfev=200 if QUICK else 1000,\n",
    "    )\n",
    "except Exception as exc:\n",
    "    print(f\"Warmup fit skipped: {exc}\")\n",
    "\n",
    "# Benchmark: 10 fits (reduced in quick mode)\n",
    "n_runs = 3 if QUICK else 10\n",
    "times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Slightly vary initial guess to avoid trivial caching\n",
    "    p0 = [3.0 + i * 0.1, 0.5, 2.0, 1.0]\n",
    "    start = time.time()\n",
    "    popt, _ = cf_gpu.curve_fit(\n",
    "        complex_model, x_large, y_large, p0=p0, maxiter=20 if QUICK else 50\n",
    "    )\n",
    "    times.append((time.time() - start) * 1000)\n",
    "\n",
    "mean_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "\n",
    "print(f\"\\nDevice: {jax.devices()[0]}\")\n",
    "print(f\"Average fit time: {mean_time:.1f} ± {std_time:.1f} ms\")\n",
    "print(f\"Throughput: {1000 / mean_time:.1f} fits/second\")\n",
    "print()\n",
    "\n",
    "if has_gpu:\n",
    "    print(\"✓ Running on GPU - performance is optimized\")\n",
    "    print(\"  Expected speedup vs CPU: 5-20x for this problem size\")\n",
    "else:\n",
    "    print(\"Running on CPU - results are valid but slower than GPU\")\n",
    "    print(\"  With GPU: Expect 5-50x speedup for large datasets\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 3: Batch Processing Strategies\n",
    "#\n",
    "# Process thousands of fits efficiently with vectorization.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Batch processing with vmap for maximum throughput\n",
    "\n",
    "print(\"Batch Processing Benchmark:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate batch of datasets\n",
    "n_datasets = min(20, cap_samples(200)) if QUICK else max(10, cap_samples(1000))\n",
    "n_points_per_dataset = 30 if QUICK else 50\n",
    "\n",
    "x_batch_data = jnp.linspace(0, 5, n_points_per_dataset)\n",
    "\n",
    "# Random true parameters for each dataset\n",
    "np.random.seed(42)\n",
    "a_true_batch = np.random.uniform(2, 4, n_datasets)\n",
    "b_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n",
    "\n",
    "y_batch_data = jnp.array(\n",
    "    [\n",
    "        a * jnp.exp(-b * x_batch_data) + np.random.normal(0, 0.05, n_points_per_dataset)\n",
    "        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {n_datasets} datasets\")\n",
    "print(f\"Points per dataset: {n_points_per_dataset}\")\n",
    "print(f\"Total data points: {n_datasets * n_points_per_dataset:,}\")\n",
    "print()\n",
    "\n",
    "# Method 1: Sequential (slow)\n",
    "print(\"Method 1: Sequential fitting (baseline)\")\n",
    "start = time.time()\n",
    "results_sequential = []\n",
    "cf_seq = CurveFit()\n",
    "sequential_runs = min(20 if QUICK else 100, n_datasets)\n",
    "for i in range(sequential_runs):  # Only fit a subset for speed\n",
    "    popt, _ = cf_seq.curve_fit(\n",
    "        exponential_model, x_batch_data, y_batch_data[i], p0=[3.0, 0.5], maxiter=30\n",
    "    )\n",
    "    results_sequential.append(popt)\n",
    "time_sequential = time.time() - start\n",
    "\n",
    "print(\n",
    "    f\"  Time for {sequential_runs} datasets: {time_sequential * 1000:.0f} ms \"\n",
    "    f\"({time_sequential * 1000 / sequential_runs:.1f} ms/fit)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Estimated time for {n_datasets}: {time_sequential * n_datasets / sequential_runs:.1f} s\"\n",
    ")\n",
    "print()\n",
    "\n",
    "# Method 2: Vectorized with vmap (fast)\n",
    "print(\"Method 2: Batched fitting with vmap (optimized)\")\n",
    "\n",
    "\n",
    "# Simplified optimizer for vectorization\n",
    "def fit_one_dataset(y_single):\n",
    "    \"\"\"Fit single dataset (simplified gradient descent).\"\"\"\n",
    "    params = jnp.array([3.0, 0.5])\n",
    "\n",
    "    def loss(p):\n",
    "        return jnp.sum((y_single - exponential_model(x_batch_data, *p)) ** 2)\n",
    "\n",
    "    # A few gradient descent steps for demonstration\n",
    "    for _ in range(5 if QUICK else 20):\n",
    "        g = jax.grad(loss)(params)\n",
    "        params = params - 0.05 * g\n",
    "    return params\n",
    "\n",
    "\n",
    "# Vectorize over batch dimension\n",
    "fit_batch = jit(vmap(fit_one_dataset))\n",
    "\n",
    "# Warm up JIT\n",
    "warmup_size = min(10, n_datasets)\n",
    "_ = fit_batch(y_batch_data[:warmup_size])\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "results_batch = fit_batch(y_batch_data)\n",
    "# Block until computation completes (JAX is async)\n",
    "results_batch[0].block_until_ready()\n",
    "time_batch = time.time() - start\n",
    "\n",
    "print(\n",
    "    f\"  Time for {n_datasets} datasets: {time_batch * 1000:.0f} ms ({time_batch * 1000 / n_datasets:.3f} ms/fit)\"\n",
    ")\n",
    "print(f\"  Throughput: {n_datasets / time_batch:.0f} fits/second\")\n",
    "print()\n",
    "\n",
    "# Speedup\n",
    "estimated_sequential_time = time_sequential * n_datasets / sequential_runs\n",
    "speedup = estimated_sequential_time / time_batch\n",
    "\n",
    "print(f\"Speedup: {speedup:.0f}x faster with vmap + JIT ✓\")\n",
    "print()\n",
    "print(\"Key insight: vmap parallelizes across datasets, JIT compiles once\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 4: Memory Optimization\n",
    "#\n",
    "# Avoiding out-of-memory (OOM) errors with large datasets.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Memory optimization strategies\n",
    "\n",
    "print(\"Memory Optimization Strategies:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "print(\"1. Use float32 instead of float64:\")\n",
    "x_f64 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float64)\n",
    "x_f32 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)\n",
    "print(f\"   float64 memory: {x_f64.nbytes} bytes per element\")\n",
    "print(f\"   float32 memory: {x_f32.nbytes} bytes per element\")\n",
    "print(f\"   Savings: {(1 - x_f32.nbytes / x_f64.nbytes) * 100:.0f}%\")\n",
    "print(\"   → Use float32 unless high precision is critical\\n\")\n",
    "\n",
    "print(\"2. Process data in chunks (streaming):\")\n",
    "print(\"   # For very large datasets (millions of points)\")\n",
    "print(\"   chunk_size = 100000\")\n",
    "print(\"   for i in range(0, len(data), chunk_size):\")\n",
    "print(\"       chunk = data[i:i+chunk_size]\")\n",
    "print(\"       result = fit(chunk)\")\n",
    "print(\"       results.append(result)\\n\")\n",
    "\n",
    "print(\"3. Clear JAX cache if needed:\")\n",
    "print(\"   from jax import clear_caches\")\n",
    "print(\"   clear_caches()  # Frees compilation cache\\n\")\n",
    "\n",
    "print(\"4. Monitor memory usage:\")\n",
    "\n",
    "\n",
    "def get_array_memory_mb(arr):\n",
    "    return arr.nbytes / (1024**2)\n",
    "\n",
    "\n",
    "large_array = jnp.ones((cap_samples(10000), cap_samples(1000)), dtype=jnp.float32)\n",
    "print(\n",
    "    f\"   Example: {large_array.shape} array uses {get_array_memory_mb(large_array):.1f} MB\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"5. Typical memory requirements:\")\n",
    "print(\"   10K points:     ~0.1 MB (negligible)\")\n",
    "print(\"   1M points:      ~10 MB (easy)\")\n",
    "print(\"   100M points:    ~1 GB (manageable)\")\n",
    "print(\"   1B points:      ~10 GB (need chunking or distributed)\")\n",
    "print()\n",
    "print(\"→ For datasets >100M points, use chunked processing or streaming\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 5: Performance Benchmarking\n",
    "#\n",
    "# Systematic performance measurement and optimization.\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Comprehensive performance benchmark\n",
    "\n",
    "\n",
    "def benchmark_nlsq(n_points_list, n_params=2, n_runs=5):\n",
    "    \"\"\"Benchmark NLSQ across different problem sizes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_points_list : list\n",
    "        List of dataset sizes to test\n",
    "    n_params : int\n",
    "        Number of parameters to fit\n",
    "    n_runs : int\n",
    "        Number of runs to average\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Benchmark results\n",
    "    \"\"\"\n",
    "    results = {\"n_points\": [], \"mean_time_ms\": [], \"std_time_ms\": []}\n",
    "\n",
    "    cf_bench = CurveFit()\n",
    "\n",
    "    for n_points in n_points_list:\n",
    "        x = jnp.linspace(0, 5, n_points)\n",
    "        y = 3.0 * jnp.exp(-0.5 * x) + np.random.normal(0, 0.1, n_points)\n",
    "\n",
    "        # Warm up\n",
    "        _ = cf_bench.curve_fit(exponential_model, x, y, p0=[2.0, 0.3], maxiter=20)\n",
    "\n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            popt, _ = cf_bench.curve_fit(\n",
    "                exponential_model, x, y, p0=[2.0, 0.3], maxiter=20\n",
    "            )\n",
    "            # Note: popt is numpy array (already synchronous), no need for block_until_ready\n",
    "            times.append((time.time() - start) * 1000)\n",
    "\n",
    "        results[\"n_points\"].append(n_points)\n",
    "        results[\"mean_time_ms\"].append(np.mean(times))\n",
    "        results[\"std_time_ms\"].append(np.std(times))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Running comprehensive benchmark...\")\n",
    "print(\"(This may take 30-60 seconds in full mode)\")\n",
    "print()\n",
    "\n",
    "# Test different problem sizes\n",
    "size_candidates = [50, 100, 200] if QUICK else [100, 500, 1000, 5000, 10000]\n",
    "sizes = sorted({cap_samples(s) for s in size_candidates})\n",
    "bench_results = benchmark_nlsq(sizes, n_runs=2 if QUICK else 5)\n",
    "\n",
    "# Display results\n",
    "print(\"Benchmark Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'N Points':<12} {'Mean Time (ms)':<20} {'Throughput (fits/s)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, n in enumerate(bench_results[\"n_points\"]):\n",
    "    mean_t = bench_results[\"mean_time_ms\"][i]\n",
    "    std_t = bench_results[\"std_time_ms\"][i]\n",
    "    throughput = 1000 / mean_t\n",
    "    print(f\"{n:<12} {mean_t:>8.2f} ± {std_t:<8.2f} {throughput:>12.1f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Plot scaling\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time vs problem size\n",
    "ax1.errorbar(\n",
    "    bench_results[\"n_points\"],\n",
    "    bench_results[\"mean_time_ms\"],\n",
    "    yerr=bench_results[\"std_time_ms\"],\n",
    "    marker=\"o\",\n",
    "    capsize=5,\n",
    "    label=\"NLSQ\",\n",
    ")\n",
    "ax1.set_xlabel(\"Number of Data Points\")\n",
    "ax1.set_ylabel(\"Time (ms)\")\n",
    "ax1.set_title(\"Performance Scaling\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Log-log plot to see scaling behavior\n",
    "ax2.loglog(bench_results[\"n_points\"], bench_results[\"mean_time_ms\"], \"o-\", label=\"NLSQ\")\n",
    "ax2.set_xlabel(\"Number of Data Points\")\n",
    "ax2.set_ylabel(\"Time (ms)\")\n",
    "ax2.set_title(\"Scaling Behavior (log-log)\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3, which=\"both\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"gpu_optimization_deep_dive\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Nearly flat scaling: Well-optimized (GPU benefits)\")\n",
    "print(\"  - Linear scaling: Expected for iterative optimization\")\n",
    "print(\"  - Superlinear scaling: May indicate memory issues or poor caching\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Summary and Best Practices\n",
    "#\n",
    "# ### Performance Optimization Checklist\n",
    "#\n",
    "# **For Maximum Speed:**\n",
    "#\n",
    "# 1. ✅ **Use GPU** if available (5-50x speedup for large problems)\n",
    "# 2. ✅ **Keep array shapes consistent** to avoid recompilation\n",
    "# 3. ✅ **Use float32** unless high precision is needed (2x memory savings)\n",
    "# 4. ✅ **Batch process** with `vmap` for multiple datasets (10-100x faster)\n",
    "# 5. ✅ **Warm up JIT** with small dataset before benchmarking\n",
    "# 6. ✅ **Use `block_until_ready()`** when timing (JAX is async)\n",
    "#\n",
    "# **For Large Datasets:**\n",
    "#\n",
    "# 1. ✅ **Chunk data** if >100M points\n",
    "# 2. ✅ **Monitor memory** usage\n",
    "# 3. ✅ **Consider downsampling** for smooth, oversampled data\n",
    "# 4. ✅ **Use streaming** for datasets that don't fit in memory\n",
    "#\n",
    "# ### Performance Expectations\n",
    "#\n",
    "# | **Scenario** | **Typical Time** | **Optimization** |\n",
    "# |--------------|------------------|------------------|\n",
    "# | First call (cold start) | 0.5-2 seconds | Expected (JIT compilation) |\n",
    "# | Subsequent calls (warm) | 1-50 ms | Cached compilation |\n",
    "# | Large dataset (10K points) | 5-100 ms | Use GPU if available |\n",
    "# | Batch (1000 fits) | 100-5000 ms | Use vmap for parallelization |\n",
    "# | Huge dataset (1M points) | 50-500 ms | GPU + chunking |\n",
    "#\n",
    "# ### Troubleshooting Performance Issues\n",
    "#\n",
    "# **Problem**: First call is slow (>5 seconds)\n",
    "# - **Solution**: Normal for JIT. Subsequent calls will be fast.\n",
    "#\n",
    "# **Problem**: All calls are slow (>1 second for small data)\n",
    "# - **Solution**: Check if recompiling each time (varying shapes/dtypes)\n",
    "#\n",
    "# **Problem**: Out of memory errors\n",
    "# - **Solution**: Use float32, chunk data, or downsample\n",
    "#\n",
    "# **Problem**: GPU not being used\n",
    "# - **Solution**: Check `jax.devices()`, install jax[cuda] or jax[rocm]\n",
    "#\n",
    "# **Problem**: Batch processing not faster than sequential\n",
    "# - **Solution**: Problem may be too small, try larger batches or datasets\n",
    "#\n",
    "# ### Advanced Profiling\n",
    "#\n",
    "# For detailed profiling:\n",
    "#\n",
    "# ```python\n",
    "# # JAX profiling (requires jax[profiling])\n",
    "# import jax.profiler\n",
    "#\n",
    "# # Profile a code block\n",
    "# with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
    "#     # Your NLSQ code here\n",
    "#     popt, pcov = cf.curve_fit(model, x, y, p0=...)\n",
    "#\n",
    "# # Opens profiling UI in browser\n",
    "# ```\n",
    "#\n",
    "# ### Production Recommendations\n",
    "#\n",
    "# ```python\n",
    "# # Example: Optimized production setup\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from nlsq import CurveFit\n",
    "#\n",
    "# # Configure JAX for production\n",
    "# jax.config.update('jax_enable_x64', False)  # Use float32\n",
    "#\n",
    "# # Pre-warm JIT cache at startup\n",
    "# cf = CurveFit()\n",
    "# x_dummy = jnp.linspace(0, 1, 100)\n",
    "# y_dummy = jnp.ones(100)\n",
    "# _ = cf.curve_fit(model, x_dummy, y_dummy, p0=initial_guess)\n",
    "#\n",
    "# # Now ready for fast production fitting\n",
    "# ```\n",
    "#\n",
    "# ### Next Steps\n",
    "#\n",
    "# - **Scale up**: Try batch processing 10,000+ datasets with vmap\n",
    "# - **Optimize models**: Simplify model functions for faster evaluation\n",
    "# - **Profile**: Use JAX profiler to identify bottlenecks\n",
    "# - **Distribute**: For massive scale, consider JAX's `pmap` for multi-GPU\n",
    "#\n",
    "# ### References\n",
    "#\n",
    "# 1. **JAX Performance**: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "# 2. **JAX Profiling**: https://jax.readthedocs.io/en/latest/profiling.html\n",
    "# 3. **GPU Acceleration**: https://jax.readthedocs.io/en/latest/gpu_performance_tips.html\n",
    "# 4. **Related examples**:\n",
    "#    - `custom_algorithms_advanced.ipynb` - vmap for batch fitting\n",
    "#    - `troubleshooting_guide.ipynb` - Performance debugging\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# **Remember**: Premature optimization is the root of all evil. Profile first, optimize what matters!\n",
    "# ======================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
