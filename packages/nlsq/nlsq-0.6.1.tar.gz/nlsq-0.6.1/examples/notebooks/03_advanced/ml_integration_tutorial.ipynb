{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c689828",
   "metadata": {},
   "source": [
    "# Ml Integration Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef1eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T16:58:36.952354Z",
     "iopub.status.busy": "2026-01-06T16:58:36.951989Z",
     "iopub.status.idle": "2026-01-06T16:59:21.073868Z",
     "shell.execute_reply": "2026-01-06T16:59:21.072878Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converted from ml_integration_tutorial.ipynb\n",
    "\n",
    "This script was automatically generated from a Jupyter notebook.\n",
    "Plots are saved to the figures/ directory instead of displayed inline.\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================================\n",
    "# # NLSQ + JAX ML Ecosystem Integration\n",
    "#\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/ml_integration_tutorial.ipynb)\n",
    "#\n",
    "# **Level**: Advanced | **Time**: 45-60 min | **Prerequisites**: NLSQ Quickstart, JAX basics\n",
    "#\n",
    "# ## Overview\n",
    "#\n",
    "# This tutorial demonstrates how NLSQ integrates with the JAX machine learning ecosystem for:\n",
    "#\n",
    "# 1. **Neural ODEs**: Fitting dynamic systems with learned neural network components\n",
    "# 2. **Physics-Informed Neural Networks (PINNs)**: Incorporating physical constraints into ML models\n",
    "# 3. **Differentiable Physics**: End-to-end differentiable simulations\n",
    "# 4. **Hybrid Models**: Combining mechanistic and data-driven approaches\n",
    "# 5. **Optax Integration**: Using advanced optimizers with NLSQ\n",
    "# 6. **Parameter Estimation**: Fitting ML model parameters from experimental data\n",
    "#\n",
    "# ### What You'll Learn\n",
    "#\n",
    "# - ‚úÖ Integrate NLSQ with Flax neural networks\n",
    "# - ‚úÖ Build hybrid mechanistic-ML models\n",
    "# - ‚úÖ Implement Neural ODEs for dynamics\n",
    "# - ‚úÖ Create physics-informed loss functions\n",
    "# - ‚úÖ Fit complex multi-component models\n",
    "# - ‚úÖ Leverage automatic differentiation for scientific computing\n",
    "#\n",
    "# ### Prerequisites\n",
    "#\n",
    "# **Required Knowledge**:\n",
    "# - NLSQ basics (complete Quickstart first)\n",
    "# - JAX fundamentals\n",
    "# - Basic neural networks\n",
    "# - Differential equations (for Neural ODE section)\n",
    "#\n",
    "# **Python Version**: 3.12+\n",
    "# **NLSQ Version**: 0.2.0+\n",
    "#\n",
    "# ---\n",
    "# ======================================================================\n",
    "# ======================================================================\n",
    "# ## Setup and Imports\n",
    "# ======================================================================\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install nlsq flax optax equinox\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import jit\n",
    "\n",
    "# NLSQ imports\n",
    "from nlsq import CurveFit, __version__\n",
    "\n",
    "QUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\n",
    "if QUICK:\n",
    "    print(\n",
    "        \"Quick mode: skipping ML integration demo (unset NLSQ_EXAMPLES_QUICK for full run).\"\n",
    "    )\n",
    "    QUICK = False\n",
    "\n",
    "# ML ecosystem imports\n",
    "try:\n",
    "    import flax.linen as nn\n",
    "    import optax\n",
    "    from flax.training import train_state\n",
    "\n",
    "    FLAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Flax/Optax not available. Install with: pip install flax optax\")\n",
    "    FLAX_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    pass\n",
    "\n",
    "    EQUINOX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Equinox not available. Install with: pip install equinox\")\n",
    "    EQUINOX_AVAILABLE = False\n",
    "\n",
    "print(f\"NLSQ version: {__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Flax available: {FLAX_AVAILABLE}\")\n",
    "print(f\"Equinox available: {EQUINOX_AVAILABLE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate synthetic data: exponential decay + systematic deviation\n",
    "# (defined outside FLAX_AVAILABLE block so statistics can be printed)\n",
    "x_data = np.linspace(0, 5, 200)\n",
    "true_a, true_b = 5.0, 1.2\n",
    "\n",
    "# Physics component\n",
    "y_physics = true_a * np.exp(-true_b * x_data)\n",
    "\n",
    "# Systematic deviation (sinusoidal correction)\n",
    "y_correction = 0.5 * np.sin(3 * x_data) * np.exp(-0.3 * x_data)\n",
    "\n",
    "# Observed data = physics + correction + noise\n",
    "y_data = y_physics + y_correction + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ---\n",
    "#\n",
    "# ## Part 1: Hybrid Mechanistic-ML Models\n",
    "#\n",
    "# ### Concept: Combining Physics and Learning\n",
    "#\n",
    "# Many scientific problems have **known physics** but **unknown corrections**:\n",
    "#\n",
    "# $$\n",
    "# y(x; \\theta, w) = f_{\\text{physics}}(x; \\theta) + g_{\\text{NN}}(x; w)\n",
    "# $$\n",
    "#\n",
    "# Where:\n",
    "# - $f_{\\text{physics}}$: Known mechanistic model (e.g., exponential decay)\n",
    "# - $g_{\\text{NN}}$: Learned neural network correction\n",
    "# - $\\theta$: Physical parameters (fitted with NLSQ)\n",
    "# - $w$: Neural network weights (pre-trained or jointly optimized)\n",
    "#\n",
    "# **Benefits**:\n",
    "# - ‚úÖ Interpretable physical parameters\n",
    "# - ‚úÖ Data-efficient (physics provides structure)\n",
    "# - ‚úÖ Extrapolates better than pure ML\n",
    "# - ‚úÖ Captures systematic deviations\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Example 1.1: Exponential Decay with Neural Network Correction\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "if FLAX_AVAILABLE:\n",
    "    # Define a simple MLP for corrections\n",
    "    class CorrectionMLP(nn.Module):\n",
    "        \"\"\"Small MLP to learn systematic deviations from physics model.\"\"\"\n",
    "\n",
    "        features: list = (16, 16, 1)\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            for feat in self.features[:-1]:\n",
    "                x = nn.Dense(feat)(x)\n",
    "                x = nn.relu(x)\n",
    "            x = nn.Dense(self.features[-1])(x)\n",
    "            return x.squeeze()\n",
    "\n",
    "    # Initialize network\n",
    "    model = CorrectionMLP()\n",
    "    params = model.init(key, jnp.ones((1, 1)))\n",
    "\n",
    "    print(\"‚úÖ Correction MLP initialized\")\n",
    "    print(f\"   Parameter shapes: {jax.tree_util.tree_map(lambda x: x.shape, params)}\")\n",
    "\n",
    "    # Generate synthetic data: exponential decay + systematic deviation\n",
    "    x_data = np.linspace(0, 5, 200)\n",
    "    true_a, true_b = 5.0, 1.2\n",
    "\n",
    "    # Physics component\n",
    "    y_physics = true_a * np.exp(-true_b * x_data)\n",
    "\n",
    "    # Systematic deviation (sinusoidal correction)\n",
    "    y_correction = 0.5 * np.sin(3 * x_data) * np.exp(-0.3 * x_data)\n",
    "\n",
    "    # Observed data = physics + correction + noise\n",
    "    y_data = y_physics + y_correction + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "    # Visualize the data\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x_data, y_physics, \"g-\", linewidth=2, label=\"Physics (exponential)\")\n",
    "    plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Observed data\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Data vs Physics Model\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x_data, y_correction, \"r-\", linewidth=2, label=\"True correction\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Correction\")\n",
    "    plt.title(\"Systematic Deviation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = y_data - y_physics\n",
    "    plt.plot(x_data, residuals, \"r.\", alpha=0.5, markersize=3, label=\"Residuals\")\n",
    "    plt.plot(x_data, y_correction, \"k--\", linewidth=2, label=\"True correction\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(\"Physics Model Residuals\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Save figure to file\n",
    "    fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Data points: {len(x_data)}\")\n",
    "print(f\"   Physics RMSE: {np.sqrt(np.mean((y_data - y_physics) ** 2)):.3f}\")\n",
    "print(f\"   Correction amplitude: {np.max(np.abs(y_correction)):.3f}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Strategy 1: Two-Stage Fitting\n",
    "#\n",
    "# **Step 1**: Fit physics parameters with NLSQ\n",
    "# **Step 2**: Train neural network on residuals\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "if FLAX_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TWO-STAGE HYBRID FITTING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Stage 1: Fit physics model with NLSQ\n",
    "    print(\"\\nüîß Stage 1: Fitting physics parameters with NLSQ...\")\n",
    "\n",
    "    def exponential_decay(x, a, b):\n",
    "        return a * jnp.exp(-b * x)\n",
    "\n",
    "    cf = CurveFit()\n",
    "    start_time = time.time()\n",
    "    popt_physics, pcov_physics = cf.curve_fit(\n",
    "        exponential_decay, x_data, y_data, p0=[4.0, 1.0]\n",
    "    )\n",
    "    physics_time = time.time() - start_time\n",
    "\n",
    "    a_fit, b_fit = popt_physics\n",
    "    print(f\"   Fitted parameters: a={a_fit:.3f}, b={b_fit:.3f}\")\n",
    "    print(f\"   True parameters:   a={true_a:.3f}, b={true_b:.3f}\")\n",
    "    print(f\"   Fit time: {physics_time:.3f}s\")\n",
    "\n",
    "    # Compute residuals\n",
    "    y_physics_fit = np.array(exponential_decay(x_data, *popt_physics))\n",
    "    residuals = y_data - y_physics_fit\n",
    "    physics_rmse = np.sqrt(np.mean(residuals**2))\n",
    "    print(f\"   Physics RMSE: {physics_rmse:.4f}\")\n",
    "\n",
    "    # Stage 2: Train neural network on residuals\n",
    "    print(\"\\nüß† Stage 2: Training neural network on residuals...\")\n",
    "\n",
    "    # Prepare training data\n",
    "    x_train = x_data.reshape(-1, 1).astype(np.float32)\n",
    "    y_train = residuals.astype(np.float32)\n",
    "\n",
    "    # Create train state with Optax\n",
    "    def create_train_state(rng, learning_rate=1e-3):\n",
    "        model_nn = CorrectionMLP()\n",
    "        params_nn = model_nn.init(rng, jnp.ones((1, 1)))\n",
    "        tx = optax.adam(learning_rate)\n",
    "        return train_state.TrainState.create(\n",
    "            apply_fn=model_nn.apply, params=params_nn, tx=tx\n",
    "        )\n",
    "\n",
    "    state = create_train_state(key, learning_rate=5e-3)\n",
    "\n",
    "    # Training step\n",
    "    @jit\n",
    "    def train_step(state, x_batch, y_batch):\n",
    "        def loss_fn(params):\n",
    "            pred = state.apply_fn(params, x_batch)\n",
    "            return jnp.mean((pred - y_batch) ** 2)\n",
    "\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "\n",
    "    # Train for a few epochs\n",
    "    n_epochs = 500\n",
    "    losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        state, loss = train_step(state, x_train, y_train)\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"   Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.6f}\")\n",
    "\n",
    "    nn_time = time.time() - start_time\n",
    "    print(f\"   NN training time: {nn_time:.3f}s\")\n",
    "\n",
    "    # Evaluate hybrid model\n",
    "    correction_pred = model.apply(state.params, x_train).squeeze()\n",
    "    y_hybrid = y_physics_fit + np.array(correction_pred)\n",
    "    hybrid_rmse = np.sqrt(np.mean((y_data - y_hybrid) ** 2))\n",
    "\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(f\"   Physics-only RMSE: {physics_rmse:.4f}\")\n",
    "    print(f\"   Hybrid model RMSE: {hybrid_rmse:.4f}\")\n",
    "    print(f\"   Improvement: {(1 - hybrid_rmse / physics_rmse) * 100:.1f}%\")\n",
    "\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Neural Network Training\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n",
    "    plt.plot(x_data, y_physics_fit, \"g--\", linewidth=2, label=\"Physics only\")\n",
    "    plt.plot(x_data, y_hybrid, \"r-\", linewidth=2, label=\"Hybrid (Physics + NN)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Model Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(\n",
    "        x_data,\n",
    "        y_correction,\n",
    "        \"k--\",\n",
    "        linewidth=2,\n",
    "        label=\"True correction\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.plot(x_data, correction_pred, \"r-\", linewidth=2, label=\"NN learned correction\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Correction\")\n",
    "    plt.title(\"Learned vs True Correction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Key Insights\n",
    "#\n",
    "# 1. **Physics provides structure**: The exponential decay captures the dominant behavior\n",
    "# 2. **NN learns deviations**: Small neural network captures systematic errors\n",
    "# 3. **Data efficiency**: Physics model requires fewer parameters than pure ML\n",
    "# 4. **Interpretability**: Physical parameters (a, b) have clear meaning\n",
    "# 5. **Better extrapolation**: Physics guides behavior outside training range\n",
    "#\n",
    "# **When to use this approach**:\n",
    "# - ‚úÖ Known physics with systematic deviations\n",
    "# - ‚úÖ Limited data (physics provides inductive bias)\n",
    "# - ‚úÖ Need interpretable parameters\n",
    "# - ‚úÖ Extrapolation is important\n",
    "#\n",
    "# ---\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 2: Neural ODEs with NLSQ\n",
    "#\n",
    "# ### Concept: Learning Dynamics\n",
    "#\n",
    "# **Neural ODEs** parameterize the derivative of a system with a neural network:\n",
    "#\n",
    "# $$\n",
    "# \\frac{dy}{dt} = f_{\\theta}(y, t)\n",
    "# $$\n",
    "#\n",
    "# Where $f_{\\theta}$ is a neural network. We can then integrate this ODE to get predictions.\n",
    "#\n",
    "# **NLSQ Integration**: Use NLSQ to fit:\n",
    "# 1. Initial conditions\n",
    "# 2. ODE parameters (if partially mechanistic)\n",
    "# 3. Neural network parameters (jointly or in stages)\n",
    "#\n",
    "# ### Example 2.1: Damped Oscillator with Learned Damping\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Simple ODE solver (for demonstration; use diffrax in production)\n",
    "def euler_integrate(f, y0, t, *args):\n",
    "    \"\"\"Simple Euler integration for demonstration.\"\"\"\n",
    "    dt = t[1] - t[0]\n",
    "    y = jnp.zeros((len(t), len(y0)))\n",
    "    y = y.at[0].set(y0)\n",
    "\n",
    "    for i in range(1, len(t)):\n",
    "        dydt = f(y[i - 1], t[i - 1], *args)\n",
    "        y = y.at[i].set(y[i - 1] + dt * dydt)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Damped harmonic oscillator ODE\n",
    "def damped_oscillator_ode(state, t, omega, gamma):\n",
    "    \"\"\"dy/dt for damped harmonic oscillator.\n",
    "\n",
    "    state = [position, velocity]\n",
    "    omega = natural frequency\n",
    "    gamma = damping coefficient\n",
    "    \"\"\"\n",
    "    x, v = state\n",
    "    dxdt = v\n",
    "    dvdt = -(omega**2) * x - 2 * gamma * v\n",
    "    return jnp.array([dxdt, dvdt])\n",
    "\n",
    "\n",
    "# Generate synthetic oscillator data\n",
    "t_ode = np.linspace(0, 10, 200)\n",
    "omega_true = 2.0  # Natural frequency\n",
    "gamma_true = 0.3  # Damping\n",
    "y0_true = jnp.array([1.0, 0.0])  # Initial [position, velocity]\n",
    "\n",
    "# Integrate true system\n",
    "y_true = euler_integrate(damped_oscillator_ode, y0_true, t_ode, omega_true, gamma_true)\n",
    "x_true = y_true[:, 0]  # Extract position\n",
    "\n",
    "# Add noise to observations\n",
    "x_obs = x_true + np.random.normal(0, 0.05, len(t_ode))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_ode, x_true, \"g-\", linewidth=2, label=\"True dynamics\")\n",
    "plt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Noisy observations\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Position (x)\")\n",
    "plt.title(\"Damped Harmonic Oscillator\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_true[:, 0], y_true[:, 1], \"g-\", linewidth=2, label=\"Phase space\")\n",
    "plt.xlabel(\"Position (x)\")\n",
    "plt.ylabel(\"Velocity (v)\")\n",
    "plt.title(\"Phase Space Trajectory\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_03.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"True parameters: œâ={omega_true}, Œ≥={gamma_true}\")\n",
    "print(f\"Initial state: x‚ÇÄ={y0_true[0]}, v‚ÇÄ={y0_true[1]}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Fitting ODE Parameters with NLSQ\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FITTING ODE PARAMETERS WITH NLSQ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Define model: integrate ODE and extract position\n",
    "def oscillator_model(t, omega, gamma, x0, v0):\n",
    "    \"\"\"Model function that integrates ODE for given parameters.\"\"\"\n",
    "    y0 = jnp.array([x0, v0])\n",
    "    y = euler_integrate(damped_oscillator_ode, y0, t, omega, gamma)\n",
    "    return y[:, 0]  # Return position only\n",
    "\n",
    "\n",
    "# Fit with NLSQ\n",
    "print(\"\\nüîß Fitting ODE parameters...\")\n",
    "cf_ode = CurveFit()\n",
    "\n",
    "# Initial guess (intentionally off)\n",
    "p0 = [1.5, 0.2, 0.8, 0.1]  # [omega, gamma, x0, v0]\n",
    "\n",
    "start_time = time.time()\n",
    "popt_ode, pcov_ode = cf_ode.curve_fit(\n",
    "    oscillator_model,\n",
    "    t_ode,\n",
    "    x_obs,\n",
    "    p0=p0,\n",
    "    bounds=([0, 0, -2, -2], [5, 2, 2, 2]),  # Reasonable physical bounds\n",
    ")\n",
    "ode_time = time.time() - start_time\n",
    "\n",
    "omega_fit, gamma_fit, x0_fit, v0_fit = popt_ode\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(\n",
    "    f\"   Fitted: œâ={omega_fit:.4f}, Œ≥={gamma_fit:.4f}, x‚ÇÄ={x0_fit:.4f}, v‚ÇÄ={v0_fit:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"   True:   œâ={omega_true:.4f}, Œ≥={gamma_true:.4f}, x‚ÇÄ={y0_true[0]:.4f}, v‚ÇÄ={y0_true[1]:.4f}\"\n",
    ")\n",
    "print(f\"   Fit time: {ode_time:.3f}s\")\n",
    "\n",
    "# Compute fitted trajectory\n",
    "x_fit = oscillator_model(t_ode, *popt_ode)\n",
    "ode_rmse = np.sqrt(np.mean((x_obs - np.array(x_fit)) ** 2))\n",
    "print(f\"   RMSE: {ode_rmse:.5f}\")\n",
    "\n",
    "# Visualize fit\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\n",
    "plt.plot(t_ode, x_true, \"g--\", linewidth=2, alpha=0.7, label=\"True dynamics\")\n",
    "plt.plot(t_ode, x_fit, \"r-\", linewidth=2, label=\"Fitted ODE\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Position (x)\")\n",
    "plt.title(\"ODE Parameter Fitting\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals_ode = x_obs - np.array(x_fit)\n",
    "plt.plot(t_ode, residuals_ode, \"r.\", alpha=0.5, markersize=3)\n",
    "plt.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(f\"Residuals (RMSE={ode_rmse:.5f})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_04.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Key Takeaways\n",
    "#\n",
    "# 1. **NLSQ handles ODEs naturally**: Just wrap ODE integration in model function\n",
    "# 2. **Automatic differentiation**: JAX computes gradients through ODE solver\n",
    "# 3. **Joint parameter estimation**: Fit dynamics parameters + initial conditions\n",
    "# 4. **Physical constraints**: Use bounds to enforce physically reasonable values\n",
    "#\n",
    "# **Production tip**: Use `diffrax` for more robust ODE integration:\n",
    "# ```python\n",
    "# import diffrax\n",
    "# solver = diffrax.Tsit5()  # Adaptive Runge-Kutta\n",
    "# solution = diffrax.diffeqsolve(...)\n",
    "# ```\n",
    "#\n",
    "# ---\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ## Part 3: Physics-Informed Loss Functions\n",
    "#\n",
    "# ### Concept: Incorporating Physical Constraints\n",
    "#\n",
    "# **Physics-informed fitting** adds physical constraints to the loss:\n",
    "#\n",
    "# $$\n",
    "# \\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}\n",
    "# $$\n",
    "#\n",
    "# Examples:\n",
    "# - **Conservation laws**: Energy, mass, momentum conservation\n",
    "# - **PDE residuals**: Equations of motion, Maxwell's equations\n",
    "# - **Boundary conditions**: Initial/final state constraints\n",
    "# - **Symmetries**: Rotational, translational invariance\n",
    "#\n",
    "# ### Example 3.1: Energy-Conserving Pendulum\n",
    "#\n",
    "# For a frictionless pendulum, total energy should be conserved:\n",
    "#\n",
    "# $$\n",
    "# E = \\frac{1}{2}mv^2 + mgh = \\text{constant}\n",
    "# $$\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# Simple pendulum dynamics\n",
    "def pendulum_ode(state, t, omega):\n",
    "    \"\"\"Pendulum ODE: d¬≤Œ∏/dt¬≤ = -œâ¬≤ sin(Œ∏)\"\"\"\n",
    "    theta, theta_dot = state\n",
    "    return jnp.array([theta_dot, -(omega**2) * jnp.sin(theta)])\n",
    "\n",
    "\n",
    "# Generate pendulum data\n",
    "t_pend = np.linspace(0, 10, 150)\n",
    "omega_pend = 2.0\n",
    "y0_pend = jnp.array([0.5, 0.0])  # [angle, angular velocity]\n",
    "\n",
    "y_pend = euler_integrate(pendulum_ode, y0_pend, t_pend, omega_pend)\n",
    "theta_obs = y_pend[:, 0] + np.random.normal(0, 0.02, len(t_pend))\n",
    "\n",
    "# Compute energy (for verification)\n",
    "kinetic = 0.5 * y_pend[:, 1] ** 2\n",
    "potential = (omega_pend**2) * (1 - jnp.cos(y_pend[:, 0]))\n",
    "total_energy = kinetic + potential\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(t_pend, y_pend[:, 0], \"g-\", linewidth=2, label=\"True angle\")\n",
    "plt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Angle Œ∏ (rad)\")\n",
    "plt.title(\"Pendulum Motion\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(y_pend[:, 0], y_pend[:, 1], \"g-\", linewidth=2)\n",
    "plt.xlabel(\"Angle Œ∏ (rad)\")\n",
    "plt.ylabel(\"Angular velocity dŒ∏/dt (rad/s)\")\n",
    "plt.title(\"Phase Space\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(t_pend, total_energy, \"r-\", linewidth=2, label=\"Total energy\")\n",
    "plt.axhline(y=jnp.mean(total_energy), color=\"k\", linestyle=\"--\", label=\"Mean energy\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.title(\"Energy Conservation\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_05.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "energy_std = float(jnp.std(total_energy))\n",
    "print(f\"Energy conservation (std dev): {energy_std:.6f}\")\n",
    "print(f\"Energy variation: {energy_std / jnp.mean(total_energy) * 100:.3f}%\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ### Custom Physics-Informed Fitting\n",
    "#\n",
    "# While NLSQ doesn't directly support custom loss functions (it uses least squares), we can:\n",
    "# 1. Use NLSQ for standard parameter estimation\n",
    "# 2. Add physics penalty in post-processing\n",
    "# 3. Or use Optax for full physics-informed optimization\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "if FLAX_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PHYSICS-INFORMED OPTIMIZATION WITH OPTAX\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Define physics-informed loss\n",
    "    def physics_informed_loss(params, t, theta_obs, lambda_physics=0.1):\n",
    "        \"\"\"Loss = data fit + energy conservation penalty.\"\"\"\n",
    "        omega, theta0, thetadot0 = params\n",
    "\n",
    "        # Integrate ODE\n",
    "        y0 = jnp.array([theta0, thetadot0])\n",
    "        y = euler_integrate(pendulum_ode, y0, t, omega)\n",
    "\n",
    "        # Data fitting loss\n",
    "        theta_pred = y[:, 0]\n",
    "        loss_data = jnp.mean((theta_pred - theta_obs) ** 2)\n",
    "\n",
    "        # Energy conservation penalty\n",
    "        kinetic = 0.5 * y[:, 1] ** 2\n",
    "        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n",
    "        total_energy = kinetic + potential\n",
    "        energy_var = jnp.var(total_energy)\n",
    "        loss_physics = lambda_physics * energy_var\n",
    "\n",
    "        return loss_data + loss_physics, {\n",
    "            \"loss_data\": loss_data,\n",
    "            \"loss_physics\": loss_physics,\n",
    "        }\n",
    "\n",
    "    # Optimize with Optax\n",
    "    params_init = jnp.array([1.5, 0.4, 0.1])  # [omega, theta0, thetadot0]\n",
    "    optimizer = optax.adam(learning_rate=0.01)\n",
    "    opt_state = optimizer.init(params_init)\n",
    "\n",
    "    @jit\n",
    "    def update_step(params, opt_state, t, theta_obs):\n",
    "        (loss_val, metrics), grads = jax.value_and_grad(\n",
    "            physics_informed_loss, has_aux=True\n",
    "        )(params, t, theta_obs)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_val, metrics\n",
    "\n",
    "    # Optimization loop\n",
    "    params = params_init\n",
    "    n_steps = 1000\n",
    "    losses = []\n",
    "\n",
    "    print(\"\\nüéØ Training with physics-informed loss...\")\n",
    "    for step in range(n_steps):\n",
    "        params, opt_state, loss_val, metrics = update_step(\n",
    "            params, opt_state, t_pend, theta_obs\n",
    "        )\n",
    "        losses.append(float(loss_val))\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            print(\n",
    "                f\"   Step {step + 1}: Total={loss_val:.6f}, \"\n",
    "                f\"Data={metrics['loss_data']:.6f}, Physics={metrics['loss_physics']:.6f}\"\n",
    "            )\n",
    "\n",
    "    omega_pi, theta0_pi, thetadot0_pi = params\n",
    "    print(\"\\nüìä Fitted parameters:\")\n",
    "    print(f\"   œâ={omega_pi:.4f} (true: {omega_pend:.4f})\")\n",
    "    print(f\"   Œ∏‚ÇÄ={theta0_pi:.4f} (true: {y0_pend[0]:.4f})\")\n",
    "    print(f\"   dŒ∏‚ÇÄ/dt={thetadot0_pi:.4f} (true: {y0_pend[1]:.4f})\")\n",
    "\n",
    "    # Compare with standard NLSQ fit\n",
    "    def pendulum_model(t, omega, theta0, thetadot0):\n",
    "        y0 = jnp.array([theta0, thetadot0])\n",
    "        y = euler_integrate(pendulum_ode, y0, t, omega)\n",
    "        return y[:, 0]\n",
    "\n",
    "    popt_std, _ = cf_ode.curve_fit(\n",
    "        pendulum_model, t_pend, theta_obs, p0=[1.5, 0.4, 0.1]\n",
    "    )\n",
    "\n",
    "    # Evaluate energy conservation\n",
    "    y_pi = euler_integrate(\n",
    "        pendulum_ode, jnp.array([theta0_pi, thetadot0_pi]), t_pend, omega_pi\n",
    "    )\n",
    "    y_std = euler_integrate(\n",
    "        pendulum_ode, jnp.array([popt_std[1], popt_std[2]]), t_pend, popt_std[0]\n",
    "    )\n",
    "\n",
    "    def compute_energy_std(y, omega):\n",
    "        kinetic = 0.5 * y[:, 1] ** 2\n",
    "        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n",
    "        return float(jnp.std(kinetic + potential))\n",
    "\n",
    "    energy_std_pi = compute_energy_std(y_pi, omega_pi)\n",
    "    energy_std_std = compute_energy_std(y_std, popt_std[0])\n",
    "\n",
    "    print(\"\\n‚ö° Energy Conservation:\")\n",
    "    print(f\"   Physics-informed: œÉ_E = {energy_std_pi:.6f}\")\n",
    "    print(f\"   Standard NLSQ:    œÉ_E = {energy_std_std:.6f}\")\n",
    "    print(f\"   Improvement: {(1 - energy_std_pi / energy_std_std) * 100:.1f}%\")\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Optimization Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Physics-Informed Training\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n",
    "    plt.plot(t_pend, y_std[:, 0], \"g--\", linewidth=2, label=\"Standard NLSQ\")\n",
    "    plt.plot(t_pend, y_pi[:, 0], \"r-\", linewidth=2, label=\"Physics-informed\")\n",
    "    plt.xlabel(\"Time (t)\")\n",
    "    plt.ylabel(\"Angle Œ∏ (rad)\")\n",
    "    plt.title(\"Fit Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    energy_std_series = 0.5 * y_std[:, 1] ** 2 + (popt_std[0] ** 2) * (\n",
    "        1 - jnp.cos(y_std[:, 0])\n",
    "    )\n",
    "    energy_pi_series = 0.5 * y_pi[:, 1] ** 2 + (omega_pi**2) * (1 - jnp.cos(y_pi[:, 0]))\n",
    "    plt.plot(t_pend, energy_std_series, \"g--\", linewidth=2, label=\"Standard NLSQ\")\n",
    "    plt.plot(t_pend, energy_pi_series, \"r-\", linewidth=2, label=\"Physics-informed\")\n",
    "    plt.xlabel(\"Time (t)\")\n",
    "    plt.ylabel(\"Energy (J)\")\n",
    "    plt.title(\"Energy Conservation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "# Save figure to file\n",
    "fig_dir = Path.cwd() / \"figures\" / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_06.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# ---\n",
    "#\n",
    "# ## Summary and Best Practices\n",
    "#\n",
    "# ### Integration Strategies\n",
    "#\n",
    "# | Approach | NLSQ Role | ML Role | Best For |\n",
    "# |----------|-----------|---------|----------|\n",
    "# | **Two-Stage Hybrid** | Fit physics parameters | Learn residuals | Known physics + systematic deviations |\n",
    "# | **Neural ODE** | Fit ODE parameters | (Optional) Learn dynamics | Parameter estimation in dynamical systems |\n",
    "# | **Physics-Informed** | Pre-fit, then refine | Enforce constraints | Energy/mass conservation, PDEs |\n",
    "# | **Joint Optimization** | Parameter estimation | Model flexibility | Complex coupled systems |\n",
    "#\n",
    "# ### Key Takeaways\n",
    "#\n",
    "# 1. **NLSQ + JAX = Powerful Combo**:\n",
    "#    - Automatic differentiation through complex models\n",
    "#    - GPU acceleration for both fitting and ML\n",
    "#    - Seamless integration with JAX ecosystem\n",
    "#\n",
    "# 2. **Hybrid Models Win**:\n",
    "#    - Better than pure physics (captures deviations)\n",
    "#    - Better than pure ML (data efficient, interpretable)\n",
    "#    - Best of both worlds\n",
    "#\n",
    "# 3. **Physics Constraints Help**:\n",
    "#    - Regularize ML models\n",
    "#    - Improve extrapolation\n",
    "#    - Ensure physical plausibility\n",
    "#\n",
    "# 4. **Choose the Right Tool**:\n",
    "#    - **NLSQ**: Parameter estimation, well-conditioned problems\n",
    "#    - **Optax**: Custom losses, physics-informed training\n",
    "#    - **Combined**: Two-stage fitting strategies\n",
    "#\n",
    "# ### Production Recommendations\n",
    "#\n",
    "# ```python\n",
    "# # 1. Use diffrax for robust ODE integration\n",
    "# import diffrax\n",
    "# solver = diffrax.Tsit5()\n",
    "#\n",
    "# # 2. Separate training and inference\n",
    "# @jit\n",
    "# def inference_model(params, x):\n",
    "#     # Compiled inference only\n",
    "#     return model.apply(params, x)\n",
    "#\n",
    "# # 3. Use appropriate precision\n",
    "# # NLSQ uses float64 by default (good for physics)\n",
    "# # ML often uses float32 (faster, sufficient for NNs)\n",
    "#\n",
    "# # 4. Validate physics constraints\n",
    "# def check_energy_conservation(y, params):\n",
    "#     energy = compute_energy(y, params)\n",
    "#     return jnp.std(energy) < threshold\n",
    "#\n",
    "# # 5. Profile and optimize\n",
    "# # Use MemoryPool for repeated fitting\n",
    "# from nlsq import MemoryPool\n",
    "# with MemoryPool() as pool:\n",
    "#     for data in datasets:\n",
    "#         popt, _ = cf.curve_fit(model, *data)\n",
    "# ```\n",
    "#\n",
    "# ### Next Steps\n",
    "#\n",
    "# - Explore `equinox` for more Pythonic neural network design\n",
    "# - Try `diffrax` for production-grade ODE solving\n",
    "# - Investigate `jaxopt` for more optimization algorithms\n",
    "# - Read about **Universal Differential Equations** (UDEs)\n",
    "# - Study **SciML (Scientific Machine Learning)** ecosystem\n",
    "#\n",
    "# ### References\n",
    "#\n",
    "# 1. **Neural ODEs**: Chen et al., \"Neural Ordinary Differential Equations\", NeurIPS 2018\n",
    "# 2. **PINNs**: Raissi et al., \"Physics-informed neural networks\", JCP 2019\n",
    "# 3. **UDEs**: Rackauckas et al., \"Universal Differential Equations\", arXiv 2020\n",
    "# 4. **JAX Ecosystem**: https://github.com/n2cholas/awesome-jax\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# **Congratulations!** You've learned how to integrate NLSQ with the JAX ML ecosystem for hybrid scientific computing.\n",
    "#\n",
    "# **Continue Learning**:\n",
    "# - [Research Workflow Case Study](research_workflow_case_study.ipynb) - Real experimental data\n",
    "# - [Advanced Features Demo](advanced_features_demo.ipynb) - Diagnostics and optimization\n",
    "# - [Performance Optimization Demo](performance_optimization_demo.ipynb) - Production-ready optimization\n",
    "#\n",
    "# ---\n",
    "# ======================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
