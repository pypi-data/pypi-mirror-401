redis = "12800"
mode = "colocated"

[train]
resume = false
epoch = 400  # max_epochs from RLinf
output_dir = "./outputs/pi05-libero10-grpo"
epsilon = 1e-5
optm_name = "AdamW"
optm_lr = 5.0e-6  # actor.optim.lr
optm_impl = "fused"
optm_weight_decay = 0.01
optm_betas = [0.9, 0.95]  # actor.optim.adam_beta1, adam_beta2
optm_warmup_steps = 0
optm_grad_norm_clip = 1.0  # actor.optim.clip_grad
async_tp_enabled = false
compile = false
param_dtype = "bfloat16"  # actor.model.precision
fsdp_reduce_dtype = "float32"
master_dtype = "float32"
transfer_dtype = "float32"
fsdp_offload = false
fsdp_reshard_after_forward = "default"
train_batch_per_replica = 256
sync_weight_interval = 1
non_text = true  # Required for VLA models

[policy]
model_name_or_path = "sunshk/pi05_libero_pytorch"
model_max_length = 4096
model_gradient_checkpointing = false

[logging]
logger = ['console', 'wandb']
project_name = "cosmos-rl-pi05-libero10"
experiment_name = "pi05-grpo-colocated"

[train.train_policy]
type = "grpo"
trainer_type = "grpo_pi05"
variant = "dapo"
dataset.name = "libero"
dataset.subset = "libero_10"
dataset.split = "train"
dataset.test_size = 496
enable_dataset_cache = false
dataloader_num_workers = 4
dataloader_prefetch_factor = 4
conversation_column_name = "messages"
max_retry_for_on_policy = -1  # try infinitely
temperature = 1.0
epsilon_low = 0.2  # algorithm.clip_ratio_low
epsilon_high = 0.28  # algorithm.clip_ratio_high
clip_ratio_c = 3.0  # algorithm.clip_ratio_c (dual-clip)
lower_bound_ratio = 10.0
kl_beta = 0.0
mu_iterations = 1
mini_batch = 256
min_filter_prefix_tokens = 1
allowed_outdated_steps = 4

[train.ckpt]
enable_checkpoint = true
save_freq = 80
save_mode = "async"
max_keep = 5
export_safetensors = true

[rollout]
backend = "vla"
gpu_memory_utilization = 0.7
enable_chunked_prefill = true
n_generation = 8
batch_size = 4
quantization = "none"
max_response_length = 1024
enforce_eager = false

[policy.parallelism]
n_init_replicas = 8
tp_size = 1  # PI05 does not support tensor parallelism
cp_size = 1  # PI05 does not support context parallelism
dp_shard_size = 1  # FSDP sharding (auto-detect based on GPUs)
pp_size = 1  # PI05 does not support pipeline parallelism
dp_replicate_size = 1
cp_rotate_method = "allgather"

[rollout.parallelism]
n_init_replicas = 8
tp_size = 1
cp_size = 1
dp_shard_size = 1
pp_size = 1
dp_replicate_size = 1

[rollout.sampling_config]
temperature = 0.6  # algorithm.sampling_params.temperature_train
top_p = 1.0  # algorithm.sampling_params.top_p
top_k = 50  # algorithm.sampling_params.top_k

[validation]
enable = true
freq = 20
val_before_train = true
batch_size = 64
dataset.name = "libero"
dataset.subset = "libero_10"
dataset.split = "val"

[vla]
vla_type = "pi05"  # Custom VLA type for PI05
use_proprio = false
num_envs = 8
proprio_dim = 7
num_images_in_input = 3  # base_0_rgb, left_wrist_0_rgb, right_wrist_0_rgb
training_chunk_size = 16  # Number of action chunks to train in one iteration

# Reward filtering (aligned with RLinf algorithm.filter_rewards)
filter_lower_bound = 0.1  # algorithm.rewards_lower_bound
filter_upper_bound = 0.9  # algorithm.rewards_upper_bound

# Custom PI05 configurations (injected into model via custom field)
[custom.pi05]
num_steps = 5  # actor.model.num_steps (denoise steps)
action_chunk = 10  # Action chunking size
action_env_dim = 7  # Environment action dimensions
replan_steps = 5
noise_method = "flow_sde"  # Noise method: "flow_sde", "flow_cps", "flow_noise"
noise_level = 0.5
noise_anneal = false
noise_params = [0.7, 0.3, 400.0]  # [noise_start, noise_end, anneal_steps]
noise_logvar_range = [0.04, 0.10]
joint_logprob = false  # Whether to use joint log probability
safe_get_logprob = false
ignore_last = false  # Whether to ignore last denoise step in training
train_expert_only = true  # Freeze VLM, only train action expert
discrete_state_input = false  # IMPORTANT: Pi05 LIBERO uses continuous state input, not discrete!
max_token_len = 200

