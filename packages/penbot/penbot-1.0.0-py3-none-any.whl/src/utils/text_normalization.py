"""Text normalization utilities to prevent detector evasion.

This module provides text normalization functions to handle various
obfuscation techniques like unicode tricks, homoglyphs, zero-width
characters, and bidirectional text manipulation.
"""

import re
import unicodedata
from typing import Dict


# Homoglyph mapping - common lookalikes to ASCII
HOMOGLYPH_MAP = {
    # Cyrillic to Latin
    "Ð°": "a",
    "Ðµ": "e",
    "Ð¾": "o",
    "Ñ€": "p",
    "Ñ": "c",
    "Ñƒ": "y",
    "Ñ…": "x",
    "Ð": "A",
    "Ð’": "B",
    "Ð•": "E",
    "Ðš": "K",
    "Ðœ": "M",
    "Ð": "H",
    "Ðž": "O",
    "Ð ": "P",
    "Ð¡": "C",
    "Ð¢": "T",
    "Ð¥": "X",
    # Greek to Latin
    "Î±": "a",
    "Î²": "b",
    "Î³": "g",
    "Î´": "d",
    "Îµ": "e",
    "Î¶": "z",
    "Î·": "h",
    "Î¸": "th",
    "Î¹": "i",
    "Îº": "k",
    "Î»": "l",
    "Î¼": "m",
    "Î½": "n",
    "Î¾": "x",
    "Î¿": "o",
    "Ï€": "p",
    "Ï": "r",
    "Ïƒ": "s",
    "Ï„": "t",
    "Ï…": "u",
    "Ï†": "ph",
    "Ï‡": "ch",
    "Ïˆ": "ps",
    "Ï‰": "o",
    "Î‘": "A",
    "Î’": "B",
    "Î“": "G",
    "Î”": "D",
    "Î•": "E",
    "Î–": "Z",
    "Î—": "H",
    "Î˜": "TH",
    "Î™": "I",
    "Îš": "K",
    "Î›": "L",
    "Îœ": "M",
    "Î": "N",
    "Îž": "X",
    "ÎŸ": "O",
    "Î ": "P",
    "Î¡": "R",
    "Î£": "S",
    "Î¤": "T",
    "Î¥": "U",
    "Î¦": "PH",
    "Î§": "CH",
    "Î¨": "PS",
    "Î©": "O",
    # Common substitutions
    "ï¼": "0",
    "ï¼‘": "1",
    "ï¼’": "2",
    "ï¼“": "3",
    "ï¼”": "4",
    "ï¼•": "5",
    "ï¼–": "6",
    "ï¼—": "7",
    "ï¼˜": "8",
    "ï¼™": "9",
    "â…°": "i",
    "â…±": "ii",
    "â…²": "iii",
    "â…³": "iv",
    "â…´": "v",
    "â…µ": "vi",
    "â…¶": "vii",
    "â…·": "viii",
    "â…¸": "ix",
    "â…¹": "x",
    # Math and special alphanumerics
    "ðš": "a",
    "ð›": "b",
    "ðœ": "c",
    "ð": "d",
    "ðž": "e",
    "ð—®": "a",
    "ð—¯": "b",
    "ð—°": "c",
    "ð—±": "d",
    "ð—²": "e",
    "ð˜¢": "a",
    "ð˜£": "b",
    "ð˜¤": "c",
    "ð˜¥": "d",
    "ð˜¦": "e",
    "ð™–": "a",
    "ð™—": "b",
    "ð™˜": "c",
    "ð™™": "d",
    "ð™š": "e",
}


# Zero-width and invisible characters
ZERO_WIDTH_CHARS = [
    "\u200b",  # Zero-width space
    "\u200c",  # Zero-width non-joiner
    "\u200d",  # Zero-width joiner
    "\ufeff",  # Zero-width no-break space (BOM)
    "\u2060",  # Word joiner
    "\u2062",  # Invisible times
    "\u2063",  # Invisible separator
    "\u2064",  # Invisible plus
]


# Bidirectional text override characters
BIDI_CHARS = [
    "\u202a",  # Left-to-right embedding
    "\u202b",  # Right-to-left embedding
    "\u202c",  # Pop directional formatting
    "\u202d",  # Left-to-right override
    "\u202e",  # Right-to-left override
    "\u2066",  # Left-to-right isolate
    "\u2067",  # Right-to-left isolate
    "\u2068",  # First strong isolate
    "\u2069",  # Pop directional isolate
]


def remove_zero_width_chars(text: str) -> str:
    """Remove zero-width and invisible Unicode characters.

    Args:
        text: Input text

    Returns:
        Text with zero-width characters removed
    """
    for char in ZERO_WIDTH_CHARS:
        text = text.replace(char, "")
    return text


def remove_bidi_chars(text: str) -> str:
    """Remove bidirectional text control characters.

    Args:
        text: Input text

    Returns:
        Text with bidi characters removed
    """
    for char in BIDI_CHARS:
        text = text.replace(char, "")
    return text


def replace_homoglyphs(text: str) -> str:
    """Replace homoglyphs with ASCII equivalents.

    Args:
        text: Input text

    Returns:
        Text with homoglyphs replaced
    """
    result = []
    for char in text:
        result.append(HOMOGLYPH_MAP.get(char, char))
    return "".join(result)


def normalize_unicode(text: str, form: str = "NFKC") -> str:
    """Normalize Unicode using specified form.

    NFKC (Compatibility Decomposition, followed by Canonical Composition)
    is most aggressive and converts things like â‘  to 1, ï¬ to fi, etc.

    Args:
        text: Input text
        form: Unicode normalization form (NFC, NFD, NFKC, NFKD)

    Returns:
        Normalized text
    """
    return unicodedata.normalize(form, text)


def remove_soft_hyphens(text: str) -> str:
    """Remove soft hyphens (invisible break hints).

    Args:
        text: Input text

    Returns:
        Text with soft hyphens removed
    """
    return text.replace("\u00ad", "")


def collapse_whitespace(text: str) -> str:
    """Collapse multiple whitespace to single space.

    Args:
        text: Input text

    Returns:
        Text with normalized whitespace
    """
    return re.sub(r"\s+", " ", text).strip()


def normalize_text(text: str, aggressive: bool = True) -> str:
    """Apply full text normalization pipeline.

    This is the main function to use for detector input normalization.
    It applies all transformations to maximize detection and prevent evasion.

    Args:
        text: Input text to normalize
        aggressive: If True, applies all normalizations including homoglyphs

    Returns:
        Fully normalized text

    Example:
        >>> text = "HÐµllo\\u200bWorld"  # Cyrillic 'Ðµ' + zero-width space
        >>> normalize_text(text)
        'Hello World'
    """
    if not text:
        return text

    # Step 1: Remove invisible/zero-width characters
    text = remove_zero_width_chars(text)
    text = remove_soft_hyphens(text)

    # Step 2: Remove bidirectional text controls
    text = remove_bidi_chars(text)

    # Step 3: Unicode normalization (NFKC is most aggressive)
    text = normalize_unicode(text, form="NFKC")

    # Step 4: Replace homoglyphs (optional but recommended)
    if aggressive:
        text = replace_homoglyphs(text)

    # Step 5: Collapse whitespace
    text = collapse_whitespace(text)

    return text


def normalize_for_comparison(text: str) -> str:
    """Normalize text for case-insensitive comparison.

    Applies normalization and converts to lowercase.

    Args:
        text: Input text

    Returns:
        Normalized lowercase text
    """
    return normalize_text(text, aggressive=True).lower()


def get_normalization_stats(original: str, normalized: str) -> Dict[str, int]:
    """Get statistics about what was normalized.

    Useful for logging and debugging obfuscation attempts.

    Args:
        original: Original text
        normalized: Normalized text

    Returns:
        Dictionary with normalization statistics
    """
    stats = {
        "length_change": len(original) - len(normalized),
        "zero_width_removed": sum(1 for c in original if c in ZERO_WIDTH_CHARS),
        "bidi_removed": sum(1 for c in original if c in BIDI_CHARS),
        "homoglyphs_replaced": sum(1 for c in original if c in HOMOGLYPH_MAP),
        "had_soft_hyphens": "\u00ad" in original,
    }
    stats["total_changes"] = sum(
        v for k, v in stats.items() if k != "length_change" and isinstance(v, int)
    )
    return stats
