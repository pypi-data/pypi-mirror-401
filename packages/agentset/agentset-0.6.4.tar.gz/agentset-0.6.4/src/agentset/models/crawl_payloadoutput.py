"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from agentset.types import BaseModel, UNSET_SENTINEL
from agentset.utils import validate_const
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import AfterValidator
from typing import Dict, List, Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class CrawlPayloadOutputTypedDict(TypedDict):
    url: str
    r"""The starting URL to crawl."""
    type: Literal["CRAWL"]
    max_depth: NotRequired[int]
    r"""Maximum depth to follow links from the starting URL. Depth 1 means only the initial page. Defaults to `5`."""
    limit: NotRequired[int]
    r"""Maximum number of pages to crawl before stopping. Helps bound large sites. Defaults to `50`."""
    include_paths: NotRequired[List[str]]
    r"""Only crawl URLs whose path matches at least one of these prefixes."""
    exclude_paths: NotRequired[List[str]]
    r"""Never crawl URLs whose path matches these prefixes."""
    headers: NotRequired[Dict[str, str]]
    r"""Custom HTTP headers to send with crawl requests (for example, auth headers)."""


class CrawlPayloadOutput(BaseModel):
    url: str
    r"""The starting URL to crawl."""

    TYPE: Annotated[
        Annotated[Literal["CRAWL"], AfterValidator(validate_const("CRAWL"))],
        pydantic.Field(alias="type"),
    ] = "CRAWL"

    max_depth: Annotated[Optional[int], pydantic.Field(alias="maxDepth")] = None
    r"""Maximum depth to follow links from the starting URL. Depth 1 means only the initial page. Defaults to `5`."""

    limit: Optional[int] = None
    r"""Maximum number of pages to crawl before stopping. Helps bound large sites. Defaults to `50`."""

    include_paths: Annotated[
        Optional[List[str]], pydantic.Field(alias="includePaths")
    ] = None
    r"""Only crawl URLs whose path matches at least one of these prefixes."""

    exclude_paths: Annotated[
        Optional[List[str]], pydantic.Field(alias="excludePaths")
    ] = None
    r"""Never crawl URLs whose path matches these prefixes."""

    headers: Optional[Dict[str, str]] = None
    r"""Custom HTTP headers to send with crawl requests (for example, auth headers)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            ["maxDepth", "limit", "includePaths", "excludePaths", "headers"]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
