# councillm

**councillm** is a lightweight, transparent *LLM Council* framework built on **Ollama**. It orchestrates multiple local language models into a structured decision-making pipeline inspired by Andrej Karpathyâ€™s *LLM Council* concept â€” but designed for **local-first**, **observable**, and **practical CLI usage**.

This project focuses on **correctness, transparency, and control**, not theatrics.
---
<a href="https://github.com/Rktim/ezyml/blob/main/LICENSE">
  <img alt="License" src="https://img.shields.io/github/license/Rktim/ezyml?color=blue">
</a>

[![PyPI Downloads](https://static.pepy.tech/personalized-badge/councillm?period=total&units=INTERNATIONAL_SYSTEM&left_color=BLACK&right_color=BRIGHTGREEN&left_text=downloads)](https://pepy.tech/projects/councillm)
---
## âœ¨ Key Features

* ðŸ” **Multiâ€‘Model Reasoning** (Generator â†’ Critic â†’ Chairman)
* ðŸ§  **Fast / Lite / Full execution modes**
* ðŸ” Optional **webâ€‘search grounding**
* ðŸ‘ï¸ **Transparent execution logs** â€” see each model work
* ðŸ–¥ï¸ **Localâ€‘only** (no OpenAI / no cloud)
* âš¡ **Fast install with `uv`**

---

## ðŸ—ï¸ Council Architecture

The system follows a strict, inspectable pipeline:

```
User Question
   â†“
[Generators]  â†’ produce independent drafts
   â†“
[Critics]     â†’ review & rank drafts (optional)
   â†“
[Chairman]   â†’ synthesize final answer
```

Each stage is logged in real time so users can verify that the council is *actually running*.

---

## ðŸ“¦ Installation (Fast with uv)

### Prerequisites

* Python **3.10+**
* **Ollama** installed and running
* At least **2 local Ollama models** pulled

```bash
ollama pull mistral
ollama pull llama3
```

### Install with `uv`

```bash
uv pip install councillm
```

Or for development:

```bash
git clone https://github.com/yourname/councillm.git
cd councillm
uv pip install -e .
```

---

## ðŸš€ Quick Start

Run the CLI:

```bash
councillm
```

You will be prompted to configure the council **once per session**.

---

## âš™ï¸ Interactive Configuration

Instead of editing YAML files, **councillm asks you directly**:

```
Assign GENERATOR models (commaâ€‘separated):
> mistral:latest, llama3:8b

Assign CRITIC models (commaâ€‘separated):
> phi3:latest

Assign CHAIRMAN model:
> gemma3:1b
```

âœ” No files are written
âœ” No autoâ€‘detection
âœ” No hidden state

---

## ðŸ§© Execution Modes

After configuration, choose how the council runs:

### 1ï¸âƒ£ Fast Mode

```
Chairman answers directly
```

* Fastest
* Lowest cost
* Least robust

### 2ï¸âƒ£ Lite Mode (Default)

```
Generator â†’ Chairman
```

* Balanced
* Good for daily use

### 3ï¸âƒ£ Full Mode

```
Multiple Generators â†’ Critics â†’ Chairman
```

* Most reliable
* Slowest
* Maximum crossâ€‘checking

---

## ðŸ” Web Search Grounding

You can optionally enable web search:

```
Enable web search grounding? [y/N]: y
```

This uses DuckDuckGo search results to reduce hallucinations for factual queries.

---

## ðŸ–¥ï¸ Example Session

```text
$ councillm

======================================================================
        LLM COUNCIL â€” OLLAMA CONSOLE MODE
======================================================================
Type your question and press Enter.
Type 'exit' to quit.

Council ready (mode=full, search=True).

You: who won the 2020 F1 drivers championship?

[Stage 1] Generating responses
  â€¢ mistral:latest âœ“
  â€¢ llama3:8b âœ“

[Stage 2] Peer review
  â€¢ phi3:latest âœ“

[Stage 3] Chairman synthesis
  â€¢ gemma3:1b âœ“

Final Answer:
Lewis Hamilton won the 2020 Formula 1 Drivers' Championship.
```

---

## ðŸ§ª Hallucination Mitigation Strategy

councillm reduces hallucinations by:

* Multiple independent generations
* Crossâ€‘model critique
* Chairman synthesis
* Optional web grounding

âš ï¸ **Still not perfect** â€” this is *risk reduction*, not elimination.

---


## âŒ What councillm Is NOT

* âŒ Not a chatbot UI
* âŒ Not a prompt playground
* âŒ Not a guarantee of truth
* âŒ Not cloudâ€‘based

This is a **reasoning orchestrator**, not a demo app.

---

## ðŸ“œ License

MIT License

You are free to use, modify, and distribute this project.

---


## ðŸ¤ Contributing

Contributions are welcome if they:

* Improve correctness
* Reduce hallucinations
* Increase transparency
* Keep the system simple

---

## ðŸ§  Philosophy

> "If you canâ€™t observe it, you canâ€™t trust it."

councillm exists to make **local LLM reasoning inspectable, not magical**.

---

Happy reasoning ðŸš€
