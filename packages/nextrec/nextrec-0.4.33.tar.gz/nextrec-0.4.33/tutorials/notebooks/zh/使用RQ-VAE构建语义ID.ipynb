{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a958351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nextrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3c1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "logger = logging.getLogger() \n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n",
    "logger.handlers = [handler] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43500b8",
   "metadata": {},
   "source": [
    "在这个示例里，我们将指导您使用NextRec框架，利用RQ-VAE模型训练和生成语义ID。在`dataset/`路径下，我们为您提供了示例电商数据集，其中包含了用户id，物品id，商品文本描述，埋点时间和一些常见的特征。我将会用到其中的文本特征来进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52e0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_bucket</th>\n",
       "      <th>city</th>\n",
       "      <th>device</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>impression_position</th>\n",
       "      <th>user_active_days_7</th>\n",
       "      <th>user_ctr</th>\n",
       "      <th>text_desc</th>\n",
       "      <th>click</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-14 09:16:00</td>\n",
       "      <td>10001</td>\n",
       "      <td>item_BEAU_01399</td>\n",
       "      <td>F</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Web</td>\n",
       "      <td>search</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>PureSkin</td>\n",
       "      <td>277.62</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Sunscreen cruelty-free brightening</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-15 17:18:00</td>\n",
       "      <td>10001</td>\n",
       "      <td>item_BEAU_01530</td>\n",
       "      <td>F</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Web</td>\n",
       "      <td>push</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>BrandN</td>\n",
       "      <td>814.57</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Essence for sensitive skin with niacinamide hy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-18 09:51:00</td>\n",
       "      <td>10001</td>\n",
       "      <td>item_HOME_01211</td>\n",
       "      <td>F</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Web</td>\n",
       "      <td>organic</td>\n",
       "      <td>Home</td>\n",
       "      <td>BrandB</td>\n",
       "      <td>904.47</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Office chair with lumbar support handcrafted</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-23 09:59:00</td>\n",
       "      <td>10001</td>\n",
       "      <td>item_CLOT_00426</td>\n",
       "      <td>F</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Web</td>\n",
       "      <td>search</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>BrandD</td>\n",
       "      <td>358.71</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Cotton t-shirt short sleeve athletic fit with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-08 19:39:00</td>\n",
       "      <td>10001</td>\n",
       "      <td>item_HOME_01178</td>\n",
       "      <td>F</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Web</td>\n",
       "      <td>search</td>\n",
       "      <td>Home</td>\n",
       "      <td>ModernLiving</td>\n",
       "      <td>315.23</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Plant pot space-saving durable material easy a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              log_time  user_id          item_id gender age_bucket     city  \\\n",
       "0  2025-11-14 09:16:00    10001  item_BEAU_01399      F      35-44  Beijing   \n",
       "1  2025-11-15 17:18:00    10001  item_BEAU_01530      F      35-44  Beijing   \n",
       "2  2025-11-18 09:51:00    10001  item_HOME_01211      F      35-44  Beijing   \n",
       "3  2025-11-23 09:59:00    10001  item_CLOT_00426      F      35-44  Beijing   \n",
       "4  2025-12-08 19:39:00    10001  item_HOME_01178      F      35-44  Beijing   \n",
       "\n",
       "  device  channel  category         brand   price  impression_position  \\\n",
       "0    Web   search    Beauty      PureSkin  277.62                   10   \n",
       "1    Web     push    Beauty        BrandN  814.57                    9   \n",
       "2    Web  organic      Home        BrandB  904.47                    5   \n",
       "3    Web   search  Clothing        BrandD  358.71                   10   \n",
       "4    Web   search      Home  ModernLiving  315.23                   10   \n",
       "\n",
       "   user_active_days_7  user_ctr  \\\n",
       "0                   7      0.29   \n",
       "1                   7      0.29   \n",
       "2                   7      0.29   \n",
       "3                   7      0.29   \n",
       "4                   7      0.29   \n",
       "\n",
       "                                           text_desc  click  conversion  \n",
       "0                 Sunscreen cruelty-free brightening      0           0  \n",
       "1  Essence for sensitive skin with niacinamide hy...      0           0  \n",
       "2       Office chair with lumbar support handcrafted      0           0  \n",
       "3  Cotton t-shirt short sleeve athletic fit with ...      1           1  \n",
       "4  Plant pot space-saving durable material easy a...      0           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/NextRec/dataset/ecommerce_task.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6094f8",
   "metadata": {},
   "source": [
    "# RQ-VAE\n",
    "\n",
    "在开始之前，先简单介绍一下RQ-VAE。RQ-VAE是对VAE（变分自编码器）的改进，后者是一个生成模型，目标是通过学习输入数据的分布，输出类似的分布，使用KL散度作为评估指标。简单来说，VAE类似于一个embedding层，以文本数据为例，将其映射到低维空间。由于VAE需要输入一个向量，因此需要对文本进行分词和嵌入/one hot，这样每个token都会得到一个高维度向量，随后将这个高维序列传入编码器（RNN/GRN/CNN/Transformer），来将序列压缩为低维向量，这就是最终需要的低维向量。\n",
    "\n",
    "VAE已经将输入的高维embedding降低为了一定程度上的低维embedding，不过要对所有item都保存这个向量，对工业的数据存储压力依旧很大，因此希望对这个输出再进行一次压缩，将连续向量压缩为离散低维向量，这就是RQ-VAE试图解决的问题。这里的RQ指的是向量量化（Vector Quantization, VQ）和残差编码（Residual Encoding）。RQ-VAE试图对Item保存为索引ID，计算时通过ID → 查表 → 点积的方式进行召回。\n",
    "\n",
    "RQ-VAE通过离散码本（Codebook）索引的组合向量来表示原来的VAE embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8f4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrec.basic.features import DenseFeature\n",
    "from nextrec.data.dataloader import RecDataLoader\n",
    "from nextrec.models.representation import RQVAE\n",
    "\n",
    "from nextrec.data.data_processing import split_dict_random\n",
    "from nextrec.utils.embedding import encode_multimodel_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9b19",
   "metadata": {},
   "source": [
    "要通过RQ-VAE生成sid，首先就需要输入的多模态信息，在这个示例里，我们采用文本信息作为原始输出，使用bert模型将其嵌入为高维稠密向量。NextRec提供的工具函数`encode_multimodel_content`可以帮你实现这一步。它底层调用了transformers库来进行嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ec96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nextrec\n",
      "Version: 0.4.21\n",
      "Summary: A comprehensive recommendation library with match, ranking, and multi-task learning models\n",
      "Home-page: https://github.com/zerolovesea/NextRec\n",
      "Author: Yang Zhou\n",
      "Author-email: zyaztec@gmail.com\n",
      "License: \n",
      "Location: /opt/anaconda3/envs/nextrec/lib/python3.10/site-packages\n",
      "Editable project location: /Users/zyaztec/DailyWork/建模代码整理/NextRec\n",
      "Requires: numpy, pandas, pyarrow, pyyaml, rich, scikit-learn, scipy, swanlab, torch, torchvision, transformers, wandb\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show nextrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42137bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 10000 samples\n",
      "Encoded text_desc into embeddings with shape torch.Size([10000, 768])\n",
      "tensor([-6.9366e-02, -2.5368e-01, -4.2544e-01,  2.4386e-01, -2.3643e-01,\n",
      "        -2.3092e-02, -7.0698e-02,  2.4566e-01, -4.0289e-02, -1.3284e-01,\n",
      "         9.2764e-02,  4.7631e-01, -2.7675e-02,  1.6549e-01, -2.5294e-01,\n",
      "         1.0878e-01, -2.4900e-01,  4.8581e-01,  3.5214e-02,  1.4288e-01,\n",
      "        -8.4659e-02, -8.7917e-02, -6.5080e-01,  1.9214e-01,  9.4129e-02,\n",
      "        -1.0084e-01,  3.9686e-01,  2.1918e-01,  1.7440e-01,  1.8361e-01,\n",
      "         2.5563e-01,  1.7434e-01, -5.5120e-02, -5.0393e-02,  5.2894e-01,\n",
      "        -5.3674e-01,  2.0702e-02, -8.3346e-01, -2.1497e-01, -9.4487e-02,\n",
      "        -1.1800e-02,  1.2713e-01,  6.3430e-01, -1.8029e-01,  5.2011e-02,\n",
      "        -3.1240e-01, -2.5302e+00,  2.3571e-01, -1.6146e-01, -1.8182e-01,\n",
      "         6.8951e-01, -4.4509e-01,  6.4377e-02,  1.4384e-01,  6.2194e-01,\n",
      "         8.5790e-01, -2.3083e-02,  5.3937e-01,  3.3104e-01,  1.1583e-01,\n",
      "         8.1912e-02, -3.3135e-01, -2.5769e-01,  3.6626e-02,  1.5965e-01,\n",
      "         3.3988e-02, -1.3385e-01,  3.4125e-01, -2.2593e-01,  2.2698e-01,\n",
      "        -4.8414e-01,  3.7880e-02, -7.9133e-02, -4.3425e-02, -9.6151e-02,\n",
      "        -1.5596e-01,  1.2116e-01,  7.5390e-02, -5.4109e-02, -1.1437e-01,\n",
      "         2.0788e-02,  5.3831e-01,  1.2107e-01,  4.4698e-01,  1.8357e-01,\n",
      "         4.0756e-01, -2.0482e-01, -2.2030e-01,  5.8290e-01,  8.9612e-01,\n",
      "        -8.8884e-02,  1.7875e-01,  5.7594e-01,  2.7254e-01,  5.9928e-01,\n",
      "        -1.7287e-01,  3.7728e-02,  2.1079e-01,  3.4164e-01, -8.3466e-02,\n",
      "        -2.5629e-02, -3.8143e-02, -1.0855e-01, -4.1824e-01,  2.4296e-01,\n",
      "        -6.6965e-02, -6.6354e-03, -1.0347e-01,  1.9249e-01, -2.3163e+00,\n",
      "         3.8747e-01,  1.5098e-01,  1.4482e-01,  2.3235e-01, -5.8179e-01,\n",
      "         3.8337e-01, -6.4055e-02,  1.2763e-01,  8.3698e-02,  9.9421e-02,\n",
      "        -9.9673e-02,  2.8180e-01,  2.1983e-01, -1.3839e-02, -8.4813e-02,\n",
      "         5.0904e-01, -3.0666e-01,  4.5403e-01,  2.8270e-02, -9.8717e-02,\n",
      "         3.8649e-02,  4.0795e-01,  9.8648e-02, -6.8703e-01, -3.3533e-01,\n",
      "         8.3121e-02,  3.5431e-01,  1.0801e-01,  1.6950e-01, -5.0428e-01,\n",
      "        -7.8143e-01, -4.0190e-01, -3.1062e+00, -2.3790e-01,  1.0108e+00,\n",
      "        -1.4190e-01,  2.7980e-01,  6.9310e-01,  1.4091e-01,  7.5458e-02,\n",
      "         5.6058e-01,  2.9992e-01, -3.6911e-01,  9.7665e-02,  2.9242e-01,\n",
      "        -8.1825e-02,  5.2271e-02, -2.2228e-01,  2.1968e-01, -6.5691e-02,\n",
      "         1.5747e-01, -2.0537e-01,  7.9325e-02,  1.0311e-01, -2.0537e-01,\n",
      "         1.7432e-01,  7.2282e-02,  2.2525e-01,  6.2064e-01,  3.7530e-01,\n",
      "         1.5799e-01,  5.6158e-02,  3.6148e-01, -4.2427e-02,  7.8699e-01,\n",
      "        -1.2320e-02, -3.4269e-01,  5.1480e-01,  1.3052e-01,  1.3414e-01,\n",
      "        -2.2502e-01,  6.0436e-01,  5.9342e-01,  9.2160e-02,  5.5220e-01,\n",
      "        -1.7860e-01,  1.8045e-01, -1.7164e-01,  2.9960e-01,  3.9801e-01,\n",
      "        -6.2805e-01, -3.2539e-01,  2.2267e-01,  1.8860e-01, -2.6236e-01,\n",
      "        -2.0792e-03,  1.7970e-01,  9.6719e-02,  1.3834e-01,  1.6465e-01,\n",
      "         4.3941e-01,  4.0408e-01, -1.1593e-01,  5.0312e-01, -7.9638e-01,\n",
      "         3.8036e+00, -2.5938e-02, -2.9573e-01, -5.5707e-01,  2.0672e-01,\n",
      "        -1.6628e-02,  7.6502e-02,  5.0318e-02, -3.7029e-01, -9.4420e-02,\n",
      "         2.1649e-02,  3.6363e-01,  3.4216e-01, -4.4548e-01, -3.1554e-01,\n",
      "        -1.4671e-01,  2.2865e-01, -4.0624e-01,  1.7409e-01,  1.9627e-01,\n",
      "        -2.5682e-02, -5.0682e-01,  4.1476e-02, -3.7071e-02, -1.1322e+00,\n",
      "         1.5225e-01, -3.9729e-01, -3.8710e-01,  1.8852e-01, -1.3645e-01,\n",
      "        -5.0950e-02, -6.5730e-02, -3.6738e-01,  4.2436e-01, -1.7697e-01,\n",
      "        -2.4992e-02, -1.9092e-01,  4.3674e-01,  1.7234e-01, -5.2417e-01,\n",
      "         4.1063e-01,  4.1986e-01,  2.7603e-01,  3.6187e-01, -2.1469e-01,\n",
      "         9.1558e-01, -4.1393e-01,  2.2331e-01, -2.8814e-01, -2.2860e-01,\n",
      "         7.0311e-02,  1.9104e-02,  2.3647e-01, -1.2071e-01, -1.4964e-01,\n",
      "        -2.4925e-01,  8.5475e-02,  3.6431e-01, -3.7920e-01, -3.1736e-01,\n",
      "        -3.2072e-01, -1.2138e-01, -4.0661e-01, -6.8701e-02, -2.4337e-01,\n",
      "         1.2148e-01, -2.2233e-01, -4.3085e-01, -3.5164e+00, -4.7643e-01,\n",
      "         4.9333e-02,  8.0546e-01,  6.6038e-01, -6.2852e-02, -5.0351e-02,\n",
      "         2.7127e-01, -7.3066e-02, -3.8528e-01,  6.4198e-01, -1.1995e-01,\n",
      "        -2.9475e-01, -5.3356e-02, -6.7284e-01,  2.7137e-01,  2.2273e-01,\n",
      "         2.2851e-01,  1.5039e-01, -2.8687e-03,  1.6053e-01, -6.0219e-02,\n",
      "        -2.2570e-01, -1.7218e-01,  3.7920e-01,  1.3907e-01, -7.7125e-01,\n",
      "        -9.4169e-03, -6.2083e-03, -3.9824e-01,  4.5904e-01, -4.2053e-01,\n",
      "         1.4814e-01, -1.8934e-01, -5.1180e-01, -3.0337e+00,  6.3251e-01,\n",
      "        -3.4695e-01, -3.6198e-01, -1.3974e-01, -2.3074e-01,  5.3864e-01,\n",
      "         1.8220e-01, -4.0131e-01,  2.5629e-01,  2.0530e-01,  5.6823e-02,\n",
      "         5.6716e-02,  9.6733e-02,  1.5985e-01,  5.3779e-01,  2.8825e-01,\n",
      "        -1.5445e-01,  4.8056e-01,  7.9940e-02,  1.6704e-01, -2.5466e-01,\n",
      "        -2.5154e-01, -1.2814e-01, -4.3058e-01,  4.5326e-01, -3.8428e-01,\n",
      "         2.2742e-01, -2.6829e-01, -4.8025e-02, -1.1971e-01,  7.1794e-02,\n",
      "         3.0362e-01, -4.6630e-02,  1.4646e-01, -2.6315e-01, -9.1856e-04,\n",
      "         1.4888e-01,  3.8987e-01,  2.2798e-01, -8.3920e-02,  1.0038e+00,\n",
      "         5.8821e-02,  6.1741e-01,  8.2228e-01,  1.0828e-01,  5.2113e-01,\n",
      "        -6.3850e-02,  4.3798e-01,  8.2911e-02, -5.8361e-02,  1.5287e-01,\n",
      "         6.3643e-01, -3.0526e-01, -4.8996e-01,  8.3750e-02,  3.0410e-01,\n",
      "        -4.6447e-02,  1.9311e-01, -1.4793e-01,  6.5270e-01, -6.8207e-01,\n",
      "         2.3250e-01,  2.2581e-01,  1.0160e-01, -1.7039e-01,  4.5763e-01,\n",
      "        -6.6117e-01, -1.0915e-01,  3.1759e-01, -5.6742e-01,  3.6369e-01,\n",
      "         1.1118e-01, -9.2273e-01, -1.8318e-01,  1.9630e-01, -3.5768e-01,\n",
      "        -1.1572e-01,  6.1387e-01, -1.7925e-01, -3.7512e-01,  2.9159e-02,\n",
      "        -3.4944e-01,  6.7496e-01, -4.1452e-01,  1.4668e-01, -2.1319e-01,\n",
      "        -1.2230e-01, -6.6191e-01, -5.7266e-02, -1.5278e-01,  7.0484e-01,\n",
      "        -6.3267e-03,  1.2094e-01, -1.3995e-01,  2.9667e-01,  4.2080e-01,\n",
      "        -1.1279e+00,  4.6778e-01, -2.3268e-01, -3.4625e-01, -2.7924e-01,\n",
      "         1.6204e-01, -3.6720e-01, -3.4509e-01, -1.3106e-01, -2.0637e-02,\n",
      "        -2.9770e-01,  3.5473e-01, -1.0500e-02, -6.8337e-02, -1.8009e-01,\n",
      "         8.4196e-02,  4.3347e-01,  9.9328e-01, -7.0923e-02,  9.2673e-02,\n",
      "         6.6957e-01,  3.4144e-01,  5.4814e-01,  6.2526e-01,  3.7684e-01,\n",
      "        -8.4530e-02, -4.1268e-01, -2.8410e-01, -1.9068e-01,  2.6755e-02,\n",
      "        -6.8923e-01, -7.0618e-01,  1.5478e-01,  1.2836e-01, -3.5345e-01,\n",
      "        -1.7234e-01, -5.8153e-01, -4.6847e-01, -4.1033e-01, -4.6941e-01,\n",
      "        -1.0821e-01, -1.0341e-01, -6.4984e-02, -2.9586e-01, -1.7551e-01,\n",
      "        -6.6527e-02,  1.1980e-01, -7.1435e-03,  5.9394e-01,  1.7895e-02,\n",
      "        -1.7739e-01, -1.2278e-01,  7.0724e-01, -1.2774e-01, -1.9259e-01,\n",
      "        -4.2095e-01, -4.5515e-01, -2.8818e-01, -1.5670e-01,  4.3510e-01,\n",
      "        -1.9231e-02,  3.5389e-01, -4.7203e-01, -2.2583e-01, -1.3106e-01,\n",
      "        -1.7989e+00,  2.3723e-01,  3.4040e-01,  2.2196e-01,  2.7090e-01,\n",
      "        -6.2533e-01, -7.6352e-01,  6.4266e-01,  1.7336e-01,  1.4678e-01,\n",
      "        -5.6795e-01, -1.2158e-01,  1.1580e-02, -1.3967e-02, -8.1238e-02,\n",
      "         1.1433e-02,  1.2196e-02, -9.5423e-02,  2.2765e-01, -5.1093e-01,\n",
      "        -1.2084e-01,  2.8321e-01,  5.6126e-01, -3.9942e-01, -5.8153e-01,\n",
      "         3.8004e-03,  1.1020e-01,  9.6521e-02,  5.3683e-02,  3.0277e-01,\n",
      "        -5.5503e-01, -5.4410e-01, -5.9991e-01, -2.0292e-01,  2.5257e-02,\n",
      "         1.4013e-01,  4.8088e-01, -3.4687e-02, -3.3648e-01,  2.2430e-01,\n",
      "        -5.4277e-01,  5.2681e-01,  2.1319e-02, -9.8654e-02,  5.3981e-01,\n",
      "         3.1239e-01, -7.1116e-01,  6.4740e-02, -8.6942e-02, -6.1064e-01,\n",
      "        -2.8150e-01,  1.4165e-01, -2.4054e-01,  1.4434e-01,  7.1285e-02,\n",
      "        -3.3866e-01, -6.1422e-01,  2.1060e-01, -2.0925e-01, -3.1046e-01,\n",
      "         3.9100e-01, -1.9506e-01, -2.8720e-01,  2.8261e-04,  6.0405e-02,\n",
      "        -5.6653e-01, -6.1056e-02, -7.9092e-02, -4.7727e-01, -2.1269e-02,\n",
      "         5.3112e-01,  4.8477e-01, -1.4412e-01,  8.8439e-01, -2.7032e-01,\n",
      "         7.8288e-02,  2.2952e-01, -7.7833e-04,  3.5408e-01, -2.3626e-01,\n",
      "        -2.1180e-01, -3.2179e-01, -4.3833e-01,  2.3894e-01, -6.7503e-01,\n",
      "         8.1350e-01, -1.0447e-01, -1.8474e-01, -2.5916e-01, -2.0528e-02,\n",
      "        -5.8164e-01, -4.4628e-01,  2.0156e-01,  1.5272e-02, -8.0457e-02,\n",
      "         1.4970e-01, -2.8312e-01,  3.4177e-01, -3.4481e-01,  4.2938e-01,\n",
      "        -1.6818e-01,  4.3744e-01,  6.1219e-01,  1.1482e-01,  1.0051e-01,\n",
      "         4.6275e-02,  4.9423e-01,  2.9493e-01, -2.0491e-01, -4.8518e-01,\n",
      "         8.6826e-02, -1.6348e-01, -5.1374e-01,  2.9356e-03,  1.9949e-01,\n",
      "        -3.2420e-01,  1.6379e-01, -3.4601e-01,  2.0076e+00,  3.2521e-01,\n",
      "         2.6308e-01, -1.6533e-01,  4.2788e-02, -2.7729e-01, -4.2491e-01,\n",
      "         1.2408e-01, -6.0217e-01,  4.4291e-01,  5.7114e-01,  3.1247e-01,\n",
      "         2.8356e-01,  6.4406e-02,  2.3504e-01,  6.9948e-01, -3.0132e-01,\n",
      "        -4.7122e-01, -7.5120e-01, -1.9443e-01, -2.0153e-01,  8.6684e-02,\n",
      "         3.6493e-01,  3.7720e-02,  2.8007e-02, -1.3535e-01,  1.1384e-01,\n",
      "        -2.2620e-01, -1.4043e-01,  1.6133e-01, -1.7186e-01,  4.0180e-02,\n",
      "        -3.1330e-02,  4.8217e-01, -1.5895e-01,  3.3973e-01,  1.1754e-01,\n",
      "        -2.3574e-02,  2.3798e-01, -5.6425e-03,  3.8240e-01, -4.0302e-01,\n",
      "         7.4354e-01,  1.6381e-02,  1.8022e-01,  1.0966e+00, -1.4816e-01,\n",
      "        -2.7472e-01,  4.0889e-01,  5.5912e-01, -3.8880e-01,  5.6891e-02,\n",
      "         1.2181e-02, -2.3278e-01, -1.2547e-01, -2.2284e-01, -6.6471e-02,\n",
      "        -3.3570e-01, -7.4227e-01,  5.1837e-01,  9.1139e-02,  2.4492e-01,\n",
      "         2.6933e-01, -4.8274e-01,  3.1287e-01,  3.9045e-02, -1.3839e-01,\n",
      "         1.9426e-01,  8.3204e-01, -1.8849e-01,  7.3139e-02, -6.3581e-02,\n",
      "         2.9828e-01, -1.8007e-01,  3.6696e-01,  5.1320e-02,  4.9431e-01,\n",
      "        -3.7047e-02, -2.8472e-01, -2.9826e+00, -2.8985e-01,  3.1972e-01,\n",
      "        -8.1519e-02,  6.4848e-03,  5.5849e-01,  1.1519e-01, -2.0170e-01,\n",
      "         3.4829e-02, -1.4676e-01,  3.0870e-01,  2.4527e-01,  1.0434e-01,\n",
      "         2.0022e-01,  1.5045e-01,  1.0219e-01,  2.2780e-02, -4.5604e-01,\n",
      "        -1.3816e-02,  3.2144e-01,  9.0319e-01, -5.9987e-02, -6.2334e-01,\n",
      "        -3.3442e-01, -4.5039e-01,  4.1877e-01,  6.0070e-01, -3.7741e-01,\n",
      "        -1.7775e-01,  3.2265e-01, -5.6936e-02,  1.9614e-01, -1.1707e-01,\n",
      "        -2.3342e-01,  4.0205e-01, -6.8591e-01,  1.6166e-01,  3.3266e-02,\n",
      "         2.3545e-02, -6.9738e-03, -3.6437e-01,  4.1949e-01,  1.7129e-01,\n",
      "         2.6154e-01,  2.4273e-01,  1.1987e-01,  4.6432e-01, -6.8651e-02,\n",
      "         7.3708e-01, -8.8414e-01, -1.3060e-01,  1.4399e-01,  3.0227e-01,\n",
      "        -2.4266e-01,  8.9823e-02,  1.6494e-01,  4.3161e-01, -5.8104e-02,\n",
      "         1.9733e-01, -8.3772e-01,  4.2085e-01,  6.2676e-01, -1.0064e-01,\n",
      "         1.0587e-01,  7.1138e-02, -2.2048e-02,  1.2831e-01, -4.3649e-02,\n",
      "         2.3304e-02,  3.4029e-02,  1.0609e-01, -5.9020e-01,  6.4011e-01,\n",
      "        -8.3592e-02,  1.4647e-01, -2.9901e-01,  3.1314e-01,  2.9735e-01,\n",
      "         2.0583e-01, -3.2639e-01,  1.0104e-01, -1.8237e-01, -7.3217e-01,\n",
      "         2.3650e-01,  7.4988e-02, -7.0153e+00, -4.9691e-01, -4.0131e-01,\n",
      "         2.7561e-01,  2.8285e-01, -5.9808e-01,  6.0939e-02, -2.0890e-01,\n",
      "         1.2492e-02,  7.5696e-02,  1.9983e-01, -1.1688e-01, -4.0965e-02,\n",
      "        -1.8963e-01, -1.9239e-01,  4.6898e-01])\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"text_desc\"].fillna(\"\").tolist()\n",
    "print(f\"Dataset loaded: {len(df)} samples\")\n",
    "\n",
    "embeddings = encode_multimodel_content(texts, model_name=\"bert-base-uncased\", device=\"cpu\", batch_size=32)\n",
    "print(f\"Encoded text_desc into embeddings with shape {embeddings.shape}\")\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1df0e",
   "metadata": {},
   "source": [
    "现在我们拥有原始输入了，我们需要让RQ-VAE来学习和重建输入数据的分布。因此，我们需要先为原始数据拆分成训练和验证集，并构建成dataloader。NextRec提供了`RecDataLoader`来帮助你实现这一步。\n",
    "\n",
    "由于我们已经将文本特征变为了稠密特征`Dense Feature`，我们只需要对特征进行定义，随后传入`RecDataLoader`即可。未来NextRec将会支持多模态特征定义和自动transform，不过在此之前，您还需要使用工具函数来进行手动transform。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268663cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders for RQ-VAE\n",
    "text_feature = DenseFeature(name=\"text_embedding\", input_dim=embeddings.shape[1])\n",
    "loader_builder = RecDataLoader(dense_features=[text_feature])\n",
    "emb_np = embeddings.cpu().numpy()\n",
    "rqvae_train_dict, rqvae_valid_dict = split_dict_random(\n",
    "    {\"text_embedding\": emb_np}, test_size=0.1, random_state=2025\n",
    ")\n",
    "rqvae_train_loader = loader_builder.create_dataloader(\n",
    "    rqvae_train_dict, batch_size=256, shuffle=True\n",
    ")\n",
    "rqvae_valid_loader = loader_builder.create_dataloader(\n",
    "    rqvae_valid_dict, batch_size=256, shuffle=False\n",
    ")\n",
    "rqvae_full_loader = loader_builder.create_dataloader(\n",
    "    {\"text_embedding\": emb_np}, batch_size=256, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef7c1b",
   "metadata": {},
   "source": [
    "现在我们需要实例化RQ-VAE，可以看到它提供了多个参数，我们将为您一一解释：\n",
    "\n",
    "- input_dim: 输入嵌入的维度（如 BERT 向量 768）；编码器接收的维度，解码器最终输出也回到该维度。\n",
    "- hidden_dims: 编码器/解码器的中间层维度列表。\n",
    "- latent_dim: 编码后潜在空间及码本向量的维度；越小压缩越强，越大表达力更高。\n",
    "- num_codebooks: 残差量化层数（分层深度）；层数越多语义 ID 粒度越细，但推理/训练稍慢。\n",
    "- codebook_size: 每一层码本的词表大小列表，长度应等于 num_codebooks；例如 [256,256,256] 意味 3 层，每层 256 个码字，总组合 256³。\n",
    "- shared_codebook: 是否让所有层共享同一套码本；共享码本时参数量更少，不共享时每层的表达力更强，默认为不共享。\n",
    "- kmeans_method: 码本初始化方式；\"kmeans\" 常规 KMeans，\"bkmeans\" 平衡 KMeans（推荐），其他值则随机初始化。\n",
    "- kmeans_iters: KMeans 初始化的最大迭代次数。\n",
    "- distances_method: 量化距离度量；\"l2\" 欧氏距离（默认 VAE 用法），此外也支持\"cosine\" 。\n",
    "- loss_beta: 承诺损失权重 β（量化损失中的第二项）；越大越强制编码器贴近码本，通常 0.25 左右。\n",
    "- dense_features: RQ-VAE需要传入的原始embedding分布是稠密向量，我们需要告诉模型哪些向量是需要被学习和压缩的。\n",
    "\n",
    "在配置完成后，我们使用`fit`方法来开始训练。与其他精排模型的`fit`不同，RQ-VAE需要配置`init_batches`参数来指定使用多少批次的数据进行码本初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de26531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[94mModel Summary: RQVAE\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mFeature Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Dense Features (1):\n",
      "  1. text_embedding      \n",
      "\n",
      "\u001b[1m\u001b[36mModel Parameters\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Model Architecture:\n",
      "RQVAE(\n",
      "  (encoder): RQEncoder(\n",
      "    (stages): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): RQDecoder(\n",
      "    (stages): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rq): RQ(\n",
      "    (vqmodules): ModuleList(\n",
      "      (0-1): 2 x VQEmbedding(128, 128)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Total Parameters:        363,648\n",
      "Trainable Parameters:    363,648\n",
      "Non-trainable Parameters: 0\n",
      "Layer-wise Parameters:\n",
      "  encoder                       : 165,120\n",
      "  decoder                       : 165,760\n",
      "  rq                            : 32,768\n",
      "\n",
      "\u001b[1m\u001b[36mTraining Configuration\u001b[0m\n",
      "\u001b[36m--------------------------------------------------------------------------------\u001b[0m\n",
      "Task Type:               regression\n",
      "Number of Tasks:         1\n",
      "Metrics:                 ['loss']\n",
      "Target Columns:          []\n",
      "Device:                  cpu\n",
      "GradNorm Enabled:        False\n",
      "Regularization:\n",
      "  Embedding L1:          0.0\n",
      "  Embedding L2:          0.0\n",
      "  Dense L1:              0.0\n",
      "  Dense L2:              0.0\n",
      "Other Settings:\n",
      "  Max Gradient Norm:     1.0\n",
      "  Session ID:            rqvae_tutorial\n",
      "  Features Config Path:  /Users/zyaztec/DailyWork/建模代码整理/NextRec/tutorials/notebooks/zh/nextrec_logs/rqvae_tutorial/features_config.pkl\n",
      "  Latest Checkpoint:     /Users/zyaztec/DailyWork/建模代码整理/NextRec/tutorials/notebooks/zh/nextrec_logs/rqvae_tutorial/RQVAE_checkpoint.pt\n",
      "\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\u001b[1mStart training\u001b[0m\n",
      "\u001b[1m================================================================================\u001b[0m\n",
      "\n",
      "\u001b[1mModel device: cpu\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 36/36 elapsed=0:00:00 speed=108.30/s ETA=0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.3739\n",
      "\u001b[36m  Epoch 1/5 - Valid Loss: 0.1670\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 36/36 elapsed=0:00:00 speed=103.61/s ETA=0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.1144\n",
      "\u001b[36m  Epoch 2/5 - Valid Loss: 0.0847\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 36/36 elapsed=0:00:00 speed=116.19/s ETA=0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.0735\n",
      "\u001b[36m  Epoch 3/5 - Valid Loss: 0.0657\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 36/36 elapsed=0:00:00 speed=120.19/s ETA=0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.0621\n",
      "\u001b[36m  Epoch 4/5 - Valid Loss: 0.0598\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 36/36 elapsed=0:00:00 speed=123.27/s ETA=0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.0609\n",
      "\u001b[36m  Epoch 5/5 - Valid Loss: 0.0624\u001b[0m\n",
      "\n",
      "\u001b[1mTraining finished.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RQVAE(\n",
       "  (encoder): RQEncoder(\n",
       "    (stages): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): RQDecoder(\n",
       "    (stages): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Linear(in_features=128, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (rq): RQ(\n",
       "    (vqmodules): ModuleList(\n",
       "      (0-1): 2 x VQEmbedding(128, 128)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rqvae = RQVAE(\n",
    "    input_dim=embeddings.shape[1],\n",
    "    hidden_dims=[128, 256],\n",
    "    latent_dim=128,\n",
    "    num_codebooks=2,\n",
    "    codebook_size=[128, 128],\n",
    "    shared_codebook=False,\n",
    "    kmeans_method=\"bkmeans\",\n",
    "    kmeans_iters=50,\n",
    "    distances_method=\"cosine\",\n",
    "    loss_beta=0.25,\n",
    "    device=\"cpu\",\n",
    "    dense_features=[DenseFeature(name=\"text_embedding\", input_dim=embeddings.shape[1])],\n",
    "    session_id=\"rqvae_tutorial\",\n",
    ")\n",
    "rqvae.fit(\n",
    "    train_data=rqvae_train_loader,\n",
    "    valid_data=rqvae_valid_loader,\n",
    "    epochs=5,\n",
    "    batch_size=256,\n",
    "    lr=1e-3,\n",
    "    init_batches=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44762411",
   "metadata": {},
   "source": [
    "现在我们已经训练完成了，让我们简单看一下输出的结果。可以看到，每个样本原本768维的连续嵌入向量，现在只用2维的离散向量即可表达，大大节省了计算量和存储量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcb5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic IDs shape: torch.Size([10000, 2])\n",
      "Semantic IDs sample (first 5 items):\n",
      "tensor([[ 72,  30],\n",
      "        [ 72,  52],\n",
      "        [106,  38],\n",
      "        [  0,  38],\n",
      "        [ 28, 101]])\n"
     ]
    }
   ],
   "source": [
    "semantic_ids = rqvae.predict(\n",
    "    rqvae_full_loader, batch_size=256, return_reconstruction=False, as_numpy=False\n",
    ")\n",
    "semantic_ids = semantic_ids.to(\"cpu\")\n",
    "print(f\"Semantic IDs shape: {semantic_ids.shape}\")\n",
    "print(f\"Semantic IDs sample (first 5 items):\\n{semantic_ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be14c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
