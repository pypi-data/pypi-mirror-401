model: hmoe

params:
  num_experts: 4

  expert_mlp_params:
    hidden_dims: [256, 128]
    activation: relu
    dropout: 0.1

  gate_mlp_params:
    hidden_dims: [64]
    activation: relu
    dropout: 0.1

  tower_mlp_params_list:
    - hidden_dims: [128, 64]
      activation: relu
      dropout: 0.1
    - hidden_dims: [128, 64]
      activation: relu
      dropout: 0.1

  task_weight_mlp_params:
    - hidden_dims: [64]
      activation: relu
      dropout: 0.1
    - hidden_dims: [64]
      activation: relu
      dropout: 0.1

  embedding_l1_reg: 1.e-6
  embedding_l2_reg: 1.e-5
  dense_l1_reg: 1.e-6
  dense_l2_reg: 1.e-4
