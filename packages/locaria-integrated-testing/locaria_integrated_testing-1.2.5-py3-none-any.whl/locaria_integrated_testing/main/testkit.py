"""
TestKit - Core orchestration engine for integrated testing framework.

Orchestrates test execution, result logging, email alerting, and acknowledgment tracking.
Manages shared resources (BigQuery client, Firestore client) across test runs for efficiency.
Handles test result aggregation, filtering by acknowledgment status, and finalization.
"""

import os
import json
import requests
import uuid
import pandas as pd
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Set
from google.cloud import firestore
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

from .acknowledge_manager import AcknowledgeManager
from ..utils.permission_resolver import PermissionResolver
from ..utils.config_manager import DEFAULT_ISSUE_OWNER_PERMISSION
from .. import __version__

class TestFailureException(Exception):
    """Custom exception raised when a test fails and fail_on_error is True."""
    pass

class TestKit:
    """
    Central orchestrator for test execution, logging, and alerting.
    
    Manages the full test lifecycle: initialization, result collection, acknowledgment
    filtering, email notifications, and finalization. Uses shared resource caching to
    minimize initialization overhead across multiple test runs.
    
    Key responsibilities:
    - Test result logging (pass/warn/fail) with structured metrics
    - Email alerting with acknowledgment-aware filtering
    - Firestore configuration management
    - BigQuery logging for historical tracking
    - Idempotent finalization to prevent duplicate notifications
    """
    
    # Class-level cache for shared resources
    _shared_bigquery_client = None
    _shared_firestore_client = None
    _shared_config = None
    
    def __init__(self, repository_name: str, pipeline_name: str = None, fail_on_error: bool = False, 
                 firestore_project_id: str = None, bigquery_client = None, bigquery_project_id: str = None):
        """
        Initialize the TestKit with repository and pipeline information.
        
        Sets up shared resources (BigQuery client, Firestore client) with caching for efficiency.
        Loads configuration from Firestore and initializes acknowledgment manager.
        
        Args:
            repository_name: Name of the repository (e.g., 'locate_2_pulls')
            pipeline_name: Name of the specific pipeline (e.g., 'plunet_employee_table')
            fail_on_error: If True, pipeline stops on test failures. If False, continues with alerts.
            firestore_project_id: Optional GCP project ID (defaults to 'locaria-dev-config-store')
            bigquery_client: Optional BigQuery client instance for test result logging
            
        Example:
            >>> testkit = TestKit(
            ...     repository_name='locate_2_pulls',
            ...     pipeline_name='plunet_employee_table',
            ...     fail_on_error=False,
            ...     firestore_project_id='locaria-dev-config-store'
            ... )
            >>> # Initializes with shared resource caching, loads config from Firestore
        """
        # Print version on initialization
        print(f"[INFO] Locaria Integrated Testing Framework v{__version__}")
        
        self.repository_name = repository_name
        self.pipeline_name = pipeline_name or "unknown"
        self.fail_on_error = fail_on_error
        self.run_id = self._generate_run_id()
        self.test_results = []
        self.warnings = []
        self.failures = []
        self.failure_emails_sent = False  # Track if failure email has been sent
        self.warning_emails_sent = False  # Track if warning digest email has been sent
        self.finalized = False  # Track if finalize_run() has been called to prevent duplicate execution
        self.start_time = datetime.now(timezone.utc)
        
        
        # Store provided parameters and fallbacks
        from ..utils.config_manager import (
            DEFAULT_FIRESTORE_PROJECT_ID,
            DEFAULT_BIGQUERY_PROJECT_ID,
            DEFAULT_BIGQUERY_DATASET_ID,
            DEFAULT_BIGQUERY_TEST_LOGS_TABLE,
            DEFAULT_PERMISSION_RESOLVER_PROJECT_ID,
            DEFAULT_ACK_MANAGER_ACCESS_GRANTS_COLLECTION
        )
        
        # --- GCP resource configuration block ---
        self.collection_integrated_testing_config = DEFAULT_FIRESTORE_PROJECT_ID
        self._firestore_project_id = firestore_project_id or DEFAULT_FIRESTORE_PROJECT_ID
        self._bigquery_project_id = bigquery_project_id or DEFAULT_BIGQUERY_PROJECT_ID
        self._bigquery_client = bigquery_client
        self._default_dataset_id = DEFAULT_BIGQUERY_DATASET_ID
        self._default_test_logs_table = DEFAULT_BIGQUERY_TEST_LOGS_TABLE
        env_permission_project = os.getenv("TESTKIT_ACCESS_CONTROL_PROJECT")
        self._permission_project_id = (
            env_permission_project
            or DEFAULT_PERMISSION_RESOLVER_PROJECT_ID
        )
        
        # Initialize BigQuery client for test result logging
        self.bigquery_client = self._init_bigquery_client()
        
        # Initialize Firestore client for configuration
        self.firestore_client = self._init_firestore_client()
        
        # Load configuration from Firestore
        self.config = self._load_config_from_firestore()
        self.bigquery_logging_config = self.config.get(
            'bigquery_logging',
            self._get_default_bigquery_logging_config()
        )
        
        # Store acknowledgment manager access grants collection name (configurable via config, defaults to constant)
        self.ack_manager_access_grants_collection = self.config.get(
            'ack_manager_access_grants_collection',
            DEFAULT_ACK_MANAGER_ACCESS_GRANTS_COLLECTION
        )

        # Permission resolver for access-controller lookups (Fire
        # store project defaults to locaria-prod-authenticator).
        self._permission_resolver: Optional[PermissionResolver] = None
        
        # Initialize acknowledgment manager with config (for collection names etc.)
        self.acknowledge_manager = AcknowledgeManager(
            self.firestore_client,
            self.repository_name,
            self.pipeline_name,
            config=self.config,
        )
        
        # Log test run start
        self._log_run_start()
    
    
    def _generate_run_id(self) -> str:
        """Generate unique run identifier with timestamp and UUID."""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"{self.repository_name}_{self.pipeline_name}_{timestamp}_{unique_id}"
    
    def _init_bigquery_client(self) -> Optional[bigquery.Client]:
        """
        Initialize BigQuery client with caching and fallback logic.
        
        Priority: cached instance > provided instance > default credentials.
        Uses class-level cache to share instance across TestKit instances.
        
        Returns:
            BigQuery client instance or None if initialization fails
        """
        # Use cached BigQuery client if available
        if TestKit._shared_bigquery_client is not None:
            return TestKit._shared_bigquery_client
            
        # First priority: Use provided BigQuery client
        if self._bigquery_client is not None:
            TestKit._shared_bigquery_client = self._bigquery_client
            print("Using provided BigQuery client instance")
            return self._bigquery_client
            
        try:
            # Initialize BigQuery client with default credentials
            # Will use Application Default Credentials (ADC) from environment
            bq_client = bigquery.Client(project=self._bigquery_project_id)
            print("BigQuery client initialized with default credentials")
            TestKit._shared_bigquery_client = bq_client
            return bq_client
            
        except Exception as e:
            print(f"Warning: Could not initialize BigQuery client: {e}")
            return None
    
    def _init_firestore_client(self) -> Optional[firestore.Client]:
        """
        Initialize Firestore client with caching and fallback project ID.
        
        Uses class-level cache to share client across TestKit instances.
        Falls back to default project ID if none provided.
        
        Returns:
            Firestore client instance or None if initialization fails
        """
        # Use cached Firestore client if available
        if TestKit._shared_firestore_client is not None:
            return TestKit._shared_firestore_client
            
        try:
            project_id = self._firestore_project_id
            print(f"Using Firestore project ID: {project_id}")
            firestore_client = firestore.Client(project=project_id)
            print(f"Firestore client initialized for project: {project_id}")
            
            # Cache the Firestore client for future use
            TestKit._shared_firestore_client = firestore_client
            return firestore_client
            
        except Exception as e:
            print(f"Warning: Could not initialize Firestore client: {e}")
            return None
    
    def _load_config_from_firestore(self) -> Dict[str, Any]:
        """
        Load repository-specific configuration from Firestore.
        
        Attempts to load config for this repository. Falls back to default config
        if repository config not found or Firestore unavailable.
        
        Returns:
            Configuration dictionary (repository-specific or default)
        """
        try:
            # Import ConfigManager here to avoid circular imports
            from ..utils.config_manager import ConfigManager
            
            # Initialize ConfigManager
            config_manager = ConfigManager(project_id=self._firestore_project_id)
            
            # Load repository-specific configuration (all at once)
            config = config_manager.get_repository_config(self.repository_name)
            
            if config:
                print(f"Loaded configuration for repository: {self.repository_name}")
                return config
            else:
                print(f"No configuration found for repository: {self.repository_name}. Using default configuration.")
                return config_manager.get_default_config()
                
        except Exception as e:
            print(f"Warning: Could not load config from Firestore: {e}")
            # Fallback to default config if Firestore fails
            try:
                from ..utils.config_manager import ConfigManager
                config_manager = ConfigManager(project_id=self._firestore_project_id)
                return config_manager.get_default_config()
            except Exception as fallback_exc:
                print(
                    "Warning: Could not load default config from ConfigManager: "
                    f"{fallback_exc}"
                )
                # Final fallback: return minimal config so testkit can proceed
                return {}
    
    def _get_default_bigquery_logging_config(self) -> Dict[str, str]:
        """Default BigQuery logging config when not provided via Firestore."""
        return {
            'project_id': self._bigquery_project_id,
            'dataset_id': self._default_dataset_id,
            'test_logs_table': self._default_test_logs_table
        }

    def _get_permission_resolver(self) -> Optional[PermissionResolver]:
        """
        Lazily initialize permission resolver used for recipient lookups.
        """
        if not self._permission_project_id:
            return None

        if self._permission_resolver is not None:
            return self._permission_resolver

        try:
            print(
                "Initializing PermissionResolver for project "
                f"{self._permission_project_id}"
            )
            self._permission_resolver = PermissionResolver(
                project_id=self._permission_project_id
            )
        except Exception as exc:  # pragma: no cover - defensive logging
            print(
                f"Warning: Could not initialize PermissionResolver for "
                f"project {self._permission_project_id}: {exc}"
            )
            self._permission_resolver = None
        return self._permission_resolver

    def _collect_issue_owner_permissions(self, issues: List[Dict[str, Any]]) -> Set[str]:
        """
        Collect unique issue_owner permission strings from issue metrics.
        """
        return self._collect_permissions_from_metrics(issues, 'issue_owner')

    def _collect_issue_ack_access_permissions(self, issues: List[Dict[str, Any]]) -> Set[str]:
        """
        Collect unique issue_ack_access permission strings from issue metrics.
        """
        return self._collect_permissions_from_metrics(issues, 'issue_ack_access')

    def _collect_email_routing_grant_permissions(self, issues: List[Dict[str, Any]]) -> Set[str]:
        """
        Collect permissions from email routing grants that match issues.
        
        Queries Firestore for grants with email_routing=True and matches them
        against each issue's repository, pipeline, and test_name. Returns
        permissions from matching grants.
        """
        try:
            # Query email routing grants from Firestore
            grants_collection = self.firestore_client.collection(self.ack_manager_access_grants_collection)
            email_routing_grants = list(grants_collection.where('email_routing', '==', True).stream())
            
            print(f"[DEBUG] Found {len(email_routing_grants)} email routing grant(s)")
            
            if not email_routing_grants:
                print("[DEBUG] No email routing grants found - returning empty set")
                return set()
            
            matching_permissions = set()
            
            # For each issue, check if any grant matches
            for issue in issues:
                test_name = issue.get('test_name', '')
                if not test_name:
                    print(f"[DEBUG] Issue missing test_name: {issue}")
                    continue
                
                print(f"[DEBUG] Checking grants for issue: repo={self.repository_name}, pipeline={self.pipeline_name}, test={test_name}")
                
                # Match grants against this issue
                for grant_doc in email_routing_grants:
                    grant = grant_doc.to_dict() or {}
                    grant_permission = grant.get('permission', '')
                    grant_repos = grant.get('repositories', [])
                    grant_pipelines = grant.get('pipelines', [])
                    grant_tests = grant.get('tests', [])
                    
                    print(f"[DEBUG] Checking grant: permission={grant_permission}, repos={grant_repos}, pipelines={grant_pipelines}, tests={grant_tests}")
                    
                    # Check repository match (empty list = all repositories)
                    if grant_repos and self.repository_name not in grant_repos:
                        print(f"[DEBUG] Grant skipped: repository '{self.repository_name}' not in {grant_repos}")
                        continue
                    
                    # Check pipeline match (empty list = all pipelines)
                    if grant_pipelines and self.pipeline_name not in grant_pipelines:
                        print(f"[DEBUG] Grant skipped: pipeline '{self.pipeline_name}' not in {grant_pipelines}")
                        continue
                    
                    # Check test match (empty list = all tests)
                    if grant_tests and test_name not in grant_tests:
                        print(f"[DEBUG] Grant skipped: test '{test_name}' not in {grant_tests}")
                        continue
                    
                    # Grant matches - add its permission
                    if grant_permission:
                        print(f"[DEBUG] Grant MATCHED! Adding permission: {grant_permission}")
                        matching_permissions.add(grant_permission)
            
            print(f"[DEBUG] Email routing grant permissions collected: {matching_permissions}")
            return matching_permissions
        except Exception as exc:
            # Gracefully handle errors - don't break email sending if grant query fails
            print(f"Warning: Could not query email routing grants: {exc}")
            import traceback
            traceback.print_exc()
            return set()

    def _get_permission_based_recipients(
        self, issues: List[Dict[str, Any]]
    ) -> List[str]:
        """
        Resolve issue_owner permissions to concrete email addresses.
        
        Includes permissions from:
        1. issue_owner field (from pipeline code)
        2. issue_ack_access field (from pipeline code)
        3. Email routing grants (from Access Control GUI)
        
        Falls back to DEFAULT_ISSUE_OWNER_PERMISSION if no permissions are found
        from any source, ensuring backward compatibility when issue_owner/issue_ack_access
        are removed from pipeline code.
        """
        permissions = set()
        permissions.update(self._collect_issue_owner_permissions(issues))
        permissions.update(self._collect_issue_ack_access_permissions(issues))
        permissions.update(self._collect_email_routing_grant_permissions(issues))
        
        # Fallback to default permission if no permissions found
        # This ensures backward compatibility when issue_owner/issue_ack_access are removed from code
        if not permissions:
            permissions.add(DEFAULT_ISSUE_OWNER_PERMISSION)

        resolver = self._get_permission_resolver()
        if not resolver:
            return []

        try:
            resolved = resolver.get_emails_for_permissions(list(permissions))
            if resolved:
                print(
                    "Resolved permission-based recipients: "
                    f"{', '.join(resolved)} (permissions: {permissions})"
                )
            else:
                print(
                    "No permission-based recipients resolved "
                    f"for permissions: {permissions}"
                )
            return resolved
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"Warning: Could not resolve permission recipients: {exc}")
            return []

    def _get_configured_recipients(self, alert_type: str) -> List[str]:
        """Return statically configured recipients for alert type."""
        email_alerts = self.config.get('email_alerts', {})
        if alert_type == 'failure':
            return email_alerts.get('failure_recipients', []) or []
        return email_alerts.get('warning_recipients', []) or []

    def _build_append_recipients(
        self, alert_type: str, issues: List[Dict[str, Any]]
    ) -> List[str]:
        """
        Combine configured recipients with permission-based recipients.
        """
        recipients = []
        recipients.extend(self._get_configured_recipients(alert_type))
        recipients.extend(self._get_permission_based_recipients(issues))
        return self._dedupe_emails(recipients)

    @staticmethod
    def _dedupe_emails(recipients: List[str]) -> List[str]:
        """Deduplicate emails while preserving original casing/order."""
        if not recipients:
            return []

        deduped = []
        seen = set()
        for recipient in recipients:
            if not isinstance(recipient, str):
                continue
            email = recipient.strip()
            if not email:
                continue
            normalized = email.lower()
            if normalized in seen:
                continue
            seen.add(normalized)
            deduped.append(email)
        return deduped

    def _collect_permissions_from_metrics(
        self,
        issues: List[Dict[str, Any]],
        field_name: str,
    ) -> Set[str]:
        """Generic permission extractor used for issue_owner and issue_ack_access."""
        collected: Set[str] = set()
        if not issues:
            return collected

        for issue in issues:
            metrics = issue.get('metrics') or {}
            permissions = metrics.get(field_name)
            if not permissions:
                continue

            if isinstance(permissions, str):
                candidate = permissions.strip()
                if candidate:
                    collected.add(candidate)
            elif isinstance(permissions, (list, tuple, set)):
                for candidate in permissions:
                    if isinstance(candidate, str):
                        trimmed = candidate.strip()
                        if trimmed:
                            collected.add(trimmed)
        return collected
    
    def _log_run_start(self):
        """Log the start of a test run."""
        start_message = f"Test run started - Repository: {self.repository_name}, Pipeline: {self.pipeline_name}, Run ID: {self.run_id}"
        print(f"üöÄ {start_message}")
    
    def log_pass(self, test_name: str, message: str = "", metrics: Dict[str, Any] = None, logging_message: str = None):
        """
        Log a passing test result.
        
        Args:
            test_name: Name of the test that passed
            message: Optional message describing the test result (used for emails/ack manager)
            metrics: Optional dictionary of metrics/measurements
            logging_message: Optional plain text message for BigQuery logs. If not provided, uses message.
        """
        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'status': 'PASS',
            'message': message,
            'logging_message': logging_message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        self.test_results.append(result)
        # self._log_to_sheet(result) # logging all the passed tests clutters the logs
        print(f"‚úÖ PASS: {test_name} - {message}")
    
    def log_warn(
        self, 
        test_name: str, 
        issue_identifier: str,
        message: str, 
        metrics: Dict[str, Any] = None, 
        acknowledgeable: bool = True,
        logging_message: str = None,
        email_per_alert: bool = False,
    ):
        """
        Log a warning test result.
        
        Warnings don't stop pipeline execution but trigger email digests. Can be
        acknowledged to prevent email spam. Metrics are stored in Firestore for
        the Analytics Hub web interface.
        
        Args:
            test_name: Name of the test (e.g., 'check_consistent_daily_hours_per_person')
            issue_identifier: Unique identifier for the issue (e.g., 'john.doe@locaria.com').
                Use stable identifiers like email or ID, not timestamps.
            message: Warning message describing the issue (supports HTML for email formatting)
            metrics: Dictionary of metrics/measurements. Should include:
                - 'issue_owner': Union[str, List[str]] (permission(s) required to acknowledge)
                - 'issue_details': str (detailed description for Analytics Hub)
                - Any pipeline-specific fields (email, person_name, percentages, etc.)
            acknowledgeable: If True, this warning can be acknowledged and muted
            logging_message: Optional plain text message for BigQuery logs. If not provided, uses message.
            email_per_alert: If True, sends one email per alert (bypasses daily digest limit).
                If False (default), uses digest mode with one email per day per alert.
        
        Example:
            >>> testkit.log_warn(
            ...     'check_hours',
            ...     'john.doe@locaria.com',
            ...     'Expected 35h/week, found 28h/week',
            ...     metrics={
            ...         'issue_owner': 'analytics_hub.data_team_ack',
            ...         'issue_details': 'John Doe has inconsistent weekly hours',
            ...         'email': 'john.doe@locaria.com',
            ...         'expected_hours': 35,
            ...         'actual_hours': 28
            ...     }
            ... )
        """
        metrics = self._ensure_issue_ack_access(metrics)

        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'issue_identifier': issue_identifier,
            'status': 'WARN',
            'message': message,
            'logging_message': logging_message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'acknowledgeable': acknowledgeable,
            'email_per_alert': email_per_alert
        }
        
        self.warnings.append(result)
        self.test_results.append(result)
        
        print(f"‚ö†Ô∏è  WARN: {test_name} - {message}")
    
    def log_fail(
        self, 
        test_name: str, 
        issue_identifier: str,
        message: str, 
        metrics: Dict[str, Any] = None, 
        stop_pipeline: bool = None, 
        acknowledgeable: bool = True,
        logging_message: str = None,
        email_per_alert: bool = False,
    ):
        """
        Log a failing test result and trigger alerts.
        
        Failures trigger immediate email notifications (unless acknowledged). Can optionally
        stop pipeline execution. Metrics are stored in Firestore for the Analytics Hub.
        
        Args:
            test_name: Name of the test that failed (e.g., 'check_data_completeness')
            issue_identifier: Unique identifier for the issue (e.g., 'plunet_employee_table').
                Use stable identifiers, not timestamps.
            message: Failure message describing the issue (supports HTML for email formatting)
            metrics: Dictionary of metrics/measurements. Should include:
                - 'issue_owner': Union[str, List[str]] (permission(s) required to acknowledge)
                - 'issue_details': str (detailed description for Analytics Hub)
                - Any pipeline-specific fields
            stop_pipeline: If True, send email and stop pipeline. If False, send email and continue.
                          If None, uses the fail_on_error setting from TestKit initialization.
            acknowledgeable: If True, this failure can be acknowledged and muted
            logging_message: Optional plain text message for BigQuery logs. If not provided, uses message.
            email_per_alert: If True, sends one email per alert (bypasses daily digest limit).
                If False (default), uses digest mode with one email per day per alert.
            
        Raises:
            TestFailureException: If stop_pipeline is True, raises exception to stop pipeline
            
        Example:
            >>> testkit.log_fail(
            ...     'check_data_completeness',
            ...     'plunet_employee_table',
            ...     'Data completeness 85% below threshold 95%',
            ...     metrics={
            ...         'issue_owner': 'analytics_hub.data_team_ack',
            ...         'issue_details': 'Plunet employee table has low completeness',
            ...         'completeness': 0.85,
            ...         'threshold': 0.95,
            ...         'null_cells': 150
            ...     },
            ...     stop_pipeline=False  # Continue pipeline, just alert
            ... )
        """
        # Determine if we should stop the pipeline
        should_stop = stop_pipeline if stop_pipeline is not None else self.fail_on_error
        
        metrics = self._ensure_issue_ack_access(metrics)

        result = {
            'run_id': self.run_id,
            'repository': self.repository_name,
            'pipeline': self.pipeline_name,
            'test_name': test_name,
            'issue_identifier': issue_identifier,
            'status': 'FAIL',
            'message': message,
            'logging_message': logging_message,
            'metrics': metrics or {},
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'acknowledgeable': acknowledgeable,
            'email_per_alert': email_per_alert
        }
        
        self.failures.append(result)
        self.test_results.append(result)
        
        print(f"‚ùå FAIL: {test_name} - {message}")
        
        # If stop_pipeline=True, send summary email with all failures and stop
        if should_stop:
            self._send_failure_summary_email()
            raise TestFailureException(f"Pipeline stopped due to test failure: {test_name} - {message}")
           
    def _write_test_results_to_bigquery(self):
        """
        Write consolidated test results to BigQuery table.
        
        Groups test results by test_name and consolidates multiple issues from the same test
        into a single summary row. Each test gets one row with a succinct summary message
        that includes key details (e.g., first few issue identifiers) and full details in metrics.
        """
        if not self.bigquery_client or not self.test_results:
            return
        
        try:
            # Read BigQuery logging configuration
            logging_project_id = self.bigquery_logging_config.get('project_id', self._bigquery_project_id)
            dataset_id = self.bigquery_logging_config.get('dataset_id', self._default_dataset_id)
            table_name = self.bigquery_logging_config.get('test_logs_table', self._default_test_logs_table)
            table_ref = f"{logging_project_id}.{dataset_id}.{table_name}"
            
            # Group test results by test_name
            tests_grouped = {}
            for result in self.test_results:
                test_name = result.get('test_name', 'unknown')
                if test_name not in tests_grouped:
                    tests_grouped[test_name] = []
                tests_grouped[test_name].append(result)
            
            # Build consolidated rows (one per test)
            rows = []
            for test_name, test_results in tests_grouped.items():
                # Get common fields from first result
                first_result = test_results[0]
                
                # Count by status
                passed_count = len([r for r in test_results if r.get('status') == 'PASS'])
                warn_count = len([r for r in test_results if r.get('status') == 'WARN'])
                fail_count = len([r for r in test_results if r.get('status') == 'FAIL'])
                
                # Determine overall status (FAIL > WARN > PASS)
                if fail_count > 0:
                    overall_status = 'FAIL'
                elif warn_count > 0:
                    overall_status = 'WARN'
                else:
                    overall_status = 'PASS'
                
                # Build consolidated message and metrics
                if len(test_results) == 1:
                    # Single result - use logging_message if provided, otherwise fall back to message
                    consolidated_message = first_result.get('logging_message') or first_result.get('message', '')
                    consolidated_metrics = first_result.get('metrics', {})
                    # Handle metrics if it's a JSON string
                    if isinstance(consolidated_metrics, str):
                        try:
                            consolidated_metrics = json.loads(consolidated_metrics)
                        except (json.JSONDecodeError, TypeError):
                            consolidated_metrics = {}
                    issue_identifier = first_result.get('issue_identifier')
                else:
                    # Multiple results - create summary
                    issue_identifiers = [r.get('issue_identifier', '') for r in test_results if r.get('issue_identifier')]
                    unique_identifiers = list(set([id for id in issue_identifiers if id]))[:5]  # First 5 unique non-empty identifiers
                    
                    # Build succinct summary message
                    status_summary = []
                    if fail_count > 0:
                        status_summary.append(f"{fail_count} failure{'s' if fail_count > 1 else ''}")
                    if warn_count > 0:
                        status_summary.append(f"{warn_count} warning{'s' if warn_count > 1 else ''}")
                    if passed_count > 0:
                        status_summary.append(f"{passed_count} passed")
                    
                    summary_parts = [f"Found {len(test_results)} issue{'s' if len(test_results) > 1 else ''}"]
                    if status_summary:
                        summary_parts.append(f"({', '.join(status_summary)})")
                    if unique_identifiers:
                        identifiers_str = ', '.join(unique_identifiers)
                        if len(issue_identifiers) > 5:
                            identifiers_str += f" ... and {len(issue_identifiers) - 5} more"
                        summary_parts.append(f": {identifiers_str}")
                    
                    consolidated_message = " | ".join(summary_parts)
                    
                    # Consolidate metrics
                    # Use logging_message if available, otherwise message
                    sample_messages = [
                        (r.get('logging_message') or r.get('message', ''))[:100] 
                        for r in test_results[:3]
                    ]
                    consolidated_metrics = {
                        'total_issues': len(test_results),
                        'passed': passed_count,
                        'warnings': warn_count,
                        'failures': fail_count,
                        'issue_identifiers': issue_identifiers,
                        'sample_messages': sample_messages
                    }
                    # Merge any common metrics from first result
                    first_metrics = first_result.get('metrics', {})
                    if isinstance(first_metrics, str):
                        try:
                            first_metrics = json.loads(first_metrics)
                        except (json.JSONDecodeError, TypeError):
                            first_metrics = {}
                    if isinstance(first_metrics, dict):
                        # Copy non-issue-specific metrics
                        for key, value in first_metrics.items():
                            if key not in ['issue_identifier', 'issue_details', 'email', 'person_name']:
                                consolidated_metrics[key] = value
                    
                    issue_identifier = f"multiple_{test_name}" if issue_identifiers else None
                
                row = {
                    'run_id': first_result.get('run_id', self.run_id),
                    'repository': first_result.get('repository', self.repository_name),
                    'pipeline': first_result.get('pipeline', self.pipeline_name),
                    'test_name': test_name,
                    'issue_identifier': issue_identifier,
                    'status': overall_status,
                    'message': consolidated_message,
                    'metrics': json.dumps(consolidated_metrics) if consolidated_metrics else None,
                    'timestamp': first_result.get('timestamp', datetime.now(timezone.utc).isoformat()),
                    'acknowledgeable': first_result.get('acknowledgeable', False),
                    'log_timestamp': datetime.now(timezone.utc).isoformat()
                }
                rows.append(row)
            
            # Convert to DataFrame
            df = pd.DataFrame(rows)
            
            # Define schema
            schema = [
                bigquery.SchemaField("run_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("repository", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("pipeline", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("test_name", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("issue_identifier", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("status", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("message", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("metrics", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("timestamp", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("acknowledgeable", "BOOLEAN", mode="REQUIRED"),
                bigquery.SchemaField("log_timestamp", "STRING", mode="REQUIRED"),
            ]
            
            # Ensure dataset exists (create if it doesn't)
            dataset_ref = self.bigquery_client.dataset(dataset_id, project=logging_project_id)
            try:
                self.bigquery_client.get_dataset(dataset_ref)
                print(f"[INFO] BigQuery dataset {logging_project_id}.{dataset_id} already exists")
            except NotFound:
                print(f"[INFO] BigQuery dataset {logging_project_id}.{dataset_id} does not exist. Creating it...")
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = "US"  # Set dataset location
                dataset.description = "Cross-project logging for integrated test results"
                self.bigquery_client.create_dataset(dataset, exists_ok=True)
                print(f"[INFO] BigQuery dataset {logging_project_id}.{dataset_id} created successfully")
            
            # Ensure table exists (create if it doesn't)
            try:
                self.bigquery_client.get_table(table_ref)
                print(f"[INFO] BigQuery table {table_ref} already exists")
            except NotFound:
                print(f"[INFO] BigQuery table {table_ref} does not exist. Creating it...")
                table = bigquery.Table(table_ref, schema=schema)
                self.bigquery_client.create_table(table)
                print(f"[INFO] BigQuery table {table_ref} created successfully")
            
            # Replace NaN values with None for proper BigQuery handling
            df = df.where(pd.notnull(df), None)
            
            # Write DataFrame to BigQuery (append mode)
            job_config = bigquery.LoadJobConfig(
                schema=schema,
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND
            )
            
            # Use load_table_from_dataframe for better compatibility
            job = self.bigquery_client.load_table_from_dataframe(df, table_ref, job_config=job_config)
            job.result()  # Wait for job to complete
            
            # Verify the write succeeded by checking job errors
            if job.errors:
                print(f"[ERROR] BigQuery job completed with errors: {job.errors}")
            else:
                total_tests = len(tests_grouped)
                total_passed = len([r for r in self.test_results if r.get('status') == 'PASS'])
                total_warnings = len(self.warnings)
                total_failures = len(self.failures)
                print(f"‚úÖ Successfully wrote {len(df)} consolidated test result{'s' if len(df) > 1 else ''} to BigQuery: {table_ref} (Total: {total_tests} test{'s' if total_tests > 1 else ''}, {total_passed} passed, {total_warnings} warnings, {total_failures} failures)")
            
        except Exception as e:
            print(f"Warning: Could not write test results to BigQuery: {e}")
            import traceback
            print(f"Full traceback: {traceback.format_exc()}")
    
    def _send_failure_summary_email(self):
        """
        Send failure summary email, excluding acknowledged issues to prevent spam.
        
        Only sends if there are new (unacknowledged) failures. Includes context about
        muted issues for transparency. Uses config-based email API URL with env fallback.
        """
        # Only short-circuit if we've already sent (or intentionally skipped) the failure email.
        # Do NOT mark as sent just because there are no failures yet; failures may be added later.
        if self.failure_emails_sent:
            return
        if not self.failures:
            return
        
        # Filter out acknowledged issues to prevent email spam
        filtered_results = self.acknowledge_manager.filter_acknowledged_issues(self.failures)
        
        if not filtered_results['new_issues']:
            print("üìß All failure issues are acknowledged and muted - skipping email")
            self.failure_emails_sent = True
            return

        # Separate issues by email_per_alert flag
        per_alert_issues = [issue for issue in filtered_results['new_issues'] if issue.get('email_per_alert', False)]
        digest_issues = [issue for issue in filtered_results['new_issues'] if not issue.get('email_per_alert', False)]
        
        # Send individual emails for per-alert issues (bypasses daily limit)
        if per_alert_issues:
            self._send_individual_alert_emails(per_alert_issues, alert_type='failure')
        
        # Handle digest issues with daily throttle (existing behavior)
        if not digest_issues:
            # All issues were per-alert, mark as sent
            if per_alert_issues:
                self.failure_emails_sent = True
            return
        
        # Per-day throttle: if we've already sent a summary email today (UTC), skip.
        try:
            if self.acknowledge_manager.was_email_sent_today_for_issues(
                digest_issues,
                field_name="last_email_sent_at",
            ):
                print("üìß Failure summary email already sent today (UTC) - skipping")
                self.failure_emails_sent = True
                return
        except Exception as e:
            # Non-blocking: proceed with usual logic if check fails
            print(f"Warning: Could not check last_email_sent_at throttle in Firestore: {e}")
            
        try:
            # Try config first, then environment variable fallback
            email_api_url = self.config.get('api_config', {}).get('email_api_url')
            if not email_api_url:
                email_api_url = os.getenv('EMAIL_API_URL')
                
            if not email_api_url:
                return
                
            # Build email content: new issues first, then acknowledged context
            # Use digest_issues (not all new_issues) since per-alert issues are sent separately
            failure_summary = []
            
            # Section 1: New issues that need attention
            if digest_issues:
                failure_summary.append("üö® NEW ISSUES DETECTED:")
                for i, failure in enumerate(digest_issues, 1):
                    failure_summary.append(f"{i}. {failure['test_name']}<br/>   Message: {failure['message']}<br/>")
            
            # Section 2: Acknowledged issues (for context, but muted)
            if filtered_results['acknowledged_issues']:
                if failure_summary:
                    failure_summary.append("")  # Spacing between sections
                failure_summary.append("‚úÖ PREVIOUSLY ACKNOWLEDGED (muted):")
                for i, failure in enumerate(filtered_results['acknowledged_issues'], 1):
                    metrics = failure.get('metrics', {})
                    # Support multiple field name variations for backward compatibility
                    person_name = metrics.get('person_name') or metrics.get('issue_name', 'Unknown')
                    person_email = metrics.get('person_email') or metrics.get('issue_identifier', 'Unknown')
                    failure_summary.append(f"{i}. {failure['test_name']} - {person_name} ({person_email}) - Acknowledged and muted")
            
            failure_summary_text = "<br/><br/>".join(failure_summary)
            
            payload = {
                "task_name": self.config.get('api_config', {}).get('email_template_failure', 'Test Failure Alert'),
                "custom_variables": {
                    "repository_name": self.repository_name,
                    "pipeline_name": self.pipeline_name,
                    "run_id": self.run_id,
                    "failure_count": len(digest_issues),
                    "failure_summary": failure_summary_text,
                    "run_duration": self._get_run_duration(),
                    "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
                }
            }

            append_recipients = self._build_append_recipients(
                alert_type='failure',
                issues=digest_issues,
            )
            if append_recipients:
                payload["append_recipients"] = append_recipients
            
            response = requests.post(email_api_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    print(f"üìß Failure summary email sent successfully")
                    self.failure_emails_sent = True
                    # Stamp per-issue Firestore docs with last successful email send time.
                    try:
                        self.acknowledge_manager.stamp_summary_email_call(
                            filtered_results['new_issues'],
                            field_name="last_email_sent_at",
                        )
                    except Exception as e:
                        print(f"Warning: Could not stamp last_email_sent_at in Firestore: {e}")
                else:
                    print(f"Warning: Failure summary email failed: {result.get('message')}")
            else:
                print(f"Warning: Failure summary email failed with status {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"Warning: Could not send failure summary email: {e}")
    
    def send_warning_digest(self):
        """Send digest of all warnings at the end of a run, filtered by acknowledgment status."""
        if not self.warnings or self.warning_emails_sent:
            return
        
        # Filter warnings into new vs acknowledged issues
        filtered_results = self.acknowledge_manager.filter_acknowledged_issues(self.warnings)
        
        if not filtered_results['new_issues']:
            print("üìß All warning issues are acknowledged and muted - skipping email")
            self.warning_emails_sent = True
            return
        
        # Separate issues by email_per_alert flag
        per_alert_issues = [issue for issue in filtered_results['new_issues'] if issue.get('email_per_alert', False)]
        digest_issues = [issue for issue in filtered_results['new_issues'] if not issue.get('email_per_alert', False)]
        
        # Send individual emails for per-alert issues (bypasses daily limit)
        if per_alert_issues:
            self._send_individual_alert_emails(per_alert_issues, alert_type='warning')
        
        # Handle digest issues with daily throttle (existing behavior)
        if not digest_issues:
            # All issues were per-alert, mark as sent
            if per_alert_issues:
                self.warning_emails_sent = True
            return
        
        # Per-day throttle: if we've already sent a summary email today (UTC), skip.
        try:
            if self.acknowledge_manager.was_email_sent_today_for_issues(
                digest_issues,
                field_name="last_email_sent_at",
            ):
                print("üìß Warning summary email already sent today (UTC) - skipping")
                self.warning_emails_sent = True
                return
        except Exception as e:
            # Non-blocking: proceed with usual logic if check fails
            print(f"Warning: Could not check last_email_sent_at throttle in Firestore: {e}")
            
        try:
            # Get email API URL from centralized config
            email_api_url = self.config.get('api_config', {}).get('email_api_url')
            
            if not email_api_url:
                # Fallback to environment variable
                email_api_url = os.getenv('EMAIL_API_URL')
                
            if not email_api_url:
                return
                
            # Prepare warning digest with acknowledgment context
            # Use digest_issues (not all new_issues) since per-alert issues are sent separately
            warning_summary = []
            
            # Add new issues
            if digest_issues:
                warning_summary.append("‚ö†Ô∏è NEW WARNINGS DETECTED:")
                for i, warning in enumerate(digest_issues, 1):
                    warning_summary.append(f"{i}. {warning['test_name']}<br/>   Message: {warning['message']}<br/>")
            
            # Add acknowledged issues context
            if filtered_results['acknowledged_issues']:
                if warning_summary:
                    warning_summary.append("")  # Add spacing
                warning_summary.append("‚úÖ PREVIOUSLY ACKNOWLEDGED (muted):")
                for i, warning in enumerate(filtered_results['acknowledged_issues'], 1):
                    metrics = warning.get('metrics', {})
                    # Try multiple field names for backward compatibility
                    person_name = metrics.get('person_name') or metrics.get('issue_name', 'Unknown')
                    person_email = metrics.get('person_email') or metrics.get('issue_identifier', 'Unknown')
                    warning_summary.append(f"{i}. {warning['test_name']} - {person_name} ({person_email}) - Acknowledged and muted")
            
            warning_summary_text = "<br/><br/>".join(warning_summary)
            
            payload = {
                "task_name": self.config.get('api_config', {}).get('email_template_warning', 'Test Warning Digest'),
                "custom_variables": {
                    "repository_name": self.repository_name,
                    "pipeline_name": self.pipeline_name,
                    "run_id": self.run_id,
                    "warning_count": len(digest_issues),
                    "warning_summary": warning_summary_text,
                    "run_duration": self._get_run_duration(),
                    "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
                }
            }

            append_recipients = self._build_append_recipients(
                alert_type='warning',
                issues=digest_issues,
            )
            if append_recipients:
                payload["append_recipients"] = append_recipients
            
            response = requests.post(email_api_url, json=payload, timeout=30)
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    print(f"Warning digest sent successfully for {len(self.warnings)} warnings")
                    self.warning_emails_sent = True
                    # Stamp per-issue Firestore docs with last successful email send time.
                    try:
                        self.acknowledge_manager.stamp_summary_email_call(
                            digest_issues,
                            field_name="last_email_sent_at",
                        )
                    except Exception as e:
                        print(f"Warning: Could not stamp last_email_sent_at in Firestore: {e}")
            else:
                print(f"Warning: Warning digest failed with status {response.status_code}")
                
        except Exception as e:
            print(f"Warning: Could not send warning digest: {e}")
    
    def _send_individual_alert_emails(self, issues: List[Dict[str, Any]], alert_type: str = 'warning'):
        """
        Send individual emails for each alert when email_per_alert=True.
        
        Bypasses the daily digest limit and sends one email per alert immediately.
        Each alert gets its own email with full details.
        
        Args:
            issues: List of issue dictionaries (warnings or failures)
            alert_type: 'warning' or 'failure'
        """
        if not issues:
            return
        
        try:
            email_api_url = self.config.get('api_config', {}).get('email_api_url')
            if not email_api_url:
                email_api_url = os.getenv('EMAIL_API_URL')
            
            if not email_api_url:
                print(f"Warning: Email API URL not configured - cannot send individual {alert_type} emails")
                return
            
            template_key = 'email_template_failure' if alert_type == 'failure' else 'email_template_warning'
            default_template = 'Test Failure Alert' if alert_type == 'failure' else 'Test Warning Digest'
            template_name = self.config.get('api_config', {}).get(template_key, default_template)
            
            sent_count = 0
            for issue in issues:
                try:
                    # Build individual email for this alert
                    # Use same variable names as digest emails (warning_summary/warning_count) for template compatibility
                    issue_message = issue.get('message', '')
                    test_name = issue.get('test_name', 'unknown')
                    issue_identifier = issue.get('issue_identifier', 'unknown')
                    
                    # Use the same variable names as digest emails so the template can process them
                    if alert_type == 'failure':
                        count_var = "failure_count"
                        summary_var = "failure_summary"
                    else:  # warning
                        count_var = "warning_count"
                        summary_var = "warning_summary"
                    
                    payload = {
                        "task_name": template_name,
                        "custom_variables": {
                            "repository_name": self.repository_name,
                            "pipeline_name": self.pipeline_name,
                            "run_id": self.run_id,
                            count_var: 1,
                            summary_var: issue_message,  # Full HTML message with table
                            "run_duration": self._get_run_duration(),
                            "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
                        }
                    }
                    
                    append_recipients = self._build_append_recipients(
                        alert_type=alert_type,
                        issues=[issue],
                    )
                    if append_recipients:
                        payload["append_recipients"] = append_recipients
                    
                    response = requests.post(email_api_url, json=payload, timeout=30)
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result.get('success'):
                            sent_count += 1
                            # Stamp this individual issue with email timestamp
                            try:
                                self.acknowledge_manager.stamp_summary_email_call(
                                    [issue],
                                    field_name="last_email_sent_at",
                                )
                            except Exception as e:
                                print(f"Warning: Could not stamp last_email_sent_at for individual alert: {e}")
                        else:
                            print(f"Warning: Individual {alert_type} email failed: {result.get('message')}")
                    else:
                        print(f"Warning: Individual {alert_type} email failed with status {response.status_code}")
                        
                except Exception as e:
                    print(f"Warning: Could not send individual {alert_type} email for {issue.get('test_name', 'unknown')}: {e}")
            
            if sent_count > 0:
                print(f"üìß Sent {sent_count} individual {alert_type} email(s) (email_per_alert=True)")
                
        except Exception as e:
            print(f"Warning: Could not send individual {alert_type} emails: {e}")
    
    def _get_run_duration(self) -> str:
        """Get the duration of the test run."""
        duration = datetime.now(timezone.utc) - self.start_time
        return str(duration).split('.')[0]  # Remove microseconds
    
    def get_threshold(self, path: str, default_value: Any = None) -> Any:
        """
        Retrieve threshold value from config using dot-notation path.
        
        Supports both global and pipeline-specific thresholds:
        - Global: 'global.row_count_change.warn_percentage'
        - Pipeline-specific: 'test_name.threshold_name' (auto-prefixed with pipeline name)
        
        Args:
            path: Dot-notation path to threshold value
            default_value: Returned if path not found
            
        Returns:
            Threshold value or default_value
        """
        try:
            # Determine if this is a global or pipeline-specific threshold
            if path.startswith('global.'):
                full_path = f"thresholds.{path}"
            else:
                # Pipeline-specific: prefix with pipeline name
                full_path = f"thresholds.{self.pipeline_name}.{path}"
            
            # Navigate nested dict structure using dot-notation
            current = self.config
            for key in full_path.split('.'):
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    self._log_missing_threshold(full_path, default_value)
                    return default_value
            
            return current
            
        except Exception as e:
            print(f"Error getting threshold for {self.pipeline_name}.{path}: {e}")
            return default_value

    def _ensure_issue_ack_access(self, metrics: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Ensure every metrics payload carries issue_ack_access for downstream consumers.
        """
        normalized = metrics.copy() if metrics else {}
        if 'issue_ack_access' not in normalized and 'issue_owner' in normalized:
            normalized['issue_ack_access'] = normalized['issue_owner']
        return normalized
    
    def set_threshold(self, path: str, value: Any) -> bool:
        """
        Set a threshold value using dot notation path.
        
        Args:
            path: Dot notation path to threshold. Can be:
                  - Pipeline-specific: 'test_name.threshold_name' (e.g., 'check_employee_data_completeness.completeness_threshold')
                  - Global: 'global.category.threshold_name' (e.g., 'global.row_count_change.warn_percentage')
            value: New threshold value
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Handle global thresholds
            if path.startswith('global.'):
                full_path = f"thresholds.{path}"
            else:
                # Pipeline-specific thresholds
                full_path = f"thresholds.{self.pipeline_name}.{path}"
            
            # Navigate to the parent of the target key and set the value
            keys = full_path.split('.')
            current = self.config
            
            # Navigate to the parent container
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            
            # Set the final value
            current[keys[-1]] = value
            
            # Update in Firestore if available
            if self.firestore_client:
                from ..utils.config_manager import ConfigManager
                config_manager = ConfigManager(project_id=self._firestore_project_id)
                return config_manager.set_threshold(self.repository_name, full_path, value)
            
            return True
            
        except Exception as e:
            print(f"Error setting threshold for {self.pipeline_name}.{path}: {e}")
            return False

    def _log_missing_threshold(self, full_path: str, default_value: Any) -> None:
        """Surface helpful debugging info when a threshold path is missing."""
        try:
            thresholds = self.config.get('thresholds', {}) if isinstance(self.config, dict) else {}
            pipeline_thresholds = thresholds.get(self.pipeline_name, {})
            global_thresholds = thresholds.get('global', {})
            print(
                "[TestKit] Missing threshold path '{path}' for repo '{repo}' "
                "pipeline '{pipeline}'. Returning default: {default_value}".format(
                    path=full_path,
                    repo=self.repository_name,
                    pipeline=self.pipeline_name,
                    default_value=default_value,
                )
            )
            print(
                "[TestKit] Available pipeline thresholds keys: "
                f"{list(pipeline_thresholds.keys())}"
            )
            print(
                "[TestKit] Available global thresholds keys: "
                f"{list(global_thresholds.keys())}"
            )
        except Exception as log_exc:
            print(f"[TestKit] Failed to log missing threshold details: {log_exc}")
    
    def is_test_enabled(self, test_category: str) -> bool:
        """
        Check if a test category is enabled.
        
        Args:
            test_category: Test category to check (e.g., 'enable_schema_validation')
            
        Returns:
            True if test is enabled, False otherwise
        """
        try:
            return self.config.get('test_switches', {}).get(test_category, True)
        except Exception:
            return True
    
    def update_config_in_firestore(self, config_updates: Dict[str, Any]) -> bool:
        """Update configuration in Firestore for this repository."""
        if not self.firestore_client:
            print("Warning: Firestore client not available. Cannot update configuration.")
            return False
        
        try:
            repo_doc_ref = self.firestore_client.collection(self.collection_integrated_testing_config).document(self.repository_name)
            
            # Add metadata
            config_updates['last_updated'] = datetime.now(timezone.utc)
            config_updates['updated_by'] = os.getenv('USER', 'unknown')
            
            # Update the document
            repo_doc_ref.set(config_updates, merge=True)
            print(f"Configuration updated for repository: {self.repository_name}")
            
            # Reload configuration
            self.config = self._load_config_from_firestore()
            return True
            
        except Exception as e:
            print(f"Error updating configuration in Firestore: {e}")
            return False
    
    def _batch_update_acknowledgments(self):
        """
        Batch-update Firestore with all acknowledgeable issues to minimize API calls.
        
        Groups issues by test_name and performs one batch write per test instead of
        one write per issue. Processes both warnings and failures, preserving their
        status and metadata for the acknowledgment system.
        """
        # Group issues by test_name for efficient batch processing
        issues_by_test: Dict[str, List[Dict[str, Any]]] = {}
        
        # Process warnings: extract issue data and group by test
        for result in self.warnings:
            if not result.get('acknowledgeable', True):
                continue
            
            issue_identifier = result.get('issue_identifier')
            test_name = result.get('test_name')
            
            if not issue_identifier or not test_name:
                continue
            
            metrics = result.get('metrics', {}) or {}
            # Prefer explicit issue_details from metrics; fall back to message
            issue_details = metrics.get('issue_details', result.get('message', ""))
            additional_metadata = metrics.copy()
            # Include status so acknowledgment system knows this is a warning
            additional_metadata['status'] = result.get('status', 'WARN')
            
            issues_by_test.setdefault(test_name, []).append({
                "issue_identifier": issue_identifier,
                "issue_details": issue_details,
                "additional_metadata": additional_metadata,
            })
        
        # Process failures: same logic as warnings but with FAIL status
        for result in self.failures:
            if not result.get('acknowledgeable', True):
                continue
            
            issue_identifier = result.get('issue_identifier')
            test_name = result.get('test_name')
            
            if not issue_identifier or not test_name:
                continue
            
            metrics = result.get('metrics', {}) or {}
            issue_details = metrics.get('issue_details', result.get('message', ""))
            additional_metadata = metrics.copy()
            additional_metadata['status'] = result.get('status', 'FAIL')
            
            issues_by_test.setdefault(test_name, []).append({
                "issue_identifier": issue_identifier,
                "issue_details": issue_details,
                "additional_metadata": additional_metadata,
            })
        
        # Batch write all issues per test to Firestore
        for test_name, issues in issues_by_test.items():
            try:
                success = self.acknowledge_manager.batch_update_issue_detections(
                    test_name=test_name,
                    issues=issues,
                )
                if not success:
                    print(f"[WARNING] Failed to write {len(issues)} issues to Firestore for test '{test_name}'")
            except Exception as e:
                print(f"Warning: Could not batch update acknowledgment entries for test {test_name}: {e}")
                import traceback
                traceback.print_exc()
    
    def finalize_run(self):
        """
        Finalize test run: batch-update acknowledgments, send emails, log summary.
        
        Idempotent - safe to call multiple times (e.g., in finally blocks).
        Prevents duplicate emails and logs by checking finalized flag.
        
        Execution order:
        1. Batch-update Firestore with all acknowledgeable issues
        2. Send failure email (if new unacknowledged failures exist)
        3. Send warning digest (if new unacknowledged warnings exist)
        4. Log summary to sheet and console
        
        Returns:
            Dict with test summary:
                {
                    'total_tests': int,
                    'passed': int,
                    'warnings': int,
                    'failures': int,
                    'run_duration': str
                }
            
        Example:
            >>> try:
            ...     # Run tests...
            ...     testkit.log_warn('test1', 'issue1', 'Warning message')
            ...     testkit.log_pass('test2', 'All good')
            ... finally:
            ...     summary = testkit.finalize_run()
            ...     # Returns: {
            ...     #     'total_tests': 2,
            ...     #     'passed': 1,
            ...     #     'warnings': 1,
            ...     #     'failures': 0,
            ...     #     'run_duration': '0:05:23'
            ...     # }
        """
        # Early return if already finalized to prevent duplicate emails/logs
        if self.finalized:
            print("Test run already finalized - skipping duplicate finalization")
            summary = {
                'total_tests': len(self.test_results),
                'passed': len([r for r in self.test_results if r['status'] == 'PASS']),
                'warnings': len(self.warnings),
                'failures': len(self.failures),
                'run_duration': self._get_run_duration()
            }
            return summary
        
        try:
            # Step 1: Batch-write all test results to BigQuery
            self._write_test_results_to_bigquery()
            
            # Step 2: Batch-update Firestore with all acknowledgeable issues
            # This must happen before email filtering so acknowledgment status is current
            try:
                self._batch_update_acknowledgments()
            except Exception as e:
                print(f"Warning: Could not batch update acknowledgment entries: {e}")
            
            # Step 3: Send failure email (only if new unacknowledged failures exist)
            self._send_failure_summary_email()
                       
            # Step 4: Send warning digest (only if new unacknowledged warnings exist)
            self.send_warning_digest()
            
            # Step 5: Calculate and log summary statistics
            summary = {
                'total_tests': len(self.test_results),
                'passed': len([r for r in self.test_results if r['status'] == 'PASS']),
                'warnings': len(self.warnings),
                'failures': len(self.failures),
                'run_duration': self._get_run_duration()
            }
            
            print(f"üèÅ Test run completed: {summary}")
            
            # Mark as finalized to prevent duplicate execution
            self.finalized = True
            
            # Return summary for programmatic use
            return summary
            
        except Exception as e:
            print(f"Error finalizing test run: {e}")
            # Don't mark as finalized if there was an error, so it can be retried
            return None


# Convenience functions for easy import
def create_testkit(repository_name: str, pipeline_name: str = None, fail_on_error: bool = False, 
                   firestore_project_id: str = None, bigquery_client = None) -> TestKit:
    """Create a new TestKit instance."""
    return TestKit(repository_name, pipeline_name, fail_on_error, 
                   firestore_project_id=firestore_project_id, bigquery_client=bigquery_client)


def log_pass(testkit: TestKit, test_name: str, message: str = "", metrics: Dict[str, Any] = None, logging_message: str = None):
    """Log a passing test."""
    testkit.log_pass(test_name, message, metrics, logging_message)


def log_warn(testkit: TestKit, test_name: str, issue_identifier: str, message: str, metrics: Dict[str, Any] = None, acknowledgeable: bool = True, logging_message: str = None, email_per_alert: bool = False):
    """Log a warning."""
    testkit.log_warn(test_name, issue_identifier, message, metrics, acknowledgeable, logging_message, email_per_alert)


def log_fail(testkit: TestKit, test_name: str, issue_identifier: str, message: str, metrics: Dict[str, Any] = None, stop_pipeline: bool = None, acknowledgeable: bool = True, logging_message: str = None, email_per_alert: bool = False):
    """Log a failure."""
    testkit.log_fail(test_name, issue_identifier, message, metrics, stop_pipeline, acknowledgeable, logging_message, email_per_alert)
