# Quickstart

Get started with BehaviorCI in 5 minutes.

## Installation

```bash
pip install behaviorci
```

Verify installation:
```bash
behaviorci --version
# behaviorci, version 0.1.0
```

## Create Your First Bundle

Scaffold an example bundle:

```bash
behaviorci init bundles/my-first-test
```

This creates:
```
bundles/my-first-test/
â”œâ”€â”€ bundle.yaml       # Configuration
â”œâ”€â”€ prompt.md         # Prompt template
â”œâ”€â”€ dataset.jsonl     # Test cases
â””â”€â”€ schema.json       # Output schema
```

## Validate the Bundle

Check that your configuration is valid:

```bash
behaviorci validate bundles/my-first-test/bundle.yaml
```

Expected output:
```
âœ“ Bundle is valid: example-bundle

 Name        example-bundle 
 Version     1.0            
 Provider    mock           
 Model       (default)      
 Dataset     dataset.jsonl  
 Cases       3              
 Thresholds  1
```

## Run with Mock Provider

Test without API calls:

```bash
behaviorci run bundles/my-first-test/bundle.yaml
```

The mock provider returns deterministic responsesâ€”great for testing your setup.

## Run with Real LLM

Set your API key and run:

```bash
# OpenAI
export OPENAI_API_KEY=sk-your-key-here
behaviorci run bundles/my-first-test/bundle.yaml --provider openai

# Anthropic
export ANTHROPIC_API_KEY=your-key-here
behaviorci run bundles/my-first-test/bundle.yaml --provider anthropic
```

## Customize Your Bundle

### Edit the Prompt

Open `bundles/my-first-test/prompt.md`:

```markdown
You are a helpful assistant.

User question: {{ question }}

Respond with a JSON object:
{
  "answer": "your answer",
  "confidence": 0.95
}
```

Variables like `{{ question }}` come from your dataset.

### Add Test Cases

Edit `bundles/my-first-test/dataset.jsonl`:

```jsonl
{"input": {"question": "What is 2+2?"}}
{"input": {"question": "What is the capital of France?"}}
{"input": {"question": "Who wrote Hamlet?"}}
```

Each line is a test case. The `input` object provides template variables.

### Configure Thresholds

Edit `bundles/my-first-test/bundle.yaml`:

```yaml
name: my-first-test
version: "1.0"

prompt_path: prompt.md
dataset_path: dataset.jsonl

output_contract:
  schema_path: schema.json
  invariants:
    - "len(raw_output) < 500"

thresholds:
  - metric: pass_rate
    operator: ">="
    value: 0.9  # 90% of tests must pass

provider:
  name: openai
  model: gpt-4o-mini
  temperature: 0.0
```

## Add to CI

Create `.github/workflows/behavior-tests.yaml`:

```yaml
name: Behavior Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with:
        python-version: "3.11"
    - run: pip install behaviorci
    - run: behaviorci run bundles/my-first-test/bundle.yaml
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
```

Now PRs will fail if your LLM behavior regresses!

## Next Steps

- ğŸ“– [Behavior Bundles](behavior-bundles.md) â€” Full bundle specification
- ğŸ–¥ï¸ [CLI Reference](cli.md) â€” All commands and options
- ğŸ”§ [CI Integration](ci-integration.md) â€” GitHub, GitLab, CircleCI guides
- ğŸ—ï¸ [Architecture](architecture.md) â€” How BehaviorCI works
- â“ [FAQ](faq.md) â€” Common questions

## Example: Real-World Bundle

Here's a more complete example:

```yaml
# bundles/customer-support/bundle.yaml
name: customer-support-responses
version: "2.1"
description: Tests customer support bot response quality

prompt_path: support-prompt.md
dataset_path: support-cases.jsonl

output_contract:
  schema_path: response-schema.json
  invariants:
    - "len(raw_output) < 2000"
    - "'apolog' in raw_output.lower() or 'sorry' in raw_output.lower()"
    - "output.get('tone') in ['helpful', 'empathetic', 'professional']"

thresholds:
  - metric: pass_rate
    operator: ">="
    value: 0.95
  - metric: schema_valid_rate
    operator: ">="
    value: 0.99

provider:
  name: openai
  model: gpt-4o
  temperature: 0.0
  max_tokens: 512

timeout: 30.0
retries: 2
```
