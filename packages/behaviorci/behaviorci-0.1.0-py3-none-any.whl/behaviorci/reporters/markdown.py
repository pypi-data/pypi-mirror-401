"""Markdown reporter for GitHub-friendly output."""

from __future__ import annotations

from behaviorci.reporters.base import Reporter
from behaviorci.reporters.registry import register_reporter
from behaviorci.runner.engine import RunResult


@register_reporter("markdown")
class MarkdownReporter(Reporter):
    """Markdown reporter for GitHub-friendly output.

    Produces markdown suitable for:
    - GitHub PR comments
    - CI artifacts
    - Documentation
    """

    def emit(self, result: RunResult, verbose: bool = False) -> str:
        """Generate markdown report.

        Args:
            result: Run result to report
            verbose: Include case-by-case details

        Returns:
            Markdown string
        """
        lines: list[str] = []

        # Header with status badge
        status_badge = "✅ PASSED" if result.passed else "❌ FAILED"
        lines.append(f"# BehaviorCI Report: {result.bundle_name}")
        lines.append("")
        lines.append(f"**Status**: {status_badge}")
        lines.append(f"**Duration**: {result.duration_ms:.0f}ms")
        lines.append(f"**Provider**: {result.provider} ({result.model})")
        lines.append("")

        # Summary
        lines.append("## Summary")
        lines.append("")
        summary = result.summary
        lines.append("| Metric | Value |")
        lines.append("|--------|-------|")
        lines.append(f"| Total Cases | {summary['total_cases']} |")
        lines.append(f"| Passed | {summary['passed_cases']} |")
        lines.append(f"| Failed | {summary['failed_cases']} |")
        lines.append(f"| Pass Rate | {summary['pass_rate']:.1%} |")
        lines.append("")

        # Thresholds
        if result.threshold_evaluation and result.threshold_evaluation.results:
            lines.append("## Thresholds")
            lines.append("")
            lines.append("| Metric | Expected | Actual | Status |")
            lines.append("|--------|----------|--------|--------|")

            for t in result.threshold_evaluation.results:
                status = "✅" if t.passed else "❌"
                lines.append(
                    f"| {t.metric} | {t.operator} {t.expected_value:.2f} | {t.actual_value:.2f} | {status} |"
                )
            lines.append("")

        # Failed cases
        failed_cases = [c for c in result.case_results if not c.passed]
        if failed_cases:
            lines.append("## Failed Cases")
            lines.append("")

            for case in failed_cases[:10]:  # Limit to 10
                lines.append(f"### ❌ {case.case_id}")
                lines.append("")

                if case.error:
                    lines.append(f"**Error**: {case.error}")
                    lines.append("")

                if case.evaluation:
                    if case.evaluation.schema_errors:
                        lines.append("**Schema Errors**:")
                        for err in case.evaluation.schema_errors[:3]:
                            lines.append(f"- {err}")
                        lines.append("")

                    failed_invariants = [
                        inv for inv, passed in case.evaluation.invariants_passed.items() if not passed
                    ]
                    if failed_invariants:
                        lines.append("**Failed Invariants**:")
                        for inv in failed_invariants:
                            err = case.evaluation.invariant_errors.get(inv, "")
                            lines.append(f"- `{inv}`: {err}")
                        lines.append("")

                if verbose and case.output:
                    lines.append("<details>")
                    lines.append("<summary>Output</summary>")
                    lines.append("")
                    lines.append("```")
                    lines.append(case.output[:500])
                    if len(case.output) > 500:
                        lines.append("... (truncated)")
                    lines.append("```")
                    lines.append("</details>")
                    lines.append("")

            if len(failed_cases) > 10:
                lines.append(f"*...and {len(failed_cases) - 10} more failures*")
                lines.append("")

        # All cases table (verbose only)
        if verbose:
            lines.append("## All Cases")
            lines.append("")
            lines.append("| ID | Status | Latency |")
            lines.append("|----|--------|---------|")

            for case in result.case_results:
                status = "✅" if case.passed else "❌"
                latency = f"{case.latency_ms:.0f}ms" if case.latency_ms else "-"
                lines.append(f"| {case.case_id} | {status} | {latency} |")
            lines.append("")

        # Footer
        lines.append("---")
        lines.append(f"*Generated by BehaviorCI at {result.started_at.isoformat()}*")

        return "\n".join(lines)
