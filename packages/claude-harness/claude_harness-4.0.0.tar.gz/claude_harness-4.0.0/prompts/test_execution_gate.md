# Quality Gate #10: Test Execution (MANDATORY)

**The test must actually RUN and PASS - not just exist!**

---

## ğŸš¨ THE PROBLEM (From AutoGraph v3.1)

**What happened:**
```python
# Agent created test_save_diagram.py âœ…
# Agent marked feature #660 as passing âœ…
# Agent NEVER ran: python3 test_save_diagram.py âŒ
# Result: Test passes when run manually, but feature broken in browser!
```

**This is a FALSE POSITIVE - the worst kind of bug!**

---

## âœ… THE SOLUTION

### Step 1: Create Test (Already Required)

```python
# Agent creates test file
# test_feature_X.py or test_feature_X.spec.ts
```

---

### Step 2: **EXECUTE THE TEST (NEW - MANDATORY!)**

```bash
#!/bin/bash
# Execute test and verify it passes

echo "Running test for feature..."

# Detect test type and run appropriately
if [ -f "test_*.py" ]; then
    # Python test
    python3 test_*.py
    test_result=$?
elif [ -f "test_*.spec.ts" ] || [ -f "test_*.test.js" ]; then
    # JavaScript/TypeScript test
    npm test
    test_result=$?
elif [ -f "test_*.sh" ]; then
    # Bash test
    bash test_*.sh
    test_result=$?
else
    echo "âŒ No test file found!"
    exit 1
fi

# Check result
if [ $test_result -eq 0 ]; then
    echo "âœ… Test PASSED"
else
    echo "âŒ Test FAILED - cannot mark feature as passing!"
    echo "Fix the implementation until test passes!"
    exit 1
fi
```

---

### Step 3: Verify Test Output

```markdown
**The test must:**
- Exit with code 0 (success)
- Print "âœ… PASSING" or similar
- Show test steps completed
- No errors in output

**NOT acceptable:**
- Test file exists but wasn't run
- Test skipped or commented out
- Test fails but ignored
- Test mocks everything (not real)
```

---

### Step 4: Verify in Context (Not Just Isolation)

**For web apps:**
```markdown
Test must verify in BROWSER (not just curl!)

1. Run test script (creates data)
2. Open browser (http://localhost:PORT)
3. Login with test user
4. Verify feature works in UI
5. Check browser console (zero errors)

NOT enough:
âŒ curl to API works (but browser fails!)
âŒ Test script passes (but real user can't use it!)
```

**For CLIs:**
```markdown
Test must verify actual CLI usage

1. Run CLI command
2. Verify output correct
3. Run related commands (list/show/etc.)
4. Verify data accessible
5. Restart CLI
6. Data still there

NOT enough:
âŒ Internal function works (but CLI command broken!)
âŒ Test in Python passes (but bash command fails!)
```

---

## ğŸ¯ Enforcement Strategy

**Add to coding_prompt.md (before marking passing):**

```markdown
### STEP X: EXECUTE AND VERIFY TESTS (MANDATORY!)

**You created a test - now RUN it!**

```bash
# Find the test file you created
test_file=$(ls -t test_*.py test_*.spec.ts test_*.sh 2>/dev/null | head -1)

if [ -z "$test_file" ]; then
    echo "âŒ No test file found - create test first!"
    exit 1
fi

echo "Executing test: $test_file"

# Run the test based on type
case "$test_file" in
    *.py)
        python3 "$test_file"
        ;;
    *.spec.ts|*.test.js|*.test.ts)
        npm test "$test_file"
        ;;
    *.sh)
        bash "$test_file"
        ;;
esac

if [ $? -ne 0 ]; then
    echo "âŒ TEST FAILED!"
    echo "Fix implementation until test passes!"
    echo "DO NOT mark feature as passing!"
    exit 1
fi

echo "âœ… Test executed and PASSED"
```

**Verification:**
1. Test executed âœ…
2. Test passed âœ…
3. No errors âœ…

**Only NOW can you mark "passes": true**

**NEVER mark passing if:**
- âŒ Test wasn't run
- âŒ Test failed
- âŒ Test skipped
- âŒ "Will test later"
```

---

## ğŸ“Š Examples from AutoGraph

### âŒ What Agent Did (WRONG):

```markdown
Session 3:
- Created test_save_diagram.py âœ…
- Marked feature #660 as passing âœ…
- NEVER ran the test âŒ

Result: False positive!
```

### âœ… What Agent SHOULD Do (CORRECT):

```markdown
Session 3:
- Create test_save_diagram.py âœ…
- RUN: python3 test_save_diagram.py âœ…
- Test PASSES âœ…
- Verify in browser (open and test) âœ…
- Browser works âœ…
- THEN mark feature #660 as passing âœ…

Result: True positive!
```

---

## ğŸ¯ Success Criteria

**Feature is ONLY passing when:**
1. âœ… Test file created
2. âœ… Test executed
3. âœ… Test passed (exit 0)
4. âœ… Verified in actual interface (browser/CLI/etc.)
5. âœ… Data persists correctly
6. âœ… No errors in logs/console

**ALL 6 must be true!**

---

**This gate prevents false positives like we saw in AutoGraph!**

