import os
import unittest.mock
import pandas as pd
import json
from typing import Optional, Any
from collections import Counter
from contextlib import contextmanager

from medcat import cat
from medcat.data.model_card import ModelCard
from medcat.vocab import Vocab
from medcat.config import Config
from medcat.config.config_meta_cat import ConfigMetaCAT
from medcat.model_creation.cdb_maker import CDBMaker
from medcat.cdb import CDB
from medcat.tokenizing.tokens import UnregisteredDataPathException
from medcat.tokenizing.tokenizers import TOKENIZER_PREFIX
from medcat.utils.cdb_state import captured_state_cdb
from medcat.components.addons.meta_cat import MetaCATAddon
from medcat.utils.defaults import AVOID_LEGACY_CONVERSION_ENVIRON
from medcat.utils.defaults import LegacyConversionDisabledError
from medcat.utils.config_utils import temp_changed_config
from medcat.plugins.registry import create_empty_reg_comps, plugin_registry, PluginInfo
from medcat.components.types import CoreComponentType, AbstractCoreComponent
from medcat.components.addons.addons import AddonComponent

import unittest
from unittest.mock import MagicMock
import tempfile
import pickle
import shutil

from . import EXAMPLE_MODEL_PACK_ZIP
from . import V1_MODEL_PACK_PATH, UNPACKED_V1_MODEL_PACK_PATH
from .utils.legacy.test_conversion_all import ConvertedFunctionalityTests


orig_init = cat.CAT.__init__

expected_model_pack_path = EXAMPLE_MODEL_PACK_ZIP.replace(".zip", "")


class ModelLoadTests(unittest.TestCase):

    def assert_has_model_name(self, func):

        def wrapper(*args, **kwargs):
            if 'model_load_path' in kwargs:
                self.assertEqual(kwargs['model_load_path'],
                                 expected_model_pack_path)
            else:
                self.assertEqual(args[-1], expected_model_pack_path)
            return func(*args, **kwargs)
        return wrapper

    def setUp(self):
        cat.CAT.__init__ = self.assert_has_model_name(cat.CAT.__init__)

    def tearDown(self):
        cat.CAT.__init__ = orig_init

    def test_loaded_model_knows_model_path(self):
        # NOTE: the assertion is checked due to wrapper on CAT.__init__
        inst = cat.CAT.load_model_pack(EXAMPLE_MODEL_PACK_ZIP)
        self.assertIsInstance(inst, cat.CAT)

    def test_can_load_CDB_from_model_pack(self):
        cdb = cat.CAT.load_cdb(EXAMPLE_MODEL_PACK_ZIP)
        self.assertIsInstance(cdb, CDB)

    def test_can_load_model_card_off_disk_from_zip_to_json(self):
        out = cat.CAT.load_model_card_off_disk(
            EXAMPLE_MODEL_PACK_ZIP, as_dict=False)
        self.assertIsInstance(out, str)

    def test_can_load_model_card_off_disk_from_folder_to_dict(self):
        # NOTE: the model gets unpacked automatically due to __init__.py
        out = cat.CAT.load_model_card_off_disk(
            expected_model_pack_path, as_dict=True)
        self.assertIsInstance(out, dict)

    def test_can_load_model_ard_without_unzipping(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            zip_path = os.path.join(temp_dir, "model_pazk.zip")
            # copy to another location to avoid a previoulsy unpacked model
            shutil.copy(EXAMPLE_MODEL_PACK_ZIP, zip_path)
            out = cat.CAT.load_model_card_off_disk(
                expected_model_pack_path, avoid_unpack=True)
            # make sure the folder doesn't exist
            self.assertFalse(os.path.exists(zip_path.removesuffix(".zip")))
            self.assertIsInstance(out, str)


class ModelLoadIWithHiddenFilesTests(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls._temp_dir = tempfile.TemporaryDirectory()
        cls.model_path = os.path.join(cls._temp_dir.name, "model")
        shutil.copytree(expected_model_pack_path, cls.model_path)
        # add file
        file_path = os.path.join(cls.model_path, '.some_file.txt')
        with open(file_path, 'w') as f:
            f.write("Nothing")
        # add folder
        folder_path = os.path.join(cls.model_path, '.some_folder')
        os.mkdir(folder_path)

    @classmethod
    def tearDownClass(cls):
        cls._temp_dir.cleanup()

    def test_can_load_with_add_hidden_files_and_folders(self):
        # try load
        inst = cat.CAT.load_model_pack(self.model_path)
        self.assertIsInstance(inst, cat.CAT)


class TrainedModelTests(unittest.TestCase):
    TRAINED_MODEL_PATH = EXAMPLE_MODEL_PACK_ZIP

    @classmethod
    def setUpClass(cls):
        cls.model = cat.CAT.load_model_pack(cls.TRAINED_MODEL_PATH)
        if cls.model.config.components.linking.train:
            cls.model.config.components.linking.train = False


class ConfigMergeTests(unittest.TestCase):
    spacy_model_name = 'en_core_web_lg'
    model_dict = {
        "general": {'nlp': {"modelname": spacy_model_name}}
    }

    def test_can_merge_config(self):
        model = cat.CAT.load_model_pack(
            EXAMPLE_MODEL_PACK_ZIP, config_dict=self.model_dict)
        # NOTE: this is converted to a (non-existent) path
        self.assertIn(
            self.spacy_model_name, model.config.general.nlp.modelname)


class OntologiesMapTests(TrainedModelTests):

    def test_does_not_have_auto(self):
        self.assertNotEqual(self.model.config.general.map_to_other_ontologies,
                            "auto")

    def test_is_empty(self):
        self.assertFalse(self.model.config.general.map_to_other_ontologies)


class OntologiesMapWithOntologiesTests(TrainedModelTests):
    MY_ONT_NAME = "My_Ontology"
    EXP_GET = [MY_ONT_NAME]
    MY_ONT_MAPPING = {
        # mapping doens't matter here, really
        "ABC": "BBC"
    }

    @classmethod
    def reset_mappings(cls):
        # set to auto
        cls.model.config.general.map_to_other_ontologies = "auto"
        # redo process
        cls.model._set_and_get_mapped_ontologies()

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        # add "mapping"
        cls.model.cdb.addl_info[f"cui2{cls.MY_ONT_NAME}"] = cls.MY_ONT_MAPPING
        cls.reset_mappings()

    def test_has_correct_results(self):
        got = sorted(self.model.config.general.map_to_other_ontologies)
        self.assertEqual(len(got), len(self.EXP_GET))
        self.assertEqual(got, self.EXP_GET)


class OntologiesMapWithOntologiesAndNoIgnoresTests(
        OntologiesMapWithOntologiesTests):
    EXTRA_ONTS = ["original_names"]

    @classmethod
    def reset_mappings(cls):
        # set to auto
        cls.model.config.general.map_to_other_ontologies = "auto"
        # redo process
        cls.model._set_and_get_mapped_ontologies(ignore_set=set())

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        # I need to redefine for specific class
        # instead of changing instance in base class
        cls.EXP_GET = OntologiesMapWithOntologiesTests.EXP_GET.copy()
        cls.EXP_GET.extend(cls.EXTRA_ONTS)
        cls.EXP_GET.sort()
        cls.reset_mappings()


class OntologiesMapWithOntologiesAndAllowEmpty(
        OntologiesMapWithOntologiesAndNoIgnoresTests):
    EXTRA_ONTS = ["icd10", "opcs4"]

    @classmethod
    def reset_mappings(cls):
        # set to auto
        cls.model.config.general.map_to_other_ontologies = "auto"
        # redo process
        cls.model._set_and_get_mapped_ontologies(ignore_empty=False)


class InferenceFromLoadedTests(TrainedModelTests):

    def test_can_load_model(self):
        self.assertIsInstance(self.model, cat.CAT)

    def test_has_training(self):
        self.assertTrue(self.model.cdb.cui2info)
        self.assertTrue(self.model.cdb.name2info)

    def test_inference_works(self):
        ents = self.model.get_entities(
            ConvertedFunctionalityTests.TEXT)['entities']
        for nr, ent in enumerate(ents.values()):
            with self.subTest(f"{nr}"):
                ConvertedFunctionalityTests.assert_has_ent(ent)

    @classmethod
    @contextmanager
    def _faster_spacy_inference(cls):
        with temp_changed_config(
            cls.model.config.general.nlp,
            "faster_spacy_tokenization",
            True
        ):
            with temp_changed_config(
                cls.model.config.general.nlp,
                "modelname",
                "en_core_web_md"
            ):
                cls.model._recreate_pipe()
                yield
        cls.model._recreate_pipe()

    def _is_spacy_model(self):
        if self.model.config.general.nlp.provider != "spacy":
            raise unittest.SkipTest("Only applicable for spacy models")

    def test_default_spacy_runs_pipe(self):
        self._is_spacy_model()
        self.assertFalse(self.model.pipe._tokenizer._avoid_pipe)

    def test_faster_spacy_inference_is_set(self):
        self._is_spacy_model()
        with self._faster_spacy_inference():
            self.assertTrue(self.model.pipe._tokenizer._avoid_pipe)

    def test_faster_spacy_inference_works(self):
        self._is_spacy_model()
        with self._faster_spacy_inference():
            ents = self.model.get_entities(
                ConvertedFunctionalityTests.TEXT)['entities']
            self.assertTrue(ents)
            for nr, ent in enumerate(ents.values()):
                with self.subTest(f"{nr}"):
                    ConvertedFunctionalityTests.assert_has_ent(ent)

    def test_faster_spacy_inference_is_used(self):
        self._is_spacy_model()
        with self._faster_spacy_inference():
            with unittest.mock.patch.object(
                    self.model.pipe._tokenizer._nlp,
                    '__call__') as dunder_call_mock:
                with unittest.mock.patch.object(
                        self.model.pipe._tokenizer._nlp,
                        'make_doc') as make_doc_mock:
                    self.model.get_entities(
                        ConvertedFunctionalityTests.TEXT)
                    dunder_call_mock.assert_not_called()
                    make_doc_mock.assert_called()

    def test_entities_in_correct_order(self):
        # NOTE: the issue wouldn't show up with smaller amount of text
        doc = self.model(ConvertedFunctionalityTests.TEXT * 3)
        cur_start = 0
        for ent in doc.linked_ents:
            with self.subTest(f"Ent: {ent}"):
                self.assertGreaterEqual(ent.base.start_char_index, cur_start)
                cur_start = ent.base.start_char_index


class InferenceIntoOntologyTests(TrainedModelTests):
    ont_name = "FAKE_ONT"

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        # create mapping
        cls.ont_map = {
            cui: [f"{cls.ont_name}:{cui}"]
            for cui in cls.model.cdb.cui2info
        }
        # add to addl_info
        cls.model.cdb.addl_info[f"cui2{cls.ont_name}"] = cls.ont_map
        # ask to be mapped
        cls.model.config.general.map_to_other_ontologies.append(cls.ont_name)

    def assert_has_mapping(self, ent: dict):
        # has value
        self.assertIn(self.ont_name, ent)
        val = ent[self.ont_name]
        # 1 value
        self.assertEqual(len(val), 1)
        # value in our map
        self.assertIn(val, self.ont_map.values())

    def test_gets_mappings(self):
        ents = self.model.get_entities(
            ConvertedFunctionalityTests.TEXT)['entities']
        for nr, ent in enumerate(ents.values()):
            with self.subTest(f"{nr}"):
                self.assert_has_mapping(ent)


class CATIncludingTests(unittest.TestCase):
    TOKENIZING_PROVIDER = 'regex'
    EXPECT_TRAIN = {}

    # paths
    VOCAB_DATA_PATH = os.path.join(
        os.path.dirname(__file__), 'resources', 'vocab_data.txt'
    )
    CDB_PREPROCESSED_PATH = os.path.join(
        os.path.dirname(__file__), 'resources', 'preprocessed4cdb.txt'
    )

    @classmethod
    def setUpClass(cls):

        # vocab

        vocab = Vocab()
        vocab.add_words(cls.VOCAB_DATA_PATH)

        # CDB
        config = Config()

        # tokenizer
        config.general.nlp.provider = cls.TOKENIZING_PROVIDER

        maker = CDBMaker(config)

        cls.cdb: CDB = maker.prepare_csvs([cls.CDB_PREPROCESSED_PATH])

        # usage monitoring
        cls._temp_logs_folder = tempfile.TemporaryDirectory()
        config.general.usage_monitor.enabled = True
        config.general.usage_monitor.log_folder = cls._temp_logs_folder.name

        # CAT
        cls.cat = cat.CAT(cls.cdb, vocab)
        cls.cat.config.components.linking.train = False

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        cls._temp_logs_folder.cleanup()

    def tearDown(self):
        # remove existing contents / empty file log file
        log_file_path = self.cat.usage_monitor.log_file
        if os.path.exists(log_file_path):
            os.remove(log_file_path)


class CATCreationTests(CATIncludingTests):
    # should be persistent as long as we don't change the underlying model
    EXPECTED_HASH = "558019fd37ed2167"

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.prev_hash = cls.cat.config.meta.hash

    @classmethod
    def get_cui2ct(cls, cat: Optional[cat.CAT] = None):
        if cat is None:
            cat = cls.cat
        return {
            cui: info['count_train'] for cui, info in cat.cdb.cui2info.items()
            if info['count_train']}

    def test_has_expected_training(self):
        self.assertEqual(self.get_cui2ct(), self.EXPECT_TRAIN)

    def test_versioning_updates_config_hash(self):
        self.assert_hashes_to(self.EXPECTED_HASH)

    def assert_hashes_to(self, exp_hash: str) -> None:
        self.cat._versioning(None)
        new_hash = self.cat.config.meta.hash
        self.assertNotEqual(self.prev_hash, new_hash)
        self.assertEqual(new_hash, exp_hash)
        self.assertEqual(self.cat.config.meta.history[-1], new_hash)

    def test_versioning_does_not_overpopulate_history(self):
        # run multiple times
        self.cat._versioning(None)
        self.cat._versioning(None)
        # and expect it not to append multiple times in the history
        # if there were multiple instances, the set would remove duplicates
        sorted_set = sorted(set(self.cat.config.meta.history))
        sorted_list = sorted(self.cat.config.meta.history)
        self.assertEqual(sorted_set, sorted_list)

    def test_can_get_model_card_str(self):
        model_card = self.cat.get_model_card(as_dict=False)
        self.assertIsInstance(model_card, str)

    def test_can_get_model_card_dict(self):
        model_card = self.cat.get_model_card(as_dict=True)
        self.assertIsInstance(model_card, dict)

    def test_model_card_has_required_keys(self):
        model_card = self.cat.get_model_card(as_dict=True)
        for ann in ModelCard.__annotations__:
            with self.subTest(f"Ann: {ann}"):
                self.assertIn(ann, model_card)

    def test_model_card_has_no_extra_keys(self):
        model_card = self.cat.get_model_card(as_dict=True)
        for key in model_card:
            with self.subTest(f"Key: {key}"):
                self.assertIn(key, ModelCard.__annotations__)


class CatWithMetaCATTests(CATCreationTests):
    EXPECTED_HASH = "8c3de3f171a87132"
    EXPECT_SAME_INSTANCES = True

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        meta_cat_cnf = ConfigMetaCAT()
        # NOTE: need to set for consistent hashing
        meta_cat_cnf.train.last_train_on = -1.0
        meta_cat_cnf.general.category_name = 'Status'
        meta_cat_cnf.general.tokenizer_name = 'bert-tokenizer'
        meta_cat_cnf.model.model_name = 'bert'
        meta_cat_cnf.model.model_variant = 'prajjwal1/bert-tiny'
        cls.addon = MetaCATAddon.create_new(
            meta_cat_cnf, cls.cat._pipeline.tokenizer)
        cls.cat.add_addon(cls.addon)
        cls.init_addons = list(cls.cat._pipeline._addons)

    def test_can_recreate_pipe(self):
        self.cat._recreate_pipe()
        addons_after = list(self.cat._pipeline._addons)
        self.assertGreater(len(self.init_addons), 0)
        self.assertEqual(len(self.init_addons), len(addons_after))
        if self.EXPECT_SAME_INSTANCES:
            self.assertEqual(self.init_addons, addons_after)
        else:
            # otherwise they should differ
            self.assertNotEqual(self.init_addons, addons_after)

    def test_get_entities_gets_monitored(self,
                                         text="Some text"):
        repeats = self.cat.config.general.usage_monitor.batch_size
        # ensure something gets written to the file
        for _ in range(repeats):
            self.cat.get_entities(text)
        log_file_path = self.cat.usage_monitor.log_file
        self.assertTrue(os.path.exists(log_file_path))
        with open(log_file_path) as f:
            contents = f.readline()
        self.assertTrue(contents)

    def test_get_entities_logs_usage(
            self,
            text="The dog is sitting outside the house."):
        # clear usage monitor buffer
        self.cat.usage_monitor.log_buffer.clear()
        self.cat.get_entities(text)
        self.assertTrue(self.cat.usage_monitor.log_buffer)
        self.assertEqual(len(self.cat.usage_monitor.log_buffer), 1)
        line = self.cat.usage_monitor.log_buffer[0]
        # the 1st element is the input text length
        input_text_length = line.split(",")[1]
        self.assertEqual(str(len(text)), input_text_length)


class CatWithMetaCATSaveLoadTests(CatWithMetaCATTests):

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.temp_dir = tempfile.TemporaryDirectory()
        cls.mpp = cls.cat.save_model_pack(cls.temp_dir.name)

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        cls.temp_dir.cleanup()

    def test_can_save_pack(self):
        self.assertTrue(os.path.exists(self.mpp))

    def test_can_load_saved(self):
        loaded = cat.CAT.load_model_pack(self.mpp)
        self.assertIsInstance(loaded, cat.CAT)
        # test that it has an addon
        addons = list(loaded._pipeline.iter_addons())
        self.assertEqual(len(addons), 1)
        addon = addons[0]
        self.assertIsInstance(addon, MetaCATAddon)
        # test that loaded model pack has the same addon config as the addon
        self.assertIs(addon.config, loaded.config.components.addons[0])

    def test_can_load_meta_cat(self):
        addons = cat.CAT.load_addons(self.mpp)
        self.assertEqual(len(addons), 1)
        _, addon = addons[0]
        self.assertIsInstance(addon, MetaCATAddon)

    def test_can_load_meta_cat_with_addon_cnf(self, seed: int = -41):
        mc: MetaCATAddon = cat.CAT.load_addons(
            self.mpp, addon_config_dict={
                "meta_cat.Status": {
                    "general": {"seed": seed}}})[0][1]
        self.assertEqual(mc.config.general.seed, seed)

    def test_can_merge_cnf_upon_load(self, use_seed: int = -4):
        loaded = cat.CAT.load_model_pack(
            self.mpp,
            addon_config_dict={
                "meta_cat.Status": {"general": {"seed": use_seed}}
            })
        addon: MetaCATAddon = list(loaded._pipeline.iter_addons())[0]
        self.assertEqual(addon.config.general.seed, use_seed)


class CatWithChangesMetaCATTests(CatWithMetaCATTests):
    EXPECTED_HASH = "0b22401059a08380"
    EXPECT_SAME_INSTANCES = False

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.addon.config.general.batch_size_eval = 10


class CATUnsupTrainingTests(CATCreationTests):
    SELF_SUPERVISED_DATA_PATH = os.path.join(
        os.path.dirname(__file__), 'resources', 'selfsupervised_data.txt'
    )
    EXPECT_TRAIN = {'C01': 2, 'C02': 2, 'C03': 2, 'C04': 1, 'C05': 1}
    # NOTE: should remain consistent unless we change the model or data
    EXPECTED_HASH = "e9989cc2dde739ff"

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        data = pd.read_csv(cls.SELF_SUPERVISED_DATA_PATH)
        cls.cat.trainer.train_unsupervised(data.text.values)

    def test_lists_unsup_train_in_config(self):
        self.assertTrue(self.cat.config.meta.unsup_trained)


class CATSupTrainingTests(CATUnsupTrainingTests):
    SUPERVISED_DATA_PATH = os.path.join(
        os.path.dirname(__file__), 'resources', 'supervised_mct_export.json'
    )
    # NOTE: should remain consistent unless we change the model or data
    EXPECTED_HASH = "7bfe01e8e36eb07d"

    @classmethod
    def _get_cui_counts(cls) -> dict[str, int]:
        counter = Counter()
        data = cls._get_data()
        for proj in data['projects']:
            for doc in proj['documents']:
                for ann in doc['annotations']:
                    counter[ann['cui']] += 1
        return counter

    @classmethod
    def _get_data(cls) -> dict:
        with open(cls.SUPERVISED_DATA_PATH) as f:
            return json.load(f)

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        # copy from parent
        cls.EXPECT_TRAIN = CATUnsupTrainingTests.EXPECT_TRAIN.copy()
        cui_counts_in_data = cls._get_cui_counts()
        # add extra CUIs in supervised training example
        for cui, extra_cnt in cui_counts_in_data.items():
            cls.EXPECT_TRAIN[cui] += extra_cnt
        cls._perform_training()

    @classmethod
    def _perform_training(cls):
        data = cls._get_data()
        cls.cat.trainer.train_supervised_raw(data)

    def test_lists_sup_train_in_config(self):
        self.assertTrue(self.cat.config.meta.sup_trained)

    def test_clearing_training_works(self):
        with captured_state_cdb(self.cat.cdb):
            self.cat.cdb.reset_training()
            self.assertEqual(self.cat.cdb.get_cui2count_train(), {})
            self.assertEqual(self.cat.cdb.get_name2count_train(), {})
            self.assertEqual(self.cat.config.meta.unsup_trained, [])
            self.assertEqual(self.cat.config.meta.sup_trained, [])


class CATWithDictNERSupTrainingTests(CATSupTrainingTests):
    from medcat.components.types import CoreComponentType
    from medcat.components.ner.dict_based_ner import NER as DNER
    from medcat.components.ner.vocab_based_ner import NER as VNER

    @classmethod
    def _dummy_pt(cls):
        pass

    @classmethod
    def setUpClass(cls):
        # NOTE: need to do training AFTER changes
        #       so stopping it from happening here
        orig_training = cls._perform_training
        cls._perform_training = cls._dummy_pt
        super().setUpClass()
        cls._perform_training = orig_training
        cls.cdb.config.components.ner.comp_name = 'dict'
        cls.cat._recreate_pipe()
        # cls.cat.cdb.reset_training()
        cls._perform_training()

    def test_has_dict_based_ner(self):
        comp = self.cat._pipeline.get_component(self.CoreComponentType.ner)
        self.assertNotIsInstance(comp, self.VNER)
        self.assertIsInstance(comp, self.DNER)

    def test_can_get_entities(self,
                              expected_cuis: list[str] = ['C01', 'C05']):
        _ents = self.cat.get_entities(
            "The fittest most fit of chronic kidney failure", only_cui=True)
        ents = _ents['entities']
        self.assertEqual(len(ents), len(expected_cuis))
        self.assertEqual(set(ents.values()), set(expected_cuis))

    def test_can_get_multiple_entities(self):
        texts = [
            "The fittest most fit of chronic kidney failure",
            "The dog is sitting outside the house."
        ]*10
        ents = list(self.cat.get_entities_multi_texts(
            texts, batch_size=2, batch_size_chars=-1))
        self.assert_ents(ents, texts)

    def assert_ents(self, ents: list[tuple], texts: list[str]):
        self.assertEqual(len(ents), len(texts))
        # NOTE: text IDs are integers starting from 0
        exp_ids = set(str(i) for i in range(len(texts)))
        for ent_id_str, ent in ents:
            with self.subTest(f"Entity: {ent_id_str} [{ent}]"):
                self.assertIn(ent_id_str, exp_ids)

    def test_can_multiprocess_empty(self):
        texts = []
        ents = list(self.cat.get_entities_multi_texts(texts, n_process=3))
        self.assert_ents(ents, texts)

    def test_can_get_multiprocess(self):
        texts = [
            "The fittest most fit of chronic kidney failure",
            "The dog is sitting outside the house."
        ]*10
        ents = list(self.cat.get_entities_multi_texts(
            texts, n_process=3, batch_size=2, batch_size_chars=-1))
        self.assert_ents(ents, texts)

    def _do_mp_run_with_save(
            self, save_to: str,
            chars_per_batch: int = 165,
            batches_per_save: int = 5,
            exp_parts: int = 8,
            n_process: int = 1,
            ) -> tuple[list[str], list[tuple], dict[str, Any], int]:
        in_data = [
            f"The patient presented with {name} and "
            f"did not have {negname}"
            for name in self.cdb.name2info
            for negname in self.cdb.name2info if name != negname
        ]
        out_data = self.cat.get_entities_multi_texts(
            in_data,
            save_dir_path=save_to,
            batch_size_chars=chars_per_batch,
            batches_per_save=batches_per_save,
            n_process=n_process,
            )
        out_data = list(out_data)
        out_dict_all = {
            key: cdata for key, cdata in out_data
        }
        return in_data, out_data, out_dict_all, exp_parts

    def assert_mp_runs_with_save_and_load(
            self, save_to: str,
            chars_per_batch: int = 165,
            batches_per_save: int = 5,
            exp_parts: int = 8,
            n_process: int = 1,
            ) -> tuple[
                tuple[list[str], list[tuple], dict[str, Any], int],
                tuple[tuple[list[str], int], list[str], int],
            ]:
        in_data, out_data, out_dict_all, exp_parts = (
            self._do_mp_run_with_save(
                save_to, chars_per_batch, batches_per_save, exp_parts,
                n_process=n_process))
        anns_file = os.path.join(save_to, 'annotated_ids.pickle')
        self.assertTrue(os.path.exists(anns_file))
        with open(anns_file, 'rb') as f:
            loaded_data = pickle.load(f)
        self.assertEqual(len(loaded_data), 2)
        ids, last_part_num = loaded_data
        return (in_data, out_data, out_dict_all, exp_parts), (
            loaded_data, ids, last_part_num)

    def assert_mp_runs_save_load_gather(
            self, save_to: str,
            chars_per_batch: int = 165,
            batches_per_save: int = 5,
            exp_parts: int = 8,
            n_process: int = 1,
            ) -> tuple[
                tuple[list[str], list[tuple], dict[str, Any], int],
                tuple[tuple[list[str], int], list[str], int],
                dict[str, Any]
            ]:
        (in_data, out_data, out_dict_all, exp_parts), (
            loaded_data, ids, num_last_part
        ) = self.assert_mp_runs_with_save_and_load(
            save_to, chars_per_batch, batches_per_save, exp_parts,
            n_process=n_process)
        all_loaded_output = {}
        for num in range(num_last_part + 1):
            with self.subTest(f"Part {num}"):
                part_name = f"part_{num}.pickle"
                part_path = os.path.join(save_to, part_name)
                self.assertTrue(os.path.exists(part_path))
                with open(part_path, 'rb') as f:
                    part_data = pickle.load(f)
                self.assertIsInstance(part_data, dict)
                self.assertTrue(
                    all(key not in all_loaded_output for key in part_data))
                all_loaded_output.update(part_data)
        return (in_data, out_data, out_dict_all, exp_parts), (
            loaded_data, ids, num_last_part), all_loaded_output

    def test_multiprocessing_can_save_indices(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            (in_data, out_data,
             out_dict_all, exp_parts), (
                 loaded_data, ids, num_last_part
             ) = self.assert_mp_runs_with_save_and_load(temp_dir)
            self.assertEqual(len(out_data), len(in_data))
            self.assertEqual(len(in_data), len(ids))

    def test_mp_saves_all_parts(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            (in_data, out_data,
             out_dict_all, exp_parts), (
                 loaded_data, ids, num_last_part
             ), all_loaded_output = self.assert_mp_runs_save_load_gather(
                 temp_dir)
            # NOTE: the number of parts is 1 greater
            self.assertEqual(num_last_part + 1, exp_parts)

    def assert_correct_loaded_output(
            self,
            in_data: list[str],
            out_dict_all: dict[str, Any],
            all_loaded_output: dict[str, Any]):
        self.assertEqual(len(all_loaded_output), len(in_data))
        self.assertEqual(all_loaded_output.keys(), out_dict_all.keys())
        self.assertEqual(all_loaded_output, out_dict_all)

    def test_mp_saves_correct_data(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            (in_data, out_data,
             out_dict_all, exp_parts), (
                 loaded_data, ids, num_last_part
             ), all_loaded_output = self.assert_mp_runs_save_load_gather(
                 temp_dir)
            self.assert_correct_loaded_output(
                in_data, out_dict_all, all_loaded_output)

    def test_mp_saves_correct_data_with_2_proc(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            (in_data, out_data,
             out_dict_all, exp_parts), (
                 loaded_data, ids, num_last_part
             ), all_loaded_output = self.assert_mp_runs_save_load_gather(
                 temp_dir, n_process=2)
            self.assert_correct_loaded_output(
                in_data, out_dict_all, all_loaded_output)

    def test_mp_saves_correct_data_with_3_proc(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            (in_data, out_data,
             out_dict_all, exp_parts), (
                 loaded_data, ids, num_last_part
             ), all_loaded_output = self.assert_mp_runs_save_load_gather(
                 temp_dir, n_process=3)
            self.assert_correct_loaded_output(
                in_data, out_dict_all, all_loaded_output)

    def test_get_entities_multi_texts_with_save_dir_lazy(self):
        texts = ["text1", "text2"]
        with tempfile.TemporaryDirectory() as tmp_dir:
            out = self.cat.get_entities_multi_texts(
                texts,
                save_dir_path=tmp_dir)
            # nothing before manual iter
            self.assertFalse(os.listdir(tmp_dir))
            out_list = list(out)
            # something was saved
            self.assertTrue(os.listdir(tmp_dir))
            # and something was yielded
            self.assertEqual(len(out_list), len(texts))

    def test_save_entities_multi_texts(self):
        texts = ["text1", "text2"]
        with tempfile.TemporaryDirectory() as tmp_dir:
            self.cat.save_entities_multi_texts(
                texts,
                save_dir_path=tmp_dir)
            # stuff was already saved
            self.assertTrue(os.listdir(tmp_dir))


class CATWithDocAddonTests(CATIncludingTests):
    EXAMPLE_TEXT = "Example text to tokenize"
    ADDON_PATH = 'SMTH'
    EXAMPLE_VALUE = 'something else'

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        doc = cls.cat(cls.EXAMPLE_TEXT)
        cls.doc_cls = doc.__class__
        cls.doc_cls.register_addon_path(cls.ADDON_PATH)
        # add for MutableEntity as well
        doc[:].register_addon_path(cls.ADDON_PATH)

    def setUp(self):
        self.doc = self.cat(self.EXAMPLE_TEXT)

    def test_can_set_value(self):
        self.doc.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)

    def test_cannot_set_incorrect_value(self):
        with self.assertRaises(UnregisteredDataPathException):
            self.doc.set_addon_data(self.ADDON_PATH * 2 + "#",
                                    self.EXAMPLE_TEXT)

    def test_cannot_get_incorrect_value(self):
        with self.assertRaises(UnregisteredDataPathException):
            self.doc.get_addon_data(self.ADDON_PATH * 2 + "#")

    def test_can_load_value(self):
        self.doc.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)
        got = self.doc.get_addon_data(self.ADDON_PATH)
        self.assertEqual(self.EXAMPLE_VALUE, got)

    def test_empty_doc_has_no_addon_data_paths(self):
        avail = self.doc.get_available_addon_paths()
        datas = {
            path: (data := self.doc.get_addon_data(path), bool(data),
                   self.doc.has_addon_data(path))
            for path in avail
        }
        self.assertFalse(avail, f"Available: {avail};\nDATA: {datas}")

    def test_doc_can_have_addon_data_path(self):
        # set some data
        self.doc.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)
        avail = self.doc.get_available_addon_paths()
        self.assertTrue(avail)
        datas = {
            path: (data := self.doc.get_addon_data(path), bool(data),
                   self.doc.has_addon_data(path))
            for path in avail
        }
        self.assertEqual(len(avail), 1, f"Available: {avail};\nDATA: {datas}")
        self.assertEqual(avail[0], self.ADDON_PATH)

    def test_empty_ent_has_no_addon_data_paths(self):
        ent = self.doc[:]
        avail = ent.get_available_addon_paths()
        self.assertFalse(avail)
        datas = {
            path: (data := self.doc.get_addon_data(path), bool(data),
                   self.doc.has_addon_data(path))
            for path in avail
        }
        self.assertFalse(ent.has_addon_data(self.ADDON_PATH),
                         f"Available: {avail};\nDATA: {datas}")

    def test_ent_can_have_addon_data_path(self):
        ent = self.doc[:]
        # set some data
        ent.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)
        avail = ent.get_available_addon_paths()
        self.assertTrue(avail)
        self.assertEqual(len(avail), 1)
        self.assertEqual(avail[0], self.ADDON_PATH)
        self.assertTrue(ent.has_addon_data(self.ADDON_PATH))


class MethodSpy:
    def __init__(self, obj: object, method_name: str):
        self.obj = obj
        self.is_module = obj.__class__.__name__ == 'module'
        self.method_name = method_name
        self.call_args = []
        self.call_results = []
        self._patcher = None
        self._original_method = getattr(obj, method_name)

    def __enter__(self):
        def wrapper(*args, **kwargs):
            self.call_args.append((args, kwargs))
            if self.is_module:
                result = self._original_method(*args, **kwargs)
            else:
                result = self._original_method(self.obj, *args, **kwargs)
            self.call_results.append(result)
            return result

        self._patcher = unittest.mock.patch.object(
            self.obj, self.method_name, side_effect=wrapper
        )
        self._patcher.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._patcher.stop()

    def assert_called_with(self, *args, **kwargs):
        assert (args, kwargs) in self.call_args, (
            f"No such args for, {self._original_method}")

    def assert_called_once_with(self, *args, **kwargs):
        assert len(self.call_args) == 1
        self.assert_called_with(
            *args, **kwargs)

    def assert_returned(self, ret_val):
        assert ret_val in self.call_results


class CATWithDocAddonSpacyTests(CATWithDocAddonTests):
    TOKENIZING_PROVIDER = 'spacy'

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls._save_folder = tempfile.TemporaryDirectory()
        cls.saved_model_path = cls.cat.save_model_pack(
            cls._save_folder.name, make_archive=False)
        # NOTE: that has changed config
        cls.saved_spacy_path = os.path.join(
            cls.saved_model_path,
            cls.cat.config.general.nlp.modelname)

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        cls._save_folder.cleanup()

    def test_saves_spacy_model(self):
        # make sure was saved in the current folder
        self.assertIn(self._save_folder.name, self.saved_spacy_path)
        self.assertIn(TOKENIZER_PREFIX, self.saved_spacy_path)
        self.assertTrue(os.path.exists(self.saved_spacy_path))
        self.assertTrue(os.path.isdir(self.saved_spacy_path))

    def test_loads_spacy_model(self):
        import medcat.tokenizing.spacy_impl.tokenizers
        import spacy
        with MethodSpy(
                medcat.tokenizing.spacy_impl.tokenizers.SpacyTokenizer,
                "load_internals_from") as mock_load_internal:
            with MethodSpy(spacy, "load") as mock_load:
                model = cat.CAT.load_model_pack(self.saved_model_path)
        self.assertIsInstance(model, cat.CAT)
        mock_load_internal.assert_called_once_with(self.saved_spacy_path)
        mock_load.assert_called_once_with(
            self.saved_spacy_path,
            disable=self.cat.config.general.nlp.disabled_components)


class CATWithEntityAddonTests(CATIncludingTests):
    EXAMPLE_TEXT = "Example text to tokenize"
    EXAMPLE_ENT_START = 0
    EXAMPLE_ENT_END = 2
    ADDON_PATH = 'SMTH'
    EXAMPLE_VALUE = 'something else'

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        doc = cls.cat(cls.EXAMPLE_TEXT)
        doc.__getitem__
        entity = doc[0:-1]
        cls.entity_cls = entity.__class__
        cls.entity_cls.register_addon_path(cls.ADDON_PATH)

    def setUp(self):
        self.doc = self.cat(self.EXAMPLE_TEXT)
        self.entity = self.doc[self.EXAMPLE_ENT_START: self.EXAMPLE_ENT_END]

    def test_can_add_data(self):
        self.entity.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)

    def test_cannot_add_data_to_wrong_path(self):
        with self.assertRaises(UnregisteredDataPathException):
            self.entity.set_addon_data(self.ADDON_PATH * 2 + "£",
                                       self.EXAMPLE_VALUE)

    def test_cannot_get_data_to_wrong_path(self):
        with self.assertRaises(UnregisteredDataPathException):
            self.entity.get_addon_data(self.ADDON_PATH * 2 + "£")

    def test_can_get_data(self):
        self.entity.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)
        got = self.entity.get_addon_data(self.ADDON_PATH)
        self.assertEqual(self.EXAMPLE_VALUE, got)

    def test_data_is_persistent(self):
        self.entity.set_addon_data(self.ADDON_PATH, self.EXAMPLE_VALUE)
        ent = self.doc[self.EXAMPLE_ENT_START: self.EXAMPLE_ENT_END]
        # new instance
        self.assertFalse(ent is self.entity)
        got = ent.get_addon_data(self.ADDON_PATH)
        self.assertEqual(self.EXAMPLE_VALUE, got)


class CATWithEntityAddonSpacyTests(CATWithEntityAddonTests):
    TOKENIZING_PROVIDER = 'spacy'


class CATLegacyLoadTests(unittest.TestCase):

    def test_can_load_legacy_model_zip(self):
        self.assertIsInstance(
            cat.CAT.load_model_pack(V1_MODEL_PACK_PATH), cat.CAT)

    def test_can_load_legacy_model_unpacked(self):
        self.assertIsInstance(
            cat.CAT.load_model_pack(UNPACKED_V1_MODEL_PACK_PATH), cat.CAT)

    def test_cannot_load_legacy_with_environ_set(self):
        with unittest.mock.patch.dict(os.environ, {
                AVOID_LEGACY_CONVERSION_ENVIRON: "true"}, clear=True):
            with self.assertRaises(LegacyConversionDisabledError):
                cat.CAT.load_model_pack(V1_MODEL_PACK_PATH)


class CATSaveTests(CATIncludingTests):
    DESCRIPTION = "Test CAT save functionality"

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.temp_folder = tempfile.TemporaryDirectory()
        cls.saved_path = cls.cat.save_model_pack(
            cls.temp_folder.name, change_description=cls.DESCRIPTION)
        cls.model_card_path = os.path.join(cls.saved_path, "model_card.json")

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        cls.temp_folder.cleanup()

    def test_can_save_model_pack(self):
        self.assertTrue(os.path.exists(self.saved_path))

    def test_model_adds_description(self):
        self.assertIn(self.DESCRIPTION, self.cat.config.meta.description)

    def test_saved_has_model_card(self):
        self.assertTrue(os.path.exists(self.model_card_path))

    def test_model_card_is_json(self):
        with open(self.model_card_path) as f:
            mc = json.load(f)
        self.assertIsInstance(mc, dict)

    def test_model_card_has_pipe_description(self):
        with open(self.model_card_path) as f:
            mc = json.load(f)
        self.assertIn('Pipeline Description', mc)
        core_descr = mc["Pipeline Description"]["core"]
        for cct in CoreComponentType:
            with self.subTest(f"Core component {cct.name}"):
                self.assertIn(cct.name, core_descr)

    def test_model_card_has_empty_required_plugins_setion(self):
        with open(self.model_card_path) as f:
            mc = json.load(f)
        self.assertIn('Required Plugins', mc)
        self.assertFalse(mc['Required Plugins'])


class BatchingTests(unittest.TestCase):
    NUM_TEXTS = 100
    all_texts = [
        f"Text {num:04d} -> " + "a" * num
        for num in range(NUM_TEXTS)
    ]
    total_text_length = sum(len(text) for text in all_texts)

    @classmethod
    def setUpClass(cls):
        cnf = Config()
        cls.cat = cat.CAT(cdb=CDB(cnf), vocab=Vocab())

    # per doc batching tests

    def test_batching_gets_full(self):
        batches = list(self.cat._generate_simple_batches(
            iter(self.all_texts), batch_size=self.NUM_TEXTS,
            only_cui=False))
        self.assertEqual(len(batches), 1)
        self.assertEqual(len(batches[0]), self.NUM_TEXTS)
        # NOTE: the contents has the text and the index and the only_cui bool
        #       so can't check equality directly
        # self.assertEqual(batches[0], self.all_texts)

    def test_batching_gets_in_sequence(self):
        batches = list(self.cat._generate_simple_batches(
            iter(self.all_texts), batch_size=self.NUM_TEXTS // 2,
            only_cui=False))
        self.assertEqual(len(batches), 2)
        self.assertEqual(len(batches[0]), self.NUM_TEXTS // 2)
        self.assertEqual(len(batches[1]), self.NUM_TEXTS // 2)
        # self.assertEqual(batches[0] + batches[1], self.all_texts)

    def test_batching_gets_all_1_at_a_time(self):
        batches = list(self.cat._generate_simple_batches(
            iter(self.all_texts), batch_size=1, only_cui=False))
        self.assertEqual(len(batches), self.NUM_TEXTS)
        for num, batch in enumerate(batches):
            with self.subTest(f"Batch {num}"):
                self.assertEqual(len(batch), 1)
                # self.assertEqual(batch[0], f"Text {num}")

    # per character batching tests

    def test_batching_gets_full_char(self):
        batches = list(self.cat._generate_batches_by_char_length(
            iter(self.all_texts), batch_size_chars=self.total_text_length,
            only_cui=False))
        self.assertEqual(len(batches), 1)
        # has all texts
        self.assertEqual(sum(len(batch) for batch in batches), self.NUM_TEXTS)
        # has all characters
        self.assertEqual(sum(len(text[0]) for text in batches[0]),
                         self.total_text_length)

    def test_batching_gets_all_half_at_a_time(self):
        exp_chars = int(0.7 * self.total_text_length)
        batches = list(self.cat._generate_batches_by_char_length(
            iter(self.all_texts), batch_size_chars=exp_chars,
            only_cui=False))
        # NOTE: should have 2 batches at 40% overlap
        self.assertEqual(len(batches), 2)
        # each batch should have less than expected characters
        for batch_num, batch in enumerate(batches):
            with self.subTest(f"Batch {batch_num}"):
                cur_total_chars = sum(len(text[1]) for text in batch)
                self.assertLessEqual(cur_total_chars, exp_chars)
        # has all texts
        self.assertEqual(sum(len(batch) for batch in batches), self.NUM_TEXTS)
        # has all characters
        self.assertEqual(sum(len(text[0])
                             for batch in batches for text in batch),
                         self.total_text_length)

    # overal batching (i.e joint methods)

    def test_cannot_set_both_neg(self):
        with self.assertRaises(ValueError):
            list(self.cat._generate_batches(
                iter(self.all_texts), batch_size_chars=-1,
                batch_size=-1, only_cui=False))

    def test_cannot_set_both_pos(self):
        with self.assertRaises(ValueError):
            list(self.cat._generate_batches(
                iter(self.all_texts), batch_size_chars=100,
                batch_size=10, only_cui=False))

    def test_can_do_char_based(self):
        exp_chars = int(0.3 * self.total_text_length)
        batches = list(self.cat._generate_batches(
            iter(self.all_texts), batch_size_chars=exp_chars,
            batch_size=-1, only_cui=False))
        self.assertGreater(len(batches), 0)
        batch_lens = [len(batch) for batch in batches]
        # has different number of texts in some batches -> not doc based
        self.assertGreater(max(batch_lens), min(batch_lens))

    def test_can_set_batch_size_per_doc(self):
        exp_batches = 10
        batches = list(self.cat._generate_batches(
            iter(self.all_texts), batch_size=exp_batches,
            batch_size_chars=-1, only_cui=False))
        self.assertGreater(len(batches), 0)
        batch_lens = [len(batch) for batch in batches]
        # has same number of texts in each batch -> doc based
        self.assertEqual(max(batch_lens), min(batch_lens))
        self.assertEqual(max(batch_lens), exp_batches)

class TestModelCardEnhancements(unittest.TestCase):

    def setUp(self):
        # Clear the plugin registry before each test
        plugin_registry._plugins = {}

        self.mock_config = MagicMock(spec=Config())
        self.mock_config.general.nlp.provider = 'regex'
        self.mock_config.meta.hash = "testhash123"
        self.mock_config.meta.last_saved.isoformat.return_value = "2025-12-19T12:00:00"
        self.mock_config.meta.history = ["testhash123"]
        self.mock_config.meta.description = "Test description"
        self.mock_config.meta.ontology = ["SNOMEDCT"]
        self.mock_config.meta.location = "/path/to/model"
        self.mock_config.meta.medcat_version = "1.0.0"
        # these will be used in model card so need values
        self.mock_config.components.ner.min_name_len = 3
        self.mock_config.components.ner.upper_case_limit_len = 3
        self.mock_config.components.linking.similarity_threshold = 0.3
        self.mock_config.components.linking.filters.cuis = {}
        self.mock_config.general.spell_check = True
        self.mock_config.general.spell_check_len_limit = 3

        self.mock_cdb = MagicMock(spec=CDB)
        self.mock_cdb.get_basic_info.return_value = {"Number of concepts": 100}

        self.mock_pipeline = MagicMock(spec=cat.Pipeline)
        self.mock_pipeline.iter_all_components.return_value = [] # Default empty

        self.mock_cat = cat.CAT(self.mock_cdb, config=self.mock_config)
        self.mock_cat._pipeline = self.mock_pipeline # Override with our mock pipeline

    def test_describe_pipeline_core_components(self):
        mock_core_comp = MagicMock(spec=AbstractCoreComponent)
        mock_core_comp.is_core.return_value = True
        mock_core_comp.get_type.return_value = CoreComponentType.ner
        mock_core_comp.name = "my_ner_component"
        mock_core_comp.full_name = "core:ner:my_ner_component"

        self.mock_pipeline.iter_all_components.return_value = [mock_core_comp]

        pipeline_desc = self.mock_cat.describe_pipeline()

        self.assertIn(CoreComponentType.ner.name, pipeline_desc["core"])
        self.assertEqual(pipeline_desc["core"][CoreComponentType.ner.name]["name"], "my_ner_component")
        self.assertEqual(pipeline_desc["core"][CoreComponentType.ner.name]["provider"], "medcat")

    def test_describe_pipeline_addons(self):
        mock_addon_comp = MagicMock(spec=AddonComponent)
        mock_addon_comp.is_core.return_value = False
        mock_addon_comp.name = "my_addon_component"
        mock_addon_comp.full_name = "addon:my_addon_component"

        self.mock_pipeline.iter_all_components.return_value = [mock_addon_comp]

        pipeline_desc = self.mock_cat.describe_pipeline()

        self.assertEqual(len(pipeline_desc["addons"]), 1)
        self.assertEqual(pipeline_desc["addons"][0]["name"], "my_addon_component")
        self.assertEqual(pipeline_desc["addons"][0]["provider"], "medcat")

    def test_get_required_plugins(self):
        # Mock a plugin in the registry
        mock_plugin_info = PluginInfo(
            name="MockPlugin",
            version="1.0",
            author="Mock Author",
            url="http://mock.com",
            registered_components={
                "core": {CoreComponentType.ner.name: [("my_ner_component", "MockNER.create")]},
                "addons": [("my_addon_component", "MockAddon.create")]
            }
        )
        plugin_registry.register_plugin(mock_plugin_info)

        # Mock pipeline components that this plugin provides
        mock_core_comp = MagicMock(spec=AbstractCoreComponent)
        mock_core_comp.is_core.return_value = True
        mock_core_comp.get_type.return_value = CoreComponentType.ner
        mock_core_comp.name = "my_ner_component"
        mock_core_comp.full_name = "core:ner:my_ner_component"

        mock_addon_comp = MagicMock(spec=AddonComponent)
        mock_addon_comp.is_core.return_value = False
        mock_addon_comp.name = "my_addon_component"
        mock_addon_comp.full_name = "addon:my_addon_component"

        self.mock_pipeline.iter_all_components.return_value = [mock_core_comp, mock_addon_comp]

        required_plugins = self.mock_cat.get_required_plugins()

        self.assertEqual(len(required_plugins), 1)
        self.assertEqual(required_plugins[0]["name"], "MockPlugin")
        self.assertIn(("ner", "my_ner_component"), required_plugins[0]["provides"])
        self.assertIn(("addon", "my_addon_component"), required_plugins[0]["provides"])
        self.assertEqual(required_plugins[0]["author"], "Mock Author")
        self.assertEqual(required_plugins[0]["url"], "http://mock.com")

    @unittest.mock.patch('medcat.cat.CAT.describe_pipeline')
    @unittest.mock.patch('medcat.cat.CAT.get_required_plugins')
    def test_get_model_card_with_pipeline_and_plugins(self, mock_get_required_plugins, mock_describe_pipeline):
        mock_describe_pipeline.return_value = {"core": {CoreComponentType.ner.name: {"name": "test_ner", "provider": "medcat"}}, "addons": []}
        mock_get_required_plugins.return_value = [{"name": "TestPlugin", "provides": [("ner", "test_ner")], "author": "Test Author", "url": "http://test.com"}]

        model_card = self.mock_cat.get_model_card(as_dict=True)

        self.assertIn("Pipeline Description", model_card)
        self.assertEqual(model_card["Pipeline Description"], {"core": {CoreComponentType.ner.name: {"name": "test_ner", "provider": "medcat"}}, "addons": []})
        self.assertIn("Required Plugins", model_card)
        self.assertEqual(model_card["Required Plugins"], [{"name": "TestPlugin", "provides": [("ner", "test_ner")], "author": "Test Author", "url": "http://test.com"}])

    @unittest.mock.patch('medcat.cat.CAT.describe_pipeline')
    @unittest.mock.patch('medcat.cat.CAT.get_required_plugins')
    def test_model_card_saved_and_loaded_from_disk(self, mock_get_required_plugins, mock_describe_pipeline):
        # Setup mocks for content to be in the model card
        mock_describe_pipeline.return_value = {"core": {CoreComponentType.ner.name: {"name": "test_ner_disk", "provider": "medcat"}}, "addons": []}
        mock_get_required_plugins.return_value = [{"name": "TestPluginDisk", "provides": [("ner", "test_ner_disk")], "author": "Test Author Disk", "url": "http://test-disk.com"}]

        with tempfile.TemporaryDirectory() as temp_dir:
            model_card_path = os.path.join(temp_dir, "model_card.json")
            # Save the model pack
            self.mock_cat.save_model_card(model_card_path)

            # Load the model card from disk
            loaded_model_card = cat.CAT.load_model_card_off_disk(temp_dir, as_dict=True)

            self.assertIn("Pipeline Description", loaded_model_card)
            self.assertEqual(loaded_model_card["Pipeline Description"], {"core": {CoreComponentType.ner.name: {"name": "test_ner_disk", "provider": "medcat"}}, "addons": []})
            self.assertIn("Required Plugins", loaded_model_card)
            # NOTE: tuples get loaded as lists
            self.assertEqual(loaded_model_card["Required Plugins"], [{"name": "TestPluginDisk", "provides": [["ner", "test_ner_disk"]], "author": "Test Author Disk", "url": "http://test-disk.com"}])

    def test_describe_pipeline_with_module_path_fallback(self):
        # Define a mock component class with a specific module path
        class MockComponentWithModule(AbstractCoreComponent):
            def is_core(self): return True
            def get_type(self): return CoreComponentType.ner
            name = "fallback_ner_component"
            full_name = "core:ner:fallback_ner_component"
            __module__ = "my_plugin_package.some_module"

        # Register a plugin with a matching module path, but no explicit registration
        mock_plugin_info = PluginInfo(
            name="MyPluginPackage",
            version="1.0",
            author="Module Author",
            url="http://module-plugin.com",
            module_paths=["my_plugin_package"],
            registered_components=create_empty_reg_comps(),
        )
        plugin_registry.register_plugin(mock_plugin_info)

        # Mock the pipeline to return an instance of our component
        mock_comp_instance = MockComponentWithModule()
        self.mock_pipeline.iter_all_components.return_value = [mock_comp_instance]

        pipeline_desc = self.mock_cat.describe_pipeline()

        self.assertIn(CoreComponentType.ner.name, pipeline_desc["core"])
        self.assertEqual(pipeline_desc["core"][CoreComponentType.ner.name]["name"], "fallback_ner_component")
        self.assertEqual(pipeline_desc["core"][CoreComponentType.ner.name]["provider"], "MyPluginPackage")

    @unittest.mock.patch("medcat.cat.deserialise")
    @unittest.mock.patch('importlib.util.find_spec')
    def test_load_model_pack_with_missing_plugin_raises_error(self, mock_find_spec, mock_deserialise):
        mock_find_spec.return_value = None  # Simulate plugin not found
        mock_deserialise.side_effect = ImportError
        with tempfile.TemporaryDirectory() as temp_dir:
            model_card_path = os.path.join(temp_dir, "model_card.json")
            model_card_content = {
                "Required Plugins": [{
                    "name": "MissingPlugin",
                    "provides": [["core", "test_core_comp"]],
                    "author": "Missing Author",
                    "url": "http://missing.com"
                }]
            }
            # overwrite model card
            with open(model_card_path, "w") as f:
                json.dump(model_card_content, f)

            with self.assertRaises(cat.MissingPluginError) as cm:
                cat.CAT.load_model_pack(temp_dir)

            self.assertEqual(len(cm.exception.missing_plugins), 1)
            self.assertEqual(cm.exception.missing_plugins[0]["name"], "MissingPlugin")
            self.assertIn("MissingPlugin", str(cm.exception))

    @unittest.mock.patch('importlib.util.find_spec')
    def test_load_model_pack_with_available_plugin_succeeds(self, mock_find_spec):
        mock_find_spec.return_value = MagicMock()  # Simulate plugin found
        with tempfile.TemporaryDirectory() as temp_dir:
            model_card_path = os.path.join(temp_dir, "model_card.json")
            model_card_content = {
                "Required Plugins": [{
                    "name": "AvailablePlugin",
                    "provides": [["core", "test_core_comp"]],
                    "author": "Available Author",
                    "url": "http://available.com"
                }]
            }
            with open(model_card_path, "w") as f:
                json.dump(model_card_content, f)

            # Mock deserialise to return a valid CAT object to avoid deeper loading issues
            with unittest.mock.patch('medcat.cat.deserialise') as mock_deserialise:
                mock_deserialise.return_value = self.mock_cat
                loaded_cat = cat.CAT.load_model_pack(temp_dir)
                self.assertIs(loaded_cat, self.mock_cat)

    @unittest.mock.patch('importlib.util.find_spec')
    def test_load_model_pack_with_no_required_plugins_succeeds(self, mock_find_spec):
        mock_find_spec.return_value = None  # Should not be called if no required plugins
        with tempfile.TemporaryDirectory() as temp_dir:
            model_card_path = os.path.join(temp_dir, "model_card.json")
            model_card_content = {
                "Required Plugins": []
            }
            with open(model_card_path, "w") as f:
                json.dump(model_card_content, f)

            with unittest.mock.patch('medcat.cat.deserialise') as mock_deserialise:
                mock_deserialise.return_value = self.mock_cat
                loaded_cat = cat.CAT.load_model_pack(temp_dir)
                self.assertIs(loaded_cat, self.mock_cat)

