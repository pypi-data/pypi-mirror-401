import asyncio
import json
import os
import threading
import time
import uuid
from http.server import BaseHTTPRequestHandler, HTTPServer
from queue import Queue
from typing import List

import pytest
import requests
from dotenv import load_dotenv
from letta_client import AsyncLetta
from temporalio import activity
from temporalio.exceptions import ApplicationError
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import Worker
from temporalio.worker.workflow_sandbox import (
    SandboxedWorkflowRunner,
    SandboxRestrictions,
)

from letta.agents.temporal.activities import (
    create_messages,
    create_step,
    example_activity,
    execute_tool,
    llm_request,
    prepare_messages,
    refresh_context_and_system_message,
    send_step_complete_webhook,
    summarize_conversation_history,
    update_message_ids,
    update_run,
)
from letta.agents.temporal.temporal_agent_workflow import TemporalAgentWorkflow
from letta.agents.temporal.types import WorkflowInputParams
from letta.schemas.enums import RunStatus
from letta.schemas.message import MessageCreate
from letta.schemas.organization import Organization
from letta.schemas.run import Run
from letta.services.organization_manager import OrganizationManager
from letta.services.run_manager import RunManager
from letta.services.user_manager import UserManager
from tests.helpers.utils import upload_test_agentfile_from_disk_async


class WebhookHandler(BaseHTTPRequestHandler):
    """HTTP request handler for webhook callbacks."""

    def do_POST(self):
        """Handle POST requests to the webhook endpoint."""
        content_length = int(self.headers.get("Content-Length", 0))
        post_data = self.rfile.read(content_length)

        print(f"âœ“ Webhook received at: {self.path}")
        print(f"âœ“ Headers: {dict(self.headers)}")
        print(f"âœ“ Body: {post_data.decode('utf-8') if post_data else 'Empty'}")

        # Store the received webhook data
        self.server.webhook_calls.put(
            {
                "path": self.path,
                "headers": dict(self.headers),
                "body": post_data.decode("utf-8") if post_data else "",
                "timestamp": time.time(),
            }
        )

        # Send response
        self.send_response(200)
        self.send_header("Content-Type", "application/json")
        self.end_headers()
        self.wfile.write(json.dumps({"status": "received"}).encode())

    def log_message(self, format, *args):
        """Suppress default HTTP server logging."""
        pass


class TestWebhookServer:
    """Test webhook server that runs in a separate thread."""

    def __init__(self, port=0):
        """Initialize the webhook server.

        Args:
            port: Port to listen on. If 0, a random available port is chosen.
        """
        self.port = port
        self.server = None
        self.thread = None
        self.webhook_calls = Queue()
        self.url = None

    def start(self):
        """Start the webhook server in a background thread."""
        # Create server with custom handler
        self.server = HTTPServer(("localhost", self.port), WebhookHandler)
        self.server.webhook_calls = self.webhook_calls

        # Get the actual port (useful if port was 0)
        self.port = self.server.server_port
        self.url = f"http://localhost:{self.port}"

        # Start server in background thread
        self.thread = threading.Thread(target=self.server.serve_forever, daemon=True)
        self.thread.start()

        # Wait for server to be ready
        time.sleep(0.1)
        return self.url

    def stop(self):
        """Stop the webhook server."""
        if self.server:
            self.server.shutdown()
            self.server.server_close()
        if self.thread:
            self.thread.join(timeout=1)

    def get_webhook_calls(self, timeout=5):
        """Get all webhook calls received so far.

        Args:
            timeout: Maximum time to wait for calls.

        Returns:
            List of webhook call data.
        """
        calls = []
        deadline = time.time() + timeout

        while time.time() < deadline:
            if not self.webhook_calls.empty():
                try:
                    call = self.webhook_calls.get_nowait()
                    calls.append(call)
                except:
                    break
            else:
                break

        return calls

    def wait_for_webhook(self, timeout=10):
        """Wait for at least one webhook call.

        Args:
            timeout: Maximum time to wait.

        Returns:
            The first webhook call received, or None if timeout.
        """
        try:
            return self.webhook_calls.get(timeout=timeout)
        except:
            return None


def roll_dice(num_sides: int) -> int:
    """
    Returns a random number between 1 and num_sides.
    Args:
        num_sides (int): The number of sides on the die.
    Returns:
        int: A random integer between 1 and num_sides, representing the die roll.
    """
    import random

    return random.randint(1, num_sides)


USER_MESSAGE_OTID = str(uuid.uuid4())
USER_MESSAGE_GREETING: List[MessageCreate] = [
    MessageCreate(
        role="user",
        content="Hi!",
        otid=USER_MESSAGE_OTID,
    )
]
USER_MESSAGE_ROLL_DICE: List[MessageCreate] = [
    MessageCreate(
        role="user",
        content="This is an automated test message. Call the roll_dice tool with 16 sides and send me a message with the outcome.",
        otid=USER_MESSAGE_OTID,
    )
]

RESEARCH_INSTRUCTIONS: List[MessageCreate] = [
    MessageCreate(
        role="user",
        content="\n    Lead Name: Kian Jones\n    Lead Title: Software Engineer\n    Lead LinkedIn URL: https://www.linkedin.com/in/kian-jones\n    Company Name: Letta\n    Company Domain: letta.com\n    Company Industry: technology/software/ai\n    \n**Research Instructions**\n",
        otid=USER_MESSAGE_OTID,
    )
]


@pytest.fixture(scope="function")
async def webhook_server():
    """
    Fixture that provides a test webhook server for capturing callback requests.
    The server runs in a background thread and automatically cleans up after the test.
    """
    server = TestWebhookServer()
    webhook_url = server.start()
    print(f"âœ“ Webhook server fixture started at: {webhook_url}")

    yield server

    # Cleanup
    server.stop()
    print("âœ“ Webhook server fixture stopped")


@pytest.fixture(scope="module")
def server_url() -> str:
    """
    Provides the URL for the Letta server.
    If LETTA_SERVER_URL is not set, starts the server in a background thread
    and polls until it's accepting connections.
    """

    def _run_server() -> None:
        load_dotenv()
        from letta.server.rest_api.app import start_server

        start_server(debug=True)

    url: str = os.getenv("LETTA_SERVER_URL", "http://localhost:8283")

    if not os.getenv("LETTA_SERVER_URL"):
        thread = threading.Thread(target=_run_server, daemon=True)
        thread.start()

        # Poll until the server is up (or timeout)
        timeout_seconds = 60
        deadline = time.time() + timeout_seconds
        while time.time() < deadline:
            try:
                resp = requests.get(url + "/v1/health")
                if resp.status_code < 500:
                    break
            except requests.exceptions.RequestException:
                pass
            time.sleep(0.1)
        else:
            raise RuntimeError(f"Could not reach {url} within {timeout_seconds}s")

    return url


@pytest.fixture(scope="function")
async def client(server_url: str) -> AsyncLetta:
    """
    Creates and returns a synchronous Letta REST client for testing.
    """
    client_instance = AsyncLetta(base_url=server_url)
    yield client_instance


@pytest.fixture
async def default_organization():
    """Fixture to create and return the default organization."""
    manager = OrganizationManager()
    org = await manager.create_default_organization_async()
    yield org


@pytest.mark.asyncio(loop_scope="function")
async def test_execute_workflow(client: AsyncLetta, default_organization: Organization):
    """
    Test the temporal agent workflow execution.

    This test validates that the Temporal workflow infrastructure is working correctly by:
    1. Setting up a Temporal workflow environment with time-skipping
    2. Creating a worker with all required activities
    3. Executing a workflow that processes a user message
    4. Verifying the workflow completes successfully and produces expected outputs

    The focus is on integration testing of the Temporal workflow system, not on
    testing individual message schemas or agent logic.
    """
    # import os
    # import asyncio
    # import logging
    # from letta.server.db import db_registry

    # # Suppress scary database connection logs during test
    # logging.getLogger("sqlalchemy.pool").setLevel(logging.CRITICAL)
    # logging.getLogger("asyncio").setLevel(logging.CRITICAL)
    # logging.getLogger("temporalio.activity").setLevel(logging.CRITICAL)

    # # Force database pooling to be disabled for cleaner event loop handling
    # os.environ["LETTA_DISABLE_SQLALCHEMY_POOLING"] = "true"

    # # Get the current event loop to ensure consistency
    # loop = asyncio.get_running_loop()

    # # Clear any existing database connections completely
    # if hasattr(db_registry, "_async_engines"):
    #     for name, engine in list(db_registry._async_engines.items()):
    #         if engine:
    #             try:
    #                 await engine.dispose()
    #             except Exception:
    #                 pass
    #     db_registry._async_engines.clear()

    # if hasattr(db_registry, "_async_session_factories"):
    #     db_registry._async_session_factories.clear()

    # if hasattr(db_registry, "_initialized"):
    #     db_registry._initialized["async"] = False

    task_queue_name = str(uuid.uuid4())

    manager = UserManager()
    user = await manager.create_default_actor_async(org_id=default_organization.id)

    dice_tool = await client.tools.upsert_from_function(func=roll_dice)

    send_message_tool_page = await client.tools.list(name="send_message")
    send_message_tool = send_message_tool_page.items[0]
    agent = await client.agents.create(
        name="test-agent",
        agent_type="memgpt_v2_agent",
        include_base_tools=False,
        tool_ids=[send_message_tool.id, dice_tool.id],
        model="openai/gpt-4o",
        embedding="openai/text-embedding-3-small",
        tags=["test"],
    )
    run_manager = RunManager()
    run = Run(
        status=RunStatus.created,
        agent_id=agent.id,
        background=True,  # Async endpoints are always background
        metadata={
            "run_type": "send_message_async",
            "agent_id": agent.id,
            "lettuce": True,
        },
    )
    run = await run_manager.create_run(pydantic_run=run, actor=user)

    # Fetch the agent from the server to get the correct AgentState schema
    # (SDK AgentState has different field names like 'metadata' vs 'metadata_')
    from letta.services.agent_manager import AgentManager

    agent_manager = AgentManager()
    agent_state = await agent_manager.get_agent_by_id_async(agent_id=agent.id, actor=user)

    async with await WorkflowEnvironment.start_time_skipping() as env:
        # Create worker with shared event loop
        worker = Worker(
            env.client,
            task_queue=task_queue_name,
            workflows=[TemporalAgentWorkflow],
            activities=[
                prepare_messages,
                refresh_context_and_system_message,
                llm_request,
                summarize_conversation_history,
                example_activity,
                execute_tool,
                create_messages,
                create_step,
                update_message_ids,
                update_run,
                send_step_complete_webhook,
            ],
            workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_modules("letta")),
        )

        async with worker:
            workflow_input = WorkflowInputParams(
                agent_state=agent_state,
                messages=USER_MESSAGE_ROLL_DICE,
                actor=user,
                max_steps=10,
                run_id=run.id,
            )
            result = await env.client.execute_workflow(
                TemporalAgentWorkflow.run,
                workflow_input,
                id=workflow_input.run_id,
                task_queue=task_queue_name,
            )

            # Verify the workflow executed successfully
            assert result is not None, "Workflow result should not be None"
            assert hasattr(result, "messages"), "Result should have messages attribute"
            assert hasattr(result, "usage"), "Result should have usage attribute"
            assert hasattr(result, "stop_reason"), "Result should have stop_reason attribute"

            # ====== TEMPORAL WORKFLOW EXECUTION VALIDATION ======
            # The focus is on verifying the temporal workflow executed correctly,
            # not on detailed message structure validation

            # 1. Verify workflow completed successfully
            assert result is not None, "Workflow should return a result"
            assert hasattr(result, "messages"), "Result should contain messages"
            assert hasattr(result, "stop_reason"), "Result should have stop_reason"
            assert hasattr(result, "usage"), "Result should have usage tracking"

            # 2. Verify workflow produced messages (workflow activities executed)
            assert len(result.messages) > 0, "Workflow should produce at least one message"

            # 3. Verify the workflow processed the user input
            user_messages = [msg for msg in result.messages if hasattr(msg, "message_type") and msg.message_type == "user_message"]
            assert len(user_messages) >= 1, "Workflow should have processed the user message"

            # 4. Verify the workflow generated an LLM response
            assistant_messages = [
                msg for msg in result.messages if hasattr(msg, "message_type") and msg.message_type == "assistant_message"
            ]
            assert len(assistant_messages) >= 1, "Workflow should have generated at least one assistant response"

            # 5. Verify workflow executed the requested tool (roll_dice)
            # This validates that the workflow properly handled tool execution activities
            tool_executed = False
            for msg in result.messages:
                if hasattr(msg, "message_type"):
                    if msg.message_type == "tool_call_message" and hasattr(msg, "tool_call"):
                        if msg.tool_call.name == "roll_dice":
                            tool_executed = True
                            break
                    elif msg.message_type == "tool_return_message":
                        # Tool was executed and returned a result
                        tool_executed = True
                        break
            assert tool_executed, "Workflow should have executed the roll_dice tool as requested"

            # 6. Verify workflow activities tracked token usage
            # This validates the llm_request activity executed correctly
            if result.usage:
                assert hasattr(result.usage, "total_tokens"), "Workflow should track total tokens"
                if hasattr(result.usage, "total_tokens") and result.usage.total_tokens is not None:
                    assert result.usage.total_tokens > 0, "Workflow should have consumed tokens during LLM request activity"

            # 7. Verify workflow terminated properly
            assert result.stop_reason in ["max_steps", "user_interrupted", "finished", "error", "end_turn"], (
                f"Workflow should terminate with a valid reason, got: {result.stop_reason}"
            )

            # 8. Verify workflow created messages with IDs
            # Some message types might share IDs (e.g., tool calls and returns)
            # so we just verify that messages have IDs
            message_ids = [msg.id for msg in result.messages if hasattr(msg, "id")]
            assert len(message_ids) > 0, "Messages should have IDs assigned by the workflow"

            # 9. Log workflow execution summary for debugging
            print("\nâœ“ Temporal workflow executed successfully!")
            print(f"âœ“ Total messages processed: {len(result.messages)}")
            print(f"âœ“ Workflow stop reason: {result.stop_reason}")
            if result.usage and hasattr(result.usage, "total_tokens"):
                print(f"âœ“ Total tokens used: {result.usage.total_tokens}")

            message_types = {}
            for msg in result.messages:
                if hasattr(msg, "message_type"):
                    message_types[msg.message_type] = message_types.get(msg.message_type, 0) + 1
            print(f"âœ“ Message types processed: {message_types}")

    # Skip agent deletion to avoid cleanup issues
    # client.agents.delete(agent.id)

    # Final cleanup - suppress all cleanup errors to avoid scary logs
    # try:
    #     if hasattr(db_registry, "_async_engines"):
    #         for engine in list(db_registry._async_engines.values()):
    #             if engine:
    #                 try:
    #                     # Force immediate dispose without waiting for connections to close gracefully
    #                     await engine.dispose()
    #                 except Exception:
    #                     # Suppress all cleanup exceptions
    #                     pass
    #         db_registry._async_engines.clear()

    #     if hasattr(db_registry, "_async_session_factories"):
    #         db_registry._async_session_factories.clear()

    #     if hasattr(db_registry, "_initialized"):
    #         db_registry._initialized["async"] = False

    #     # Clean up environment variable
    #     if "LETTA_DISABLE_SQLALCHEMY_POOLING" in os.environ:
    #         del os.environ["LETTA_DISABLE_SQLALCHEMY_POOLING"]

    #     # Restore logging levels
    #     logging.getLogger("sqlalchemy.pool").setLevel(logging.INFO)
    #     logging.getLogger("asyncio").setLevel(logging.INFO)
    #     logging.getLogger("temporalio.activity").setLevel(logging.INFO)
    # except Exception:
    #     # Suppress any cleanup errors completely
    #     pass


@pytest.mark.asyncio(loop_scope="function")
async def test_execute_workflow_with_callback(client: AsyncLetta, default_organization: Organization, webhook_server: TestWebhookServer):
    """
    Test the temporal agent workflow execution with callback.

    This test validates that webhook callbacks are properly triggered during workflow execution:
    1. Uses the webhook_server fixture to capture callbacks
    2. Executes a workflow with a run that has a callback_url configured
    3. Verifies that the webhook server receives the expected callbacks
    4. Validates the webhook payload structure and content
    """
    webhook_url = webhook_server.url
    task_queue_name = str(uuid.uuid4())

    manager = UserManager()
    user = await manager.create_default_actor_async(org_id=default_organization.id)

    print("importing agent from file")
    imported_af = await upload_test_agentfile_from_disk_async(
        client, "../../../stress-py/11x-deep-research-stubbed/stubbed_research_agent.json"
    )
    agent = await client.agents.retrieve(imported_af.agent_ids[0])
    print(agent.id, " imported from agent file")

    run_manager = RunManager()
    run = Run(
        status=RunStatus.created,
        agent_id=agent.id,
        background=True,  # Async endpoints are always background
        callback_url=webhook_url,  # Set the webhook URL on the Run
        metadata={
            "run_type": "send_message_async",
            "agent_id": agent.id,
            "lettuce": True,
        },
    )
    run = await run_manager.create_run(pydantic_run=run, actor=user)

    # Fetch the agent from the server to get the correct AgentState schema
    # (SDK AgentState has different field names like 'metadata' vs 'metadata_')
    from letta.services.agent_manager import AgentManager

    agent_manager = AgentManager()
    agent_state = await agent_manager.get_agent_by_id_async(agent_id=agent.id, actor=user)

    async with await WorkflowEnvironment.start_time_skipping() as env:
        # Create worker with shared event loop
        worker = Worker(
            env.client,
            task_queue=task_queue_name,
            workflows=[TemporalAgentWorkflow],
            activities=[
                prepare_messages,
                refresh_context_and_system_message,
                llm_request,
                summarize_conversation_history,
                example_activity,
                execute_tool,
                create_messages,
                create_step,
                update_message_ids,
                update_run,
                send_step_complete_webhook,
            ],
            workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_modules("letta")),
        )

        async with worker:
            workflow_input = WorkflowInputParams(
                agent_state=agent_state,
                messages=RESEARCH_INSTRUCTIONS,
                actor=user,
                max_steps=10,
                run_id=run.id,
            )
            result = await env.client.execute_workflow(
                TemporalAgentWorkflow.run,
                workflow_input,
                id=workflow_input.run_id,
                task_queue=task_queue_name,
            )

            print("\nâœ“ Temporal workflow executed successfully!")
            print(f"âœ“ Total messages processed: {len(result.messages)}")
            print(f"âœ“ Workflow stop reason: {result.stop_reason}")
            if result.usage and hasattr(result.usage, "total_tokens"):
                print(f"âœ“ Total tokens used: {result.usage.total_tokens}")

            message_types = {}
            for msg in result.messages:
                if hasattr(msg, "message_type"):
                    message_types[msg.message_type] = message_types.get(msg.message_type, 0) + 1
            print(f"âœ“ Message types processed: {message_types}")

            # Check for webhook callbacks
            print("\nðŸ“¡ Checking for webhook callbacks...")
            print(f"Run status: {result.stop_reason}")

            # In time-skipping mode, callbacks might be processed differently
            # Let's wait a bit and poll for callbacks
            max_wait_time = 15
            poll_interval = 0.5
            start_time = time.time()
            webhook_calls = []

            while time.time() - start_time < max_wait_time:
                webhook_calls = webhook_server.get_webhook_calls(timeout=0.1)
                if webhook_calls:
                    break
                await asyncio.sleep(poll_interval)

            # If still no callbacks, check if the run was properly marked as completed
            print(f"Webhook URL configured: {webhook_url}")
            print(f"Run ID: {run.id}")

            # Assert webhook behavior
            assert len(webhook_calls) > 0, "Expected at least one webhook callback, but received none"
            print(f"âœ… Received {len(webhook_calls)} webhook callback(s)")

            # Validate webhook payload structure and content
            for i, call in enumerate(webhook_calls):
                print(f"\nðŸ“¬ Webhook call #{i + 1}:")
                print(f"  Path: {call['path']}")
                print(f"  Headers: {list(call['headers'].keys())}")

                # Parse and validate the JSON body
                assert call["body"], "Webhook body should not be empty"

                try:
                    payload = json.loads(call["body"])
                    print(f"  Payload keys: {list(payload.keys())}")

                    # Assert required fields in webhook payload
                    assert "run_id" in payload, f"Webhook payload missing 'run_id'. Got: {list(payload.keys())}"
                    assert "status" in payload, f"Webhook payload missing 'status'. Got: {list(payload.keys())}"
                    assert "completed_at" in payload, f"Webhook payload missing 'completed_at'. Got: {list(payload.keys())}"
                    assert "metadata" in payload, f"Webhook payload missing 'metadata'. Got: {list(payload.keys())}"

                    # Validate field values
                    assert payload["run_id"] == run.id, f"Expected run_id {run.id}, got {payload['run_id']}"
                    assert payload["status"] in ["completed", "failed"], f"Expected status 'completed' or 'failed', got {payload['status']}"
                    assert payload["completed_at"] is not None, "completed_at should not be None"

                    # Validate metadata contains result
                    metadata = payload["metadata"]
                    assert isinstance(metadata, dict), f"metadata should be a dict, got {type(metadata)}"

                    if "result" in metadata:
                        result_data = metadata["result"]
                        assert isinstance(result_data, dict), f"result should be a dict, got {type(result_data)}"

                        # Check for expected result fields
                        if "messages" in result_data:
                            assert isinstance(result_data["messages"], list), "messages should be a list"
                            assert len(result_data["messages"]) > 0, "messages list should not be empty"
                            print(f"  âœ“ Result contains {len(result_data['messages'])} messages")

                        if "stop_reason" in result_data:
                            assert result_data["stop_reason"] is not None, "stop_reason should not be None"
                            print(f"  âœ“ Stop reason: {result_data['stop_reason']}")

                        if "usage" in result_data:
                            assert isinstance(result_data["usage"], dict), "usage should be a dict"
                            print("  âœ“ Usage statistics included")

                    print(f"  âœ“ Valid webhook payload for run {payload['run_id']}")
                    print(f"  âœ“ Run status: {payload['status']}")
                    print(f"  âœ“ Completed at: {payload['completed_at']}")

                except json.JSONDecodeError as e:
                    pytest.fail(f"Failed to parse webhook JSON payload: {e}. Body: {call['body'][:500]}")
                except AssertionError:
                    # Re-raise assertion errors
                    raise
                except Exception as e:
                    pytest.fail(f"Unexpected error validating webhook payload: {e}")

    print("\nâœ… All webhook callbacks validated successfully!")


@pytest.mark.asyncio(loop_scope="function")
async def test_workflow_updates_run_on_prepare_messages_failure(client: AsyncLetta, default_organization: Organization):
    """
    When an early activity (prepare_messages) fails permanently (non-retryable),
    the workflow should best-effort update the run to a terminal failed status
    before surfacing the failure.
    """
    task_queue_name = str(uuid.uuid4())

    # Create default actor and simple agent
    manager = UserManager()
    user = await manager.create_default_actor_async(org_id=default_organization.id)

    send_message_tool_page = await client.tools.list(name="send_message")
    send_message_tool = send_message_tool_page.items[0]
    agent = await client.agents.create(
        name="test-agent-fail-prepare",
        agent_type="memgpt_v2_agent",
        include_base_tools=False,
        tool_ids=[send_message_tool.id],
        model="openai/gpt-4o",
        embedding="openai/text-embedding-3-small",
        tags=["test"],
    )

    # Create run record
    run_manager = RunManager()
    run = Run(
        status=RunStatus.created,
        agent_id=agent.id,
        background=True,
        metadata={"run_type": "send_message_async", "agent_id": agent.id, "lettuce": True},
    )
    run = await run_manager.create_run(pydantic_run=run, actor=user)

    # Fetch the agent from the server to get the correct AgentState schema
    from letta.services.agent_manager import AgentManager

    agent_manager = AgentManager()
    agent_state = await agent_manager.get_agent_by_id_async(agent_id=agent.id, actor=user)

    # Define a failing prepare_messages activity (permanent failure)
    @activity.defn(name="prepare_messages")
    async def fail_prepare_messages(_):
        # Simulate a non-retryable provider/validation failure
        raise ApplicationError("forced failure", type="ValueError", non_retryable=True)

    async with await WorkflowEnvironment.start_time_skipping() as env:
        worker = Worker(
            env.client,
            task_queue=task_queue_name,
            workflows=[TemporalAgentWorkflow],
            activities=[
                fail_prepare_messages,  # override default prepare_messages by name
                # Needed by the error path to persist final status
                update_run,
            ],
            workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_modules("letta")),
        )

        async with worker:
            workflow_input = WorkflowInputParams(
                agent_state=agent_state,
                messages=USER_MESSAGE_GREETING,
                actor=user,
                max_steps=1,
                run_id=run.id,
            )
            threw = False
            try:
                await env.client.execute_workflow(
                    TemporalAgentWorkflow.run,
                    workflow_input,
                    id=workflow_input.run_id,
                    task_queue=task_queue_name,
                )
            except Exception:
                threw = True

            # Workflow should surface failure
            assert threw, "Workflow should propagate failure after updating run"

    # Verify run marked as failed with an appropriate stop reason
    final_run = await run_manager.get_run_by_id(run_id=run.id, actor=user)
    assert final_run.status == RunStatus.failed
    # Stop reason should map to invalid_llm_response from ValueError mapping in workflow
    assert final_run.stop_reason in ("invalid_llm_response", "llm_api_error", "error")


@pytest.mark.asyncio(loop_scope="function")
async def test_workflow_marks_run_failed_on_nonretryable_llm_request_failure(client: AsyncLetta, default_organization: Organization):
    """
    If the llm_request activity fails non-retryably (e.g., JSON parsing error),
    the workflow should handle it, set an appropriate stop reason, and update the
    run to failed while still completing the workflow call.
    """
    task_queue_name = str(uuid.uuid4())

    manager = UserManager()
    user = await manager.create_default_actor_async(org_id=default_organization.id)

    send_message_tool_page = await client.tools.list(name="send_message")
    send_message_tool = send_message_tool_page.items[0]
    agent = await client.agents.create(
        name="test-agent-llm-fail",
        agent_type="memgpt_v2_agent",
        include_base_tools=False,
        tool_ids=[send_message_tool.id],
        model="openai/gpt-4o",
        embedding="openai/text-embedding-3-small",
        tags=["test"],
    )

    run_manager = RunManager()
    run = Run(
        status=RunStatus.created,
        agent_id=agent.id,
        background=True,
        metadata={"run_type": "send_message_async", "agent_id": agent.id, "lettuce": True},
    )
    run = await run_manager.create_run(pydantic_run=run, actor=user)

    # Fetch the agent from the server to get the correct AgentState schema
    # (SDK AgentState has different field names like 'metadata' vs 'metadata_')
    from letta.services.agent_manager import AgentManager

    agent_manager = AgentManager()
    agent_state = await agent_manager.get_agent_by_id_async(agent_id=agent.id, actor=user)

    # Define a failing llm_request activity (non-retryable)
    @activity.defn(name="llm_request")
    async def fail_llm_request(_):
        # Simulate invalid JSON/tool call parsing error from LLM
        raise ApplicationError("invalid llm response", type="LLMJSONParsingError", non_retryable=True)

    async with await WorkflowEnvironment.start_time_skipping() as env:
        worker = Worker(
            env.client,
            task_queue=task_queue_name,
            workflows=[TemporalAgentWorkflow],
            activities=[
                # Normal prepare + refresh + summarize to reach llm_request
                prepare_messages,
                refresh_context_and_system_message,
                summarize_conversation_history,
                # Override llm_request to fail
                fail_llm_request,
                # These are called by the normal end-of-run path
                create_messages,
                create_step,
                update_message_ids,
                update_run,
                send_step_complete_webhook,
            ],
            workflow_runner=SandboxedWorkflowRunner(restrictions=SandboxRestrictions.default.with_passthrough_modules("letta")),
        )

        async with worker:
            workflow_input = WorkflowInputParams(
                agent_state=agent_state,
                messages=USER_MESSAGE_GREETING,
                actor=user,
                max_steps=1,
                run_id=run.id,
            )
            # Workflow handles the error internally and returns a FinalResult
            result = await env.client.execute_workflow(
                TemporalAgentWorkflow.run,
                workflow_input,
                id=workflow_input.run_id,
                task_queue=task_queue_name,
            )

            # Verify workflow returned a stop_reason indicating failure mapping
            assert result.stop_reason in ("invalid_llm_response", "llm_api_error", "error")

    # Verify run status updated to failed
    final_run = await run_manager.get_run_by_id(run_id=run.id, actor=user)
    assert final_run.status == RunStatus.failed
    assert final_run.stop_reason in ("invalid_llm_response", "llm_api_error", "error")
