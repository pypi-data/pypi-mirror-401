syntax = "proto3";

package neuralforge.inference;

option python_package = "neuralforge.grpc.generated";

// Core inference service for NeuralForge
service InferenceService {
    // Single prediction
    rpc Predict(PredictRequest) returns (PredictResponse);
    
    // Streaming prediction (for LLMs)
    rpc StreamPredict(PredictRequest) returns (stream TokenResponse);
    
    // Batch prediction
    rpc BatchPredict(BatchRequest) returns (BatchResponse);
    
    // Get model info
    rpc GetModelInfo(ModelInfoRequest) returns (ModelInfoResponse);
    
    // Health check
    rpc HealthCheck(HealthRequest) returns (HealthResponse);
}

// Prediction request
message PredictRequest {
    // Model identification
    string model_name = 1;
    string model_version = 2;
    
    // Input data (serialized)
    bytes input_data = 3;
    string input_format = 4;  // "json", "numpy", "tensor"
    
    // Request metadata
    map<string, string> metadata = 5;
    
    // Optional parameters
    int32 max_tokens = 6;
    float temperature = 7;
    float top_p = 8;
}

// Prediction response
message PredictResponse {
    // Output data (serialized)
    bytes output_data = 1;
    string output_format = 2;
    
    // Performance metrics
    float latency_ms = 3;
    int32 tokens_generated = 4;
    
    // Response metadata
    map<string, string> metadata = 5;
}

// Token response for streaming
message TokenResponse {
    string token = 1;
    int32 index = 2;
    bool is_final = 3;
    float logprob = 4;
    
    // Usage info (only in final message)
    UsageInfo usage = 5;
}

// Usage information
message UsageInfo {
    int32 prompt_tokens = 1;
    int32 completion_tokens = 2;
    int32 total_tokens = 3;
    float time_to_first_token_ms = 4;
    float total_time_ms = 5;
}

// Batch request
message BatchRequest {
    string model_name = 1;
    string model_version = 2;
    
    repeated bytes inputs = 3;
    string input_format = 4;
    
    map<string, string> metadata = 5;
}

// Batch response
message BatchResponse {
    repeated bytes outputs = 1;
    string output_format = 2;
    
    float total_latency_ms = 3;
    repeated float individual_latencies_ms = 4;
    
    map<string, string> metadata = 5;
}

// Model info request
message ModelInfoRequest {
    string model_name = 1;
    string model_version = 2;  // Optional, empty = latest
}

// Model info response
message ModelInfoResponse {
    string model_name = 1;
    string model_version = 2;
    string framework = 3;  // "pytorch", "tensorflow", "onnx"
    
    // Schema info
    string input_schema = 4;
    string output_schema = 5;
    
    // Status
    bool is_loaded = 6;
    float memory_usage_mb = 7;
    int64 prediction_count = 8;
    
    // Metadata
    map<string, string> metadata = 9;
}

// Health check request
message HealthRequest {
    bool include_details = 1;
}

// Health check response
message HealthResponse {
    enum Status {
        UNKNOWN = 0;
        SERVING = 1;
        NOT_SERVING = 2;
    }
    
    Status status = 1;
    string version = 2;
    float uptime_seconds = 3;
    
    // Detailed health info
    map<string, bool> component_status = 4;
    map<string, string> metadata = 5;
}
