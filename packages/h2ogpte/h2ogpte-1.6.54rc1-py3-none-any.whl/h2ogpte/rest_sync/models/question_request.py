# coding: utf-8

"""
    h2oGPTe REST API

     # Overview   Users can easily interact with the h2oGPTe API through its REST API, allowing HTTP requests from any programming language.  ## Authorization: Getting an API key  Sign up/in at Enterprise h2oGPTe and generate one of the following two types of API keys:   - **Global API key**: If a Collection is not specified when creating a new API Key, that key is considered to be a global API Key. Use global API Keys to grant full user impersonation and system-wide access to all of your work. Anyone with access to one of your global API Keys can create, delete, or interact with any of your past, current, and future Collections, Documents, Chats, and settings.  - **Collection-specific API key**: Use Collection-specific API Keys to grant external access to only Chat with a specified Collection and make related API calls to it. Collection-specific API keys do not allow other API calls, such as creation, deletion, or access to other Collections or Chats.   Access Enterprise h2oGPTe through your [H2O Generative AI](https://genai.h2o.ai/appstore) app store account, available with a freemium tier.  ## Authorization: Using an API key   All h2oGPTe REST API requests must include an API Key in the \"Authorization\" HTTP header, formatted as follows:  ``` Authorization: Bearer sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ```  ```sh curl -X 'POST' \\   'https://h2ogpte.genai.h2o.ai/api/v1/collections' \\   -H 'accept: application/json' \\   -H 'Content-Type: application/json' \\   -H 'Authorization: Bearer sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' \\   -d '{     \"name\": \"The name of my Collection\",     \"description\": \"The description of my Collection\",     \"embedding_model\": \"BAAI/bge-large-en-v1.5\"   }' ```      ## Interactive h2oGPTe API testing  This page only showcases the h2oGPTe REST API; you can test it directly in the [Swagger UI](https://h2ogpte.genai.h2o.ai/swagger-ui/). Ensure that you are logged into your Enterprise h2oGPTe account. 

    The version of the OpenAPI document: v1.0.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from h2ogpte.rest_sync.models.guardrails_settings import GuardrailsSettings
from typing import Optional, Set
from typing_extensions import Self

class QuestionRequest(BaseModel):
    """
    QuestionRequest
    """ # noqa: E501
    text_context_list: Optional[List[StrictStr]] = Field(default=None, description="List of raw text strings to be summarized.")
    system_prompt: Optional[StrictStr] = Field(default='', description="Text sent to models which support system prompts. Gives the model overall context in how to respond. Use `auto` for the model default or None for h2oGPTe defaults. Defaults to '' for no system prompt. ")
    llm_args: Optional[Dict[str, Any]] = Field(default=None, description="A map of arguments sent to LLM with query.   * `temperature` **(type=double, default=0.0)** - A value used to modulate the next token probabilities.     0 is the most deterministic and 1 is most creative.   * `top_k` **(type=integer, default=1)** - A number of highest probability vocabulary tokens to keep for top-k-filtering.   * `top_p` **(type=double, default=0.0)** - If set to a value < 1, only the smallest set of most probable     tokens with probabilities that add up to top_p or higher are kept for generation.   * `seed` **(type=integer, default=0)** - A seed for the random number generator when sampling during      generation (if temp>0 or top_k>1 or top_p<1), seed=0 picks a random seed.   * `repetition_penalty` **(type=double, default=1.07)** - A parameter for repetition penalty. 1.0 means no penalty.   * `max_new_tokens` **(type=double, default=1024)** - A maximum number of new tokens to generate.     This limit applies to each (map+reduce) step during summarization and each (map) step during extraction.   * `min_max_new_tokens` **(type=integer, default=512)** - A minimum value for max_new_tokens when auto-adjusting for content of prompt, docs, etc.   * `response_format` **(type=enum[text, json_object, json_code], default=text)** - An output type of LLM   * `guided_json` **(type=map)** - If specified, the output will follow the JSON schema.   * `guided_regex` **(type=string)** - If specified, the output will follow the regex pattern.     Only for models that support guided generation.   * `guided_choice` **(type=array[string])** - If specified, the output will be exactly one of the choices.     Only for models that support guided generation.   * `guided_grammar` **(type=string)** - If specified, the output will follow the context free grammar.     Only for models that support guided generation.   * `guided_whitespace_pattern` **(type=string)** - If specified, will override the default whitespace pattern for guided json decoding.     Only for models that support guided generation.   * `enable_vision` **(type=enum[on, off, auto], default=auto)** - Controls vision mode,     send images to the LLM in addition to text chunks.   * `visible_vision_models` **(type=array[string], default=[auto])** - Controls which vision model to use when processing images.     Must provide exactly one model. [auto] for automatic.   * `images_num_max` **(type=integer, default=None)** - Maximum number of images to process.   * `json_preserve_system_prompt` **(type=boolean, default=None)** - Whether to preserve system prompt in JSON response.   * `client_metadata` **(type=string, default=None)** - Additional metadata to send with the request.   * `min_chars_per_yield` **(type=integer, default=1)** - Minimum characters to yield in streaming response.   * `reasoning_effort` **(type=integer, default=0)** - Level of reasoning effort for the model (higher values = deeper reasoning, e.g., 10000-65000).     Use for models that support chain-of-thought reasoning. 0 means no additional reasoning effort.   * `cost_controls` **(type=map)** A map with cost controls settings:     * `max_cost` **(type=double)** - Sets the maximum allowed cost in USD per LLM call when doing Automatic model routing.       If the estimated cost based on input and output token counts is higher than this limit,       the request will fail as early as possible.     * `max_cost_per_million_tokens` **(type=double)** - Only consider models that cost less than this value in USD per million tokens       when doing automatic routing. Using the max of input and output cost.     * `model` **(type=array[string])** - Optional subset of models to consider when doing automatic routing.       If not specified, all models are considered.     * `willingness_to_pay` **(type=double)** - Controls the willingness to pay extra for a more accurate model for every LLM call       when doing automatic routing, in units of USD per +10% increase in accuracy.       We start with the least accurate model. For each more accurate model,       we accept it if the increase in estimated cost divided by the increase in estimated accuracy       is no more than this value divided by 10%, up to the upper limit specified above.       Lower values will try to keep the cost as low as possible,       higher values will approach the cost limit to increase accuracy. 0 means unlimited.     * `willingness_to_wait` **(type=double)** - Controls the willingness to wait longer for a more accurate model for every LLM call       when doing automatic routing, in units of seconds per +10% increase in accuracy.       We start with the least accurate model. For each more accurate model,       we accept it if the increase in estimated time divided by the increase in estimated accuracy       is no more than this value divided by 10%. Lower values will try to keep the time       as low as possible, higher values will take longer to increase accuracy. 0 means unlimited.   * `use_agent` **(type=boolean, default=False)** - If True, use the AI agent (with access to tools) to generate the response.   * `agent_accuracy` **(type=string, default=\"standard\")** - Effort level by the agent. Only if use_agent=True. One of [\"quick\", \"basic\", \"standard\", \"maximum\"].   * `agent_max_turns` **(type=union[string, integer], default=\"auto\")** - Optional max. number of back-and-forth turns with the agent. Only if use_agent=True. Either \"auto\" or an integer.   * `agent_tools` **(type=union[string, array[string]], default=\"auto\")** - Either \"auto\", \"all\", \"any\" to enable all available tools, or a specific list of tools to use. Only if use_agent=True.   * `agent_type` **(type=string, default=\"auto\")** - Type of agent to use for task processing.   * `agent_original_files` **(type=array[string], default=None)** - List of file paths for agent to process.   * `agent_timeout` **(type=integer, default=None)** - Timeout in seconds for each agent turn.   * `agent_total_timeout` **(type=integer, default=3600)** - Total timeout in seconds for all agent processing.   * `agent_code_writer_system_message` **(type=string, default=None)** - System message for agent code writer.   * `agent_num_executable_code_blocks_limit` **(type=integer, default=1)** - Maximum number of executable code blocks.   * `agent_system_site_packages` **(type=boolean, default=True)** - Whether agent has access to system site packages.   * `agent_main_model` **(type=string, default=None)** - Main model to use for agent.   * `agent_max_stream_length` **(type=integer, default=None)** - Maximum stream length for agent response.   * `agent_max_memory_usage` **(type=integer, default=16*1024**3)** - Maximum memory usage for agent in bytes (16GB default).   * `agent_main_reasoning_effort` **(type=integer, default=None)** - Effort level for main reasoning.   * `agent_advanced_reasoning_effort` **(type=integer, default=None)** - Effort level for advanced reasoning.   * `agent_max_confidence_level` **(type=integer, default=None)** - Maximum confidence level for agent responses.   * `agent_planning_forced_mode` **(type=boolean, default=None)** - Whether to force planning mode for agent.   * `agent_too_soon_forced_mode` **(type=boolean, default=None)** - Whether to force \"too soon\" mode for agent.   * `agent_critique_forced_mode` **(type=integer, default=None)** - Whether to force critique mode for agent.   * `agent_query_understanding_parallel_calls` **(type=integer, default=None)** - Number of parallel calls for query understanding.   * `tool_building_mode` **(type=string, default=None)** - Mode for tool building configuration.   * `agent_stream_files` **(type=boolean, default=True)** - Whether to stream files from agent. ")
    guardrails_settings: Optional[GuardrailsSettings] = None
    timeout: Optional[StrictInt] = Field(default=None, description="Timeout in seconds.")
    pre_prompt_query: Optional[StrictStr] = Field(default=None, description="Text that is prepended before the contextual document chunks in text_context_list. Only used if text_context_list is provided.")
    prompt_query: Optional[StrictStr] = Field(default=None, description="Text that is appended after the contextual document chunks in text_context_list. Only used if text_context_list is provided.")
    question: StrictStr = Field(description="Text query to send to the LLM.")
    chat_conversation: Optional[List[Annotated[List[StrictStr], Field(min_length=2, max_length=2)]]] = Field(default=None, description="List of tuples for (human, bot) conversation that will be pre-appended to an (question, None) case for a query. ")
    additional_properties: Dict[str, Any] = {}
    __properties: ClassVar[List[str]] = ["text_context_list", "system_prompt", "llm_args", "guardrails_settings", "timeout", "pre_prompt_query", "prompt_query", "question", "chat_conversation"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of QuestionRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * Fields in `self.additional_properties` are added to the output dict.
        """
        excluded_fields: Set[str] = set([
            "additional_properties",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of guardrails_settings
        if self.guardrails_settings:
            _dict['guardrails_settings'] = self.guardrails_settings.to_dict()
        # puts key-value pairs in additional_properties in the top level
        if self.additional_properties is not None:
            for _key, _value in self.additional_properties.items():
                _dict[_key] = _value

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of QuestionRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "text_context_list": obj.get("text_context_list"),
            "system_prompt": obj.get("system_prompt") if obj.get("system_prompt") is not None else '',
            "llm_args": obj.get("llm_args"),
            "guardrails_settings": GuardrailsSettings.from_dict(obj["guardrails_settings"]) if obj.get("guardrails_settings") is not None else None,
            "timeout": obj.get("timeout"),
            "pre_prompt_query": obj.get("pre_prompt_query"),
            "prompt_query": obj.get("prompt_query"),
            "question": obj.get("question"),
            "chat_conversation": obj.get("chat_conversation")
        })
        # store additional fields in additional_properties
        for _key in obj.keys():
            if _key not in cls.__properties:
                _obj.additional_properties[_key] = obj.get(_key)

        return _obj


