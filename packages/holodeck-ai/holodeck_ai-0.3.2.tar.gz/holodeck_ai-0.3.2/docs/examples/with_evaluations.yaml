# Agent with Evaluations Example
#
# This example demonstrates the evaluation framework with:
# - DeepEval GEval metrics (custom criteria with chain-of-thought)
# - DeepEval RAG metrics (faithfulness, answer relevancy)
# - NLP metrics (F1, ROUGE)
# - Legacy Azure AI metrics (deprecated, for backwards compatibility)
# - Per-metric model overrides
# - Threshold-based pass/fail criteria
#
# Usage: holodeck test with_evaluations.yaml

name: qa-agent
description: Quality assurance agent with comprehensive evaluation

model:
  provider: ollama
  name: llama3.2:latest
  temperature: 0.3
  max_tokens: 1024

instructions:
  inline: |
    You are a quality assurance specialist. Your task is to:
    1. Review provided content
    2. Check for accuracy and completeness
    3. Identify issues or improvements needed
    4. Provide actionable feedback

evaluations:
  # Global model used for all DeepEval metrics (can be overridden per metric)
  model:
    provider: ollama
    name: llama3.2:latest
    temperature: 0.0  # Use 0 for deterministic evaluation

  metrics:
    # ============================================
    # TIER 1: DeepEval Metrics (Recommended)
    # ============================================

    # GEval: Custom criteria with chain-of-thought evaluation
    - type: geval
      name: "Coherence"
      criteria: |
        Evaluate whether the response is clear, well-structured,
        and easy to understand.
      evaluation_steps:
        - "Evaluate whether the response uses clear and direct language."
        - "Check if the explanation avoids jargon or explains it when used."
        - "Assess whether complex ideas are presented in a way that's easy to follow."
        - "Identify any vague or confusing parts that reduce understanding."
      evaluation_params:
        - actual_output
      threshold: 0.7
      enabled: true
      fail_on_error: false

    # GEval: Helpfulness check
    - type: geval
      name: "Helpfulness"
      criteria: |
        Evaluate whether the response provides actionable, practical help
        that addresses the user's needs.
      evaluation_params:
        - actual_output
        - input
      threshold: 0.75
      enabled: true

    # RAG: Faithfulness - detect hallucinations
    - type: rag
      metric_type: faithfulness
      threshold: 0.8
      include_reason: true
      enabled: true
      fail_on_error: false

    # RAG: Answer Relevancy - response addresses query
    - type: rag
      metric_type: answer_relevancy
      threshold: 0.7
      include_reason: true
      enabled: true

    # GEval with per-metric model override for critical evaluation
    - type: geval
      name: "Technical Accuracy"
      criteria: |
        Evaluate the response for accuracy in technical content.
        Check that code examples, commands, and technical details are correct.
      evaluation_params:
        - actual_output
        - input
        - expected_output
      threshold: 0.85
      enabled: true
      fail_on_error: false
      model:  # Override global model for this critical metric
        provider: openai
        name: gpt-4
        temperature: 0.0

    # ============================================
    # TIER 2: NLP Metrics (Standard)
    # ============================================

    # F1 Score: Token-level precision/recall
    - type: standard
      metric: f1_score
      threshold: 0.7
      enabled: true
      fail_on_error: true  # Hard failure for this metric

    # ROUGE: N-gram overlap for summarization
    - type: standard
      metric: rouge
      threshold: 0.6
      enabled: true
      fail_on_error: false

    # ============================================
    # TIER 3: Legacy AI Metrics (Deprecated)
    # ============================================
    # NOTE: These Azure AI-based metrics are deprecated.
    # Migrate to DeepEval metrics for better flexibility.
    #
    # Migration guide:
    # - groundedness -> type: rag, metric_type: faithfulness
    # - relevance -> type: rag, metric_type: answer_relevancy
    # - coherence -> type: geval with custom criteria
    # - safety -> type: geval with custom criteria

    # DEPRECATED - Use type: rag with metric_type: faithfulness instead
    # - type: standard
    #   metric: groundedness
    #   threshold: 0.8
    #   enabled: false  # Disabled - using faithfulness instead

    # DEPRECATED - Use type: rag with metric_type: answer_relevancy instead
    # - type: standard
    #   metric: relevance
    #   threshold: 0.75
    #   enabled: false  # Disabled - using answer_relevancy instead

test_cases:
  - name: "Review product specification"
    input: |
      Please review this product spec for completeness:
      - Feature: User authentication
      - Requirement: Support OAuth2.0
      - Status: In development
      - Timeline: Q4 2025
    ground_truth: |
      The specification is incomplete. Missing:
      1. Security requirements (encryption, token expiration)
      2. Error handling scenarios
      3. Testing strategy
      4. Rollback plan
    evaluations:
      # Per-test evaluation overrides
      - type: rag
        metric_type: faithfulness
        threshold: 0.8
      - type: rag
        metric_type: answer_relevancy
        threshold: 0.7
      - type: standard
        metric: f1_score
        threshold: 0.6

  - name: "Technical documentation quality"
    input: |
      Evaluate this API documentation:
      GET /api/users/{id} - Retrieve user by ID
      Returns: User object
      Errors: 404 if not found, 401 if unauthorized
    ground_truth: |
      Documentation is clear but needs:
      1. Request/response schema details
      2. Example payloads
      3. Rate limiting information
      4. Pagination for list endpoints (if applicable)
    evaluations:
      - type: geval
        name: "Coherence"
        criteria: "Evaluate whether the response is clear and well-structured."
        evaluation_params:
          - actual_output
        threshold: 0.7
      - type: geval
        name: "Technical Accuracy"
        criteria: "Evaluate technical accuracy of the feedback provided."
        evaluation_params:
          - actual_output
          - input
        threshold: 0.8

  - name: "Code review feedback"
    input: |
      Review this Python code:
      def calculate_discount(price, discount):
          return price - (price * discount)
    ground_truth: |
      The code needs:
      1. Type hints for parameters and return value
      2. Input validation (negative price, discount > 1)
      3. Docstring explaining the function
      4. Consider using Decimal for financial calculations
    evaluations:
      - type: geval
        name: "Helpfulness"
        criteria: "Evaluate if the feedback is actionable and helpful."
        evaluation_params:
          - actual_output
          - input
        threshold: 0.75
      - type: rag
        metric_type: answer_relevancy
        threshold: 0.7
