//! SIMD Viterbi Optimization Tests (TDD)
//!
//! This test suite verifies the correctness and performance of SIMD-optimized
//! Viterbi algorithm for the Unigram tokenizer.
//!
//! Target: 20x faster than HuggingFace Tokenizers while maintaining 100% accuracy.
//!
//! Run with: cargo test --test unigram_simd_viterbi --release

use budtiktok_core::unigram::UnigramPiece;
use budtiktok_core::unigram_fast::{UnigramFast, UnigramFastConfig};
use budtiktok_core::vocab::{Vocabulary, SpecialTokens};
use tokenizers::Tokenizer as HfTokenizer;
use ahash::AHashMap;
use std::time::Instant;

const XLNET_PATH: &str = "/home/bud/Desktop/latentbud/budtiktok/benchmark_data/xlnet/tokenizer.json";

// ============================================================================
// Test Fixtures
// ============================================================================

fn load_tokenizers() -> Option<(UnigramFast, HfTokenizer)> {
    if !std::path::Path::new(XLNET_PATH).exists() {
        return None;
    }

    let hf_tok = HfTokenizer::from_file(XLNET_PATH).ok()?;

    let json_content = std::fs::read_to_string(XLNET_PATH).ok()?;
    let json: serde_json::Value = serde_json::from_str(&json_content).ok()?;

    let model = json.get("model")?;
    let vocab_array = model.get("vocab")?.as_array()?;

    let mut vocab = Vec::with_capacity(vocab_array.len());
    let mut pieces = Vec::with_capacity(vocab_array.len());

    for item in vocab_array {
        let arr = item.as_array()?;
        let token = arr.get(0)?.as_str()?;
        let score = arr.get(1)?.as_f64()?;
        vocab.push(token.to_string());
        pieces.push(UnigramPiece {
            token: token.to_string(),
            score,
        });
    }

    let unk_id = model.get("unk_id")
        .and_then(|v| v.as_u64())
        .map(|v| v as u32)
        .unwrap_or(0);

    let config = UnigramFastConfig {
        unk_id,
        add_prefix_space: true,
        fuse_unk: true,
        ..Default::default()
    };

    let mut vocab_map = AHashMap::with_capacity(vocab.len());
    for (i, token) in vocab.iter().enumerate() {
        vocab_map.insert(token.clone(), i as u32);
    }
    let vocabulary = Vocabulary::new(vocab_map, SpecialTokens {
        unk_token: model.get("unk_token").and_then(|v| v.as_str()).map(|s| s.to_string()),
        ..Default::default()
    });

    let fast_tok = UnigramFast::new(vocabulary, pieces, config);
    Some((fast_tok, hf_tok))
}

fn generate_text(length: usize) -> String {
    let base = "The quick brown fox jumps over the lazy dog. \
                Machine learning enables computers to learn from data. \
                Natural language processing is a key component of AI. \
                Transformers have revolutionized the field of NLP. ";
    let mut result = String::with_capacity(length);
    while result.len() < length {
        result.push_str(base);
    }
    result.truncate(length);
    result
}

// ============================================================================
// Correctness Tests - SIMD must match scalar exactly
// ============================================================================

#[test]
fn test_simd_viterbi_matches_scalar_short() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => {
            eprintln!("Skipping: tokenizer not found");
            return;
        }
    };

    let test_cases = [
        "hello",
        "world",
        "hello world",
        "the quick brown fox",
        "123456789",
        "caf√© r√©sum√©",
        "Êó•Êú¨Ë™û„ÉÜ„Çπ„Éà",
        "mixed 123 text caf√©",
    ];

    for text in &test_cases {
        let scalar_ids = fast_tok.encode_hf_compat(text);
        let simd_ids = fast_tok.encode_hf_compat_simd(text);

        assert_eq!(
            scalar_ids, simd_ids,
            "SIMD mismatch for '{}': scalar={:?}, simd={:?}",
            text, scalar_ids, simd_ids
        );
    }
    println!("Short text SIMD correctness: PASSED");
}

#[test]
fn test_simd_viterbi_matches_scalar_medium() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    for length in [100, 250, 500, 1000] {
        let text = generate_text(length);
        let scalar_ids = fast_tok.encode_hf_compat(&text);
        let simd_ids = fast_tok.encode_hf_compat_simd(&text);

        assert_eq!(
            scalar_ids, simd_ids,
            "SIMD mismatch for {}-char text",
            length
        );
    }
    println!("Medium text SIMD correctness: PASSED");
}

#[test]
fn test_simd_viterbi_matches_scalar_long() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    for length in [2000, 5000, 10000] {
        let text = generate_text(length);
        let scalar_ids = fast_tok.encode_hf_compat(&text);
        let simd_ids = fast_tok.encode_hf_compat_simd(&text);

        assert_eq!(
            scalar_ids, simd_ids,
            "SIMD mismatch for {}-char text",
            length
        );
    }
    println!("Long text SIMD correctness: PASSED");
}

// ============================================================================
// Accuracy Tests - SIMD must match HuggingFace exactly
// ============================================================================

#[test]
fn test_simd_matches_huggingface() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let test_cases = [
        "hello world",
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning enables computers to learn from data.",
        "Natural language processing is a key component of AI.",
        "12345 numbers mixed with text",
        "Special characters: @#$%^&*()",
    ];

    for text in &test_cases {
        let hf_enc = hf_tok.encode(*text, false).unwrap();
        let hf_ids: Vec<u32> = hf_enc.get_ids().to_vec();
        let simd_ids = fast_tok.encode_hf_compat_simd(text);

        assert_eq!(
            hf_ids, simd_ids,
            "SIMD vs HF mismatch for '{}': hf={:?}, simd={:?}",
            text, hf_ids, simd_ids
        );
    }
    println!("SIMD vs HuggingFace accuracy: PASSED");
}

// ============================================================================
// Performance Tests - SIMD must achieve 20x speedup
// ============================================================================

#[test]
fn test_simd_performance_target_1000_chars() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let text = generate_text(1000);
    let iterations = 1000;

    // Warmup - use v2 with inlined UTF-8
    for _ in 0..100 {
        let _ = fast_tok.encode_hf_compat_simd_v2(&text);
        let _ = hf_tok.encode(text.as_str(), false);
    }

    // HuggingFace timing
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = hf_tok.encode(text.as_str(), false);
    }
    let hf_time = start.elapsed();

    // SIMD v2 timing with inlined UTF-8
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = fast_tok.encode_hf_compat_simd_v2(&text);
    }
    let simd_time = start.elapsed();

    let speedup = hf_time.as_nanos() as f64 / simd_time.as_nanos() as f64;

    println!("\n=== SIMD Performance (1000 chars, {} iterations) ===", iterations);
    println!("HuggingFace: {:?}", hf_time);
    println!("SIMD:        {:?}", simd_time);
    println!("Speedup:     {:.1}x", speedup);
    println!("Target:      20.0x");

    // Currently we're at ~10x, target is 20x
    // This test documents progress - we'll update the assertion as we optimize
    assert!(
        speedup >= 8.0,
        "SIMD speedup {:.1}x is below minimum acceptable threshold of 8x",
        speedup
    );

    if speedup >= 20.0 {
        println!("‚úì TARGET ACHIEVED: {:.1}x >= 20x", speedup);
    } else {
        println!("‚óã Progress: {:.1}x / 20x ({:.0}%)", speedup, speedup / 20.0 * 100.0);
    }
}

#[test]
fn test_simd_performance_target_5000_chars() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let text = generate_text(5000);
    let iterations = 200;

    // Warmup with v2
    for _ in 0..20 {
        let _ = fast_tok.encode_hf_compat_simd_v2(&text);
        let _ = hf_tok.encode(text.as_str(), false);
    }

    // HuggingFace timing
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = hf_tok.encode(text.as_str(), false);
    }
    let hf_time = start.elapsed();

    // SIMD v2 timing
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = fast_tok.encode_hf_compat_simd_v2(&text);
    }
    let simd_time = start.elapsed();

    let speedup = hf_time.as_nanos() as f64 / simd_time.as_nanos() as f64;

    println!("\n=== SIMD Performance (5000 chars, {} iterations) ===", iterations);
    println!("HuggingFace: {:?}", hf_time);
    println!("SIMD:        {:?}", simd_time);
    println!("Speedup:     {:.1}x", speedup);

    assert!(speedup >= 8.0, "SIMD speedup below minimum");
}

// ============================================================================
// Batch Performance Tests
// ============================================================================

#[test]
fn test_simd_batch_performance() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    // Generate a batch of texts
    let batch_size = 100;
    let texts: Vec<String> = (0..batch_size)
        .map(|i| generate_text(500 + (i * 10)))
        .collect();
    let text_refs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();

    let iterations = 10;

    // Warmup
    for _ in 0..2 {
        let _ = fast_tok.encode_batch_hf_compat_simd(&text_refs);
    }

    // HuggingFace batch timing (sequential)
    let start = Instant::now();
    for _ in 0..iterations {
        for text in &texts {
            let _ = hf_tok.encode(text.as_str(), false);
        }
    }
    let hf_time = start.elapsed();

    // Our SIMD parallel batch
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = fast_tok.encode_batch_hf_compat_simd(&text_refs);
    }
    let batch_time = start.elapsed();

    let speedup = hf_time.as_nanos() as f64 / batch_time.as_nanos() as f64;

    println!("\n=== SIMD Batch Performance ({} texts x {} iterations) ===", batch_size, iterations);
    println!("HuggingFace (seq): {:?}", hf_time);
    println!("SIMD Batch:        {:?}", batch_time);
    println!("Speedup:           {:.1}x", speedup);

    // With parallelism, we should see much higher speedups
    assert!(speedup >= 5.0, "Batch speedup below minimum");
}

// ============================================================================
// Component Microbenchmarks - to identify optimization opportunities
// ============================================================================

#[test]
fn test_trie_lookup_performance() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    // Benchmark trie prefix enumeration
    let text = "‚ñÅhello‚ñÅworld‚ñÅthis‚ñÅis‚ñÅa‚ñÅtest";
    let bytes = text.as_bytes();
    let iterations = 100_000;

    let start = Instant::now();
    let mut total_matches = 0usize;
    for _ in 0..iterations {
        for pos in 0..bytes.len() {
            fast_tok.trie_lookup(&bytes[pos..pos+1.min(bytes.len()-pos)]);
            total_matches += 1;
        }
    }
    let elapsed = start.elapsed();

    println!("\n=== Trie Lookup Microbenchmark ===");
    println!("Iterations: {}", iterations);
    println!("Total lookups: {}", total_matches);
    println!("Time: {:?}", elapsed);
    println!("Lookups/sec: {:.0}", total_matches as f64 / elapsed.as_secs_f64());
}

#[test]
fn test_score_lookup_performance() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let iterations = 10_000_000;
    let vocab_size = fast_tok.vocab_size();

    // Benchmark score array access pattern
    let start = Instant::now();
    let mut sum = 0.0f64;
    for i in 0..iterations {
        let token_id = (i % vocab_size) as u32;
        // Direct score access (simulating hot path)
        if let Some(tok) = fast_tok.id_to_token(token_id) {
            sum += tok.len() as f64; // Prevent optimization
        }
    }
    let elapsed = start.elapsed();

    println!("\n=== Score Lookup Microbenchmark ===");
    println!("Iterations: {}", iterations);
    println!("Time: {:?}", elapsed);
    println!("Lookups/sec: {:.0}", iterations as f64 / elapsed.as_secs_f64());
    println!("Sum (prevent opt): {}", sum);
}

// ============================================================================
// Memory Layout Tests
// ============================================================================

#[test]
fn test_memory_layout_efficiency() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let text = generate_text(10000);
    let iterations = 100;

    // Test with normal allocation
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = fast_tok.encode_hf_compat(&text);
    }
    let normal_time = start.elapsed();

    // Test with arena allocation
    let start = Instant::now();
    for _ in 0..iterations {
        let _ = fast_tok.encode_with_arena(&text);
    }
    let arena_time = start.elapsed();

    println!("\n=== Memory Layout Comparison (10k chars) ===");
    println!("Normal allocation: {:?}", normal_time);
    println!("Arena allocation:  {:?}", arena_time);
    println!("Arena speedup:     {:.2}x", normal_time.as_nanos() as f64 / arena_time.as_nanos() as f64);
}

// ============================================================================
// Edge Case Tests
// ============================================================================

#[test]
fn test_simd_empty_input() {
    let (fast_tok, _) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    let result = fast_tok.encode_hf_compat_simd("");
    let expected: Vec<u32> = vec![];
    assert_eq!(result, expected);
    println!("Empty input SIMD: PASSED");
}

#[test]
fn test_simd_single_char() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    for c in ['a', 'A', '1', ' ', '.', 'Êó•'] {
        let text = c.to_string();
        let hf_enc = hf_tok.encode(text.as_str(), false).unwrap();
        let hf_ids: Vec<u32> = hf_enc.get_ids().to_vec();
        let simd_ids = fast_tok.encode_hf_compat_simd(&text);

        assert_eq!(hf_ids, simd_ids, "Single char '{}' mismatch", c);
    }
    println!("Single char SIMD: PASSED");
}

#[test]
fn test_simd_unicode_boundaries() {
    let (fast_tok, hf_tok) = match load_tokenizers() {
        Some(t) => t,
        None => return,
    };

    // Unicode characters of various byte lengths
    let test_cases = [
        "a",      // 1 byte
        "√©",      // 2 bytes
        "‰∏≠",     // 3 bytes
        "ùÑû",     // 4 bytes
        "a√©‰∏≠ùÑû", // mixed
        "caf√©‰∏≠ÊñáùÑûmusic",
    ];

    for text in &test_cases {
        let hf_enc = hf_tok.encode(*text, false).unwrap();
        let hf_ids: Vec<u32> = hf_enc.get_ids().to_vec();
        let simd_ids = fast_tok.encode_hf_compat_simd(text);

        assert_eq!(hf_ids, simd_ids, "Unicode '{}' mismatch", text);
    }
    println!("Unicode boundaries SIMD: PASSED");
}
