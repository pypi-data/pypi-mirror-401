# aggregate_analysis.py
import argparse
import json
import os
import subprocess  # æ·»åŠ å¯¼å…¥
from pathlib import Path
import glob

def aggregate_stack_data(input_dir, output_dir):
    """èšåˆæ‰€æœ‰èŠ‚ç‚¹çš„å †æ ˆæ•°æ®"""
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„æ•°æ®
    all_stack_files = []
    for node_dir in input_path.iterdir():
        if node_dir.is_dir() and node_dir.name.startswith('node_'):
            stack_files = list(node_dir.glob("*.json"))
            all_stack_files.extend(stack_files)
    
    print(f"æ‰¾åˆ° {len(all_stack_files)} ä¸ªå †æ ˆæ•°æ®æ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    aggregated_data = []
    for stack_file in all_stack_files:
        try:
            with open(stack_file, 'r') as f:
                data = json.load(f)
                if isinstance(data, list):
                    aggregated_data.extend(data)
                else:
                    aggregated_data.append(data)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {stack_file} å¤±è´¥: {e}")
    
    # ä¿å­˜èšåˆæ•°æ®
    aggregated_file = output_path / "aggregated_stack_data.json"
    with open(aggregated_file, 'w') as f:
        json.dump(aggregated_data, f, indent=2)
    
    print(f"èšåˆæ•°æ®å·²ä¿å­˜åˆ°: {aggregated_file}")
    
    # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼ä»¥ä¾¿åˆ†æ
    convert_to_original_format(aggregated_data, output_path)
    
def convert_to_original_format(aggregated_data, output_path):
    """è½¬æ¢ä¸ºåŸæœ‰.stackdataæ ¼å¼"""
    for i, data in enumerate(aggregated_data):
        if isinstance(data, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸ªå…ƒç´ 
            for j, item in enumerate(data):
                filename = f"{i:05d}_{j:05d}.stackdata"
                filepath = output_path / filename
                with open(filepath, 'w') as f:
                    json.dump(item, f, indent=2)
        else:
            # å•ä¸ªå¯¹è±¡
            filename = f"{i:05d}.stackdata"
            filepath = output_path / filename
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
    
    print(f"å·²è½¬æ¢ {len(aggregated_data)} ä¸ªæ–‡ä»¶ä¸ºåŸå§‹æ ¼å¼")

def aggregate_gxdata(input_dir, output_dir):
    """èšåˆæ‰€æœ‰ .gxdata æ–‡ä»¶ï¼ŒæŒ‰ rank é¡ºåºå†™å…¥å•ä¸ªæ–‡ä»¶"""
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # æ”¶é›†æ‰€æœ‰ .gxdata æ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
    gxdata_files = []
    for path in input_path.rglob("*.gxdata"):
        if path.is_file():
            gxdata_files.append(path)

    if not gxdata_files:
        print("æœªæ‰¾åˆ°ä»»ä½• .gxdata æ–‡ä»¶")
        return

    # æŒ‰æ–‡ä»¶åå‰ç¼€ï¼ˆrankï¼‰æ’åº
    def get_rank_from_filename(filepath):
        name = filepath.name
        if '-' in name and name.endswith('.gxdata'):
            try:
                rank_str = name.split('-')[0]
                return int(rank_str)
            except ValueError:
                pass
        return float('inf')  # æ— æ³•è§£æçš„æ”¾æœ€å

    gxdata_files.sort(key=get_rank_from_filename)

    # åˆå¹¶å†…å®¹
    aggregated_gxdata_file = output_path / "aggregated_gxdata.txt"
    with open(aggregated_gxdata_file, 'w') as out_f:
        for i, gx_file in enumerate(gxdata_files):
            if i > 0:
                out_f.write("\n" + "="*80 + "\n\n")  # åˆ†éš”ä¸åŒ rank
            out_f.write(f"# Source: {gx_file}\n")
            try:
                with open(gx_file, 'r') as f:
                    out_f.write(f.read())
            except Exception as e:
                out_f.write(f"# ERROR: Failed to read {gx_file}: {e}\n")

    print(f"å·²èšåˆ {len(gxdata_files)} ä¸ª .gxdata æ–‡ä»¶åˆ°: {aggregated_gxdata_file}")
def main():
    parser = argparse.ArgumentParser(description="èšåˆåˆ†å¸ƒå¼å †æ ˆæ•°æ®")
    parser.add_argument("--input-dir", default="/tmp/stack_data_all", help="è¾“å…¥ç›®å½•ï¼ˆåè°ƒå™¨çš„dumpè·¯å¾„ï¼‰")
    parser.add_argument("--output-dir", default="/tmp/stack_analysis", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è°ƒç”¨èšåˆå‡½æ•°
    # aggregate_stack_data(args.input_dir, args.output_dir)
    
    # è¿è¡ŒåŸæœ‰åˆ†æè„šæœ¬    
    cmd = [
        "python3", "-m", "cluster.stack_processor",
        "--path", args.input_dir,
        "--output-dir", args.output_dir,
    ]
    
    print("è¿è¡Œå †æ ˆåˆ†æ...")
    result = subprocess.run(cmd)
    if result.returncode == 0:
        print("åˆ†æå®Œæˆ")
    else:
        print("åˆ†æå¤±è´¥")
        
    cmd = [
        "python3", "-m", "cluster.process_processor",
        "--dump-path", args.input_dir,
        "--output-dir", args.output_dir,
    ]
    
    print("è¿è¡Œå †æ ˆåˆ†æ...")
    result = subprocess.run(cmd)
    if result.returncode == 0:
        print("åˆ†æå®Œæˆ")
    else:
        print("åˆ†æå¤±è´¥")
    
    # ğŸ‘‡ æ–°å¢ï¼šèšåˆ .gxdata æ–‡ä»¶
    aggregate_gxdata(args.input_dir, args.output_dir)

if __name__ == "__main__":
    main()