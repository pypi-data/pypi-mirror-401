from OpenRT.models import BaseModel
from OpenRT.datasets import BaseDataset
from OpenRT.attacks import BaseAttack, AttackResult
from OpenRT.evaluators import BaseEvaluator, EvaluationMetrics
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

class Orchestrator:
    """
    Experiment workflow orchestrator (The "Conductor of the Orchestra").

    This class has a very focused and important responsibility: it receives all already-instantiated modules
    (model, dataset, attack, evaluator) and executes a complete, reproducible experiment in a fixed order.
    It does not contain any specific attack or evaluation logic itself, thereby achieving high decoupling
    between the core workflow and specific implementations.
    """
    def __init__(
        self,
        model: BaseModel,
        dataset: BaseDataset,
        attack: BaseAttack,
        evaluator: BaseEvaluator,
        max_workers: int = 16
    ):
        """
        Initialize the orchestrator.

        Args:
            model: The already instantiated model object.
            dataset: The already instantiated dataset object.
            attack: The already instantiated attack object.
            evaluator: The already instantiated evaluator object.
            max_workers: Maximum number of threads when executing attacks in parallel.
        """
        self.model = model
        self.dataset = dataset
        self.attack = attack
        self.evaluator = evaluator
        self.max_workers = max_workers

    def run(self) -> Tuple[EvaluationMetrics, List[AttackResult]]:
        """
        Execute the complete experiment workflow.

        Returns:
            A tuple containing the final evaluation metrics and a detailed list of results from each attack.
        """
        # 1. Print experiment configuration information for easy recording and debugging.
        print("--- Starting Experiment ---")
        print(f"Model: {self.model.__class__.__name__}")
        print(f"Dataset: {self.dataset.__class__.__name__} (Size: {len(self.dataset)})")
        print(f"Attack: {self.attack.__class__.__name__}")
        print(f"Evaluator: {self.evaluator.__class__.__name__}")
        print("---------------------------")

        # 2. Execute attacks in parallel using multiple threads
        all_results: List[AttackResult] = [None] * len(self.dataset)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all attack tasks
            future_to_index = {
                executor.submit(self.attack.attack, target): i
                for i, target in enumerate(self.dataset)
            }

            # Collect results
            in_success=0
            completed_count = 0
            for future in tqdm(as_completed(future_to_index), total=len(future_to_index), desc="Processing"):
                index = future_to_index[future]
                target = self.dataset[index]

                try:
                    result = future.result()
                    all_results[index] = result
                    completed_count += 1
                    in_success+=result.success
                    # print(f"\n[ Completed target {completed_count}/{len(self.dataset)}: '{str(target)}...' ]")
                    # print(f"  > Attack Outcome: {'Success' if result.success else 'Failure'}")
                except Exception as e:
                    import traceback
                    print(f"\n[ Error attacking target {index+1}/{len(self.dataset)}: '{str(target)}...' ]")
                    print(f"  > Error: {e}")
                    print(f"  > Traceback: {traceback.format_exc()}")
                    # Create a failed result
                    all_results[index] = AttackResult(success=False, target=target)

        if completed_count == 0:
            print("No attacks were completed successfully.")
            return EvaluationMetrics(attack_success_rate=0), all_results

        # 3. After all attacks complete, call the evaluator for final evaluation.
        print("\n--- Evaluating Full Experiment Results ---")
        metrics = self.evaluator.evaluate(all_results)

        if isinstance(metrics, EvaluationMetrics):
            metrics.insucess_rate = in_success/len(self.dataset)
            print(f"Final Attack Success Rate: {max(metrics.attack_success_rate,metrics.insucess_rate):.2%}")
        elif isinstance(metrics, dict) and "overall" in metrics and isinstance(metrics["overall"], EvaluationMetrics):
            metrics["overall"].insucess_rate = in_success/len(self.dataset)
            print(f"Final Attack Success Rate: {max(metrics['overall'].attack_success_rate,metrics['overall'].insucess_rate):.2%}")
        print("--- Experiment Finished ---\n")

        # 4. Return results for the caller to perform post-processing (e.g., save to file).
        return metrics, all_results