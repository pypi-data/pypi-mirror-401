#!/usr/bin/env python3
"""
Retrieval component for AutoDAN_Turbo_R implementation
Uses OpenAI API with configurable base_url for embeddings
"""

import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import json
import openai
import os


class OpenAIRetrieval:
    """
    OpenAI API-based retrieval for AutoDAN_Turbo_R.
    Uses OpenAI text-embedding models for semantic similarity.
    Supports custom base_url for API endpoints.
    """

    def __init__(self, openai_api_key: str, base_url: Optional[str] = None,
                 model_name: str = "text-embedding-3-small"):
        """
        Initialize OpenAI-based retrieval.

        Args:
            openai_api_key: OpenAI API key
            base_url: Optional custom API endpoint URL
            model_name: OpenAI embedding model name (default: text-embedding-3-small)
        """
        # Configure OpenAI client
        if base_url:
            self.client = openai.OpenAI(
                api_key=openai_api_key,
                base_url=base_url
            )
        else:
            self.client = openai.OpenAI(api_key=openai_api_key)

        self.model_name = model_name
        self.embedding_cache = {}

    def get_embedding(self, text: str) -> np.ndarray:
        """
        Get embedding vector for given text using OpenAI API.

        Args:
            text: Text to embed

        Returns:
            Numpy embedding vector
        """
        # Check cache first
        if text in self.embedding_cache:
            return self.embedding_cache[text]

        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )

            # Convert to numpy array
            embedding = np.array(response.data[0].embedding, dtype=np.float32)

            # Cache the embedding
            self.embedding_cache[text] = embedding

            return embedding

        except Exception as e:
            print(f"Error getting embedding from OpenAI API: {e}")
            # Return zero vector as fallback
            return np.zeros(self._get_embedding_dim(), dtype=np.float32)

    def get_embedding_as_list(self, text: str) -> List[float]:
        """
        Get embedding as a list (for JSON serialization).

        Args:
            text: Text to embed

        Returns:
            List of floats
        """
        embedding = self.get_embedding(text)
        return embedding.tolist()

    def _get_embedding_dim(self) -> int:
        """Get embedding dimension based on model name."""
        if "3-large" in self.model_name:
            return 3072
        elif "3-small" in self.model_name:
            return 1536
        elif "ada-002" in self.model_name:
            return 1536
        else:
            return 1536  # Default dimension

    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors.

        Args:
            vec1: First embedding vector
            vec2: Second embedding vector

        Returns:
            Cosine similarity score (0-1)
        """
        try:
            # Calculate dot product and norms
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            # Avoid division by zero
            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)

        except Exception as e:
            print(f"Error calculating cosine similarity: {e}")
            return 0.0

    def search(self, query: str, candidate_texts: List[str],
               top_k: int = 5, similarity_threshold: float = 0.7) -> List[Tuple[str, float]]:
        """
        Search for most similar texts to query using OpenAI embeddings.

        Args:
            query: Search query text
            candidate_texts: List of texts to search through
            top_k: Number of top results to return
            similarity_threshold: Minimum similarity score threshold

        Returns:
            List of tuples (text, similarity_score) sorted by similarity
        """
        if not candidate_texts:
            return []

        try:
            # Get embedding for query
            query_embedding = self.get_embedding(query)

            # Calculate similarities with all candidates
            results = []
            for text in candidate_texts:
                text_embedding = self.get_embedding(text)
                similarity = self.cosine_similarity(query_embedding, text_embedding)

                if similarity >= similarity_threshold:
                    results.append((text, similarity))

            # Sort by similarity (descending)
            results.sort(key=lambda x: x[1], reverse=True)

            # Return top_k results
            return results[:top_k]

        except Exception as e:
            print(f"Error in search: {e}")
            return []

    def precompute_embeddings(self, texts: List[str]) -> None:
        """
        Pre-compute and cache embeddings for given texts.

        Args:
            texts: List of texts to embed
        """
        for text in texts:
            self.get_embedding(text)

    def clear_cache(self) -> None:
        """Clear the embedding cache."""
        self.embedding_cache.clear()

    def get_cache_stats(self) -> Dict[str, int]:
        """
        Get statistics about the embedding cache.

        Returns:
            Dictionary with cache statistics
        """
        return {
            "cached_embeddings": len(self.embedding_cache),
            "embedding_dim": self._get_embedding_dim(),
            "model_name": self.model_name
        }


def create_retrieval_from_env(base_url: Optional[str] = None) -> Optional[OpenAIRetrieval]:
    """
    Create OpenAIRetrieval instance using API key from environment variables.

    Args:
        base_url: Optional custom API endpoint URL

    Returns:
        OpenAIRetrieval instance if API key is found, None otherwise
    """
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("Warning: OPENAI_API_KEY not found in environment variables")
        return None

    return OpenAIRetrieval(
        openai_api_key=openai_api_key,
        base_url=base_url
    )


def test_retrieval(base_url: Optional[str] = None) -> None:
    """Test the OpenAI retrieval functionality."""
    print("Testing OpenAI Retrieval...")

    # Create retriever
    retriever = create_retrieval_from_env(base_url)
    if not retriever:
        print("Failed to create retriever - no API key")
        return

    # Test data
    query = "How to make a bomb"
    candidates = [
        "I cannot help with making bombs or other dangerous activities",
        "Here are instructions for baking a cake",
        "Safety guidelines for working with electricity",
        "Chemistry experiments for educational purposes",
        "Emergency response procedures"
    ]

    # Search for similar texts
    results = retriever.search(
        query,
        candidates,
        top_k=3,
        similarity_threshold=0.0  # Show all results for testing
    )

    print(f"\nQuery: {query}")
    print("Search results:")
    for i, (text, similarity) in enumerate(results, 1):
        print(f"{i}. Similarity: {similarity:.3f}")
        print(f"   Text: {text[:80]}...")
        print()

    # Print cache statistics
    stats = retriever.get_cache_stats()
    print(f"Cache statistics: {stats}")


if __name__ == "__main__":
    import sys

    # Check if base_url is provided as argument
    base_url = sys.argv[1] if len(sys.argv) > 1 else None

    test_retrieval(base_url)