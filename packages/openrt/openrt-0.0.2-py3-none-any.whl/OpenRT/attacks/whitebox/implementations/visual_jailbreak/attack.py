import os
import random
import logging
import hashlib
import csv
from datetime import datetime, timedelta
from typing import List, Optional, Union, Tuple
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn.functional as F
from torchvision.utils import save_image
from transformers import set_seed
from PIL import Image

from ....base_attack import AttackResult
from ...base import BaseWhiteBoxAttack
from .....core.registry import attack_registry
from .config import VisualJailbreakConfig, VisualJailbreakResult

logger = logging.getLogger(__name__)

@attack_registry.register("visual_jailbreak_attack")
class VisualJailbreakAttack(BaseWhiteBoxAttack):
    """
    Visual Jailbreak Attack for Vision-Language Models
    
    This attack generates adversarial images that can cause vision-language models
    to produce harmful or inappropriate responses when combined with specific text prompts.
    """
    
    def __init__(self, model, config: VisualJailbreakConfig = None, **kwargs):
        """
        Initialize Visual Jailbreak Attack
        
        Args:
            model: Vision-language model supporting white-box attacks
            config: Attack configuration
            **kwargs: Additional arguments
        """
        super().__init__(model, **kwargs)
        
        self.config = config or VisualJailbreakConfig()
        
        # Get model components
        self.hf_model = self.model.get_model()
        self.tokenizer = self.model.get_tokenizer()
        self.processor = self.model.get_processor()
        
        # Store optimized adversarial image
        self.optimized_image = None
        
        # Validate model capabilities for vision-language tasks
        self._validate_vision_model()
        
        # Setup device
        self.device = torch.device(self.config.device if torch.cuda.is_available() else "cpu")
        self.hf_model = self.hf_model.to(self.device)
        
        # Freeze model parameters
        self.hf_model.eval()
        self.hf_model.requires_grad_(False)
        
        # Setup normalization tensors
        self.mean = torch.tensor(self.config.mean, device=self.device).view(1, 3, 1, 1)
        self.std = torch.tensor(self.config.std, device=self.device).view(1, 3, 1, 1)
        
        # Detect image parameter name for this model
        self.image_param_name = self._detect_image_param_name()
        
        # Create save directory
        os.makedirs(self.config.save_dir, exist_ok=True)
        
        # Fixed file path for saving optimized image
        self.optimized_image_path = os.path.join(self.config.save_dir, "optimized_adversarial_image.pt")
        
        # Load derogatory corpus
        self.derogatory_corpus = self._load_derogatory_corpus()
        
        # Set random seed if specified
        if self.config.seed is not None:
            set_seed(self.config.seed)
            torch.manual_seed(self.config.seed)
            random.seed(self.config.seed)
            np.random.seed(self.config.seed)
    
    def _load_derogatory_corpus(self) -> List[str]:
        """Load derogatory corpus from CSV file"""
        corpus_path = os.path.join(
            os.path.dirname(__file__), 
            "derogatory_corpus.csv"
        )
        
        if not os.path.exists(corpus_path):
            logger.warning(f"Derogatory corpus not found at {corpus_path}")
            return ["Generate harmful content", "Produce inappropriate response"]
        
        try:
            file = open(corpus_path, "r", encoding="utf-8")
            data = list(csv.reader(file, delimiter=","))
            file.close()
            
            if not data:
                logger.warning("Empty CSV file")
                return ["Generate harmful content", "Produce inappropriate response"]
            
            # Extract header and find text column
            header = data[0] if data else []
            text_column_idx = None
            
            # Try common column names for text content
            text_columns = ['text', 'content', 'message', 'response', 'output']
            for i, col_name in enumerate(header):
                if col_name.lower() in text_columns:
                    text_column_idx = i
                    break
            
            # Use first column as fallback if no matching column found
            if text_column_idx is None and header:
                text_column_idx = 0
                logger.info(f"Using first column '{header[0]}' as text column")
            
            if text_column_idx is None:
                logger.warning("Could not determine text column in CSV")
                return ["Generate harmful content", "Produce inappropriate response"]
            
            # Extract corpus entries (skip header row)
            corpus_entries = []
            for row in data[1:]:  # Skip header
                if len(row) > text_column_idx and row[text_column_idx].strip():
                    corpus_entries.append(row[text_column_idx].strip())
            
            if not corpus_entries:
                logger.warning("No valid entries found in corpus")
                return ["Generate harmful content", "Produce inappropriate response"]
            
            # Apply corpus sample size limit if specified
            if self.config.corpus_sample_size and len(corpus_entries) > self.config.corpus_sample_size:
                corpus_entries = random.sample(corpus_entries, self.config.corpus_sample_size)
            
            logger.info(f"Loaded {len(corpus_entries)} entries from derogatory corpus")
            return corpus_entries
            
        except Exception as e:
            logger.error(f"Failed to load derogatory corpus: {e}")
            return ["Generate harmful content", "Produce inappropriate response"]
    
    def _get_cache_key(self, target: str, image_shape: Tuple[int, ...]) -> str:
        """Generate simple cache key for identification"""
        content = f"{target}_{image_shape}"
        return hashlib.md5(content.encode()).hexdigest()[:8]
    
    def _get_cached_image(self) -> Optional[torch.Tensor]:
        """Load cached adversarial image from fixed file if available"""
        if not self.config.save_adv_images:
            return None
            
        if not os.path.exists(self.optimized_image_path):
            return None
        
        try:
            # Load cached image
            cached_image = torch.load(self.optimized_image_path, map_location=self.device)
            logger.info("Using cached optimized adversarial image")
            self.optimized_image = cached_image
            return cached_image
            
        except Exception as e:
            logger.warning(f"Failed to load cached image: {e}")
            return None
    
    def _save_cached_image(self, adversarial_image: torch.Tensor):
        """Save adversarial image to fixed file"""
        if not self.config.save_adv_images:
            return
            
        try:
            # Save image to class attribute and file
            self.optimized_image = adversarial_image.clone()
            torch.save(adversarial_image.cpu(), self.optimized_image_path)
            logger.info(f"Cached optimized adversarial image to {self.optimized_image_path}")
            
        except Exception as e:
            logger.warning(f"Failed to cache adversarial image: {e}")

    def _validate_vision_model(self):
        """Validate that the model supports vision-language tasks"""
        # Check if model can process images
        if not hasattr(self.hf_model, 'forward'):
            raise ValueError("Model must support forward() method")
        
        # Try to get vision encoder if available
        vision_components = ['vision_tower', 'vision_model', 'visual', 'vision_encoder', 'vision_config']
        has_vision = any(hasattr(self.hf_model, comp) for comp in vision_components)
        
        if not has_vision:
            logger.warning("Could not detect vision components in model. Attack may not work properly.")
    
    def _detect_image_param_name(self):
        """Detect the correct parameter name for images in this model"""
        # Common parameter names for images in vision-language models
        possible_names = ['pixel_values', 'images', 'image', 'vision_inputs']
        
        # Try to inspect model's forward signature
        import inspect
        try:
            sig = inspect.signature(self.hf_model.forward)
            for name in possible_names:
                if name in sig.parameters:
                    logger.info(f"Detected image parameter name: {name}")
                    return name
        except Exception as e:
            logger.warning(f"Could not inspect model signature: {e}")
        
        # Default fallback
        logger.info("Using default image parameter name: pixel_values")
        return 'pixel_values'
    
    def normalize_image(self, images: torch.Tensor) -> torch.Tensor:
        """Normalize images using ImageNet statistics"""
        return (images - self.mean) / self.std
    
    def denormalize_image(self, images: torch.Tensor) -> torch.Tensor:
        """Denormalize images"""
        return images * self.std + self.mean
    
    def _preprocess_image(self, image: Union[torch.Tensor, Image.Image, str]) -> torch.Tensor:
        """Preprocess input image to tensor format"""
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        if isinstance(image, Image.Image):
            # Convert PIL to tensor and normalize to [0,1]
            image = torch.tensor(np.array(image)).float() / 255.0
            image = image.permute(2, 0, 1).unsqueeze(0)  # (H,W,C) -> (1,C,H,W)
        
        if not isinstance(image, torch.Tensor):
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        # Ensure proper tensor format
        if image.dim() == 3:
            image = image.unsqueeze(0)  # Add batch dimension
        
        if image.shape[1] != 3:
            raise ValueError(f"Expected 3 channels, got {image.shape[1]}")
        
        return image.to(self.device)
    
    def attack(self, target: str, image: Union[torch.Tensor, Image.Image, str], 
               target_outputs: Optional[List[str]] = None, **kwargs) -> AttackResult:
        """
        Execute visual jailbreak attack
        
        Args:
            target: Text prompt/question to ask the model
            image: Input image (can be tensor, PIL Image, or path)
            target_outputs: List of target outputs for optimization (not necessarily harmful)
            **kwargs: Additional arguments
            
        Returns:
            AttackResult: Attack result with adversarial image and model response
        """
        # Use derogatory corpus if target_outputs not provided
        if target_outputs is None and self.config.use_derogatory_corpus:
            target_outputs = self.derogatory_corpus
            logger.info(f"Using derogatory corpus with {len(target_outputs)} entries for optimization")
        
        if not target_outputs:
            raise ValueError("target_outputs cannot be empty")
        
        # Process input image
        image = self._preprocess_image(image)
        
        # Check for cached adversarial image
        cached_image = self._get_cached_image()
        
        if cached_image is not None:
            logger.info("Using cached adversarial image")
            # Get model response with cached image
            final_response = self._test_generation(target, cached_image)
            
            # Save final adversarial image for reference
            image_path = ""
            try:
                save_path = os.path.join(self.config.save_dir, "cached_adversarial_image.png")
                denorm_image = self.denormalize_image(cached_image).clamp(0, 1)
                save_image(denorm_image.squeeze(0), save_path)
                image_path = save_path
            except Exception as e:
                logger.warning(f"Failed to save cached adversarial image: {e}")
            
            return AttackResult(
                target=target,
                success=False,  # Always false since we're not judging success
                final_prompt=target,
                output_text=final_response,
                history=[],
                method="visual_jailbreak",
                image_path=image_path
            )
        
        try:
            # Run the attack optimization using target_outputs as optimization targets
            if self.config.constrained:
                result = self._attack_constrained(target, image, target_outputs)
            else:
                result = self._attack_unconstrained(target, image, target_outputs)
            
            # Cache the optimized image
            if result.adversarial_image is not None:
                self._save_cached_image(result.adversarial_image)
            
            # Get model's response to the target question using optimized adversarial image
            final_response = self._test_generation(target, result.adversarial_image)
            
            # Save final adversarial image
            image_path = ""
            if result.adversarial_image is not None:
                try:
                    save_path = os.path.join(self.config.save_dir, f"final_adversarial_image.png")
                    denorm_image = self.denormalize_image(result.adversarial_image).clamp(0, 1)
                    save_image(denorm_image.squeeze(0), save_path)
                    image_path = save_path
                except Exception as e:
                    logger.warning(f"Failed to save adversarial image: {e}")
            
            return AttackResult(
                target=target,
                success=False,  # Always false since we're not judging success
                final_prompt=target,
                output_text=final_response,
                history=[{
                    'step': i,
                    'loss': loss,
                    'output': result.generation_outputs[i//self.config.test_interval] if result.generation_outputs and i % self.config.test_interval == 0 and i//self.config.test_interval < len(result.generation_outputs) else ""
                } for i, loss in enumerate(result.loss_history or [])],
                method="visual_jailbreak",
                image_path=image_path
            )
            
        except Exception as e:
            logger.error(f"Attack failed: {e}")
            return AttackResult(
                target=target,
                success=False,
                final_prompt=target,
                output_text=f"Attack failed: {str(e)}",
                history=[],
                method="visual_jailbreak",
                image_path=""
            )
    
    def _attack_unconstrained(self, text_prompt: str, image: torch.Tensor, 
                            target_outputs: List[str]) -> VisualJailbreakResult:
        """Execute unconstrained attack (noise can be arbitrary)"""
        logger.info("Starting unconstrained visual jailbreak attack")
        
        # Initialize adversarial noise
        adv_noise = torch.rand_like(image, device=self.device, requires_grad=True)
        
        loss_history = []
        generation_outputs = []
        
        for t in tqdm(range(self.config.num_iter + 1), desc="Unconstrained Attack"):
            # Sample batch of target outputs
            batch_targets = random.choices(target_outputs, 
                                         k=min(self.config.batch_size, len(target_outputs)))
            
            # Normalize adversarial image
            x_adv = self.normalize_image(adv_noise.clamp(0, 1))
            
            try:
                # Compute attack loss
                target_loss = self._compute_attack_loss(text_prompt, x_adv, batch_targets)
                
                # Backward pass
                target_loss.backward()
                
                # Update adversarial noise
                with torch.no_grad():
                    adv_noise.data = (adv_noise.data - self.config.alpha * adv_noise.grad.detach().sign()).clamp(0, 1)
                    adv_noise.grad.zero_()
                
                loss_history.append(target_loss.item())
                
                if t % 50 == 0:  # Reduce logging frequency
                    logger.info(f"Step {t}, Loss: {target_loss.item():.6f}")
                
            except Exception as e:
                logger.warning(f"Error at step {t}: {e}")
                # Use previous loss if current step fails
                if loss_history:
                    loss_history.append(loss_history[-1])
                else:
                    loss_history.append(float('inf'))
                continue
            
            # Periodic visualization and testing
            if t % self.config.log_interval == 0 and t > 0:
                self._plot_loss(loss_history)
            
            if t % self.config.test_interval == 0:
                logger.info(f'Testing generation at step {t}')
                x_adv_test = self.normalize_image(adv_noise.detach().clamp(0, 1))
                response = self._test_generation(text_prompt, x_adv_test)
                generation_outputs.append(response)
                logger.info(f'Response: {response[:100]}...')  # Truncate long responses
                
                if self.config.save_temp_images:
                    try:
                        temp_image = self.denormalize_image(x_adv_test).clamp(0, 1)
                        save_image(temp_image.squeeze(0), 
                                 os.path.join(self.config.save_dir, f'temp_step_{t}.png'))
                    except Exception as e:
                        logger.warning(f"Failed to save temp image at step {t}: {e}")
        
        final_adv_image = self.normalize_image(adv_noise.detach().clamp(0, 1))
        
        return VisualJailbreakResult(
            adversarial_image=final_adv_image,
            loss_history=loss_history,
            final_loss=loss_history[-1] if loss_history else float('inf'),
            generation_outputs=generation_outputs,
            success=False
        )
    
    def _attack_constrained(self, text_prompt: str, image: torch.Tensor, 
                          target_outputs: List[str]) -> VisualJailbreakResult:
        """Execute constrained attack (bounded perturbation)"""
        logger.info("Starting constrained visual jailbreak attack")
        
        # Initialize perturbation
        epsilon = self.config.epsilon
        perturbation = torch.rand_like(image, device=self.device) * 2 * epsilon - epsilon
        perturbation.requires_grad_(True)
        
        # Original image (denormalized for proper constraint handling)
        x_orig = image.clone()
        
        # Ensure perturbation keeps image in valid range
        with torch.no_grad():
            perturbation.data = (perturbation.data + x_orig).clamp(0, 1) - x_orig
        
        loss_history = []
        generation_outputs = []
        
        for t in tqdm(range(self.config.num_iter + 1), desc="Constrained Attack"):
            # Sample batch of target outputs
            batch_targets = random.choices(target_outputs, 
                                         k=min(self.config.batch_size, len(target_outputs)))
            
            # Create adversarial image
            x_adv = (x_orig + perturbation).clamp(0, 1)
            x_adv_norm = self.normalize_image(x_adv)
            
            try:
                # Compute attack loss
                target_loss = self._compute_attack_loss(text_prompt, x_adv_norm, batch_targets)
                
                # Backward pass
                target_loss.backward()
                
                # Project perturbation to constraint set
                with torch.no_grad():
                    perturbation.data = (perturbation.data - self.config.alpha * perturbation.grad.detach().sign()).clamp(-epsilon, epsilon)
                    perturbation.data = (perturbation.data + x_orig).clamp(0, 1) - x_orig
                    perturbation.grad.zero_()
                
                loss_history.append(target_loss.item())
                
                if t % 50 == 0:  # Reduce logging frequency
                    logger.info(f"Step {t}, Loss: {target_loss.item():.6f}")
                
            except Exception as e:
                logger.warning(f"Error at step {t}: {e}")
                # Use previous loss if current step fails
                if loss_history:
                    loss_history.append(loss_history[-1])
                else:
                    loss_history.append(float('inf'))
                continue
            
            # Periodic visualization and testing
            if t % self.config.log_interval == 0 and t > 0:
                self._plot_loss(loss_history)
            
            if t % self.config.test_interval == 0:
                logger.info(f'Testing generation at step {t}')
                x_adv_test = (x_orig + perturbation).clamp(0, 1)
                x_adv_test_norm = self.normalize_image(x_adv_test)
                response = self._test_generation(text_prompt, x_adv_test_norm)
                generation_outputs.append(response)
                logger.info(f'Response: {response[:100]}...')  # Truncate long responses
                
                if self.config.save_temp_images:
                    try:
                        save_image(x_adv_test.squeeze(0), 
                                 os.path.join(self.config.save_dir, f'temp_step_{t}.png'))
                    except Exception as e:
                        logger.warning(f"Failed to save temp image at step {t}: {e}")
        
        final_adv_image = self.normalize_image((x_orig + perturbation).clamp(0, 1))
        
        return VisualJailbreakResult(
            adversarial_image=final_adv_image,
            loss_history=loss_history,
            final_loss=loss_history[-1] if loss_history else float('inf'),
            generation_outputs=generation_outputs,
            success=False
        )
    
    def _compute_attack_loss(self, text_prompt: str, images: torch.Tensor, 
                           target_outputs: List[str]) -> torch.Tensor:
        """Compute the attack loss for given images and target outputs"""
        batch_size = len(target_outputs)
        
        # Prepare text inputs
        text_prompts = [text_prompt] * batch_size if isinstance(text_prompt, str) else text_prompt
        text_prompts_wrapped = self.wrap_prompts(text_prompts)
        
        # Expand images to match batch size
        if images.shape[0] == 1 and batch_size > 1:
            images = images.repeat(batch_size, 1, 1, 1)
        
        # Try different approaches for loss computation
        loss = None
        dummy_image = Image.new('RGB', images.shape[-2:], (255, 255, 255))
        
        # Tokenize inputs
        inputs = self.processor(
            text=text_prompts_wrapped,
            images=dummy_image,
            return_tensors="pt", 
            padding=True, 
        ).to(self.device)
        
        # Tokenize targets
        target_tokens = self.tokenizer(
            target_outputs,
            return_tensors="pt", 
            padding=True, 
            add_special_tokens=False
        ).to(self.device)
        
        # Concatenate inputs and targets
        full_input_ids = torch.cat([inputs.input_ids, target_tokens.input_ids], dim=1)
        
        # Create labels (mask input tokens with -100)
        labels = full_input_ids.clone()
        labels[:, :inputs.input_ids.shape[1]] = -100
        
        # Create attention mask
        attention_mask = torch.ones_like(full_input_ids)
        
        # Forward pass with detected image parameter name
        forward_kwargs = {
            'input_ids': full_input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            self.image_param_name: images,
            'return_dict': True
        }
        
        outputs = self.hf_model(**forward_kwargs)
        loss = outputs.loss
            
        if loss is None:
            raise ValueError("Error computing loss with full input_ids: loss is None")
        
        return loss
    
    def _test_generation(self, text_prompt: str, image: torch.Tensor) -> str:
        """Test generation with current adversarial image"""
        dummy_image = Image.new('RGB', image.shape[-2:], (255, 255, 255))
        try:
            # Tokenize prompt
            inputs = self.processor(
                text=self.wrap_prompt(text_prompt),
                images=dummy_image,
                return_tensors="pt",
                padding=True,
            ).to(self.device)
            
            # Prepare generation kwargs
            generation_kwargs = {
                'input_ids': inputs.input_ids,
                'attention_mask': inputs.attention_mask,
                self.image_param_name: image,
                'max_new_tokens': self.config.max_new_tokens,
                'temperature': self.config.temperature,
                'do_sample': self.config.do_sample,
                'pad_token_id': self.tokenizer.eos_token_id or self.tokenizer.pad_token_id
            }
            
            # Generate response
            with torch.no_grad():
                outputs = self.hf_model.generate(**generation_kwargs)
            
            # Decode response
            input_length = inputs.input_ids.shape[1]
            response = self.tokenizer.decode(
                outputs[0][input_length:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            logger.warning(f"Error in test generation: {e}")
            return f"Generation failed: {str(e)}"
    
    def wrap_prompt(self, prompt: str) -> str:
        """Wrap prompts with predefined prefix and suffix"""
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image"},
                ],
            }
        ]
        prompt_wrapped = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
        return prompt_wrapped
    
    def wrap_prompts(self, prompts: List[str]) -> List[str]:
        """Wrap a batch of prompts"""
        return [self.wrap_prompt(prompt) for prompt in prompts]
        
    
    def _plot_loss(self, loss_history: List[float]):
        """Plot and save loss curve"""
        if not loss_history:
            return
        
        try:
            sns.set_theme()
            plt.figure(figsize=(10, 6))
            
            x_ticks = list(range(len(loss_history)))
            plt.plot(x_ticks, loss_history, label='Attack Loss', linewidth=2)
            
            plt.title('Visual Jailbreak Attack Loss', fontsize=14, fontweight='bold')
            plt.xlabel('Iteration', fontsize=12)
            plt.ylabel('Loss', fontsize=12)
            plt.legend(loc='best')
            plt.grid(True, alpha=0.3)
            
            # Add some styling
            plt.tight_layout()
            
            save_path = os.path.join(self.config.save_dir, 'loss_curve.png')
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            # Also save loss data
            torch.save(loss_history, os.path.join(self.config.save_dir, 'loss_history.pt'))
            
        except Exception as e:
            logger.warning(f"Error plotting loss: {e}")
