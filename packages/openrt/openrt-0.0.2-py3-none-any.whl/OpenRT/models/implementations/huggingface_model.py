import torch
import transformers
from typing import Any, Dict, Optional, Union
from ..base_model import BaseModel
from ...core.registry import model_registry

@model_registry.register("huggingface_model")
class HuggingFaceModel(BaseModel):
    """
    HuggingFace model wrapper, supporting gradient computation and internal model access required for white-box attacks.
    """

    def __init__(
        self,
        model_name: Optional[str] = None,
        model: Optional[transformers.PreTrainedModel] = None,
        tokenizer: Optional[transformers.PreTrainedTokenizer] = None,
        processor: Optional[Any] = None,
        device: str = "auto",
        torch_dtype: Optional[torch.dtype] = None,
        use_processor: bool = False,
        cache_dir: Optional[str] = None,
        use_fast_tokenizer: bool = True,
        **kwargs
    ):
        """
        Initialize HuggingFace model

        Args:
            model_name: HuggingFace model name or path (for traditional LLMs)
            model: Pre-initialized model instance (for VLMs or direct model passing)
            tokenizer: Pre-initialized tokenizer instance (for direct LLM tokenizer passing)
            processor: Pre-initialized processor instance (for VLMs)
            device: Device ("auto", "cpu", "cuda", "cuda:0", etc.)
            torch_dtype: Model data type
            use_processor: Whether to use AutoProcessor instead of AutoTokenizer (only effective in model_name mode)
            cache_dir: Directory to cache downloaded models
            use_fast_tokenizer: Whether to use fast tokenizer (set to False for Llama 3.1 compatibility)
            **kwargs: Other parameters passed to the model
        """
        super().__init__(**kwargs)

        self.device = self._resolve_device(device)
        self.torch_dtype = torch_dtype or torch.float16
        self.use_processor = use_processor
        self.cache_dir = cache_dir
        self.use_fast_tokenizer = use_fast_tokenizer

        # Determine initialization mode
        if model is not None and processor is not None:
            # VLM mode: use pre-initialized model and processor
            print("Initializing in VLM mode with pre-initialized model and processor")
            self.model_name = None
            self.model = model
            self.tokenizer = None
            self.processor = processor
            self._setup_pretrained_model()

        elif model is not None and tokenizer is not None:
            # Direct model and tokenizer passing mode
            print("Initializing with pre-initialized model and tokenizer")
            self.model_name = None
            self.model = model
            self.tokenizer = tokenizer
            self.processor = None
            self._setup_pretrained_model()

        elif model_name is not None:
            # Traditional LLM mode: load via model_name
            print("Initializing in LLM mode with model_name")
            self.model_name = model_name
            self.model = None
            self.tokenizer = None
            self.processor = None
            self._load_model_and_tokenizer()

        else:
            raise ValueError(
                "Invalid initialization parameters. Use one of:\n"
                "1. VLM mode: provide both 'model' and 'processor'\n"
                "2. Direct mode: provide both 'model' and 'tokenizer'\n"
                "3. LLM mode: provide 'model_name'"
            )

        # Set to evaluation mode
        self.model.eval()

        # If GPU, enable gradient computation
        if self.device.type == "cuda":
            self.model.requires_grad_(True)

    def _resolve_device(self, device: str) -> torch.device:
        """Resolve device string"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                return torch.device("cpu")
        return torch.device(device)

    def _setup_pretrained_model(self):
        """Setup pre-initialized model"""
        model_type = "VLM" if self.processor is not None else "LLM"
        print(f"Using pre-initialized {model_type} model: {type(self.model).__name__}")

        # Get actual tokenizer (from processor or use tokenizer directly)
        actual_tokenizer = self._get_tokenizer()

        # Configure padding token
        if actual_tokenizer.pad_token is None:
            if actual_tokenizer.unk_token is not None:
                actual_tokenizer.pad_token = actual_tokenizer.unk_token
            elif actual_tokenizer.eos_token is not None:
                actual_tokenizer.pad_token = actual_tokenizer.eos_token
            else:
                actual_tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

        # Move model to specified device
        if self.device.type != "cpu" and next(self.model.parameters()).device != self.device:
            self.model = self.model.to(self.device)

        print(f"Model setup completed on {next(self.model.parameters()).device}")

    def _load_model_and_tokenizer(self):
        """Load model and tokenizer/processor (only for model_name mode)"""
        if self.model_name is None:
            raise ValueError("model_name is required for automatic loading")

        print(f"Loading HuggingFace model: {self.model_name}")
        if self.cache_dir:
            print(f"Using cache directory: {self.cache_dir}")

        # Prepare loading arguments for tokenizer
        tokenizer_kwargs = {}
        if self.cache_dir:
            tokenizer_kwargs["cache_dir"] = self.cache_dir

        # Prepare loading arguments for model (separate from tokenizer kwargs)
        model_kwargs = {}
        if self.cache_dir:
            model_kwargs["cache_dir"] = self.cache_dir

        # Load tokenizer or processor based on use_processor flag
        if self.use_processor:
            try:
                self.processor = transformers.AutoProcessor.from_pretrained(self.model_name, **tokenizer_kwargs)
                self.tokenizer = None
                print("Loaded processor for VLM")
            except Exception as e:
                print(f"Failed to load processor, falling back to tokenizer: {e}")
                self.processor = None
                tokenizer_kwargs["use_fast"] = self.use_fast_tokenizer
                self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, **tokenizer_kwargs)
        else:
            tokenizer_kwargs["use_fast"] = self.use_fast_tokenizer
            self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, **tokenizer_kwargs)
            self.processor = None

        # Get actual tokenizer and configure padding token
        actual_tokenizer = self._get_tokenizer()
        if actual_tokenizer.pad_token is None:
            if actual_tokenizer.unk_token is not None:
                actual_tokenizer.pad_token = actual_tokenizer.unk_token
            elif actual_tokenizer.eos_token is not None:
                actual_tokenizer.pad_token = actual_tokenizer.eos_token
            else:
                actual_tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

        # Load model
        self.model = transformers.AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=self.torch_dtype,
            device_map=self.device if self.device.type != "cpu" else None,
            trust_remote_code=True,
            **model_kwargs
        )

        # If model is not on specified device, manually move it
        if self.device.type != "cpu" and next(self.model.parameters()).device != self.device:
            self.model = self.model.to(self.device)

        print(f"Model loaded on {self.model.device}")

    def _get_tokenizer(self) -> transformers.PreTrainedTokenizer:
        """Get actual tokenizer (from processor or return tokenizer directly)"""
        if self.processor is not None:
            return self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor
        return self.tokenizer

    def query(self, text_input: str, image_input: Any = None) -> str:
        """
        Send query to model and get response

        Args:
            text_input: Input text
            image_input: Image input (for VLMs)

        Returns:
            str: Model response
        """
        with torch.no_grad():
            # Choose different processing methods based on whether processor is available
            if self.processor is not None and image_input is not None:
                # VLM mode: use processor to handle text and images
                inputs = self.processor(
                    text=text_input,
                    images=image_input,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(self.model.device)
            else:
                # Text mode: use tokenizer
                actual_tokenizer = self._get_tokenizer()
                inputs = actual_tokenizer(
                    text_input,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(self.model.device)

            actual_tokenizer = self._get_tokenizer()
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=actual_tokenizer.pad_token_id
            )

            # Only decode the newly generated tokens
            response_ids = outputs[0][inputs.input_ids.shape[1]:]
            response = actual_tokenizer.decode(response_ids, skip_special_tokens=True)

            return response.strip()

    def get_input_embeddings(self) -> torch.nn.Module:
        """Get input embedding layer"""
        return self.model.get_input_embeddings()

    def get_model(self) -> transformers.PreTrainedModel:
        """Get underlying model"""
        return self.model

    def get_tokenizer(self) -> transformers.PreTrainedTokenizer:
        """Get tokenizer"""
        return self._get_tokenizer()

    def get_processor(self) -> Optional[Any]:
        """Get processor (if exists)"""
        return self.processor

    def get_gradients(self, inputs: Dict[str, torch.Tensor], targets: torch.Tensor) -> torch.Tensor:
        """
        Compute gradients

        Args:
            inputs: Model inputs (including input_ids, attention_mask, etc.)
            targets: Target token ids

        Returns:
            torch.Tensor: Gradient tensor
        """
        self.model.zero_grad()

        # Enable gradient computation
        with torch.enable_grad():
            # Forward pass
            outputs = self.model(**inputs, return_dict=True)
            logits = outputs.logits

            # Compute loss
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = targets[..., 1:].contiguous()

            loss = torch.nn.functional.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )

            # Backward pass
            loss.backward()

        return loss

    def get_embeddings(self, token_ids: torch.Tensor) -> torch.Tensor:
        """
        Get embedding vectors for tokens

        Args:
            token_ids: Token id tensor

        Returns:
            torch.Tensor: Embedding vectors
        """
        embedding_layer = self.get_input_embeddings()
        return embedding_layer(token_ids)

    def compute_loss(self, input_embeds: torch.Tensor, target_ids: torch.Tensor) -> torch.Tensor:
        """
        Compute loss for given embeddings and target tokens

        Args:
            input_embeds: Input embeddings
            target_ids: Target token ids

        Returns:
            torch.Tensor: Loss value
        """
        outputs = self.model(inputs_embeds=input_embeds, return_dict=True)
        logits = outputs.logits

        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = target_ids[..., 1:].contiguous()

        loss = torch.nn.functional.cross_entropy(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1)
        )

        return loss
    
    def __repr__(self):
        if self.model_name:
            model_info = self.model_name
            mode = "LLM (auto-loaded)"
        else:
            model_info = f"{type(self.model).__name__} (pre-initialized)"
            mode = "VLM" if self.processor is not None else "LLM"
        
        processor_info = "processor" if self.processor is not None else "tokenizer"
        return f"HuggingFaceModel(model='{model_info}', {processor_info}, mode={mode}, device={self.device})"