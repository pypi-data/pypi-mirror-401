# Webex AI Bot Configuration
# Copy this file to .env and configure your values

# ==============================================================================
# REQUIRED: Webex Bot Settings
# ==============================================================================

# Your Webex bot access token (get from developer.webex.com)
WEBEX_ACCESS_TOKEN=your_webex_bot_token_here

# ==============================================================================
# REQUIRED: LLM Provider Settings
# ==============================================================================

# For OpenAI models (gpt-4o-mini, gpt-4o, etc.)
OPENAI_API_KEY=your_openai_api_key_here

# OR for other providers via LiteLLM:
# - Ollama: No API key needed for local models
# - OpenRouter: Set OPENROUTER_API_KEY
# - Anthropic: Set ANTHROPIC_API_KEY
# - Azure: Set AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION

# ==============================================================================
# OPTIONAL: Bot Identity
# ==============================================================================

# Bot name for mention handling (extracted from email if not set)
# BOT_NAME=Assistant

# Display name shown in Webex
# BOT_DISPLAY_NAME=AI Assistant

# ==============================================================================
# OPTIONAL: Webex Access Control
# ==============================================================================

# Comma-separated list of approved user emails
# WEBEX_APPROVED_USERS=user1@example.com,user2@example.com

# Comma-separated list of approved email domains
# WEBEX_APPROVED_DOMAINS=example.com,company.com

# Comma-separated list of approved room IDs
# WEBEX_APPROVED_ROOMS=room_id_1,room_id_2

# ==============================================================================
# OPTIONAL: LLM Configuration
# ==============================================================================

# Model to use (LiteLLM format)
# Examples:
#   gpt-4o-mini          - OpenAI GPT-4o Mini
#   gpt-4o               - OpenAI GPT-4o
#   ollama_chat/gpt-oss:120b - Ollama GPT OSS 120B
#   ollama_chat/qwen3:32b - Ollama Qwen 3 32B
#   openrouter/meta-llama/llama-3.1-70b-instruct - OpenRouter
# LLM_MODEL=gpt-4o-mini

# Temperature (0.0 - 2.0, affects response randomness)
# LLM_TEMPERATURE=0.7

# Maximum tokens in response
# LLM_MAX_TOKENS=2048

# Custom API base URL (for self-hosted models or proxies)
# LLM_API_BASE=http://localhost:11434

# API key for LLM (alternative to provider-specific keys)
# LLM_API_KEY=your_api_key_here

# Custom system prompt (template with {bot_name} placeholder)
# LLM_SYSTEM_PROMPT=You are {bot_name}, a helpful AI assistant...

# ==============================================================================
# OPTIONAL: Conversation Management
# ==============================================================================

# Maximum messages to keep per thread
# CONVERSATION_MAX_HISTORY_MESSAGES=50

# Hours before conversation is considered stale
# CONVERSATION_TIMEOUT_HOURS=24

# Path to SQLite database file for storing conversation history
# CONVERSATION_DB_PATH=conversations.db

# Enable or disable conversation persistence to database
# CONVERSATION_ENABLE_PERSISTENCE=true

# ==============================================================================
# OPTIONAL: MCP (Model Context Protocol) Integration
# ==============================================================================

# Enable MCP integration
# MCP_ENABLED=false

# Request timeout in seconds for MCP operations
# MCP_REQUEST_TIMEOUT=30

# MCP Servers configuration (JSON array)
# Each server needs: name, url, and optionally: headers, auth_token, enabled
#
# Example - single server:
# MCP_SERVERS=[{"name": "my-mcp-server", "url": "http://localhost:8000/mcp", "enabled": true}]
#
# Example - server with authentication:
# MCP_SERVERS=[{"name": "secure-server", "url": "https://api.example.com/mcp", "auth_token": "your-token", "enabled": true}]
#
# Example - multiple servers:
# MCP_SERVERS=[{"name": "server1", "url": "http://localhost:8000/mcp"}, {"name": "server2", "url": "http://localhost:8001/mcp"}]
#
# MCP_SERVERS=[]

# Sentry Error Tracking (Optional)
# Set SENTRY_DSN to enable error tracking and performance monitoring
# SENTRY_DSN=https://your-key@o12345.ingest.us.sentry.io/6789
# Optional Sentry configuration
# SENTRY_TRACES_SAMPLE_RATE=1.0
# SENTRY_SEND_DEFAULT_PII=true
# SENTRY_ENVIRONMENT=production
# SENTRY_RELEASE=1.2.3
# SENTRY_PROFILE_SESSION_SAMPLE_RATE=1.0
# SENTRY_PROFILE_LIFECYCLE=trace
# SENTRY_ENABLE_LOGS=true
