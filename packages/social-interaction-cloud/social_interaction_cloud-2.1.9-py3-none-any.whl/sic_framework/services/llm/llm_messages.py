from sic_framework import SICConfMessage, SICMessage, SICRequest


class LLMConf(SICConfMessage):
    """
    Configuration message for LLM service.

    This configuration class contains all the parameters needed to initialize and configure
    an LLM service. It includes authentication credentials, model selection,
    and various parameters that control the behavior of the language model.
    """

    def __init__(self, api_key: str, model: str, temp: float = 0.7, max_tokens: int = 100,
                 system_message: str = "", return_usage_data: bool = False):
        """
        :param api_key: Your secret API API key for authentication
        :param model: LLM model to use (availability depends on exact LLM service)
        :param temp: Temperature parameter controlling randomness (0.0 = deterministic, 1.0 = very random)
        :param max_tokens: Maximum number of tokens in the model's response
        :param system_message: System message that sets the behavior/context for the LLM
        :param return_usage_data: Boolean flag to output usage data
        """
        super().__init__()
        self.api_key = api_key
        self.model = model
        self.temp = temp
        self.max_tokens = max_tokens
        self.system_message = system_message
        self.return_usage_data = return_usage_data


class GPTConf(LLMConf):
    """
    Configuration message for OpenAI GPT service.

    This configuration class contains all the parameters needed to initialize and configure
    the OpenAI GPT service. It includes authentication credentials, model selection,
    and various parameters that control the behavior of the language model.

    Available models can be found at: https://platform.openai.com/docs/models
    For API key setup, see: https://platform.openai.com/docs/quickstart
    """

    def __init__(self, openai_key, model="gpt-4o-mini", temp=0.7, max_tokens=100, system_message="",
                 return_usage_data=False):
        """
        :param openai_key: Your secret OpenAI API key for authentication
        :type openai_key: str
        :param model: OpenAI model to use (e.g., 'gpt-4o-mini', 'gpt-4', 'gpt-3.5-turbo')
        :type model: str
        :param temp: Temperature parameter controlling randomness (0.0 = deterministic, 1.0 = very random)
        :type temp: float
        :param max_tokens: Maximum number of tokens in the model's response
        :type max_tokens: int
        :param system_message: System message that sets the behavior/context for the LLM
        :type system_message: str
        :param return_usage_data: Boolean flag to output usage data
        :type return_usage_data: bool
        """
        super().__init__(openai_key, model, temp, max_tokens, system_message, return_usage_data)


class LLMResponse(SICMessage):
    """
    Response message containing the output from the LLM model.

    This message class encapsulates the response generated by the LLM service,
    including both the text content and metadata.
    """

    def __init__(self, response: str, num_tokens: int, usage_data: dict = None):
        """
        :param response: The actual text response generated by the LLM model
        :param num_tokens: Total number of tokens used in the request (prompt + response)
        :param usage_data: Additional information about the usage of the model
        """
        super().__init__()
        self.response = response
        self.num_tokens = num_tokens
        self.usage_data = usage_data


class GPTResponse(LLMResponse):
    """OpenAI-specific LLM response."""


class LLMRequest(SICRequest):
    """
    Request message for sending prompts to an LLM model.

    This request class supports conversation context through context_messages and
    allows fine-tuning of model parameters for specific requests.
    """

    def __init__(
        self,
        prompt: str = "",
        context_messages: list[str] | None = None,
        system_message: str | None = None,
        model: str | None = None,
        temp: float | None = None,
        max_tokens: int | None = None,
    ):
        """  
        :param prompt: The main text prompt to send to the LLM model
        :param context_messages: Optional list of previous messages to provide conversation context
        :param system_message: Optional system message to set the behavior/context for the AI assistant
        :param model: Optional model override (if None, uses service default from GPTConf)
        :param temp: Optional temperature override (if None, uses service default from GPTConf)
        :param max_tokens: Optional max tokens override (if None, uses service default from GPTConf)
        """
        super().__init__()
        self.prompt = prompt
        self.context_messages = context_messages or []
        self.system_message = system_message
        self.model = model
        self.temp = temp
        self.max_tokens = max_tokens


class GPTRequest(LLMRequest):
    """OpenAI-specific LLM request."""


class AvailableModelsRequest(SICRequest):
    """
    Request message for getting all available models of the LLM service.
    """


class AvailableModels(SICMessage):
    """
    All available models of the LLM service.
    """
    def __init__(self, models: list[str] = None):
        """
        param models: list of available models of the LLM service.
        """
        super().__init__()
        self.models = models
