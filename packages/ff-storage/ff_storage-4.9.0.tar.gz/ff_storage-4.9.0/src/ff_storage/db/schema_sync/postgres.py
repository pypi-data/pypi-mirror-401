"""
PostgreSQL implementation of schema sync system.

This module provides complete PostgreSQL support for:
- Schema introspection (reading information_schema)
- SQL parsing (parsing CREATE TABLE statements)
- Migration generation (generating ALTER TABLE, CREATE INDEX, etc.)
"""

import re
from typing import List, Optional

from ...utils.postgres import quote_identifier
from .base import MigrationGeneratorBase, SchemaIntrospectorBase, SQLParserBase
from .models import ColumnDefinition, ColumnType, IndexDefinition, TableDefinition


class PostgresSchemaIntrospector(SchemaIntrospectorBase):
    """PostgreSQL-specific schema introspection using information_schema."""

    def get_tables(self, schema: str) -> List[str]:
        """Get list of table names in schema."""
        query = """
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = %s
            AND table_type = 'BASE TABLE'
            ORDER BY table_name
        """
        results = self.db.read_query(
            query,
            (schema,),
            as_dict=False,
            context={"trusted_source": True, "source": "PostgresSchemaIntrospector.get_tables"},
        )
        return [row[0] for row in results] if results else []

    def get_columns(self, table_name: str, schema: str) -> List[ColumnDefinition]:
        """Get column definitions for a table."""
        query = """
            SELECT
                column_name,
                data_type,
                is_nullable,
                column_default,
                character_maximum_length,
                numeric_precision,
                numeric_scale,
                udt_name
            FROM information_schema.columns
            WHERE table_schema = %s
            AND table_name = %s
            ORDER BY ordinal_position
        """
        results = self.db.read_query(
            query,
            (schema, table_name),
            as_dict=False,
            context={"trusted_source": True, "source": "PostgresSchemaIntrospector.get_columns"},
        )

        columns = []
        for row in results:
            col_name, data_type, nullable, default, max_len, precision, scale, udt_name = row

            # Map PostgreSQL type to generic type
            column_type = self._map_postgres_type(data_type, udt_name)

            # Normalize boolean defaults for consistent comparison
            # PostgreSQL returns lowercase 'false'/'true', but we use uppercase 'FALSE'/'TRUE'
            if default and column_type == ColumnType.BOOLEAN:
                default_lower = default.lower().strip()
                if default_lower in ("false", "f", "0", "no"):
                    default = "FALSE"
                elif default_lower in ("true", "t", "1", "yes"):
                    default = "TRUE"

            # Determine native_type representation and precision handling
            native_type_raw = udt_name or data_type
            native_lower = native_type_raw.lower()

            # Normalize types and determine precision handling in a single pass
            # Float types have inherent 53-bit precision that should be ignored
            # to avoid false schema drift (DB reports 53, model expects None)
            if native_lower in ("float8", "double precision", "double"):
                native_type_normalized = "DOUBLE PRECISION"
                final_precision = None
                final_scale = None
            elif native_lower in ("float4", "real"):
                native_type_normalized = "REAL"
                final_precision = None
                final_scale = None
            elif native_lower.startswith("_") or data_type.upper() == "ARRAY":
                # Array type: udt_name is like "_text", "_int4", etc.
                if native_lower.startswith("_"):
                    element_type = native_lower[1:]
                    element_display = {
                        "text": "TEXT",
                        "int4": "INTEGER",
                        "int8": "BIGINT",
                        "varchar": "VARCHAR",
                        "uuid": "UUID",
                    }.get(element_type, element_type.upper())
                    native_type_normalized = f"{element_display}[]"
                else:
                    native_type_normalized = "TEXT[]"
                final_precision = None
                final_scale = None
            elif native_lower == "timestamptz":
                native_type_normalized = "TIMESTAMPTZ"
                final_precision = None
                final_scale = None
            else:
                native_type_normalized = native_type_raw.upper()
                # Only NUMERIC/DECIMAL have user-specified precision/scale
                if column_type == ColumnType.DECIMAL:
                    final_precision = precision
                    final_scale = scale
                else:
                    final_precision = None
                    final_scale = None

            columns.append(
                ColumnDefinition(
                    name=col_name,
                    column_type=column_type,
                    nullable=(nullable == "YES"),
                    default=default,
                    max_length=max_len,
                    precision=final_precision,
                    scale=final_scale,
                    native_type=native_type_normalized,
                )
            )

        # Enrich columns with constraint information (primary keys, foreign keys)
        constraints = self.get_column_constraints(table_name, schema)
        for col in columns:
            if col.name in constraints:
                col_constraints = constraints[col.name]
                col.is_primary_key = col_constraints.get("is_primary_key", False)
                col.is_foreign_key = col_constraints.get("is_foreign_key", False)
                col.references = col_constraints.get("references")

        return columns

    def get_column_constraints(self, table_name: str, schema: str) -> dict:
        """Get primary key and foreign key constraints for table columns.

        Returns a dict mapping column names to their constraint info:
        {
            "column_name": {
                "is_primary_key": bool,
                "is_foreign_key": bool,
                "references": "schema.table(column)" or None
            }
        }
        """
        # Query for primary key columns
        pk_query = """
            SELECT a.attname
            FROM pg_index i
            JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
            JOIN pg_class c ON c.oid = i.indrelid
            JOIN pg_namespace n ON n.oid = c.relnamespace
            WHERE i.indisprimary
              AND n.nspname = %s
              AND c.relname = %s
        """
        pk_results = self.db.read_query(
            pk_query,
            (schema, table_name),
            as_dict=False,
            context={
                "trusted_source": True,
                "source": "PostgresSchemaIntrospector.get_column_constraints.pk",
            },
        )
        pk_columns = {row[0] for row in pk_results} if pk_results else set()

        # Query for foreign key columns with their references
        # NOTE: For composite FKs, this query joins on constraint_name only, which can
        # produce incorrect column mappings (cross-product). E.g., FK(a,b) -> REF(x,y)
        # may report a->x, a->y, b->x, b->y instead of a->x, b->y. This is acceptable
        # for detecting "is FK" and "references which table", but not for exact column
        # mapping. To fix, would need pg_constraint with array position matching.
        fk_query = """
            SELECT
                kcu.column_name,
                ccu.table_schema || '.' || ccu.table_name || '(' || ccu.column_name || ')' as references
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage kcu
                ON tc.constraint_name = kcu.constraint_name
                AND tc.table_schema = kcu.table_schema
            JOIN information_schema.constraint_column_usage ccu
                ON tc.constraint_name = ccu.constraint_name
                AND tc.table_schema = ccu.table_schema
            WHERE tc.constraint_type = 'FOREIGN KEY'
              AND tc.table_schema = %s
              AND tc.table_name = %s
        """
        fk_results = self.db.read_query(
            fk_query,
            (schema, table_name),
            as_dict=False,
            context={
                "trusted_source": True,
                "source": "PostgresSchemaIntrospector.get_column_constraints.fk",
            },
        )
        fk_map = {row[0]: row[1] for row in fk_results} if fk_results else {}

        # Build combined constraints dict
        constraints = {}
        all_columns = pk_columns | set(fk_map.keys())

        for col_name in all_columns:
            constraints[col_name] = {
                "is_primary_key": col_name in pk_columns,
                "is_foreign_key": col_name in fk_map,
                "references": fk_map.get(col_name),
            }

        return constraints

    def get_indexes(self, table_name: str, schema: str) -> List[IndexDefinition]:
        """Get index definitions for a table.

        Note: This excludes indexes that back constraints (PRIMARY KEY, UNIQUE, EXCLUDE)
        as these are managed implicitly through their constraints.
        """
        query = """
            SELECT
                i.relname as index_name,
                ARRAY_AGG(a.attname ORDER BY array_position(ix.indkey, a.attnum)) as column_names,
                ix.indisunique as is_unique,
                am.amname as index_type,
                pg_get_expr(ix.indpred, ix.indrelid) as where_clause,
                (SELECT opc.opcname FROM pg_opclass opc
                 WHERE opc.oid = ix.indclass[0]) as opclass
            FROM pg_class t
            JOIN pg_index ix ON t.oid = ix.indrelid
            JOIN pg_class i ON i.oid = ix.indexrelid
            JOIN pg_attribute a ON a.attrelid = t.oid AND a.attnum = ANY(ix.indkey)
            JOIN pg_am am ON i.relam = am.oid
            JOIN pg_namespace n ON n.oid = t.relnamespace
            LEFT JOIN pg_constraint co ON co.conindid = ix.indexrelid
            WHERE n.nspname = %s
            AND t.relname = %s
            AND t.relkind = 'r'
            AND NOT ix.indisprimary  -- Exclude primary key indexes
            AND co.conindid IS NULL  -- Exclude indexes backing constraints
            GROUP BY i.relname, ix.indisunique, am.amname, ix.indpred, ix.indrelid, ix.indclass
            ORDER BY i.relname
        """
        results = self.db.read_query(
            query,
            (schema, table_name),
            as_dict=False,
            context={"trusted_source": True, "source": "PostgresSchemaIntrospector.get_indexes"},
        )

        indexes = []
        for row in results:
            idx_name, col_names, is_unique, idx_type, where_clause, opclass = row
            # Only include opclass if it's non-default (not btree default operators)
            # Default opclasses like 'text_ops', 'int4_ops' should be treated as None
            normalized_opclass = self._normalize_opclass(opclass, idx_type)
            indexes.append(
                IndexDefinition(
                    name=idx_name,
                    table_name=table_name,
                    columns=col_names if isinstance(col_names, list) else [col_names],
                    unique=is_unique,
                    index_type=idx_type,
                    where_clause=where_clause,
                    opclass=normalized_opclass,
                )
            )

        return indexes

    def _normalize_opclass(self, opclass: Optional[str], index_type: str) -> Optional[str]:
        """Normalize opclass, returning None for default operator classes.

        Default operator classes (like 'text_ops', 'int4_ops' for btree) should
        be treated as None since they're implicit and don't need to be specified.
        Only non-default opclasses like 'gin_trgm_ops' should be preserved.
        """
        if opclass is None:
            return None

        # These are common default operator classes that don't need explicit specification
        default_opclasses = {
            # btree defaults
            "text_ops",
            "varchar_ops",
            "int4_ops",
            "int8_ops",
            "bool_ops",
            "uuid_ops",
            "timestamp_ops",
            "timestamptz_ops",
            "numeric_ops",
            # gin defaults (for arrays)
            "array_ops",
            # hash defaults
            "text_pattern_ops",
            "varchar_pattern_ops",
        }

        if opclass.lower() in default_opclasses:
            return None

        return opclass

    def table_exists(self, table_name: str, schema: str) -> bool:
        """Check if table exists."""
        query = """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_schema = %s
                AND table_name = %s
            )
        """
        result = self.db.read_query(
            query,
            (schema, table_name),
            as_dict=False,
            context={"trusted_source": True, "source": "PostgresSchemaIntrospector.table_exists"},
        )
        return result[0][0] if result else False

    def _map_postgres_type(self, data_type: str, udt_name: str) -> ColumnType:
        """Map PostgreSQL type to generic ColumnType."""
        # Use udt_name for more accurate type mapping
        type_str = (udt_name or data_type).lower()

        type_map = {
            "uuid": ColumnType.UUID,
            "character varying": ColumnType.STRING,
            "varchar": ColumnType.STRING,
            "text": ColumnType.TEXT,
            "integer": ColumnType.INTEGER,
            "int4": ColumnType.INTEGER,
            "bigint": ColumnType.BIGINT,
            "int8": ColumnType.BIGINT,
            "boolean": ColumnType.BOOLEAN,
            "bool": ColumnType.BOOLEAN,
            "timestamp without time zone": ColumnType.TIMESTAMP,
            "timestamp": ColumnType.TIMESTAMP,
            "timestamp with time zone": ColumnType.TIMESTAMPTZ,
            "timestamptz": ColumnType.TIMESTAMPTZ,
            "time": ColumnType.TIME,
            "time without time zone": ColumnType.TIME,
            "interval": ColumnType.INTERVAL,
            "bytea": ColumnType.BINARY,
            "jsonb": ColumnType.JSONB,
            "numeric": ColumnType.DECIMAL,
            "decimal": ColumnType.DECIMAL,
            # Float types
            "float8": ColumnType.DECIMAL,  # DOUBLE PRECISION internal name
            "double precision": ColumnType.DECIMAL,
            "float4": ColumnType.DECIMAL,  # REAL internal name
            "real": ColumnType.DECIMAL,
            "double": ColumnType.DECIMAL,  # Handle if someone uses just "double"
        }

        # Check for array types (case-insensitive)
        # PostgreSQL stores arrays with udt_name like "_text", "_int4", etc.
        if type_str.endswith("[]") or data_type.upper() == "ARRAY" or type_str.startswith("_"):
            return ColumnType.ARRAY

        return type_map.get(type_str, ColumnType.STRING)


class PostgresSQLParser(SQLParserBase):
    """Parse PostgreSQL CREATE TABLE statements."""

    # Pattern to match quoted or unquoted SQL identifiers
    _IDENTIFIER = r'(?:"[^"]+"|[a-zA-Z_][a-zA-Z0-9_]*)'
    _QUALIFIED_NAME = rf"{_IDENTIFIER}(?:\.{_IDENTIFIER})?"

    @staticmethod
    def _strip_quotes(identifier: str) -> str:
        """Remove surrounding quotes from identifier if present."""
        return identifier.strip('"')

    def parse_create_table(self, sql: str) -> TableDefinition:
        """Parse CREATE TABLE statement into TableDefinition."""
        # Extract schema and table name (handles both quoted and unquoted)
        table_match = re.search(
            rf"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?({self._QUALIFIED_NAME})",
            sql,
            re.IGNORECASE,
        )

        if not table_match:
            raise ValueError(f"Could not parse table name from SQL: {sql[:100]}")

        full_name = table_match.group(1)

        # Split schema.table and strip quotes
        if "." in full_name:
            # Extract schema and table (may be quoted separately)
            parts_match = re.match(rf"({self._IDENTIFIER})\.({self._IDENTIFIER})", full_name)
            if parts_match:
                schema = self._strip_quotes(parts_match.group(1))
                table_name = self._strip_quotes(parts_match.group(2))
            else:
                raise ValueError(f"Could not parse qualified name: {full_name}")
        else:
            schema = "public"
            table_name = self._strip_quotes(full_name)

        # Parse columns and indexes
        columns = self.parse_columns_from_sql(sql)
        indexes = self.parse_indexes_from_sql(sql)

        return TableDefinition(name=table_name, schema=schema, columns=columns, indexes=indexes)

    def parse_columns_from_sql(self, sql: str) -> List[ColumnDefinition]:
        """Extract column definitions from CREATE TABLE SQL."""
        # Extract the content inside CREATE TABLE (...)
        table_match = re.search(r"CREATE\s+TABLE[^(]+\((.*?)\);", sql, re.IGNORECASE | re.DOTALL)
        if not table_match:
            return []

        table_content = table_match.group(1)

        columns = []
        # Split by lines, look for column definitions
        for line in table_content.split("\n"):
            line = line.strip()

            # Skip comments, constraints, empty lines
            if (
                not line
                or line.startswith("--")
                or line.upper().startswith(
                    ("PRIMARY KEY", "FOREIGN KEY", "UNIQUE", "CHECK", "CONSTRAINT")
                )
            ):
                continue

            # Parse column definition: column_name TYPE [constraints...] [,]
            # Fixed regex to handle:
            # - Multi-word types (TIMESTAMP WITH TIME ZONE)
            # - REFERENCES constraints
            # - Foreign keys and other constraints
            # - Quoted column names
            col_match = re.match(rf"({self._IDENTIFIER})\s+(.+?)(?:,\s*$|$)", line, re.IGNORECASE)

            if col_match:
                col_name = self._strip_quotes(col_match.group(1))
                col_def = col_match.group(2).rstrip(",").strip()

                # Extract type (first word or multi-word type)
                # Handle types like: UUID, VARCHAR(255), TIMESTAMP WITH TIME ZONE, DOUBLE PRECISION, TEXT[]
                type_match = re.match(
                    r"([A-Z]+(?:\s+(?:WITH\s+TIME\s+ZONE|VARYING|PRECISION))?(?:\([^)]+\))?(?:\[\])?)",
                    col_def,
                    re.IGNORECASE,
                )
                if not type_match:
                    # Fallback: just take first word
                    type_match = re.match(r"(\S+)", col_def)

                col_type_str = type_match.group(1) if type_match else col_def.split()[0]

                # Check for constraints in definition
                nullable = "NOT NULL" not in col_def.upper()

                # Extract default value if present
                default_match = re.search(
                    r"DEFAULT\s+(.+?)(?:,|REFERENCES|$)", col_def, re.IGNORECASE
                )
                default_str = default_match.group(1).strip() if default_match else None

                # Map type string to ColumnType
                column_type = self._parse_column_type(col_type_str)

                # Extract max_length, precision, scale from type string
                max_length, precision, scale = self._extract_type_constraints(col_type_str)

                columns.append(
                    ColumnDefinition(
                        name=col_name,
                        column_type=column_type,
                        nullable=nullable,
                        default=default_str,
                        max_length=max_length,
                        precision=precision,
                        scale=scale,
                        native_type=col_type_str,
                    )
                )

        return columns

    def parse_indexes_from_sql(self, sql: str) -> List[IndexDefinition]:
        """Extract index definitions from SQL (CREATE INDEX statements)."""
        indexes = []

        # Find all CREATE INDEX statements (handles quoted identifiers)
        index_pattern = rf"CREATE\s+(UNIQUE\s+)?INDEX\s+(?:IF\s+NOT\s+EXISTS\s+)?({self._IDENTIFIER})\s+ON\s+({self._QUALIFIED_NAME})\s*(?:USING\s+([a-zA-Z]+))?\s*\(([^)]+)\)(?:\s+WHERE\s+(.+?))?;"

        for match in re.finditer(index_pattern, sql, re.IGNORECASE):
            is_unique = match.group(1) is not None
            index_name = self._strip_quotes(match.group(2))
            full_table_name = match.group(3)
            index_type = match.group(4) or "btree"
            columns_str = match.group(5)
            where_clause = match.group(6)

            # Extract table name from qualified name
            if "." in full_table_name:
                parts_match = re.match(
                    rf"({self._IDENTIFIER})\.({self._IDENTIFIER})", full_table_name
                )
                if parts_match:
                    table_name = self._strip_quotes(parts_match.group(2))
                else:
                    table_name = self._strip_quotes(full_table_name.split(".")[-1])
            else:
                table_name = self._strip_quotes(full_table_name)

            # Parse column list (may contain quoted columns)
            columns = []
            for col in columns_str.split(","):
                col = col.strip()
                # Remove quotes if present
                columns.append(self._strip_quotes(col))

            indexes.append(
                IndexDefinition(
                    name=index_name,
                    table_name=table_name,
                    columns=columns,
                    unique=is_unique,
                    index_type=index_type.lower(),
                    where_clause=where_clause,
                )
            )

        return indexes

    def _parse_column_type(self, type_str: str) -> ColumnType:
        """Parse PostgreSQL type string to ColumnType."""
        type_upper = type_str.upper()

        if type_upper == "UUID":
            return ColumnType.UUID
        elif type_upper.startswith("VARCHAR") or type_upper.startswith("CHARACTER VARYING"):
            return ColumnType.STRING
        elif type_upper == "TEXT":
            return ColumnType.TEXT
        elif type_upper.startswith("INTEGER") or type_upper in ("INT", "INT4"):
            return ColumnType.INTEGER
        elif type_upper.startswith("BIGINT") or type_upper == "INT8":
            return ColumnType.BIGINT
        elif type_upper == "BOOLEAN":
            return ColumnType.BOOLEAN
        elif "TIMESTAMP WITH TIME ZONE" in type_upper or type_upper == "TIMESTAMPTZ":
            return ColumnType.TIMESTAMPTZ
        elif "TIMESTAMP" in type_upper:
            return ColumnType.TIMESTAMP
        elif "TIME" in type_upper and "TIMESTAMP" not in type_upper:
            return ColumnType.TIME
        elif type_upper == "INTERVAL":
            return ColumnType.INTERVAL
        elif type_upper == "BYTEA":
            return ColumnType.BINARY
        elif type_upper == "JSONB":
            return ColumnType.JSONB
        elif type_upper.endswith("[]"):
            return ColumnType.ARRAY
        elif "DOUBLE" in type_upper or "FLOAT8" in type_upper:
            return ColumnType.DECIMAL
        elif "REAL" in type_upper or "FLOAT4" in type_upper or type_upper == "FLOAT":
            return ColumnType.DECIMAL
        elif type_upper.startswith("NUMERIC") or type_upper.startswith("DECIMAL"):
            return ColumnType.DECIMAL
        else:
            return ColumnType.STRING  # Default fallback

    def _extract_type_constraints(self, type_str: str) -> tuple[int | None, int | None, int | None]:
        """Extract max_length, precision, scale from SQL type string."""
        # VARCHAR(n)
        if match := re.search(r"VARCHAR\((\d+)\)", type_str, re.IGNORECASE):
            return int(match.group(1)), None, None

        # NUMERIC(p,s)
        if match := re.search(r"(?:NUMERIC|DECIMAL)\((\d+),(\d+)\)", type_str, re.IGNORECASE):
            return None, int(match.group(1)), int(match.group(2))

        return None, None, None


class PostgresMigrationGenerator(MigrationGeneratorBase):
    """Generate PostgreSQL-specific migration SQL."""

    def generate_add_column(self, table_name: str, schema: str, column: ColumnDefinition) -> str:
        """
        Generate ALTER TABLE ADD COLUMN statement.

        For NOT NULL columns without DEFAULT on populated tables, generates a safe
        multi-step process:
        1. Add column as nullable (allows existing rows to have NULL)
        2. Set NOT NULL constraint (new rows must provide value going forward)

        This is safe because:
        - New columns start empty (all NULL)
        - Application code will provide values for new inserts
        - Existing rows keep NULL (backward compatible)
        """
        full_table = quote_identifier(f"{schema}.{table_name}")
        quoted_col = quote_identifier(column.name)

        # Handle NOT NULL columns without DEFAULT
        if not column.nullable and column.default is None:
            # Multi-step approach for backward compatibility
            # Step 1: Add as nullable
            sql = (
                f"ALTER TABLE {full_table} "
                f"ADD COLUMN IF NOT EXISTS {quoted_col} {column.native_type} NULL;\n"
            )

            # Step 2: Make NOT NULL (existing rows keep NULL, new inserts must provide value)
            sql += f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} SET NOT NULL"

            return sql + ";"

        # Standard path for nullable columns or columns with DEFAULT
        default = f"DEFAULT {column.default}" if column.default else ""
        nullable = "NULL" if column.nullable else "NOT NULL"

        sql = f"ALTER TABLE {full_table} ADD COLUMN IF NOT EXISTS {quoted_col} {column.native_type}"

        if not column.nullable:
            sql += f" {nullable}"

        if default:
            sql += f" {default}"

        return sql + ";"

    def generate_create_index(self, schema: str, index: IndexDefinition) -> str:
        """Generate CREATE INDEX statement."""
        unique = "UNIQUE " if index.unique else ""
        quoted_columns = ", ".join(quote_identifier(col) for col in index.columns)
        full_table = quote_identifier(f"{schema}.{index.table_name}")
        quoted_index = quote_identifier(index.name)

        sql = f"CREATE {unique}INDEX IF NOT EXISTS {quoted_index} ON {full_table}"

        if index.index_type and index.index_type != "btree":
            sql += f" USING {index.index_type}"

        # Handle operator class for specialized indexes (e.g., gin_trgm_ops)
        if index.opclass:
            # Apply opclass to each column for multi-column indexes
            columns_with_opclass = ", ".join(
                f"{quote_identifier(col)} {index.opclass}" for col in index.columns
            )
            sql += f" ({columns_with_opclass})"
        else:
            sql += f" ({quoted_columns})"

        if index.where_clause:
            sql += f" WHERE {index.where_clause}"

        return sql + ";"

    def generate_create_table(self, table: TableDefinition) -> str:
        """Generate CREATE TABLE statement."""
        full_table = quote_identifier(f"{table.schema}.{table.name}")

        # Generate column definitions
        col_defs = []
        primary_keys = []
        foreign_keys = []

        for col in table.columns:
            quoted_col = quote_identifier(col.name)
            col_def = f"{quoted_col} {col.native_type}"

            if not col.nullable:
                col_def += " NOT NULL"

            if col.default:
                col_def += f" DEFAULT {col.default}"

            col_defs.append(col_def)

            # Track primary keys for composite PK constraint
            if col.is_primary_key:
                primary_keys.append(quoted_col)

            # Track foreign keys
            if col.is_foreign_key and col.references:
                foreign_keys.append((quoted_col, col.references))

        # Add PRIMARY KEY constraint if any primary keys exist
        if primary_keys:
            if len(primary_keys) == 1:
                # Single PK - modify the column definition directly
                for i, col_def in enumerate(col_defs):
                    if col_def.startswith(f"{primary_keys[0]} "):
                        col_defs[i] += " PRIMARY KEY"
                        break
            else:
                # Composite PK - add as table constraint
                pk_constraint = f"PRIMARY KEY ({', '.join(primary_keys)})"
                col_defs.append(pk_constraint)

        # Add FOREIGN KEY constraints
        for col_name, references in foreign_keys:
            fk_constraint = f"FOREIGN KEY ({col_name}) REFERENCES {references}"
            col_defs.append(fk_constraint)

        sql = f"CREATE TABLE IF NOT EXISTS {full_table} (\n  "
        sql += ",\n  ".join(col_defs)
        sql += "\n);"

        return sql

    def generate_drop_index(self, schema: str, index: IndexDefinition) -> str:
        """Generate DROP INDEX statement."""
        # PostgreSQL DROP INDEX requires schema-qualified index name
        full_index = quote_identifier(f"{schema}.{index.name}")
        return f"DROP INDEX IF EXISTS {full_index};"

    def generate_drop_column(self, table_name: str, schema: str, column: ColumnDefinition) -> str:
        """Generate ALTER TABLE DROP COLUMN statement."""
        full_table = quote_identifier(f"{schema}.{table_name}")
        quoted_col = quote_identifier(column.name)
        return f"ALTER TABLE {full_table} DROP COLUMN IF EXISTS {quoted_col};"

    def generate_alter_column(self, table_name: str, schema: str, column: ColumnDefinition) -> str:
        """
        Generate ALTER TABLE ALTER COLUMN statement with USING clause for type conversions.

        Uses STRICT conversion strategy: fails loudly on invalid data rather than silently
        converting to NULL. This forces manual data cleanup and prevents data loss.

        Also handles nullable changes with automatic backfilling when DEFAULT is present.
        """
        full_table = quote_identifier(f"{schema}.{table_name}")
        quoted_col = quote_identifier(column.name)

        # PostgreSQL ALTER COLUMN requires separate statements for type, nullable, and default
        statements = []

        # Determine if we need a USING clause for type conversion
        using_clause = self._get_type_conversion_using(column)

        # Change type
        alter_type = f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} TYPE {column.native_type}"
        if using_clause:
            alter_type += f" USING {using_clause}"
        statements.append(alter_type)

        # Change nullable
        # IMPORTANT: If making NOT NULL and column has DEFAULT, backfill first
        if not column.nullable:
            if column.default:
                # Backfill existing NULL values before adding constraint
                backfill = (
                    f"UPDATE {full_table} "
                    f"SET {quoted_col} = {column.default} "
                    f"WHERE {quoted_col} IS NULL"
                )
                statements.append(backfill)

            statements.append(f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} SET NOT NULL")
        else:
            statements.append(f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} DROP NOT NULL")

        # Change default
        if column.default:
            statements.append(
                f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} SET DEFAULT {column.default}"
            )
        else:
            statements.append(f"ALTER TABLE {full_table} ALTER COLUMN {quoted_col} DROP DEFAULT")

        return ";\n".join(statements) + ";"

    def _get_type_conversion_using(self, column: ColumnDefinition) -> str | None:
        """
        Generate USING clause for type conversions that PostgreSQL can't auto-cast.

        Strategy: STRICT conversions that fail on invalid data
        - NULL and empty strings are handled gracefully
        - Invalid data causes migration to FAIL (forces manual cleanup)
        - NO silent data loss or corruption

        Returns:
            USING clause string, or None if PostgreSQL can handle conversion automatically
        """
        target_type = column.native_type.upper()
        quoted_col = quote_identifier(column.name)

        # text → numeric/decimal
        # Handles NULL/empty, but FAILS on non-numeric values (e.g., "abc")
        if "NUMERIC" in target_type or "DECIMAL" in target_type:
            return f"CASE WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN NULL ELSE {quoted_col}::numeric END"

        # text → jsonb
        # Defaults to empty array for NULL/empty, but FAILS on invalid JSON
        elif "JSONB" in target_type or "JSON" in target_type:
            return f"CASE WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN '[]'::jsonb ELSE {quoted_col}::jsonb END"

        # text → integer
        # Handles NULL/empty, but FAILS on non-integer values
        elif "INTEGER" in target_type or "INT" in target_type:
            return f"CASE WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN NULL ELSE {quoted_col}::integer END"

        # text → bigint
        elif "BIGINT" in target_type:
            return f"CASE WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN NULL ELSE {quoted_col}::bigint END"

        # text → boolean
        # Standard boolean conversions, FAILS on unrecognized values
        elif "BOOLEAN" in target_type or "BOOL" in target_type:
            return (
                f"CASE "
                f"WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN NULL "
                f"WHEN {quoted_col}::text IN ('t', 'true', '1', 'yes', 'y') THEN true "
                f"WHEN {quoted_col}::text IN ('f', 'false', '0', 'no', 'n') THEN false "
                f"ELSE {quoted_col}::boolean "  # Let PostgreSQL try, will fail on invalid
                f"END"
            )

        # text → uuid
        elif "UUID" in target_type:
            return f"CASE WHEN {quoted_col}::text IS NULL OR {quoted_col}::text = '' THEN NULL ELSE {quoted_col}::uuid END"

        # For other conversions (e.g., varchar(100) → varchar(255), same type changes)
        # Let PostgreSQL handle it automatically without USING clause
        return None

    def wrap_in_transaction(self, statements: List[str]) -> str:
        """Wrap multiple statements in a transaction."""
        if not statements:
            return ""

        return "BEGIN;\n" + "\n".join(statements) + "\nCOMMIT;"
