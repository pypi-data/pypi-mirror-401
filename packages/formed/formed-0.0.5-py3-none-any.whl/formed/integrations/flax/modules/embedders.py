"""Text embedding modules for Flax models.

This module provides embedders that convert tokenized text into dense vector
representations. Embedders handle various text representations including
surface forms, part-of-speech tags, and character sequences.

Key Components:
    - BaseEmbedder: Abstract base class for all embedders
    - TokenEmbedder: Embeds token ID sequences into dense vectors
    - AnalyzedTextEmbedder: Combines multiple embedding types (surface, POS, chars)

Features:
    - Support for nested token sequences (e.g., word -> character)
    - Automatic masking and padding handling
    - Configurable vectorization for character-level embeddings
    - Concatenation of multiple embedding types

Examples:
    >>> from formed.integrations.flax.modules import TokenEmbedder, AnalyzedTextEmbedder
    >>> from flax import nnx
    >>>
    >>> # Simple token embedder
    >>> embedder = TokenEmbedder(
    ...     vocab_size=10000,
    ...     embedding_dim=128,
    ...     rngs=nnx.Rngs(0)
    ... )
    >>>
    >>> # Multi-feature embedder
    >>> embedder = AnalyzedTextEmbedder(
    ...     surface=TokenEmbedder(vocab_size=10000, embedding_dim=128, rngs=rngs),
    ...     postag=TokenEmbedder(vocab_size=50, embedding_dim=32, rngs=rngs)
    ... )

"""

import abc
from typing import Generic, NamedTuple, TypeVar

import jax
from colt import Registrable
from flax import nnx

from ..random import require_rngs
from ..types import IAnalyzedTextBatch, IIDSequenceBatch
from ..utils import ensure_jax_array, sequence_distribute, sequence_undistribute
from .vectorizers import BaseSequenceVectorizer

_TextBatchT = TypeVar("_TextBatchT")


class EmbedderOutput(NamedTuple):
    """Output from an embedder.

    Attributes:
        embeddings: Dense embeddings of shape (batch_size, seq_len, embedding_dim).
        mask: Attention mask of shape (batch_size, seq_len).

    """

    embeddings: jax.Array
    mask: jax.Array


class BaseEmbedder(nnx.Module, Registrable, Generic[_TextBatchT], abc.ABC):
    """Abstract base class for text embedders.

    Embedders convert tokenized text into dense vector representations.
    They output both embeddings and attention masks.

    Type Parameters:
        _TextBatchT: Type of input batch (e.g., IIDSequenceBatch, IAnalyzedTextBatch).

    """

    @abc.abstractmethod
    def __call__(self, inputs: _TextBatchT) -> EmbedderOutput:
        """Embed input tokens into dense vectors.

        Args:
            inputs: Batch of tokenized text.

        Returns:
            EmbedderOutput containing embeddings and mask.

        """
        raise NotImplementedError

    @abc.abstractmethod
    def get_output_dim(self) -> int:
        """Get the output embedding dimension.

        Returns:
            Embedding dimension.

        """
        raise NotImplementedError


@BaseEmbedder.register("token")
class TokenEmbedder(BaseEmbedder[IIDSequenceBatch]):
    """Embedder for token ID sequences.

    This embedder converts token IDs into dense embeddings using a learned
    embedding matrix. It supports both 2D (batch_size, seq_len) and 3D
    (batch_size, seq_len, char_len) token ID tensors.

    For 3D inputs (e.g., character-level tokens within words), the embedder
    can either average the embeddings or apply a custom vectorizer.

    Args:
        vocab_size: Size of the vocabulary.
        embedding_dim: Dimension of the embedding vectors.
        vectorizer: Optional vectorizer for 3D inputs (character sequences).
        rngs: Random number generators.

    Examples:
        >>> # Simple word embeddings
        >>> embedder = TokenEmbedder(vocab_size=10000, embedding_dim=128, rngs=rngs)
        >>> output = embedder(word_ids_batch)
        >>>
        >>> # Character-level embeddings with pooling
        >>> from formed.integrations.flax.modules import BagOfEmbeddingsSequenceVectorizer
        >>> embedder = TokenEmbedder(
        ...     vocab_size=256,
        ...     embedding_dim=32,
        ...     vectorizer=BagOfEmbeddingsSequenceVectorizer(pooling="max"),
        ...     rngs=rngs
        ... )

    """

    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int,
        *,
        vectorizer: BaseSequenceVectorizer | None = None,
        rngs: nnx.Rngs | None = None,
    ) -> None:
        rngs = rngs or require_rngs()
        self._embedding = nnx.Embed(num_embeddings=vocab_size, features=embedding_dim, rngs=rngs)
        self._vectorizer = vectorizer

    def __call__(self, inputs: IIDSequenceBatch) -> EmbedderOutput:
        nested = False
        if inputs.ids.ndim > 2:
            if inputs.ids.ndim != 3:
                raise ValueError("Token ids must be of shape (batch_size, seq_len) or (batch_size, seq_len, char_len)")
            nested = True

        if inputs.ids.shape != inputs.mask.shape:
            raise ValueError(
                f"Token ids and mask must have the same shape, got {inputs.ids.shape} and {inputs.mask.shape}"
            )

        token_ids = ensure_jax_array(inputs.ids)
        mask = ensure_jax_array(inputs.mask)

        embeddings = self._embedding(token_ids)

        if nested:
            if self._vectorizer is None:
                embeddings = (embeddings * mask[..., None]).sum(axis=-2) / mask.sum(axis=-1, keepdims=True).clip(min=1)
                mask = mask.any(axis=-1)
            else:
                dist_embeddings, dist_shape = sequence_distribute(embeddings)
                dist_mask, _ = sequence_distribute(mask)
                dist_embeddings = self._vectorizer(dist_embeddings, mask=dist_mask)
                embeddings = sequence_undistribute(dist_embeddings, dist_shape)
                mask = mask.any(axis=-1)

        return EmbedderOutput(embeddings=embeddings, mask=mask)

    def get_output_dim(self) -> int:
        return self._embedding.features


@BaseEmbedder.register("analyzed_text")
class AnalyzedTextEmbedder(BaseEmbedder[IAnalyzedTextBatch]):
    """Embedder for analyzed text with multiple linguistic features.

    This embedder combines embeddings from multiple linguistic representations
    (surface forms, part-of-speech tags, character sequences) by concatenating
    them along the feature dimension.

    Args:
        surface: Optional embedder for surface form tokens.
        postag: Optional embedder for part-of-speech tags.
        character: Optional embedder for character sequences.

    Raises:
        ValueError: If all embedders are None (at least one is required).

    Examples:
        >>> from formed.integrations.flax.modules import (
        ...     AnalyzedTextEmbedder,
        ...     TokenEmbedder
        ... )
        >>>
        >>> embedder = AnalyzedTextEmbedder(
        ...     surface=TokenEmbedder(vocab_size=10000, embedding_dim=128, rngs=rngs),
        ...     postag=TokenEmbedder(vocab_size=50, embedding_dim=32, rngs=rngs),
        ...     character=TokenEmbedder(vocab_size=256, embedding_dim=32, rngs=rngs)
        ... )
        >>>
        >>> # Output dimension is sum of all embedding dimensions (128 + 32 + 32 = 192)
        >>> assert embedder.get_output_dim() == 192

    Note:
        All provided embedders share the same mask, which is taken from
        the last non-None embedder processed.

    """

    def __init__(
        self,
        surface: BaseEmbedder[IIDSequenceBatch] | None = None,
        postag: BaseEmbedder[IIDSequenceBatch] | None = None,
        character: BaseEmbedder[IIDSequenceBatch] | None = None,
    ) -> None:
        if all(embedder is None for embedder in (surface, postag, character)):
            raise ValueError("At least one embedder must be provided for AnalyzedTextEmbedder.")

        self._surface = surface
        self._postag = postag
        self._character = character

    def __call__(self, inputs: IAnalyzedTextBatch) -> EmbedderOutput:
        embeddings: list[jax.Array] = []
        mask: jax.Array | None = None

        for embedder, ids in (
            (self._surface, inputs.surfaces),
            (self._postag, inputs.postags),
            (self._character, inputs.characters),
        ):
            if embedder is not None:
                output = embedder(ids)
                embeddings.append(output.embeddings)
                mask = output.mask
        if not embeddings:
            raise ValueError("No embeddings were computed in AnalyzedTextEmbedder.")
        assert mask is not None
        return EmbedderOutput(embeddings=jax.numpy.concatenate(embeddings, axis=-1), mask=mask)

    def get_output_dim(self) -> int:
        return sum(
            embedder.get_output_dim()
            for embedder in (self._surface, self._postag, self._character)
            if embedder is not None
        )
