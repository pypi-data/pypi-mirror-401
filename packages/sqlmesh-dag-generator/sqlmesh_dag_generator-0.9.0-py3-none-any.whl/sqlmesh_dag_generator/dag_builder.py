"""
Airflow DAG builder module
"""
from typing import Dict, List, Optional
from textwrap import dedent, indent
from datetime import datetime, timedelta

from sqlmesh_dag_generator.config import DAGGeneratorConfig
from sqlmesh_dag_generator.models import DAGStructure, SQLMeshModelInfo


class AirflowDAGBuilder:
    """
    Builds Airflow DAG Python code from SQLMesh models.
    """

    def __init__(self, config: DAGGeneratorConfig, dag_structure: DAGStructure):
        self.config = config
        self.dag_structure = dag_structure

    def build(self) -> str:
        """
        Build the complete Airflow DAG Python file (static generation).

        Returns:
            Python code as a string
        """
        sections = [
            self._build_header(),
            self._build_imports(),
            self._build_config(),
            self._build_dag_definition(),
            self._build_tasks(),
            self._build_dependencies(),
        ]

        return "\n\n".join(sections)

    def build_dynamic(self) -> str:
        """
        Build a dynamic DAG that discovers SQLMesh models at runtime.

        This creates a single DAG file that works for any SQLMesh project.
        Models are discovered when Airflow parses the DAG, so no regeneration
        is needed when models change.

        Returns:
            Python code as a string
        """
        sections = [
            self._build_dynamic_header(),
            self._build_dynamic_imports(),
            self._build_dynamic_config(),
            self._build_dynamic_model_discovery(),
            self._build_dynamic_dag_definition(),
            self._build_dynamic_tasks(),
        ]

        return "\n\n".join(sections)

    def _build_header(self) -> str:
        """Build file header with metadata"""
        return dedent(f'''
            """
            Airflow DAG generated from SQLMesh project
            
            DAG ID: {self.config.airflow.dag_id}
            Generated: {datetime.now().isoformat()}
            
            This DAG was automatically generated by sqlmesh-dag-generator.
            DO NOT EDIT MANUALLY - changes will be overwritten.
            """
        ''').strip()

    def _build_imports(self) -> str:
        """Build import statements"""
        imports = [
            "from datetime import datetime, timedelta",
            "import logging",
            "",
            "from airflow import DAG",
        ]

        if self.config.generation.operator_type == "python":
            imports.append("from airflow.operators.python import PythonOperator")
        elif self.config.generation.operator_type == "bash":
            imports.append("from airflow.operators.bash import BashOperator")
        elif self.config.generation.operator_type == "kubernetes":
            imports.extend([
                "from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator",
                "from kubernetes.client import models as k8s",
                "from kubernetes.client.models import V1EnvVar",
            ])

        # Add TriggerDagRunOperator if trigger_dag_id is configured
        if self.config.generation.trigger_dag_id:
            imports.append("from airflow.operators.trigger_dagrun import TriggerDagRunOperator")

        # Add EmptyOperator for source tables
        if self.config.generation.include_source_tables:
            imports.append("from airflow.operators.empty import EmptyOperator")

        imports.extend([
            "",
            "from sqlmesh import Context",
            "",
            "logger = logging.getLogger(__name__)",
        ])

        # Add callback imports if configured
        if self.config.airflow.on_failure_callback:
            module_path = self.config.airflow.on_failure_callback.rsplit('.', 1)
            if len(module_path) == 2:
                imports.insert(4, f"from {module_path[0]} import {module_path[1]}")
        if self.config.airflow.on_success_callback:
            module_path = self.config.airflow.on_success_callback.rsplit('.', 1)
            if len(module_path) == 2:
                imports.insert(4, f"from {module_path[0]} import {module_path[1]}")
        if self.config.airflow.sla_miss_callback:
            module_path = self.config.airflow.sla_miss_callback.rsplit('.', 1)
            if len(module_path) == 2:
                imports.insert(4, f"from {module_path[0]} import {module_path[1]}")

        return "\n".join(imports)

    def _build_config(self) -> str:
        """Build configuration section"""
        return dedent(f'''
            # SQLMesh Configuration
            SQLMESH_PROJECT_PATH = "{self.config.sqlmesh.project_path}"
            SQLMESH_ENVIRONMENT = "{self.config.sqlmesh.environment}"
            SQLMESH_GATEWAY = {f'"{self.config.sqlmesh.gateway}"' if self.config.sqlmesh.gateway else 'None'}
        ''').strip()

    def _build_dag_definition(self) -> str:
        """Build DAG definition"""
        default_args = self._format_default_args()

        schedule = f'"{self.config.airflow.schedule_interval}"' if self.config.airflow.schedule_interval else 'None'
        tags = repr(self.config.airflow.tags)
        description = f'"{self.config.airflow.description}"' if self.config.airflow.description else f'"SQLMesh DAG: {self.config.airflow.dag_id}"'

        # Handle start_date - use config or default to days_ago(1)
        if self.config.airflow.start_date:
            if self.config.airflow.start_date.startswith("days_ago"):
                start_date_str = self.config.airflow.start_date
            else:
                # Parse ISO date
                start_date_str = f'datetime.fromisoformat("{self.config.airflow.start_date}")'
        else:
            start_date_str = "datetime.now() - timedelta(days=1)"  # Default to yesterday

        # Build optional DAG arguments
        optional_args = ""
        if self.config.airflow.sla_miss_callback:
            optional_args += f"\n    sla_miss_callback={self.config.airflow.sla_miss_callback},"

        return f"""# DAG Definition
dag = DAG(
    dag_id="{self.config.airflow.dag_id}",
    default_args={default_args},
    description={description},
    schedule_interval={schedule},
    start_date={start_date_str},
    catchup={self.config.airflow.catchup},
    max_active_runs={self.config.airflow.max_active_runs},
    tags={tags},{optional_args}
)"""

    def _format_default_args(self) -> str:
        """Format default_args dictionary"""
        args = self.config.airflow.default_args.copy()

        # Ensure some basic defaults
        if 'owner' not in args:
            args['owner'] = 'sqlmesh'
        if 'retries' not in args:
            args['retries'] = 1
        if 'retry_delay' not in args:
            args['retry_delay'] = timedelta(minutes=5)

        # Format as Python dict
        lines = ["{"]
        for key, value in args.items():
            if key == 'retry_delay':
                # Handle timedelta - always output as timedelta object
                if isinstance(value, timedelta):
                    total_minutes = int(value.total_seconds() / 60)
                    value_str = f"timedelta(minutes={total_minutes})"
                elif isinstance(value, int):
                    value_str = f"timedelta(minutes={value})"
                else:
                    value_str = "timedelta(minutes=5)"
            elif isinstance(value, str):
                value_str = f'"{value}"'
            else:
                value_str = str(value)
            lines.append(f'    "{key}": {value_str},')
        lines.append("}")

        return "\n".join(lines)

    def _build_tasks(self) -> str:
        """Build task definitions"""
        task_defs = []

        # Build task for each model
        for model_name, model_info in self.dag_structure.models.items():
            task_def = self._build_single_task(model_info)
            task_defs.append(task_def)

        return "\n\n".join(task_defs)

    def _build_single_task(self, model_info: SQLMeshModelInfo) -> str:
        """Build a single task definition"""
        task_id = model_info.get_task_id()

        if self.config.generation.operator_type == "python":
            return self._build_python_task(model_info, task_id)
        elif self.config.generation.operator_type == "bash":
            return self._build_bash_task(model_info, task_id)
        elif self.config.generation.operator_type == "kubernetes":
            return self._build_kubernetes_task(model_info, task_id)
        else:
            raise ValueError(
                f"Unsupported operator_type: {self.config.generation.operator_type}. "
                f"Must be one of: python, bash, kubernetes"
            )

    def _build_python_task(self, model_info: SQLMeshModelInfo, task_id: str) -> str:
        """Build a PythonOperator task"""
        function_name = f"execute_{task_id}"

        # Escape model name for use in strings
        model_name_escaped = model_info.name.replace('"', '\\"')

        # Build the execution function
        func_def = f'''def {function_name}(**context):
    """Execute SQLMesh model: {model_name_escaped}"""
    logger.info("Executing SQLMesh model: {model_name_escaped}")
    
    # Load SQLMesh context
    ctx = Context(
        paths=SQLMESH_PROJECT_PATH,
        gateway=SQLMESH_GATEWAY,
    )
    
    # Get time interval from Airflow context (Airflow 2.2+)
    # Use data_interval for proper incremental model handling
    start = context.get('data_interval_start') or context.get('execution_date')
    end = context.get('data_interval_end') or context.get('execution_date')
    
    # Run the specific model
    logger.info(f"Running model {model_name_escaped} for interval {{start}} to {{end}}")
    
    # Use SQLMesh's run method with model selection
    result = ctx.run(
        environment=SQLMESH_ENVIRONMENT,
        start=start,
        end=end,
        select_models=["{model_name_escaped}"],
    )
    
    logger.info(f"Model {model_name_escaped} completed successfully")
    return result'''

        # Build the operator with additional configuration
        operator_args = [
            f'task_id="{task_id}"',
            f'python_callable={function_name}',
            'dag=dag',
        ]

        # Add pool if configured
        if self.config.generation.pool:
            operator_args.append(f'pool="{self.config.generation.pool}"')
            operator_args.append(f'pool_slots={self.config.generation.pool_slots}')

        # Add SLA if configured
        if self.config.airflow.sla:
            operator_args.append(f'sla=timedelta(seconds={self.config.airflow.sla})')

        # Add callbacks if configured
        if self.config.airflow.on_failure_callback:
            operator_args.append(f'on_failure_callback={self.config.airflow.on_failure_callback}')
        if self.config.airflow.on_success_callback:
            operator_args.append(f'on_success_callback={self.config.airflow.on_success_callback}')

        operator_def = f'''{task_id} = PythonOperator(
    {(","+chr(10)+"    ").join(operator_args)},
)'''

        return f"{func_def}\n\n{operator_def}"

    def _build_bash_task(self, model_info: SQLMeshModelInfo, task_id: str) -> str:
        """Build a BashOperator task"""
        bash_command = f"cd {self.config.sqlmesh.project_path} && sqlmesh run --select-models {model_info.name}"

        return dedent(f'''
            {task_id} = BashOperator(
                task_id="{task_id}",
                bash_command="{bash_command}",
                dag=dag,
            )
        ''').strip()

    def _build_kubernetes_task(self, model_info: SQLMeshModelInfo, task_id: str) -> str:
        """Build a KubernetesPodOperator task"""
        if not self.config.generation.docker_image:
            raise ValueError(
                "docker_image is required in generation config when using kubernetes operator_type. "
                "Add 'docker_image: your-image:tag' to your configuration."
            )

        model_name_escaped = model_info.name.replace('"', '\\"')
        namespace = self.config.generation.namespace
        image = self.config.generation.docker_image

        # Build environment variables
        env_vars = self.config.airflow.env_vars.copy()
        env_vars.update({
            'SQLMESH_PROJECT_PATH': self.config.sqlmesh.project_path,
            'SQLMESH_ENVIRONMENT': self.config.sqlmesh.environment,
        })
        if self.config.sqlmesh.gateway:
            env_vars['SQLMESH_GATEWAY'] = self.config.sqlmesh.gateway

        env_block = ",\n        ".join([
            f'V1EnvVar(name="{k}", value="{v}")'
            for k, v in env_vars.items()
        ])

        return dedent(f'''
            {task_id} = KubernetesPodOperator(
                task_id="{task_id}",
                name="{task_id}",
                namespace="{namespace}",
                image="{image}",
                cmds=["sqlmesh"],
                arguments=[
                    "run",
                    "--select-models", "{model_name_escaped}",
                    "--start", "{{{{ data_interval_start }}}}",
                    "--end", "{{{{ data_interval_end }}}}",
                ],
                env_vars=[
                    {env_block}
                ],
                get_logs=True,
                is_delete_operator_pod=True,
                dag=dag,
            )
        ''').strip()

    def _build_dependencies(self) -> str:
        """Build task dependencies"""
        dep_lines = ["# Task Dependencies"]

        for model_name, model_info in self.dag_structure.models.items():
            if not model_info.dependencies:
                continue

            task_id = model_info.get_task_id()

            for dep_name in model_info.dependencies:
                # Check if dependency exists in our models
                if dep_name not in self.dag_structure.models:
                    continue

                dep_task_id = self.dag_structure.models[dep_name].get_task_id()
                dep_lines.append(f"{dep_task_id} >> {task_id}")

        if len(dep_lines) == 1:
            dep_lines.append("# No dependencies")

        # Add trigger downstream DAG if configured
        if self.config.generation.trigger_dag_id:
            trigger_conf = repr(self.config.generation.trigger_dag_conf or {})
            dep_lines.append("")
            dep_lines.append("# Trigger downstream DAG on completion")
            dep_lines.append(f'''trigger_downstream = TriggerDagRunOperator(
    task_id="trigger_{self.config.generation.trigger_dag_id}",
    trigger_dag_id="{self.config.generation.trigger_dag_id}",
    conf={trigger_conf},
    dag=dag,
)''')
            # Find leaf tasks and set them as upstream
            leaf_tasks = [
                info.get_task_id()
                for name, info in self.dag_structure.models.items()
                if name not in {dep for m in self.dag_structure.models.values() for dep in m.dependencies}
            ]
            if leaf_tasks:
                dep_lines.append(f"[{', '.join(leaf_tasks)}] >> trigger_downstream")

        return "\n".join(dep_lines)

    # ========================================================================
    # Dynamic DAG Generation Methods
    # ========================================================================

    def _build_dynamic_header(self) -> str:
        """Build header for dynamic DAG"""
        return f'''"""
Dynamic SQLMesh DAG - Auto-discovers models at runtime

DAG ID: {self.config.airflow.dag_id}
Generated: {datetime.now().isoformat()}

This DAG automatically discovers SQLMesh models at runtime.
Place this file in Airflow's dags/ folder ONCE and forget about it!
When SQLMesh models change, the DAG automatically updates.

DO NOT EDIT MANUALLY - changes will be overwritten.
"""'''

    def _build_dynamic_imports(self) -> str:
        """Build imports for dynamic DAG"""
        imports = [
            "from datetime import datetime, timedelta",
            "import logging",
            "",
            "from airflow import DAG",
            "from airflow.operators.python import PythonOperator",
            "from airflow.models import Variable",
            "from airflow.exceptions import AirflowException",
            "",
            "from sqlmesh import Context",
            "from sqlmesh.utils.errors import SQLMeshError",
            "",
            "logger = logging.getLogger(__name__)",
        ]
        return "\n".join(imports)

    def _build_dynamic_config(self) -> str:
        """Build configuration section for dynamic DAG using Airflow Variables"""
        # Use Airflow Variables for flexible configuration
        return f'''# SQLMesh Configuration (from Airflow Variables)
# Set these in Airflow UI: Admin > Variables
SQLMESH_PROJECT_PATH = Variable.get(
    "sqlmesh_project_path", 
    default_var="{self.config.sqlmesh.project_path}"
)
SQLMESH_ENVIRONMENT = Variable.get(
    "sqlmesh_environment", 
    default_var="{self.config.sqlmesh.environment}"
)
SQLMESH_GATEWAY = Variable.get(
    "sqlmesh_gateway", 
    default_var={f'"{self.config.sqlmesh.gateway}"' if self.config.sqlmesh.gateway else 'None'}
)

logger.info(f"SQLMesh Project Path: {{SQLMESH_PROJECT_PATH}}")
logger.info(f"SQLMesh Environment: {{SQLMESH_ENVIRONMENT}}")'''

    def _build_dynamic_model_discovery(self) -> str:
        """Build model discovery section"""
        auto_schedule_code = ""
        if self.config.airflow.auto_schedule and not self.config.airflow.schedule_interval:
            auto_schedule_code = '''
    # Auto-detect schedule from SQLMesh model intervals
    def get_minimum_interval_cron(models_dict):
        """Find the most frequent interval from models and return cron expression"""
        if not models_dict:
            return "@daily"
        
        intervals = []
        for model in models_dict.values():
            interval_unit = getattr(model, 'interval_unit', None)
            if interval_unit:
                intervals.append(str(interval_unit).upper().replace("INTERVALUNIT.", ""))
        
        if not intervals:
            return "@daily"
        
        # Map intervals to minutes for comparison
        frequency_map = {
            "MINUTE": 1,
            "FIVE_MINUTE": 5,
            "TEN_MINUTE": 10,
            "QUARTER_HOUR": 15,
            "FIFTEEN_MINUTE": 15,
            "HALF_HOUR": 30,
            "THIRTY_MINUTE": 30,
            "HOUR": 60,
            "DAY": 1440,
            "WEEK": 10080,
            "MONTH": 43200,
            "QUARTER": 129600,
            "YEAR": 525600,
        }
        
        # Find minimum frequency
        min_interval = min(intervals, key=lambda x: frequency_map.get(x, 1440))
        
        # Convert to cron
        cron_map = {
            "MINUTE": "* * * * *",
            "FIVE_MINUTE": "*/5 * * * *",
            "TEN_MINUTE": "*/10 * * * *",
            "QUARTER_HOUR": "*/15 * * * *",
            "FIFTEEN_MINUTE": "*/15 * * * *",
            "HALF_HOUR": "*/30 * * * *",
            "THIRTY_MINUTE": "*/30 * * * *",
            "HOUR": "@hourly",
            "DAY": "@daily",
            "WEEK": "@weekly",
            "MONTH": "@monthly",
            "QUARTER": "0 0 1 */3 *",
            "YEAR": "@yearly",
        }
        
        return cron_map.get(min_interval, "@daily")
    
    try:
        RECOMMENDED_SCHEDULE = get_minimum_interval_cron(ctx.models)
        logger.info(f"ðŸ“… Auto-detected schedule: {{RECOMMENDED_SCHEDULE}}")
    except Exception as e:
        logger.warning(f"Failed to auto-detect schedule: {{e}}, using @daily")
        RECOMMENDED_SCHEDULE = "@daily"
'''

        return f'''# Discover SQLMesh models at DAG parse time
logger.info("Loading SQLMesh context and discovering models...")
try:
    ctx = Context(
        paths=SQLMESH_PROJECT_PATH,
        gateway=SQLMESH_GATEWAY,
    )
    
    # Extract model information
    discovered_models = {{}}
    for model_name, model in ctx.models.items():
        discovered_models[model_name] = {{
            "fqn": model.fqn,
            "name": str(model.name),
            "dependencies": [str(dep.name) for dep in model.depends_on],
        }}
    
    logger.info(f"âœ“ Discovered {{{{len(discovered_models)}}}} SQLMesh models")
{auto_schedule_code}    
except Exception as e:
    logger.error(f"Failed to load SQLMesh context: {{{{e}}}}")
    # Create empty dict to prevent DAG parse errors
    discovered_models = {{}}
    logger.warning("DAG will be created with no tasks")'''

    def _build_dynamic_dag_definition(self) -> str:
        """Build DAG definition for dynamic DAG"""
        default_args = self._format_default_args()

        # Support auto-scheduling in dynamic mode
        if self.config.airflow.auto_schedule and not self.config.airflow.schedule_interval:
            # Generate code to detect schedule at runtime - use variable directly
            schedule = 'RECOMMENDED_SCHEDULE'
            schedule_comment = '  # Auto-detected from SQLMesh models'
        else:
            schedule = f'"{self.config.airflow.schedule_interval}"' if self.config.airflow.schedule_interval else 'None'
            schedule_comment = ''

        tags = repr(self.config.airflow.tags + ['dynamic', 'auto-generated'])
        description = f'"{self.config.airflow.description}"' if self.config.airflow.description else f'"Dynamic SQLMesh DAG: {self.config.airflow.dag_id}"'

        # Handle start_date - use config or default to days_ago(1)
        if self.config.airflow.start_date:
            if self.config.airflow.start_date.startswith("days_ago"):
                start_date_str = self.config.airflow.start_date
            else:
                # Parse ISO date
                start_date_str = f'datetime.fromisoformat("{self.config.airflow.start_date}")'
        else:
            start_date_str = "datetime.now() - timedelta(days=1)"  # Default to yesterday

        return f'''# Create DAG{schedule_comment}
with DAG(
    dag_id="{self.config.airflow.dag_id}",
    start_date={start_date_str},
    schedule_interval={schedule},
    catchup={self.config.airflow.catchup},
    max_active_runs={self.config.airflow.max_active_runs},
    default_args={default_args},
    description={description},
    tags={tags},
) as dag:
    
    tasks = {{}}
    
    # Create task for each discovered model
    for model_name, model_info in discovered_models.items():
        # Sanitize task ID (remove quotes, dots, etc.)
        task_id = f"sqlmesh_{{model_name.replace('.', '_').replace('"', '').replace("'", '').replace(' ', '_')}}"
        # Remove consecutive underscores
        while "__" in task_id:
            task_id = task_id.replace("__", "_")
        task_id = task_id.strip("_")
        
        # Create callable for this model
        def make_callable(model_fqn, model_display_name):
            """Factory function to create model-specific callable"""
            def execute_model(**context):
                """Execute SQLMesh model"""
                logger.info(f"Executing SQLMesh model: {{model_display_name}}")
                
                try:
                    # Load fresh SQLMesh context
                    run_ctx = Context(
                        paths=SQLMESH_PROJECT_PATH,
                        gateway=SQLMESH_GATEWAY,
                    )
                    
                    # Get time interval (Airflow 2.2+)
                    # data_interval_start/end provides correct time range for incremental models
                    # Falls back to execution_date for backward compatibility with Airflow < 2.2
                    start = context.get('data_interval_start') or context.get('execution_date')
                    end = context.get('data_interval_end') or context.get('execution_date')
                    
                    logger.info(f"Running model {{model_display_name}} for interval {{start}} to {{end}}")
                    
                    # Run the specific model with proper time range
                    result = run_ctx.run(
                        environment=SQLMESH_ENVIRONMENT,
                        start=start,
                        end=end,
                        select_models=[model_fqn],
                    )
                    
                    logger.info(f"âœ“ Model {{model_display_name}} completed successfully")
                    # Convert CompletionStatus enum to string for XCom serialization
                    # Airflow cannot serialize enum types, so we return a simple dict
                    return {{
                        "status": result.name if hasattr(result, 'name') else str(result),
                        "model": model_fqn,
                    }}
                    
                except SQLMeshError as e:
                    logger.error(f"âœ— SQLMesh error in model {{model_display_name}}: {{e}}")
                    raise AirflowException(f"SQLMesh run failed: {{e}}")
                except Exception as e:
                    logger.error(f"âœ— Unexpected error in model {{model_display_name}}: {{e}}")
                    raise
            
            return execute_model
        
        # Create PythonOperator
        task = PythonOperator(
            task_id=task_id,
            python_callable=make_callable(model_info["fqn"], model_info["name"]),
        )
        
        tasks[model_name] = task
    
    # Set up dependencies based on SQLMesh lineage
    for model_name, model_info in discovered_models.items():
        if model_name not in tasks:
            continue
        
        current_task = tasks[model_name]
        
        for dep_name in model_info["dependencies"]:
            if dep_name in tasks:
                # Create dependency: upstream >> downstream
                tasks[dep_name] >> current_task
    
    logger.info(f"âœ“ DAG created with {{len(tasks)}} tasks")'''

    def _build_dynamic_tasks(self) -> str:
        """Build tasks section for dynamic DAG (already included in DAG definition)"""
        # Tasks are built dynamically inside the with DAG block
        # This method is here for API consistency
        return ""

