"""
Transaction manager y context manager para el ORM.

Funciones principales:
- Manejo de transacciones sync/async (BEGIN, COMMIT, ROLLBACK)
- Savepoints y transacciones anidadas
- Bulk operations (insert/update/delete masivo)
- Unit of Work y dirty tracking
- Hooks y eventos (before_commit, after_commit, etc.)
- Validación y serialización de modelos
- Gestión de errores y retry

Ejemplo de uso (sync):

from psqlmodel.engine import create_engine
from psqlmodel.model import PSQLModel
from psqlmodel.transactions import Transaction

engine = create_engine(...)

class User(PSQLModel):
    ...

with Transaction(engine) as tx:
    user = User(name="Alice", age=30)
    tx.register(user, op="insert")
    tx.savepoint("sp1")
    # ...
    tx.bulk_insert([user1, user2])

Ejemplo de uso (async, estilo clásico):

from psqlmodel.engine import create_engine
from psqlmodel.model import PSQLModel
from psqlmodel.transactions import Transaction, AsyncTransaction

engine = create_engine(..., async_=True)

class User(PSQLModel):
    ...

# Opción 1 (compatible con versiones anteriores)
async def main():
    async with Transaction(engine) as tx:
        user = User(name="Bob", age=25)
        tx.register(user, op="insert")
        await tx.savepoint_async("sp1")
        await tx.bulk_insert_async([user1, user2])

# Opción 2 (recomendada, API limpia)
async def main():
    async with AsyncTransaction(engine) as tx:
        user = User(name="Bob", age=25)
        tx.register(user, op="insert")
        await tx.savepoint("sp1")
        await tx.bulk_insert([user1, user2])
"""

from __future__ import annotations

from typing import Any, Optional, List, Dict, Tuple

import asyncio
import time
import re
import threading

import psycopg

from .engine import Engine
from ..query.crud import build_insert_sql, build_update_sql, build_bulk_insert_sql
from ..orm.model import PSQLModel


# ============================================================
# Internals: nested TX support (savepoints) without breaking API
# ============================================================

# Sync: thread-local stack per Engine (id(engine) -> list of frames)
# frame = {"conn": <psycopg conn>, "savepoint": Optional[str]}
_SYNC_LOCAL = threading.local()

# Async: context-var-like behavior (per task). We'll keep it simple with a dict stored on the task.
# We avoid importing contextvars to reduce surface; asyncio tasks already isolate attributes.


def _get_sync_stack() -> Dict[int, List[Dict[str, Any]]]:
    stk = getattr(_SYNC_LOCAL, "stack", None)
    if stk is None:
        stk = {}
        setattr(_SYNC_LOCAL, "stack", stk)
    return stk


def _get_async_stack() -> Dict[int, List[Dict[str, Any]]]:
    task = asyncio.current_task()
    if task is None:
        # Fallback: global-ish, but async without a task is rare; keep isolated in module attribute
        stk = getattr(_SYNC_LOCAL, "_async_fallback_stack", None)
        if stk is None:
            stk = {}
            setattr(_SYNC_LOCAL, "_async_fallback_stack", stk)
        return stk

    stk = getattr(task, "_psqlmodel_tx_stack", None)
    if stk is None:
        stk = {}
        setattr(task, "_psqlmodel_tx_stack", stk)
    return stk


_IDENTIFIER_RE = re.compile(r"^[A-Za-z_][A-Za-z0-9_]*$")


def _validate_identifier(name: str, *, kind: str = "identifier") -> str:
    if not name or not _IDENTIFIER_RE.match(name):
        raise ValueError(f"Invalid {kind}: {name!r}. Use [A-Za-z_][A-Za-z0-9_]*")
    return name


def _qualify_table(default_schema: str, table: str) -> str:
    # If already qualified (schema.table), respect it.
    if "." in (table or ""):
        return table
    return f"{default_schema}.{table}"


class Transaction:
    # --- Hooks y eventos transacción ---
    def add_hook(self, event: str, func):
        """Registrar un hook para un evento ('before_commit', 'after_commit', etc.)."""
        if not hasattr(self, "_hooks"):
            self._hooks = {}
        self._hooks.setdefault(event, []).append(func)

    def _run_hooks(self, event: str):
        for func in getattr(self, "_hooks", {}).get(event, []):
            func(self)

    async def _run_hooks_async(self, event: str):
        for func in getattr(self, "_hooks", {}).get(event, []):
            res = func(self)
            if asyncio.iscoroutine(res):
                await res

    # --- Gestión de errores y retry genérico (no SQL) ---
    def _retry_operation(
        self,
        func,
        *args,
        retries: int = 3,
        retry_exceptions: tuple = (Exception,),
        **kwargs,
    ):
        """Ejecuta una operación con reintentos automáticos en caso de error (no SQL)."""
        last_exc: Optional[BaseException] = None
        attempts = max(1, int(retries or 1))
        for attempt in range(attempts):
            try:
                return func(*args, **kwargs)
            except retry_exceptions as exc:
                last_exc = exc
                time.sleep(0.1 * (attempt + 1))  # backoff simple
        if last_exc is not None:
            raise last_exc
        raise RuntimeError("Retry operation failed without exception (unexpected)")

    async def _retry_operation_async(
        self,
        func,
        *args,
        retries: int = 3,
        retry_exceptions: tuple = (Exception,),
        **kwargs,
    ):
        """Ejecuta una operación async con reintentos automáticos en caso de error (no SQL)."""
        last_exc: Optional[BaseException] = None
        attempts = max(1, int(retries or 1))
        for attempt in range(attempts):
            try:
                return await func(*args, **kwargs)
            except retry_exceptions as exc:
                last_exc = exc
                await asyncio.sleep(0.1 * (attempt + 1))
        if last_exc is not None:
            raise last_exc
        raise RuntimeError("Async retry operation failed without exception (unexpected)")

    # ============================================================
    # Helpers internos: ejecutar SQL dentro de la TX usando
    # middlewares, hooks, metrics y tracer del Engine.
    # ============================================================
    def _execute_sql_in_tx_sync(
        self,
        sql: str,
        params: Optional[List[Any]] = None,
        *,
        retries: Optional[int] = None,
    ):
        """Ejecutar un SQL dentro de la transacción (sync) usando el pipeline del Engine."""
        if self._conn is None:
            raise RuntimeError("No active transaction")

        engine: Engine = self.engine

        def final(s: str, p: Any):
            # Copia de _execute_core_sync pero usando self._conn
            engine._debug("[ENGINE] SQL: {} {}", s, p)

            # Hooks before_execute
            for h in list(getattr(engine, "_hooks", {}).get("before_execute", [])):  # type: ignore[attr-defined]
                try:
                    h(s, p)
                except Exception:
                    engine._debug("[ENGINE] before_execute hook error")

            cur = self._conn.cursor()
            try:
                cur.execute(s, p or None)
                try:
                    rows = cur.fetchall()
                except psycopg.ProgrammingError:
                    rows = None
            finally:
                cur.close()

            # Hooks after_execute
            for h in list(getattr(engine, "_hooks", {}).get("after_execute", [])):  # type: ignore[attr-defined]
                try:
                    h(s, rows)
                except Exception:
                    engine._debug("[ENGINE] after_execute hook error")
            return rows

        # Construir cadena de middlewares igual que Engine._run_sync_pipeline
        import signal

        chain = final

        # IMPORTANT: igual que Engine -> sort asc; higher priority wraps outermost
        middlewares = sorted(
            getattr(engine, "_middlewares_sync", []),  # type: ignore[attr-defined]
            key=lambda x: x[0],
        )

        for _priority, mw_func, timeout in middlewares:
            prev = chain

            def make_mw(mw, prev_func, mw_timeout):
                def _wrapped(s, p):
                    # Igual que Engine: evitar señales en hilos secundarios o plataformas sin setitimer.
                    use_alarm = (
                        mw_timeout is not None
                        and mw_timeout > 0
                        and hasattr(signal, "setitimer")
                        and threading.current_thread() is threading.main_thread()
                    )
                    if use_alarm:
                        def _timeout_handler(signum, frame):
                            raise TimeoutError(f"Middleware timed out after {mw_timeout}s")

                        old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
                        try:
                            signal.setitimer(signal.ITIMER_REAL, mw_timeout)
                            return mw(s, p, prev_func)
                        finally:
                            signal.setitimer(signal.ITIMER_REAL, 0)
                            signal.signal(signal.SIGALRM, old_handler)
                    else:
                        return mw(s, p, prev_func)

                return _wrapped

            chain = make_mw(mw_func, prev, timeout)

        # Retry: si retries es None, usamos config.max_retries como en Engine.
        if retries is None:
            attempts = max(0, int(getattr(engine.config, "max_retries", 0) or 0))
        else:
            # Compat con versión anterior: retries = número TOTAL de intentos.
            attempts = max(0, int(retries) - 1)

        delay = float(getattr(engine.config, "retry_delay", 0.0) or 0.0)

        last_exc: Optional[BaseException] = None
        for i in range(attempts + 1):
            try:
                return chain(sql, params)
            except Exception as e:
                last_exc = e
                if i < attempts and delay > 0:
                    time.sleep(delay)
                    continue
                raise last_exc

    async def _execute_sql_in_tx_async(
        self,
        sql: str,
        params: Optional[List[Any]] = None,
        *,
        retries: Optional[int] = None,
    ):
        """Ejecutar un SQL dentro de la transacción (async) usando el pipeline del Engine."""
        if self._conn is None:
            raise RuntimeError("No active transaction")

        engine: Engine = self.engine

        async def final(s: str, p: Any):
            # Copia de _execute_core_async pero usando self._conn
            # Hooks before_execute
            for h in list(getattr(engine, "_hooks", {}).get("before_execute", [])):  # type: ignore[attr-defined]
                try:
                    res = h(s, p)
                    if asyncio.iscoroutine(res):
                        await res
                except Exception:
                    engine._debug("[ENGINE] before_execute hook error")

            # Convert psycopg-style %s placeholders to asyncpg-style $1, $2, $3
            if "%s" in s:
                idx = 1
                while "%s" in s:
                    s = s.replace("%s", f"${idx}", 1)
                    idx += 1

            # asyncpg requires jsonb as JSON string, convert dict/list params
            if p:
                from decimal import Decimal
                from uuid import UUID
                from datetime import datetime, date
                import json as json_stdlib
                
                def convert_special_types(obj):
                    if isinstance(obj, dict):
                        return {k: convert_special_types(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_special_types(v) for v in obj]
                    elif isinstance(obj, Decimal):
                        return float(obj)
                    elif isinstance(obj, UUID):
                        return str(obj)
                    elif isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    return obj
                
                processed_params = []
                for val in p:
                    if isinstance(val, (dict, list)):
                        # Convert special types then serialize to JSON string
                        val = convert_special_types(val)
                        processed_params.append(json_stdlib.dumps(val))
                    else:
                        processed_params.append(val)
                p = processed_params

            conn = self._conn

            # Compat: intenta fetch (como Engine) pero cae a execute si no devuelve filas.
            # Esto evita crashes en INSERT/UPDATE sin RETURNING.
            stmt_type = (s.strip().split(None, 1)[0].upper() if s and s.strip() else "UNKNOWN")
            expects_rows = ("RETURNING" in s.upper()) or (stmt_type in {"SELECT", "SHOW", "WITH"})

            rows = None
            if expects_rows:
                stmt = await conn.prepare(s)
                rows = await stmt.fetch(*p) if p else await stmt.fetch()
            else:
                # para DDL/DML sin RETURNING
                await conn.execute(s, *(p or []))
                rows = None

            # Hooks after_execute
            for h in list(getattr(engine, "_hooks", {}).get("after_execute", [])):  # type: ignore[attr-defined]
                try:
                    res = h(s, rows)
                    if asyncio.iscoroutine(res):
                        await res
                except Exception:
                    engine._debug("[ENGINE] after_execute hook error")
            return rows

        # Cadena de middlewares igual que Engine._run_async_pipeline
        chain = final

        # IMPORTANT: igual que Engine -> sort asc; higher priority wraps outermost
        middlewares = sorted(
            getattr(engine, "_middlewares_async", []),  # type: ignore[attr-defined]
            key=lambda x: x[0],
        )

        for _priority, mw_func, timeout in middlewares:
            prev = chain

            def make_mw(mw, prev_func, mw_timeout):
                async def _wrapped(s, p):
                    if mw_timeout is not None and mw_timeout > 0:
                        try:
                            return await asyncio.wait_for(mw(s, p, prev_func), timeout=mw_timeout)
                        except asyncio.TimeoutError:
                            raise TimeoutError(f"Async middleware timed out after {mw_timeout}s")
                    else:
                        return await mw(s, p, prev_func)

                return _wrapped

            chain = make_mw(mw_func, prev, timeout)

        if retries is None:
            attempts = max(0, int(getattr(engine.config, "max_retries", 0) or 0))
        else:
            # Compat con versión anterior: retries = número TOTAL de intentos.
            attempts = max(0, int(retries) - 1)

        delay = float(getattr(engine.config, "retry_delay", 0.0) or 0.0)

        last_exc: Optional[BaseException] = None
        for i in range(attempts + 1):
            try:
                return await chain(sql, params)
            except Exception as e:
                last_exc = e
                if i < attempts and delay > 0:
                    await asyncio.sleep(delay)
                    continue
                raise last_exc

    # --- Ejecutar SQL con retry explícito (compat) ---
    def execute_with_retry(
        self,
        sql: str,
        params: Optional[List[Any]] = None,
        retries: int = 3,
    ) -> None:
        """Ejecuta una sentencia SQL con reintentos automáticos (sync).

        Compatibilidad:
        - La firma se mantiene igual.
        - `retries` sigue siendo el número TOTAL de intentos (3 => hasta 3 intentos).
        """
        if self._conn is None:
            raise RuntimeError("No active transaction")
        self._execute_sql_in_tx_sync(sql, params or None, retries=retries)

    async def execute_with_retry_async(
        self,
        sql: str,
        params: Optional[List[Any]] = None,
        retries: int = 3,
    ) -> None:
        """Ejecuta una sentencia SQL con reintentos automáticos (async)."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        await self._execute_sql_in_tx_async(sql, params or None, retries=retries)

    # --- Bulk operations ---
    def BulkInsert(self, models, mode: str = "insert") -> "BulkOp":
        """
        Inicia una operación Bulk Insert (Sync).
        
        Args:
            models: Iterable of models to insert.
            mode: "insert" (DEFAULT, usa VALUES) o "copy" (usa COPY FROM).
        """
        return BulkOp(self, models, op="insert", mode=mode)

    def BulkUpdate(self, models) -> "BulkOp":
        """Inicia una operación Bulk Update (Sync).
        
        Args:
            models: Iterable of models to update.
        """
        return BulkOp(self, models, op="update")

    def BulkDelete(self, models) -> "BulkOp":
        """Inicia una operación Bulk Delete (Sync).
        
        Args:
            models: Iterable of models to delete.
        """
        return BulkOp(self, models, op="delete")


    async def bulk_insert_async(self, models: list) -> None:
        """Insertar múltiples modelos en una sola operación (async)."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        if not models:
            return
        for model in models:
            sql, values, returning_pk = build_insert_sql(model, style="asyncpg")
            result = await self._execute_sql_in_tx_async(sql, list(values))
            # Populate returned serial ID if available
            if returning_pk and result:
                try:
                    if hasattr(result, '__iter__') and len(result) > 0:
                        row = result[0]
                        if isinstance(row, dict):
                            setattr(model, returning_pk, row.get(returning_pk))
                        elif hasattr(row, returning_pk):
                            setattr(model, returning_pk, getattr(row, returning_pk))
                        elif hasattr(row, '__getitem__'):
                            setattr(model, returning_pk, row[0])
                except Exception:
                    pass

    async def bulk_update_async(self, models: list) -> None:
        """Actualizar múltiples modelos en una sola operación (async)."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        if not models:
            return
        for model in models:
            dirty = getattr(model, "dirty_fields", getattr(model, "_dirty_fields", {}))
            sql, values = build_update_sql(model, dirty, style="asyncpg")
            if sql:
                await self._execute_sql_in_tx_async(sql, list(values))

    # --- Savepoints y transacciones anidadas ---
    def savepoint(self, name: Optional[str] = None) -> str:
        """Crear un savepoint con nombre único o dado (sync)."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        if name is None:
            import uuid
            name = f"sp_{uuid.uuid4().hex[:8]}"
        _validate_identifier(name, kind="savepoint name")
        cur = self._conn.cursor()
        try:
            cur.execute(f"SAVEPOINT {name}")
        finally:
            cur.close()
        self.engine._debug("[TX] SAVEPOINT {}", name)
        return name

    def rollback_to_savepoint(self, name: str) -> None:
        """Hacer rollback parcial a un savepoint (sync)."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        _validate_identifier(name, kind="savepoint name")
        cur = self._conn.cursor()
        try:
            cur.execute(f"ROLLBACK TO SAVEPOINT {name}")
        finally:
            cur.close()
        self.engine._debug("[TX] ROLLBACK TO SAVEPOINT {}", name)

    def _release_savepoint(self, name: str) -> None:
        """Release savepoint (sync). Internal helper for nested TX."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        _validate_identifier(name, kind="savepoint name")
        cur = self._conn.cursor()
        try:
            cur.execute(f"RELEASE SAVEPOINT {name}")
        finally:
            cur.close()
        self.engine._debug("[TX] RELEASE SAVEPOINT {}", name)

    async def savepoint_async(self, name: Optional[str] = None) -> str:
        """Crear un savepoint en modo async."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        if name is None:
            import uuid
            name = f"sp_{uuid.uuid4().hex[:8]}"
        _validate_identifier(name, kind="savepoint name")
        await self._conn.execute(f"SAVEPOINT {name}")
        self.engine._debug("[TX] SAVEPOINT (async) {}", name)
        return name

    async def rollback_to_savepoint_async(self, name: str) -> None:
        """Hacer rollback parcial a un savepoint en modo async."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        _validate_identifier(name, kind="savepoint name")
        await self._conn.execute(f"ROLLBACK TO SAVEPOINT {name}")
        self.engine._debug("[TX] ROLLBACK TO SAVEPOINT (async) {}", name)

    async def _release_savepoint_async(self, name: str) -> None:
        """Release savepoint (async). Internal helper for nested TX."""
        if self._conn is None:
            raise RuntimeError("No active transaction")
        _validate_identifier(name, kind="savepoint name")
        await self._conn.execute(f"RELEASE SAVEPOINT {name}")
        self.engine._debug("[TX] RELEASE SAVEPOINT (async) {}", name)

    # --- Paralelismo delegando al Engine (lecturas fuera de la TX) ---
    def parallel_execute(self, tasks: list[Any], *, max_workers: Optional[int] = None) -> list[Any]:
        """
        Ejecutar múltiples tareas en paralelo usando el Engine (modo sync).

        IMPORTANTE:
        - Usa el pool del Engine, no la conexión de esta transacción.
        - Las consultas NO forman parte de esta transacción actual.
        - Úsalo para operaciones de solo lectura o independientes.
        """
        return self.engine.parallel_execute(tasks, max_workers=max_workers)

    async def parallel_execute_async(
        self,
        tasks: list[Any],
        *,
        max_concurrency: Optional[int] = None,
    ) -> list[Any]:
        """
        Versión async que delega al Engine.parallel_execute_async.

        Igual que parallel_execute(): las operaciones NO forman parte de la TX.
        """
        return await self.engine.parallel_execute_async(tasks, max_concurrency=max_concurrency)

    """Context manager de transacción de alto nivel.

    Uso sync:
        with Transaction(engine) as tx:
            # mutar modelos, ejecutar queries, etc.

    Uso async (modo compat):
        async with Transaction(engine) as tx:
            ...
    """

    def __init__(self, engine: Engine):
        self.engine = engine
        self._conn = None
        self._models: List[PSQLModel] = []
        self._inserts: List[PSQLModel] = []
        self._updates: List[PSQLModel] = []
        self._deletes: List[PSQLModel] = []

        # Internals for nested/savepoint mode
        self._own_conn: bool = True
        self._nested_savepoint: Optional[str] = None

        # Optional tokens if Engine supports binding/unbinding tx conn (forward-compatible)
        self._bind_token: Any = None

    # Registro de modelos afectados en esta transacción
    def register(self, model: PSQLModel, op: str = "auto") -> None:
        """Registrar modelo y operación: 'insert', 'update', 'delete', o 'auto'."""
        if op == "insert":
            if model not in self._inserts:
                self._inserts.append(model)
            return
        if op == "update":
            if model not in self._updates:
                self._updates.append(model)
            return
        if op == "delete":
            if model not in self._deletes:
                self._deletes.append(model)
            return

        # auto: decide por PK y dirty
        pk_name = None
        cols = getattr(model.__class__, "__columns__", {})
        for name, col in cols.items():
            if getattr(col, "primary_key", False):
                pk_name = name
                break

        dirty = getattr(model, "dirty_fields", getattr(model, "_dirty_fields", {}))
        is_tracked = hasattr(model, "_original_values")
        
        # Check if model was already flushed (SQLAlchemy-like behavior)
        # If it was flushed before, it's already in the DB -> should be UPDATE
        was_flushed = getattr(model, "_psqlmodel_flushed_", False)

        # Lógica mejorada:
        # - Si ya fue flusheado y tiene PK -> UPDATE (no INSERT de nuevo)
        # - Si no está tracked y no hay PK o PK es None -> INSERT
        # - Si hay dirty -> UPDATE
        if was_flushed and pk_name and getattr(model, pk_name, None) is not None:
            # Ya existe en DB, registrar como UPDATE si no está ya registrado
            if model not in self._updates and model not in self._inserts:
                self._updates.append(model)
            # Remove from inserts if it was there (shouldn't happen but safety)
            if model in self._inserts:
                self._inserts.remove(model)
        elif not is_tracked or pk_name is None or getattr(model, pk_name, None) is None:
            if model not in self._inserts:
                self._inserts.append(model)
        elif dirty:
            if model not in self._updates:
                self._updates.append(model)

    # --- sync ---
    def __enter__(self) -> "Transaction":
        if self.engine.config.async_:
            raise RuntimeError(
                "Engine configurado en modo async; usa 'async with Transaction(engine)' "
                "o 'async with AsyncTransaction(engine)'"
            )

        eng_id = id(self.engine)
        stack = _get_sync_stack().get(eng_id, [])

        # Nested TX => use savepoint on same connection
        if stack:
            self._conn = stack[-1]["conn"]
            self._own_conn = False
            sp = self.savepoint()  # validated
            self._nested_savepoint = sp
            stack.append({"conn": self._conn, "savepoint": sp})
            _get_sync_stack()[eng_id] = stack
            return self

        # Outermost TX => acquire and BEGIN
        self._own_conn = True
        self._conn = self.engine.acquire_sync()
        cur = self._conn.cursor()
        try:
            cur.execute("BEGIN")
        finally:
            cur.close()
        self.engine._debug("[TX] BEGIN")

        # Forward-compatible: bind tx conn into Engine if supported
        if hasattr(self.engine, "_bind_tx_conn_sync"):
            try:
                self._bind_token = self.engine._bind_tx_conn_sync(self._conn)  # type: ignore[attr-defined]
            except Exception:
                self._bind_token = None

        _get_sync_stack()[eng_id] = [{"conn": self._conn, "savepoint": None}]
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self._conn is None:
            return

        eng_id = id(self.engine)
        stack = _get_sync_stack().get(eng_id, [])

        # Nested: rollback/release savepoint only; never release conn to pool
        if not self._own_conn:
            sp = self._nested_savepoint
            try:
                if sp:
                    if exc_type is None:
                        # Flush inside nested scope (keeps old behavior)
                        self._run_hooks("before_flush")
                        for model in self._inserts:
                            self._flush_model(None, model, op="insert")
                        for model in self._updates:
                            self._flush_model(None, model, op="update")
                        for model in self._deletes:
                            self._flush_model(None, model, op="delete")
                        self._run_hooks("after_flush")

                        self._run_hooks("before_commit")
                        self._release_savepoint(sp)
                        self._run_hooks("after_commit")
                    else:
                        self._run_hooks("before_rollback")
                        self.rollback_to_savepoint(sp)
                        self._run_hooks("after_rollback")
            finally:
                # pop this frame
                if stack:
                    stack.pop()
                    if stack:
                        _get_sync_stack()[eng_id] = stack
                    else:
                        _get_sync_stack().pop(eng_id, None)
                self._nested_savepoint = None
            # propagate exception (default __exit__ behavior) by returning None
            return

        # Outermost: commit/rollback + release conn
        try:
            if exc_type is None:
                try:
                    # Flush models
                    self._run_hooks("before_flush")
                    for model in self._inserts:
                        self._flush_model(None, model, op="insert")
                    for model in self._updates:
                        self._flush_model(None, model, op="update")
                    for model in self._deletes:
                        self._flush_model(None, model, op="delete")
                    self._run_hooks("after_flush")

                    # Commit
                    self._run_hooks("before_commit")
                    cur = self._conn.cursor()
                    try:
                        cur.execute("COMMIT")
                    finally:
                        cur.close()
                    self.engine._debug("[TX] COMMIT")
                    self._run_hooks("after_commit")
                except Exception:
                    # If flush/commit fails, rollback to keep DB consistent
                    try:
                        self._run_hooks("before_rollback")
                        cur = self._conn.cursor()
                        try:
                            cur.execute("ROLLBACK")
                        finally:
                            cur.close()
                        self.engine._debug("[TX] ROLLBACK (commit failed)")
                        self._run_hooks("after_rollback")
                    finally:
                        raise
            else:
                # Rollback
                self._run_hooks("before_rollback")
                cur = self._conn.cursor()
                try:
                    cur.execute("ROLLBACK")
                finally:
                    cur.close()
                self.engine._debug("[TX] ROLLBACK")
                self._run_hooks("after_rollback")
        finally:
            # unbind if supported
            if self._bind_token is not None and hasattr(self.engine, "_unbind_tx_conn_sync"):
                try:
                    self.engine._unbind_tx_conn_sync(self._bind_token)  # type: ignore[attr-defined]
                except Exception:
                    pass
                self._bind_token = None

            # pop stack for this engine
            if stack:
                stack.pop()
            _get_sync_stack().pop(eng_id, None)

            self.engine.release_sync(self._conn)
            self._conn = None

    # --- async ---
    async def __aenter__(self) -> "Transaction":
        if not self.engine.config.async_:
            raise RuntimeError(
                "Engine configurado en modo sync; usa 'with Transaction(engine)' "
                "o 'async with AsyncTransaction(engine)' con un Engine async"
            )

        eng_id = id(self.engine)
        stack = _get_async_stack().get(eng_id, [])

        # Nested TX => savepoint on same connection
        if stack:
            self._conn = stack[-1]["conn"]
            self._own_conn = False
            sp = await self.savepoint_async()
            self._nested_savepoint = sp
            stack.append({"conn": self._conn, "savepoint": sp})
            _get_async_stack()[eng_id] = stack
            return self

        # Outermost
        self._own_conn = True
        self._conn = await self.engine.acquire()
        # Track connection acquisition for watchdog
        if hasattr(self.engine, '_track_connection_acquired'):
            self.engine._track_connection_acquired(self._conn)
        await self._conn.execute("BEGIN")
        self.engine._debug("[TX] BEGIN (async)")

        # Forward-compatible: bind tx conn into Engine if supported
        if hasattr(self.engine, "_bind_tx_conn_async"):
            try:
                self._bind_token = self.engine._bind_tx_conn_async(self._conn)  # type: ignore[attr-defined]
            except Exception:
                self._bind_token = None

        _get_async_stack()[eng_id] = [{"conn": self._conn, "savepoint": None}]
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        if self._conn is None:
            return

        eng_id = id(self.engine)
        stack = _get_async_stack().get(eng_id, [])

        # Nested: rollback/release savepoint only
        if not self._own_conn:
            sp = self._nested_savepoint
            try:
                if sp:
                    if exc_type is None:
                        try:
                            await self._run_hooks_async("before_flush")
                            for model in self._inserts:
                                await self._flush_model_async(self._conn, model, op="insert")
                            for model in self._updates:
                                await self._flush_model_async(self._conn, model, op="update")
                            for model in self._deletes:
                                await self._flush_model_async(self._conn, model, op="delete")
                            await self._run_hooks_async("after_flush")

                            await self._run_hooks_async("before_commit")
                            await self._release_savepoint_async(sp)
                            await self._run_hooks_async("after_commit")
                        except Exception:
                            await self._run_hooks_async("before_rollback")
                            await self.rollback_to_savepoint_async(sp)
                            await self._run_hooks_async("after_rollback")
                            raise
                    else:
                        await self._run_hooks_async("before_rollback")
                        await self.rollback_to_savepoint_async(sp)
                        await self._run_hooks_async("after_rollback")
            finally:
                if stack:
                    stack.pop()
                    if stack:
                        _get_async_stack()[eng_id] = stack
                    else:
                        _get_async_stack().pop(eng_id, None)
                self._nested_savepoint = None
            return

        # Outermost
        try:
            if exc_type is None:
                try:
                    await self._run_hooks_async("before_flush")
                    for model in self._inserts:
                        await self._flush_model_async(self._conn, model, op="insert")
                    for model in self._updates:
                        await self._flush_model_async(self._conn, model, op="update")
                    for model in self._deletes:
                        await self._flush_model_async(self._conn, model, op="delete")
                    await self._run_hooks_async("after_flush")

                    await self._run_hooks_async("before_commit")
                    await self._conn.execute("COMMIT")
                    self.engine._debug("[TX] COMMIT (async)")
                    await self._run_hooks_async("after_commit")
                except Exception:
                    try:
                        await self._run_hooks_async("before_rollback")
                        await self._conn.execute("ROLLBACK")
                        self.engine._debug("[TX] ROLLBACK (async, commit failed)")
                        await self._run_hooks_async("after_rollback")
                    finally:
                        raise
            else:
                await self._run_hooks_async("before_rollback")
                await self._conn.execute("ROLLBACK")
                self.engine._debug("[TX] ROLLBACK (async)")
                await self._run_hooks_async("after_rollback")
        finally:
            # unbind if supported
            if self._bind_token is not None and hasattr(self.engine, "_unbind_tx_conn_async"):
                try:
                    self.engine._unbind_tx_conn_async(self._bind_token)  # type: ignore[attr-defined]
                except Exception:
                    pass
                self._bind_token = None

            if stack:
                stack.pop()
            _get_async_stack().pop(eng_id, None)

            # Track connection release for watchdog
            if hasattr(self.engine, '_track_connection_released'):
                self.engine._track_connection_released(self._conn)
            await self.engine.release(self._conn)
            self._conn = None

    # -----------------------------
    # Flush de modelos (sync/async)
    # -----------------------------
    def _flush_model(self, cur, model: PSQLModel, op: str = "auto") -> None:
        """Flush de un modelo a la DB (sync). El parámetro `cur` se mantiene por compatibilidad."""
        table_name = getattr(model, "__tablename__", "unknown")
        self.engine._debug(f"[TX] FLUSH op={op} table={table_name}")

        from ..query.crud import DirtyTrackingMixin  # evitar ciclos fuertes

        if hasattr(model, "validate"):
            model.validate()
        if hasattr(model, "to_dict"):
            _ = model.to_dict()

        cols = getattr(model.__class__, "__columns__", {})
        pk_name = None
        for name, col in cols.items():
            if getattr(col, "primary_key", False):
                pk_name = name
                break

        dirty = getattr(model, "dirty_fields", getattr(model, "_dirty_fields", {}))
        
        # Auto-compute dirty if model was flushed and has original state saved
        if not dirty and hasattr(model, "_psqlmodel_original_"):
            original = getattr(model, "_psqlmodel_original_", {})
            for name in cols:
                old_val = original.get(name)
                new_val = getattr(model, name, None)
                if old_val != new_val:
                    dirty[name] = (old_val, new_val)

        sql: Optional[str] = None
        values: List[Any] = []
        returning_pk = None  # Will be set for serial PK inserts

        if op == "insert" or (op == "auto" and (pk_name is None or getattr(model, pk_name) is None)):
            sql, vals, returning_pk = build_insert_sql(model)
            values = list(vals)
        elif op == "update" or (op == "auto" and dirty):
            sql, vals = build_update_sql(model, dirty)
            if not sql:
                return
            values = list(vals)
        elif op == "delete":
            if pk_name is None:
                return
            pk_value = getattr(model, pk_name)
            table = getattr(model, "__tablename__")
            schema = getattr(model, "__schema__", "public") or "public"
            sql = f"DELETE FROM {schema}.{table} WHERE {pk_name} = %s"
            values = [pk_value]

            # ORM-level cascade: delete junction rows and one-to-many dependents
            for rel in getattr(model.__class__, "__relations__", {}).values():
                try:
                    rel._detect_relationship_type()
                except Exception:
                    continue

                if getattr(rel, "_relationship_type", None) == "many_to_many" and rel.secondary:
                    owner_table = getattr(model.__class__, "__tablename__")
                    owner_fk = (owner_table[:-1] if owner_table.endswith("s") else owner_table) + "_id"
                    junction = _qualify_table(schema, rel.secondary)
                    self._execute_sql_in_tx_sync(
                        f"DELETE FROM {junction} WHERE {owner_fk} = %s",
                        [pk_value],
                    )

                if getattr(rel, "_relationship_type", None) == "one_to_many":
                    target_model = rel._resolve_target()
                    if target_model is not None:
                        target_schema = getattr(target_model, "__schema__", "public") or "public"
                        target_table = getattr(target_model, "__tablename__")
                        owner_table = getattr(model.__class__, "__tablename__")
                        fk_name = rel._foreign_key or (
                            (owner_table[:-1] if owner_table.endswith("s") else owner_table) + "_id"
                        )
                        target_cols = getattr(target_model, "__columns__", {})
                        fk_col = target_cols.get(fk_name)
                        on_delete = getattr(fk_col, "on_delete", None) if fk_col else None

                        if on_delete in {"RESTRICT", "NO ACTION"}:
                            rows = self._execute_sql_in_tx_sync(
                                f"SELECT 1 FROM {target_schema}.{target_table} "
                                f"WHERE {fk_name} = %s LIMIT 1",
                                [pk_value],
                            )
                            if rows:
                                raise RuntimeError(f"Delete restricted: related rows exist in {target_table}")
                        elif on_delete == "SET NULL":
                            self._execute_sql_in_tx_sync(
                                f"UPDATE {target_schema}.{target_table} "
                                f"SET {fk_name} = NULL WHERE {fk_name} = %s",
                                [pk_value],
                            )
                        elif on_delete == "SET DEFAULT" and getattr(fk_col, "default", None) is not None:
                            default_val = fk_col.default() if callable(fk_col.default) else fk_col.default
                            self._execute_sql_in_tx_sync(
                                f"UPDATE {target_schema}.{target_table} "
                                f"SET {fk_name} = %s WHERE {fk_name} = %s",
                                [default_val, pk_value],
                            )
                        else:
                            self._execute_sql_in_tx_sync(
                                f"DELETE FROM {target_schema}.{target_table} "
                                f"WHERE {fk_name} = %s",
                                [pk_value],
                            )
        else:
            return

        if sql:
            result = self._execute_sql_in_tx_sync(sql, values)
            # Populate returned serial ID for inserts
            if op == "insert" and returning_pk and result:
                try:
                    if hasattr(result, '__iter__') and len(result) > 0:
                        row = result[0]
                        if isinstance(row, dict):
                            setattr(model, returning_pk, row.get(returning_pk))
                        elif hasattr(row, returning_pk):
                            setattr(model, returning_pk, getattr(row, returning_pk))
                        elif hasattr(row, '__getitem__'):
                            setattr(model, returning_pk, row[0])
                except Exception:
                    pass
            # Mark as flushed so subsequent add() calls will UPDATE, not INSERT
            if op == "insert":
                try:
                    setattr(model, "_psqlmodel_flushed_", True)
                    # Save original state for dirty tracking comparison on future updates
                    cols = getattr(model.__class__, "__columns__", {})
                    original = {name: getattr(model, name, None) for name in cols}
                    setattr(model, "_psqlmodel_original_", original)
                except Exception:
                    pass
            if isinstance(model, DirtyTrackingMixin):
                model.clear_dirty()

    async def _flush_model_async(self, conn, model: PSQLModel, op: str = "auto") -> None:
        """Flush de un modelo (async). El parámetro `conn` se mantiene por compatibilidad."""
        table_name = getattr(model, "__tablename__", "unknown")
        self.engine._debug(f"[TX] FLUSH op={op} table={table_name}")

        from ..query.crud import DirtyTrackingMixin

        if hasattr(model, "validate"):
            model.validate()
        if hasattr(model, "to_dict"):
            _ = model.to_dict()

        cols = getattr(model.__class__, "__columns__", {})
        pk_name = None
        for name, col in cols.items():
            if getattr(col, "primary_key", False):
                pk_name = name
                break

        dirty = getattr(model, "dirty_fields", getattr(model, "_dirty_fields", {}))
        
        # Auto-compute dirty if model was flushed and has original state saved
        if not dirty and hasattr(model, "_psqlmodel_original_"):
            original = getattr(model, "_psqlmodel_original_", {})
            for name in cols:
                old_val = original.get(name)
                new_val = getattr(model, name, None)
                if old_val != new_val:
                    dirty[name] = (old_val, new_val)

        sql: Optional[str] = None
        values: List[Any] = []
        returning_pk = None  # Will be set for serial PK inserts

        if op == "insert" or (op == "auto" and (pk_name is None or getattr(model, pk_name) is None)):
            sql, vals, returning_pk = build_insert_sql(model, style="asyncpg")
            values = list(vals)
        elif op == "update" or (op == "auto" and dirty):
            sql, vals = build_update_sql(model, dirty, style="asyncpg")
            if not sql:
                return
            values = list(vals)
        elif op == "delete":
            if pk_name is None:
                return
            pk_value = getattr(model, pk_name)
            table = getattr(model, "__tablename__")
            schema = getattr(model, "__schema__", "public") or "public"
            sql = f"DELETE FROM {schema}.{table} WHERE {pk_name} = $1"
            values = [pk_value]

            for rel in getattr(model.__class__, "__relations__", {}).values():
                try:
                    rel._detect_relationship_type()
                except Exception:
                    continue

                if getattr(rel, "_relationship_type", None) == "many_to_many" and rel.secondary:
                    owner_table = getattr(model.__class__, "__tablename__")
                    owner_fk = (owner_table[:-1] if owner_table.endswith("s") else owner_table) + "_id"
                    junction = _qualify_table(schema, rel.secondary)
                    await self._execute_sql_in_tx_async(
                        f"DELETE FROM {junction} WHERE {owner_fk} = $1",
                        [pk_value],
                    )

                if getattr(rel, "_relationship_type", None) == "one_to_many":
                    target_model = rel._resolve_target()
                    if target_model is not None:
                        target_schema = getattr(target_model, "__schema__", "public") or "public"
                        target_table = getattr(target_model, "__tablename__")
                        owner_table = getattr(model.__class__, "__tablename__")
                        fk_name = rel._foreign_key or (
                            (owner_table[:-1] if owner_table.endswith("s") else owner_table) + "_id"
                        )
                        target_cols = getattr(target_model, "__columns__", {})
                        fk_col = target_cols.get(fk_name)
                        on_delete = getattr(fk_col, "on_delete", None) if fk_col else None

                        if on_delete in {"RESTRICT", "NO ACTION"}:
                            rows = await self._execute_sql_in_tx_async(
                                f"SELECT 1 FROM {target_schema}.{target_table} "
                                f"WHERE {fk_name} = $1 LIMIT 1",
                                [pk_value],
                            )
                            if rows:
                                raise RuntimeError(f"Delete restricted: related rows exist in {target_table}")
                        elif on_delete == "SET NULL":
                            await self._execute_sql_in_tx_async(
                                f"UPDATE {target_schema}.{target_table} "
                                f"SET {fk_name} = NULL WHERE {fk_name} = $1",
                                [pk_value],
                            )
                        elif on_delete == "SET DEFAULT" and getattr(fk_col, "default", None) is not None:
                            default_val = fk_col.default() if callable(fk_col.default) else fk_col.default
                            await self._execute_sql_in_tx_async(
                                f"UPDATE {target_schema}.{target_table} "
                                f"SET {fk_name} = $1 WHERE {fk_name} = $2",
                                [default_val, pk_value],
                            )
                        else:
                            await self._execute_sql_in_tx_async(
                                f"DELETE FROM {target_schema}.{target_table} "
                                f"WHERE {fk_name} = $1",
                                [pk_value],
                            )
        else:
            return

        if sql:
            result = await self._execute_sql_in_tx_async(sql, values)
            # Populate returned serial ID for inserts
            if op == "insert" and returning_pk and result:
                try:
                    if hasattr(result, '__iter__') and len(result) > 0:
                        row = result[0]
                        if isinstance(row, dict):
                            setattr(model, returning_pk, row.get(returning_pk))
                        elif hasattr(row, returning_pk):
                            setattr(model, returning_pk, getattr(row, returning_pk))
                        elif hasattr(row, '__getitem__'):
                            setattr(model, returning_pk, row[0])
                except Exception:
                    pass
            # Mark as flushed so subsequent add() calls will UPDATE, not INSERT
            if op == "insert":
                try:
                    setattr(model, "_psqlmodel_flushed_", True)
                    # Save original state for dirty tracking comparison on future updates
                    cols = getattr(model.__class__, "__columns__", {})
                    original = {name: getattr(model, name, None) for name in cols}
                    setattr(model, "_psqlmodel_original_", original)
                except Exception:
                    pass
            if isinstance(model, DirtyTrackingMixin):
                model.clear_dirty()


# ============================================================
# AsyncTransaction – API async “bonita” sin sufijos *_async
# ============================================================

class AsyncTransaction(Transaction):
    """
    Versión asíncrona de Transaction con métodos sin sufijo *_async.

    Uso:
        async with AsyncTransaction(engine) as tx:
            tx.register(model)
            await tx.bulk_insert([model1, model2])
            sp = await tx.savepoint()
            ...
    """

    def __init__(self, engine: Engine):
        if not engine.config.async_:
            raise RuntimeError("AsyncTransaction requiere un Engine configurado con async_=True")
        super().__init__(engine)

    def __enter__(self):
        raise RuntimeError("AsyncTransaction solo puede usarse con 'async with AsyncTransaction(engine)'")

    async def __aenter__(self) -> "AsyncTransaction":
        await super().__aenter__()
        return self

    async def savepoint(self, name: Optional[str] = None) -> str:
        return await super().savepoint_async(name)

    async def rollback_to_savepoint(self, name: str) -> None:
        await super().rollback_to_savepoint_async(name)

    async def bulk_insert(self, models: list) -> None:
        await super().bulk_insert_async(models)

    async def bulk_update(self, models: list) -> None:
        await super().bulk_update_async(models)

    async def execute_with_retry(
        self,
        sql: str,
        params: Optional[List[Any]] = None,
        retries: int = 3,
    ) -> None:
        await super().execute_with_retry_async(sql, params, retries)

    async def parallel_execute(
        self,
        tasks: list[Any],
        *,
        max_concurrency: Optional[int] = None,
    ) -> list[Any]:
        """Ejecuta múltiples consultas en paralelo usando el Engine (fuera de esta TX)."""
        return await self.engine.parallel_execute_async(tasks, max_concurrency=max_concurrency)

    # --- New Bulk API ---

    def BulkInsert(self, models, mode: str = "insert") -> "BulkOp":
        """
        Inicia una operación Bulk Insert.
        
        Args:
            models: Iterable of models to insert.
            mode: "insert" (DEFAULT, usa VALUES) o "copy" (usa COPY FROM).
        """
        return BulkOp(self, models, op="insert", mode=mode)

    def BulkUpdate(self, models) -> "BulkOp":
        """Inicia una operación Bulk Update.
        
        Args:
            models: Iterable of models to update.
        """
        return BulkOp(self, models, op="update")

    def BulkDelete(self, models) -> "BulkOp":
        """Inicia una operación Bulk Delete.
        
        Args:
            models: Iterable of models to delete.
        """
        return BulkOp(self, models, op="delete")


class BulkOp:
    """Builder para operaciones Bulk Async."""
    def __init__(self, session_or_tx, models, op: str, mode: str = "insert"):
        self.obj = session_or_tx
        self.models = list(models)  # Convert any iterable to list
        self.op = op
        self.mode = mode
        self.returning_cols: Optional[Tuple] = None
        self._to_dicts: bool = False
        self._limit: Optional[int] = None
        self._skip: Optional[int] = None
        self._order_by: list = []  # List of (column_name, direction) tuples
        self._returning_query = None  # Optional SelectQuery for advanced Returning
        self._where_clauses: list = []  # WHERE clauses from SelectQuery
        self._data: Optional[list] = None  # Cached results
    
    async def _ensure_data(self) -> list:
        """Execute the operation if not already done and return cached results (Async)."""
        if self._data is None:
            self._data = await self._execute_async()
        return self._data
    
    def _ensure_data_sync(self) -> list:
        """Execute the operation if not already done and return cached results (Sync)."""
        if self._data is None:
            self._data = self._execute_sync()
        return self._data

    def _is_async(self) -> bool:
        """Determine if we should behave asynchronously."""
        return hasattr(self.obj, "_execute_sql_in_tx_async") or getattr(self.obj, "async_", False)
    
    def _convert_to_dicts(self, results: list) -> list:
        """Convert results to JSON-serializable dictionaries.
        
        Uses the same logic as session.exec().to_dicts() for consistency.
        Handles UUID, datetime, and other non-serializable types.
        """
        result = []
        for item in results:
            # Get column names from __columns__ if available
            cols = getattr(item.__class__, "__columns__", None)
            if cols:
                # Use model_dump for Pydantic compatibility
                # mode='json' ensures UUID/Date -> str conversion
                if hasattr(item, "model_dump"):
                    d = item.model_dump(mode='json')
                    result.append(d)
                    continue
            
            # Explicit RowResult support
            if hasattr(item, "_asdict"):
                result.append(item._asdict())
                continue
            
            # Fallback: filter __dict__ manually
            d = {}
            for k, v in getattr(item, "__dict__", {}).items():
                if not k.startswith("_") and not hasattr(v, "primary_key"):
                    # Convert non-serializable types
                    if hasattr(v, 'isoformat'):
                        d[k] = v.isoformat()
                    elif hasattr(v, 'hex') and hasattr(v, 'int'):  # UUID
                        d[k] = str(v)
                    else:
                        d[k] = v
            result.append(d)
        return result
    
    def Where(self, *clauses) -> "BulkOp":
        """Agrega cláusulas WHERE a la operación."""
        self._where_clauses.extend(clauses)
        return self
    
    def Returning(self, *cols) -> "BulkOp":
        """Agrega cláusula RETURNING a la operación.
        
        Args:
            cols: Columns, Model class, or a SelectQuery for advanced filtering.
                  If SelectQuery is passed, its WHERE/ORDER BY/LIMIT/OFFSET will be applied.
        """
        if self.mode == "copy":
            raise ValueError("Cannot use Returning with COPY mode (use mode='insert')")
        
        # Detect if first arg is a SelectQuery
        if len(cols) == 1:
            arg = cols[0]
            # Duck type check for SelectQuery
            if hasattr(arg, "to_sql_params") and hasattr(arg, "where") and hasattr(arg, "base_model"):
                self._returning_query = arg
                # Extract model for hydration
                if hasattr(arg, "base_model") and arg.base_model:
                    self.returning_cols = (arg.base_model,)
                else:
                    self.returning_cols = cols
                return self
        
        self.returning_cols = cols
        return self
    
    def to_dicts(self) -> "BulkOp":
        """Convert returned models to dictionaries. Chain after Returning(Model)."""
        self._to_dicts = True
        return self
    
    def all(self) -> Any:
        """Execute and return all results as a list or coroutine."""
        if self._is_async():
            return self._ensure_data()
        return self._ensure_data_sync()
    
    def first(self) -> Any:
        """Execute and return the first result or None."""
        if self._is_async():
            async def _first():
                data = await self._ensure_data()
                return data[0] if data else None
            return _first()
        
        data = self._ensure_data_sync()
        return data[0] if data else None
    
    def one(self) -> Any:
        """Execute and return exactly one result or raise error."""
        if self._is_async():
            async def _one():
                data = await self._ensure_data()
                if not data: raise ValueError("No results found")
                if len(data) > 1: raise ValueError(f"Expected 1 result, got {len(data)}")
                return data[0]
            return _one()
        
        data = self._ensure_data_sync()
        if not data: raise ValueError("No results found")
        if len(data) > 1: raise ValueError(f"Expected 1 result, got {len(data)}")
        return data[0]
    
    def one_or_none(self) -> Any:
        """Execute and return one result or None."""
        if self._is_async():
            async def _one_or_none():
                data = await self._ensure_data()
                if not data: return None
                if len(data) > 1: raise ValueError(f"Expected at most 1 result, got {len(data)}")
                return data[0]
            return _one_or_none()
        
        data = self._ensure_data_sync()
        if not data: return None
        if len(data) > 1: raise ValueError(f"Expected at most 1 result, got {len(data)}")
        return data[0]
    
    def Limit(self, n: int) -> "BulkOp":
        """Limit the number of returned results (SQL-level via CTE)."""
        # Check for conflict with SelectQuery's LIMIT
        if self._returning_query and getattr(self._returning_query, "limit", None) is not None:
            raise ValueError("Cannot use Limit() on BulkOp when SelectQuery in Returning() already has Limit(). Use one or the other.")
        self._limit = n
        return self
    
    def Skip(self, n: int) -> "BulkOp":
        """Skip the first n results (SQL-level via CTE, aka OFFSET)."""
        # Check for conflict with SelectQuery's OFFSET
        if self._returning_query and getattr(self._returning_query, "offset", None) is not None:
            raise ValueError("Cannot use Skip() on BulkOp when SelectQuery in Returning() already has Offset(). Use one or the other.")
        self._skip = n
        return self
    
    def OrderBy(self, *cols) -> "BulkOp":
        """Order returned results (SQL-level via CTE). Call Asc() or Desc() after."""
        for col in cols:
            if hasattr(col, "name"):
                self._order_by.append((col.name, "ASC"))  # Default ASC
            elif isinstance(col, str):
                self._order_by.append((col, "ASC"))
            else:
                self._order_by.append((str(col), "ASC"))
        return self
    
    def Asc(self) -> "BulkOp":
        """Set ascending order for the last OrderBy column(s)."""
        if self._order_by:
            name, _ = self._order_by[-1]
            self._order_by[-1] = (name, "ASC")
        return self
    
    def Desc(self) -> "BulkOp":
        """Set descending order for the last OrderBy column(s)."""
        if self._order_by:
            name, _ = self._order_by[-1]
            self._order_by[-1] = (name, "DESC")
        return self
    
    def _needs_cte(self) -> bool:
        """Check if we need CTE wrapper (for ORDER BY, LIMIT, OFFSET, or SelectQuery)."""
        return bool(self._order_by or self._limit is not None or self._skip is not None or self._returning_query)

    def execute(self) -> Any:
        """Ejecuta la operación de forma sincrónica (solo si la sesión/transacción es sync)."""
        return self._execute_sync()

    def __await__(self):
        return self._ensure_data().__await__()

    def __iter__(self):
        return iter(self._ensure_data_sync())

    def __len__(self):
        return len(self._ensure_data_sync())

    def __getitem__(self, item):
        return self._ensure_data_sync()[item]

    def __repr__(self):
        # Evitar ejecutar la query solo para el repr si no está cargada
        if self._data is not None:
            return f"BulkOp(op={self.op}, mode={self.mode}, results={len(self._data)})"
        return f"BulkOp(op={self.op}, mode={self.mode}, models={len(self.models)})"

    def _execute_sync(self):
        if not self.models:
            return []
        
        # Resolve Transaction (Sync)
        tx = self.obj
        if hasattr(tx, "_ensure_transaction"):
             tx._ensure_transaction() 
             tx = tx._tx
        
        if tx is None:
             raise RuntimeError("Session/Transaction is not active")

        # --- BULK INSERT (SYNC) ---
        if self.op == "insert":
            if self.mode == "copy":
                 pass
            
            # Optimized INSERT ... VALUES
            sql, values = build_bulk_insert_sql(self.models, style="pyformat")

            if not sql:
                return []
            
            # Handle Returning
            model_to_hydrate = None
            names = []
            
            if self.returning_cols:
                if len(self.returning_cols) == 1 and isinstance(self.returning_cols[0], type) and hasattr(self.returning_cols[0], "__tablename__"):
                     model_to_hydrate = self.returning_cols[0]
                     cols_map = getattr(model_to_hydrate, "__columns__", {})
                     explicit_names = list(cols_map.keys())
                     for cname in explicit_names:
                         names.append(cname)
                else:
                    for c in self.returning_cols:
                        if hasattr(c, "name"): names.append(c.name)
                        elif isinstance(c, str): names.append(c)
                        elif hasattr(c, "key"): names.append(c.key)
                        elif isinstance(c, type) and hasattr(c, "__tablename__"): names.append("*")
                        else: names.append(str(c))
            
            # Build SQL with CTE if needed (for ORDER BY, LIMIT, OFFSET, or SelectQuery)
            if self.returning_cols and names:
                returning_clause = ", ".join(names)
                
                if self._needs_cte():
                    # Use CTE: WITH inserted AS (INSERT ... RETURNING ...) SELECT * FROM inserted [WHERE ...] [ORDER BY ...] [LIMIT ...] [OFFSET ...]
                    cte_sql = f"WITH inserted AS ({sql} RETURNING {returning_clause}) SELECT * FROM inserted"
                    
                    # If we have a SelectQuery, extract its clauses
                    where_sql = ""
                    query_order_by = []
                    query_limit = None
                    query_offset = None
                    query_params = []
                    
                    if self._returning_query:
                        rq = self._returning_query
                        # Extract WHERE from SelectQuery
                        if hasattr(rq, "where_clauses") and rq.where_clauses:
                            # Build WHERE SQL from the query's where clauses
                            where_parts = []
                            for clause in rq.where_clauses:
                                if hasattr(clause, "to_sql_params"):
                                    w_sql, w_params = clause.to_sql_params(style="pyformat")
                                    where_parts.append(w_sql)
                                    query_params.extend(w_params)
                                elif isinstance(clause, str):
                                    where_parts.append(clause)
                            if where_parts:
                                where_sql = " AND ".join(where_parts)
                        
                        # Extract ORDER BY from SelectQuery
                        if hasattr(rq, "_order_by") and rq._order_by:
                            for ob in rq._order_by:
                                if isinstance(ob, tuple):
                                    query_order_by.append(ob)
                                elif hasattr(ob, "name"):
                                    query_order_by.append((ob.name, "ASC"))
                        
                        # Extract LIMIT/OFFSET from SelectQuery
                        query_limit = getattr(rq, "limit", None) or getattr(rq, "_limit", None)
                        query_offset = getattr(rq, "offset", None) or getattr(rq, "_offset", None)
                    
                    # Add WHERE
                    if where_sql:
                        cte_sql += f" WHERE {where_sql}"
                    
                    # Merge ORDER BY (BulkOp takes precedence, then SelectQuery)
                    all_order_by = self._order_by + query_order_by
                    if all_order_by:
                        order_parts = [f"{col} {direction}" for col, direction in all_order_by]
                        cte_sql += " ORDER BY " + ", ".join(order_parts)
                    
                    # Use LIMIT (BulkOp takes precedence, then SelectQuery)
                    final_limit = self._limit if self._limit is not None else query_limit
                    if final_limit is not None:
                        cte_sql += f" LIMIT {final_limit}"
                    
                    # Use OFFSET (BulkOp takes precedence, then SelectQuery)
                    final_offset = self._skip if self._skip is not None else query_offset
                    if final_offset is not None:
                        cte_sql += f" OFFSET {final_offset}"
                    
                    sql = cte_sql
                    # Extend values with query params (for WHERE clause placeholders)
                    values = list(values) + query_params
                else:
                    # Simple RETURNING without CTE
                    sql += " RETURNING " + returning_clause
            
            # Execute Sync
            rows = tx._execute_sql_in_tx_sync(sql, values)
            
            if self.returning_cols and rows:
                if model_to_hydrate:
                     results = []
                     cols = getattr(model_to_hydrate, "__columns__", {})
                     col_names = list(cols.keys()) # Must match expansion order above
                     
                     # Try to access Session for Identity Map
                     session = None
                     identity_map = None
                     if hasattr(tx, "_session") and tx._session:
                         session = tx._session
                     elif hasattr(tx, "_identity_map"): # It might be a Session object itself acting as TX wrapper or similar
                         session = tx
                     
                     if session:
                         identity_map = getattr(session, "_identity_map", {})

                     for row in rows:
                         # row is tuple. Zip with keys.
                         data = dict(zip(col_names, row))
                         
                         instance = None
                         pk_name = None
                         for name, col in cols.items():
                             if getattr(col, "primary_key", False):
                                 pk_name = name
                                 break
                         
                         if pk_name and pk_name in data and identity_map is not None:
                             k = (model_to_hydrate, data[pk_name])
                             if k in identity_map:
                                 instance = identity_map[k]
                         
                         if instance is None:
                             # Handle JSON strings if needed (basic support)
                             # ... (omitted generic json parsing for brevity/perf, relying on Model init)
                             instance = model_to_hydrate(**data)
                             try:
                                 if session:
                                     setattr(instance, "__session__", session)
                                 setattr(instance, "_psqlmodel_flushed_", True)
                                 original = {name: getattr(instance, name, None) for name in cols}
                                 setattr(instance, "_psqlmodel_original_", original)
                             except Exception:
                                 pass
                             
                             if session and hasattr(session, "_add_to_cache"):
                                 session._add_to_cache(instance)
                                 
                         results.append(instance)
                     
                     # Apply to_dicts conversion if requested
                     if self._to_dicts:
                         return self._convert_to_dicts(results)
                     return results

                single_col = len(self.returning_cols) == 1
                if single_col:
                    try:
                        result = [r[0] for r in rows]
                    except (IndexError, TypeError):
                        result = rows
                    # Apply to_dicts conversion if requested for single column results
                    if self._to_dicts:
                        return self._convert_to_dicts(result)
                    return result
                
                # Default: return rows as-is or converted
                if self._to_dicts:
                    return self._convert_to_dicts(rows)
                return rows
            
            # No returning cols but rows exist
            if self._to_dicts and rows:
                return self._convert_to_dicts(rows)
            return rows

        # --- BULK UPDATE (SYNC) ---
        elif self.op == "update":
            # For now delegate to legacy iterative or implement bulk update SQL
            # Let's use the new BulkUpdate logic if possible, but build_bulk_update_sql might not exist yet?
            # Wait, I didn't verify build_bulk_update_sql exists. 
            # Checking task.md: "Implement BulkUpdate logic". A previous log said "Implement ... logic".
            # I should use the iterative approach as fallback if bulk sql builder is missing, 
            # or better, check if I implemented build_bulk_update_sql.
            # Assuming I didn't (task said 'Implement BulkUpdate logic' but the code viewed earlier showed `BulkUpdate` calling `bulk_update_async`).
            # So for sync:
            if hasattr(tx, "bulk_update"): # Use the legacy-like method I just removed from Sync Transaction? 
                # Wait, I am REMOVING bulk_update from Sync Transaction in this very call.
                # So I must implement the logic here inside BulkOp or keep a private helper.
                # The previous chunks show I AM removing bulk_update.
                # So I must reimplement iterative update here for now or bulk SQL.
                 if not self.models: return []
                 for model in self.models:
                     dirty = getattr(model, "dirty_fields", getattr(model, "_dirty_fields", {}))
                     # We need build_update_sql. It is imported in transactions.py
                     from ..query.crud import build_update_sql
                     sql_up, values_up = build_update_sql(model, dirty, style="pyformat")
                     if sql_up:
                         tx._execute_sql_in_tx_sync(sql_up, list(values_up))
            return []

        # --- BULK DELETE (SYNC) ---
        elif self.op == "delete":
            if not self.models: return []
            first = self.models[0]
            pk_name = None
            cols = getattr(first.__class__, "__columns__", {})
            for name, col in cols.items():
                if getattr(col, "primary_key", False):
                    pk_name = name
                    break
            
            if pk_name:
                table = getattr(first, "__tablename__")
                schema = getattr(first, "__schema__", "public") or "public"
                ids = [getattr(m, pk_name) for m in self.models if getattr(m, pk_name) is not None]
                if ids:
                    params = list(ids)
                    placeholders = ["%s" for _ in range(len(ids))]
                    sql = f"DELETE FROM {schema}.{table} WHERE {pk_name} IN ({', '.join(placeholders)})"
                    
                    # Extra WHERE clauses
                    all_where = list(self._where_clauses)
                    if self._returning_query and hasattr(self._returning_query, "where_clauses"):
                        all_where.extend(self._returning_query.where_clauses)
                    
                    if all_where:
                        where_parts = []
                        for clause in all_where:
                            if hasattr(clause, "to_sql_params"):
                                w_sql, w_params = clause.to_sql_params()
                                where_parts.append(w_sql)
                                params.extend(w_params)
                            elif isinstance(clause, str):
                                where_parts.append(clause)
                        if where_parts:
                            sql += " AND (" + ") AND (".join(where_parts) + ")"
                    
                    if self.returning_cols:
                         names = []
                         for c in self.returning_cols:
                            if hasattr(c, "name"): names.append(c.name)
                            else: names.append(str(c))
                         if names:
                             sql += " RETURNING " + ", ".join(names)

                    rows = tx._execute_sql_in_tx_sync(sql, params)
                    
                    if self.returning_cols and rows:
                         single_col = len(self.returning_cols) == 1
                         if single_col:
                            return [r[0] for r in rows]
                         return rows
            else:
                 for m in self.models:
                      # rudimentary delete
                      pass 
            return []
        
        return []

    async def _execute_async(self):
        if not self.models:
            return []
        
        # Resolve Transaction
        tx = self.obj
        # Check if it's AsyncSession (duck typing or check attribute)
        if hasattr(tx, "_ensure_transaction"):
             await tx._ensure_transaction()
             tx = tx._tx
             if tx is None:
                 raise RuntimeError("AsyncSession failed to create transaction")

        # --- BULK INSERT ---
        if self.op == "insert":
            if self.mode == "copy":
                 # COPY Logic (fallback or native)
                 pass
            
            # Optimized INSERT ... VALUES
            sql, values = build_bulk_insert_sql(self.models, style="asyncpg")
            if not sql:
                return []
            
            # Handle Returning
            model_to_hydrate = None
            names = []
            
            if self.returning_cols:
                if len(self.returning_cols) == 1 and isinstance(self.returning_cols[0], type) and hasattr(self.returning_cols[0], "__tablename__"):
                     model_to_hydrate = self.returning_cols[0]
                     cols_map = getattr(model_to_hydrate, "__columns__", {})
                     for cname in cols_map.keys():
                         names.append(cname)
                else:
                    for c in self.returning_cols:
                        if hasattr(c, "name"): names.append(c.name)
                        elif isinstance(c, str): names.append(c)
                        elif hasattr(c, "key"): names.append(c.key)
                        elif isinstance(c, type) and hasattr(c, "__tablename__"): 
                             names.append("*")
                        else: names.append(str(c))
            
            # Build SQL with CTE if needed (for ORDER BY, LIMIT, OFFSET, or SelectQuery)
            if self.returning_cols and names:
                returning_clause = ", ".join(names)
                
                if self._needs_cte():
                    # Use CTE: WITH inserted AS (INSERT ... RETURNING ...) SELECT * FROM inserted [WHERE ...] [ORDER BY ...] [LIMIT ...] [OFFSET ...]
                    cte_sql = f"WITH inserted AS ({sql} RETURNING {returning_clause}) SELECT * FROM inserted"
                    
                    # If we have a SelectQuery, extract its clauses
                    where_sql = ""
                    query_order_by = []
                    query_limit = None
                    query_offset = None
                    query_params = []
                    
                    if self._returning_query:
                        rq = self._returning_query
                        # Extract WHERE from SelectQuery
                        if hasattr(rq, "where_clauses") and rq.where_clauses:
                            where_parts = []
                            for clause in rq.where_clauses:
                                if hasattr(clause, "to_sql_params"):
                                    w_sql, w_params = clause.to_sql_params(style="asyncpg")
                                    where_parts.append(w_sql)
                                    query_params.extend(w_params)
                                elif isinstance(clause, str):
                                    where_parts.append(clause)
                            if where_parts:
                                where_sql = " AND ".join(where_parts)
                        
                        # Extract ORDER BY from SelectQuery
                        if hasattr(rq, "_order_by") and rq._order_by:
                            for ob in rq._order_by:
                                if isinstance(ob, tuple):
                                    query_order_by.append(ob)
                                elif hasattr(ob, "name"):
                                    query_order_by.append((ob.name, "ASC"))
                        
                        # Extract LIMIT/OFFSET from SelectQuery
                        query_limit = getattr(rq, "limit", None) or getattr(rq, "_limit", None)
                        query_offset = getattr(rq, "offset", None) or getattr(rq, "_offset", None)
                    
                    # Add WHERE
                    if where_sql:
                        cte_sql += f" WHERE {where_sql}"
                    
                    # Merge ORDER BY (BulkOp takes precedence, then SelectQuery)
                    all_order_by = self._order_by + query_order_by
                    if all_order_by:
                        order_parts = [f"{col} {direction}" for col, direction in all_order_by]
                        cte_sql += " ORDER BY " + ", ".join(order_parts)
                    
                    # Use LIMIT (BulkOp takes precedence, then SelectQuery)
                    final_limit = self._limit if self._limit is not None else query_limit
                    if final_limit is not None:
                        cte_sql += f" LIMIT {final_limit}"
                    
                    # Use OFFSET (BulkOp takes precedence, then SelectQuery)
                    final_offset = self._skip if self._skip is not None else query_offset
                    if final_offset is not None:
                        cte_sql += f" OFFSET {final_offset}"
                    
                    sql = cte_sql
                    # Extend values with query params (for WHERE clause placeholders)
                    values = list(values) + query_params
                else:
                    # Simple RETURNING without CTE
                    sql += " RETURNING " + returning_clause
            
            rows = await tx._execute_sql_in_tx_async(sql, values)
            
            if self.returning_cols and rows:
                if model_to_hydrate:
                     # Hydrate instances
                     results = []
                     cols = getattr(model_to_hydrate, "__columns__", {})
                     
                     # Try to access Session for Identity Map
                     session = None
                     identity_map = None
                     # In async, tx is often the Transaction object, but we need the Session.
                     # self.obj might be the session? 
                     # self.obj is initialized with "session" or "tx".
                     if hasattr(self.obj, "_identity_map"):
                         session = self.obj
                     elif hasattr(tx, "_session") and tx._session:
                         session = tx._session
                     
                     if session:
                         identity_map = getattr(session, "_identity_map", {})

                     for row in rows:
                         data = dict(row)
                         
                         instance = None
                         pk_name = None
                         for name, col in cols.items():
                             if getattr(col, "primary_key", False):
                                 pk_name = name
                                 break
                         
                         if pk_name and pk_name in data and identity_map is not None:
                             k = (model_to_hydrate, data[pk_name])
                             if k in identity_map:
                                 instance = identity_map[k]
                                 # Update instance with new data? Usually identity map wins or we update it.
                                 # Exec logic usually returns cached instance.
                        
                         if instance is None:
                             instance = model_to_hydrate(**data)
                             # Set state
                             try:
                                 if session:
                                     setattr(instance, "__session__", session)
                                 setattr(instance, "_psqlmodel_flushed_", True)
                                 original = {name: getattr(instance, name, None) for name in cols}
                                 setattr(instance, "_psqlmodel_original_", original)
                             except Exception:
                                 pass
                             
                             if session and hasattr(session, "_add_to_cache"):
                                 session._add_to_cache(instance)

                         results.append(instance)
                     
                     # Apply to_dicts conversion if requested
                     if self._to_dicts:
                         return self._convert_to_dicts(results)
                     return results

                # If only 1 column requested AND it's NOT a wildcard/Model (handled above)
                single_col = len(self.returning_cols) == 1
                if single_col:
                    try:
                        result = [r[0] for r in rows]
                    except (IndexError, TypeError):
                        result = rows
                    # Apply to_dicts conversion if requested for single column results
                    if self._to_dicts:
                        return self._convert_to_dicts(result)
                    return result
                
                # Default: return rows as-is or converted
                if self._to_dicts:
                    return self._convert_to_dicts(rows)
                return rows
            
            # No returning cols but rows exist
            if self._to_dicts and rows:
                return self._convert_to_dicts(rows)
            return rows

        # --- BULK UPDATE ---
        elif self.op == "update":
            await tx.bulk_update_async(self.models)
            return []

        # --- BULK DELETE ---
        elif self.op == "delete":
            # Optimized DELETE WHERE IN (...)
            if not self.models:
                return []
            
            first = self.models[0]
            pk_name = None
            for name, col in getattr(first.__class__, "__columns__", {}).items():
                if getattr(col, "primary_key", False):
                    pk_name = name
                    break
            
            if pk_name:
                table = getattr(first, "__tablename__")
                schema = getattr(first, "__schema__", "public") or "public"
                ids = [getattr(m, pk_name) for m in self.models if getattr(m, pk_name) is not None]
                if ids:
                    params = list(ids)
                    placeholders = [f"${i+1}" for i in range(len(ids))]
                    sql = f"DELETE FROM {schema}.{table} WHERE {pk_name} IN ({', '.join(placeholders)})"
                    
                    # Extra WHERE clauses
                    all_where = list(self._where_clauses)
                    if self._returning_query and hasattr(self._returning_query, "where_clauses"):
                        all_where.extend(self._returning_query.where_clauses)
                    
                    if all_where:
                        where_parts = []
                        for clause in all_where:
                            if hasattr(clause, "to_sql_params"):
                                w_sql, w_params = clause.to_sql_params(style="asyncpg")
                                # Shift placeholders
                                offset = len(params)
                                if offset > 0:
                                    w_sql = re.sub(r"\$(\d+)", lambda m: f"${int(m.group(1)) + offset}", w_sql)
                                where_parts.append(w_sql)
                                params.extend(w_params)
                            elif isinstance(clause, str):
                                where_parts.append(clause)
                        if where_parts:
                            sql += " AND (" + ") AND (".join(where_parts) + ")"
                    
                    if self.returning_cols:
                         names = []
                         for c in self.returning_cols:
                            if hasattr(c, "name"): names.append(c.name)
                            else: names.append(str(c))
                         if names:
                             sql += " RETURNING " + ", ".join(names)
                    
                    rows = await tx._execute_sql_in_tx_async(sql, params)
                    
                    # Update identity map on engine/session if possible
                    # (Requires access to session or knowing engine structure)
                    
                    if self.returning_cols and rows:
                         if len(self.returning_cols) == 1:
                            return [r[0] for r in rows]
                         return rows
            else:
                for m in self.models:
                     await tx.delete(m)
            return []
            
        return []
