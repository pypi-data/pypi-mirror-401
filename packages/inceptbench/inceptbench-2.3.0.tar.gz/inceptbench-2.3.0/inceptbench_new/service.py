"""
Main evaluation service for educational content.

This module provides the primary service that orchestrates the complete
evaluation flow: classification → routing → evaluation.
"""

import logging
import time
from typing import Dict, Any, Optional

from .classifier.content_classifier import ContentClassifier
from .decomposer.content_decomposer import ContentDecomposer
from .orchestrator.evaluation_orchestrator import EvaluationOrchestrator
from .models.base import BaseEvaluationResult
from .core.input_models import RequestMetadata
from .config.settings import settings
from .utils.failure_tracker import FailureTracker

logger = logging.getLogger(__name__)


class EvaluationService:
    """
    Main service for evaluating educational content.
    
    Provides comprehensive hierarchical evaluation:
    1. Classifies content type using LLM
    2. Decomposes into nested structure (e.g., quiz → questions, reading passage → quiz → questions)
    3. Evaluates bottom-up with context propagation:
       - Children evaluated first (in parallel where possible)
       - Parents evaluated with knowledge of child results
       - All nodes receive root content as context
    4. Returns nested evaluation results with child evaluations
    
    This is the primary interface that external systems (CLI, API) should use.
    """
    
    def __init__(self):
        """Initialize the evaluation service with classifier, decomposer, and orchestrator."""
        # Validate API keys on initialization
        settings.validate_api_keys()
        
        self.classifier = ContentClassifier()
        self.decomposer = ContentDecomposer()
        self.orchestrator = EvaluationOrchestrator(client=None)  # Orchestrator creates its own evaluators
        logger.info("Evaluation service initialized")
    
    async def evaluate(
        self,
        content: str,
        curriculum: Optional[str] = None,
        generation_prompt: Optional[str] = None,
        request_metadata: Optional[RequestMetadata] = None
    ) -> BaseEvaluationResult:
        """
        Evaluate educational content with hierarchical decomposition.
        
        This method provides comprehensive evaluation of nested content:
        1. Classifies top-level content type
        2. Decomposes into hierarchical structure (e.g., quiz → questions)
        3. Evaluates bottom-up:
           - Children evaluated first (in parallel where possible)
           - Parents evaluated with knowledge of child results
           - All nodes receive root content as context
        4. Returns nested evaluation results
        
        Use this method for all evaluations. The method automatically handles:
        - Simple content (single question, no nesting)
        - Complex content (quizzes with questions, reading passages with quizzes)
        - Context-aware evaluation (questions aware of parent quiz/passage)
        
        Args:
            content: The educational content to evaluate (any string, may contain image URLs)
            curriculum: Curriculum to use (defaults to settings.DEFAULT_CURRICULUM)
            generation_prompt: Optional prompt used to generate the content (deprecated, use request_metadata)
            request_metadata: Optional structured metadata about the content generation request
            
        Returns:
            BaseEvaluationResult (Pydantic model) with evaluation results and nested subcontent_evaluations
            
        Raises:
            ValueError: If content is invalid or curriculum not supported
            RuntimeError: If evaluation fails
        """
        if not content or not content.strip():
            raise ValueError("Content cannot be empty")
        
        # Initialize failure tracker for this request context
        # This ensures each concurrent request has isolated failure logs
        FailureTracker.init_context()
        
        curriculum = curriculum or settings.DEFAULT_CURRICULUM
        
        # Handle metadata/prompt consolidation
        # If request_metadata is provided, it's the source of truth
        # If only generation_prompt is provided, wrap it in metadata for consistency
        if request_metadata is None and generation_prompt:
            request_metadata = RequestMetadata(instructions=generation_prompt)
        
        start_time = time.time()
        logger.info(f"Starting evaluation with curriculum: {curriculum}")
        
        try:
            # Step 1: Classify top-level content
            logger.info("Step 1: Classifying content...")
            content_type = await self.classifier.classify(content, request_metadata)
            logger.info(f"Content classified as: {content_type.value}")
            
            # Step 2: Decompose into hierarchical structure
            logger.info("Step 2: Decomposing content into hierarchical structure...")
            content_tree = await self.decomposer.decompose(content, content_type, request_metadata)
            logger.info(
                f"Decomposition complete. "
                f"Depth: {content_tree.get_depth()}, "
                f"Total nodes: {content_tree.count_nodes()}"
            )
            
            # Step 3: Evaluate hierarchically (bottom-up)
            logger.info("Step 3: Evaluating hierarchically (bottom-up)...")
            result = await self.orchestrator.evaluate_hierarchical(
                content_tree, 
                curriculum, 
                request_metadata=request_metadata
            )
            
            logger.info(
                f"Evaluation complete. "
                f"Type: {content_type.value}, "
                f"Overall: {result.overall.score:.2f}, "
                f"Subcontent evaluations: {len(result.subcontent_evaluations) if result.subcontent_evaluations else 0}"
            )
            logger.info(f"Evaluation completed in {time.time() - start_time:.2f} seconds")
            return result
            
        except Exception as e:
            logger.error(f"Evaluation failed after {time.time() - start_time:.2f} seconds: {e}")
            raise RuntimeError(f"Evaluation failed: {e}")
    
    async def evaluate_json(
        self,
        content: str,
        curriculum: Optional[str] = None,
        generation_prompt: Optional[str] = None,
        request_metadata: Optional[RequestMetadata] = None
    ) -> str:
        """
        Evaluate content and return JSON string.
        
        Convenience method that wraps evaluate() and returns JSON format.
        Useful for REST APIs, CLIs, and other scenarios where JSON serialization is needed.
        
        Args:
            content: The educational content to evaluate
            curriculum: Curriculum to use (defaults to settings.DEFAULT_CURRICULUM)
            generation_prompt: Optional prompt used to generate the content
            request_metadata: Optional structured metadata about the content generation request
            
        Returns:
            JSON string with evaluation results (clean, no internal debug info)
            
        Raises:
            ValueError: If content is invalid or curriculum not supported
            RuntimeError: If evaluation fails
        """
        result = await self.evaluate(content, curriculum, generation_prompt, request_metadata)
        return result.to_json()
    
    def get_failure_summary(self) -> Optional[Dict[str, Any]]:
        """
        Get a summary of soft failures that occurred during the last evaluation.
        
        Soft failures are non-fatal errors like API timeouts after all retries.
        These are useful for debugging but should not be part of the main evaluation output.
        
        Returns:
            None if no failures, otherwise a dict with failure count and details
        """
        return FailureTracker.get_summary()
    
    def has_failures(self) -> bool:
        """Check if any soft failures occurred during the last evaluation."""
        return FailureTracker.has_failures()
