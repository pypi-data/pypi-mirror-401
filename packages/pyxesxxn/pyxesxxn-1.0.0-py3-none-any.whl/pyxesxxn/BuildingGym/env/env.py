# from energyplus.ooep.addons.progress import ProgressProvider
import asyncio
import pandas as pd
# from rl.ppo.network import Agent
import random
import numpy as np
import time
# from energyplus import ooep
import torch.nn.functional as F
import os
from stable_baselines3.common.buffers import ReplayBuffer
import wandb
import tyro
from torch.utils.tensorboard import SummaryWriter
import torch.optim as optim
from gymnasium.spaces import (
    Box,
    Discrete
)
import torch
# from energyplus.ooep import (
#     Simulator,
#     Model,
#     Weather,
#     Report,
# )

import numpy as _numpy_
import gymnasium as _gymnasium_
# from energyplus.ooep.components.variables import WallClock
# from energyplus.ooep.addons.rl import (
#     VariableBox,
#     SimulatorEnv,
# )
# from energyplus.ooep import (
#     Actuator,
#     OutputVariable,
# )
# from energyplus.ooep.addons.rl.gymnasium import ThinEnv
# from energyplus.dataset.basic import dataset as _epds_
import torch.nn as nn
import wandb
from rl.util.replaybuffer import ReplayBuffer
from controllables.energyplus.events import Event

from controllables.core.tools.gymnasium import (
    DictSpace,
    BoxSpace,
    Agent,
)
from controllables.energyplus import (
    System,
    #WeatherModel,
    #Report,
    Actuator,
    OutputVariable,
)
# async def energyplus_running(simulator, idf_file, epw_file):
#     await simulator.awaitable.run(
#         input=Simulator.InputSpecs(
#             model=(
#                 idf_file
#             ),
#             weather=(epw_file),
#         ),
#         output=Simulator.OutputSpecs(
#             #report=('/tmp/ooep-report-9e1287d2-8e75-4cf5-bbc5-f76580b56a69'),
#         ),
#         options=Simulator.RuntimeOptions(
#             #design_day=True,
#         ),
#     ) 
from abc import ABC, abstractmethod
import functools
import inspect, ast

class buildinggym_env():
    def __init__(self, idf_file,
                 epw_file,
                 ep_world,
                 ep_para,
                #  observation_space,
                #  action_space,
                #  observation_dim,
                 action_type,
                 args,
                 inter_obs_var,
                 ext_obs_var = None,
                #  ext_obs_bool = False,
                 agent = None) -> None:
        # global thinenv
        # self.simulator = Simulator().add(
        #     ProgressProvider(),
        #     #LogProvider(),
        # )
        self.buffer = ReplayBuffer(
            info=['obs', 'action', 'logprbs', 'rewards', 'values'],
            args=args
            )
        self.idf_file = idf_file
        self.epw_file = epw_file
        if ext_obs_var is not None:
            self.ext_obs_bool = True
        else:
            self.ext_obs_bool = False
        # self.simulator.add(
        #     thinenv := ThinEnv(
        #         action_space=action_space,    
        #         observation_space=observation_space,
        #     )
        # )
        # To update:
        # self.inter_obs_var = ['t_out', 't_in', 'occ', 'light', 'Equip']
        self.inter_obs_var = inter_obs_var
        # self.ext_obs_var = ['signal']
        self.ext_obs_var = ext_obs_var
        if not self.ext_obs_bool:
            self.observation_dim = len(self.inter_obs_var)
        else:
            self.observation_dim = len(self.inter_obs_var) + len(self.ext_obs_var)
        self.observation_space = Box(np.array([-np.inf] * self.observation_dim), np.array([np.inf] * self.observation_dim))
        
        if isinstance(action_type, Box):
            self.action_space = Box(action_type.low, action_type.high)
        if isinstance(action_type, Discrete):
            self.action_space = Discrete(action_type.n)
        
        # self.action_var = ['Thermostat']
        self.num_envs = 1
        self.agent = agent
        self.ready_to_train = False
        self.args = tyro.cli(args)
        self.p_loss_list = []
        self.v_loss_list = []
        self.success_n = 0
        self.batch_n = 0
        self.step_n = 0
        if args.device == 'cpu':
            self.obs_batch = torch.zeros(args.batch_size, self.observation_dim).to('cpu')
            self.action_batch = torch.zeros(args.batch_size, 1).to('cpu')
            self.return_batch = torch.zeros(args.batch_size, 1).to('cpu')
        else:
            self.obs_batch = torch.zeros(args.batch_size, self.observation_dim).to('cuda')
            self.action_batch = torch.zeros(args.batch_size, 1).to('cuda')
            self.return_batch = torch.zeros(args.batch_size, 1).to('cuda')
        # self.simulator.events.on('end_zone_timestep_after_zone_reporting', self.handler)
        # self.baseline = pd.read_csv('Data\\Day_mean.csv')
        self.com = 25
        self.best_performance = 0

        # self.world = world = System(
        #     building=self.idf_file,
        #     #world='tmp_timestep 10 min.idf',
        #     weather=self.epw_file,
        
        #     report='tmp/ooep-report-9e1287d2-8e75-4cf5-bbc5-f76580b56a69',
        #     repeat=False,
        #     # design_day=False,
        # ).add('logging:progress')

        self.world = ep_world
        self.ep_para = ep_para

        # self.ep_para = Agent(dict(
        #         action_space=DictSpace({
        #             'Thermostat': BoxSpace(
        #                 low=22., high=30.,
        #                 dtype=_numpy_.float32,
        #                 shape=(),
        #             ).bind(world[Actuator.Ref(
        #                 type='Schedule:Compact',
        #                 control_type='Schedule Value',
        #                 key='Always 26',
        #             )])
        #         }),    
        #         observation_space=DictSpace({
        #             't_in': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Zone Mean Air Temperature',
        #                         key='Perimeter_ZN_1 ZN',
        #                     )]),
        #             't_out': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Site Outdoor Air Drybulb Temperature',
        #                         key='Environment',
        #                     )]),
        #             'occ': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Schedule Value',
        #                         key='Small Office Bldg Occ',
        #                     )]),
        #             'light': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Schedule Value',
        #                         key='Office Bldg Light',
        #                     )]),
        #             'Equip': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Schedule Value',
        #                         key='Small Office Bldg Equip',
        #                     )]),
        #             'Energy_1': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Cooling Coil Total Cooling Rate',
        #                         key='CORE_ZN ZN PSZ-AC-1 1SPD DX AC CLG COIL 34KBTU/HR 9.7SEER',
        #                     )]),
        #             'Energy_2': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Cooling Coil Total Cooling Rate',
        #                         key='PERIMETER_ZN_1 ZN PSZ-AC-2 1SPD DX AC CLG COIL 33KBTU/HR 9.7SEER',
        #                     )]),
        #             'Energy_3': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Cooling Coil Total Cooling Rate',
        #                         key='PERIMETER_ZN_2 ZN PSZ-AC-3 1SPD DX AC CLG COIL 23KBTU/HR 9.7SEER',
        #                     )]),
        #             'Energy_4': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Cooling Coil Total Cooling Rate',
        #                         key='PERIMETER_ZN_3 ZN PSZ-AC-4 1SPD DX AC CLG COIL 33KBTU/HR 9.7SEER',
        #                     )]),
        #             'Energy_5': BoxSpace(
        #                         low=-_numpy_.inf, high=+_numpy_.inf,
        #                         dtype=_numpy_.float32,
        #                         shape=(),
        #                     ).bind(world[OutputVariable.Ref(
        #                         type='Cooling Coil Total Cooling Rate',
        #                         key='PERIMETER_ZN_4 ZN PSZ-AC-5 1SPD DX AC CLG COIL 25KBTU/HR 9.7SEER',
        #                     )]),                                                                                                                                                                                                                                                                                                                       
        #         }),
        #     ))                

        @self.world.on(Event.Ref('end_zone_timestep_after_zone_reporting', include_warmup=False))
        def _(_):
            global thinenv

            # from energyplus.ooep import TemporaryUnavailableError
            # try:
            #     print(self.simulator.variables.getdefault(
            #         ooep.WallClock.Ref()
            #     ).value)
            #     a = 1
            # except TemporaryUnavailableError:
            #     pass

            try:
                t = self.world['wallclock:calendar'].value
                obs = self.ep_para.observe()
                # t = self.simulator.variables.getdefault(
                #     ooep.WallClock.Ref()
                # ).value
                warm_up = False
            except:
                warm_up = True

            if not warm_up:
                state = [float(obs[i]) for i in self.inter_obs_var]
                if self.ext_obs_bool:
                    if t.hour == 0 or t.hour>self.t_index:
                        self.ext_obs_var = self.get_ext_var(t)
                        self.t_index = t.hour
                    for _, value in self.ext_obs_var.items():
                        state.append(value)
                # cooling_energy =  obs['Energy_1'].item() + obs['Energy_2'].item() + obs['Energy_3'].item() + obs['Energy_4'].item() + obs['Energy_5'].item()
                state = self.normalize_input_i(state)
                # if self.ext_obs_bool:
                #     signal = state[-1]
                # else:
                #     signal = 0.5
                state = torch.Tensor(state).cuda() if torch.cuda.is_available() and self.args.cuda else torch.Tensor(state).cpu()
                with torch.no_grad():
                    actions, value, logprob = self.agent(state)
                    # actions = torch.argmax(q_values, dim=0).cpu().item()

                # control function
                self.control_fun(actions)               
                # self.com +=  (actions.cpu().item()-1)*0.5
                # self.com = max(min(self.com, 27), 23)

                # self.com = 26
                obs = pd.DataFrame(obs, index = [self.sensor_index])                
                obs.insert(0, 'Time', t)
                obs.insert(0, 'day_of_week', t.weekday())
                obs.insert(1, 'Working time', self.label_working_time_i(t))            
                obs.insert(obs.columns.get_loc("t_in") + 1, 'Thermostat', self.com)

                # cal reward
                # cooling_energy =  obs['Energy_1'].item() + obs['Energy_2'].item() + obs['Energy_3'].item() + obs['Energy_4'].item() + obs['Energy_5'].item()
                # if self.ext_obs_bool:
                #     signal = state[-1]
                # else:
                #     signal = 0.5                
                # reward_i, result_i, baseline_i = self.cal_r_i(cooling_energy, t, signal)

                # reward_i, result_i, baseline_i, signal, cooling_energy = self.cal_r_i(obs, state, t)
                reward_output = self.cal_r_i(obs, state, t)
                reward_i = reward_output['rewards']
                for _, (k, v) in enumerate(reward_output.items()):
                    obs[k] = v
                # obs['cooling_energy'] = cooling_energy
                # obs['results'] = result_i
                # obs['rewards'] = reward_i
                # obs['baseline'] = baseline_i
                # obs['Signal'] = signal
                # obs['Target'] = 20000 * (1-0.3*signal)
                obs.insert(obs.columns.get_loc("rewards") + 1, 'actions', actions.cpu().item())
                obs.insert(obs.columns.get_loc("rewards") + 1, 'logprobs', logprob.cpu().item())
                if value is not None:
                    obs.insert(obs.columns.get_loc("rewards") + 1, 'values', value.cpu().item())


                if self.sensor_index == 0:
                    self.sensor_dic = pd.DataFrame({})
                    self.sensor_dic = obs
                    self.logprobs = [logprob]
                    # self.values = [value]
                    self.actions = [actions]
                    self.states = [state]
                    self.values = [value]
                    self.rewards = [reward_i]
                else:
                    self.sensor_dic = pd.concat([self.sensor_dic, obs])           
                    self.logprobs.append(logprob) 
                    # self.values.append(value) 
                    self.actions.append(actions)
                    self.states.append(state)
                    self.values.append(value)
                    self.rewards.append(reward_i)
                actions = actions.cpu().item()
                # com = 25. + actions * 2
                # act = thinenv.act({'Thermostat': self.com})
                # act = thinenv.act({'Thermostat': 26})
                # self.ep_para.action.value = {
                # 'Thermostat': self.com,
                # }    
                b  = self.args.outlook_steps + 1
                if self.sensor_index > b:
                    i = self.sensor_index-b
                    if i % self.args.step_size == 0:
                        if np.sum(self.sensor_dic['Working time'].iloc[i:(self.sensor_index)]) == b:
                            ob_i = self.states[i]
                            r_i = self.rewards[i+1]
                            logp_i = self.logprobs[i]
                            action_i = self.actions[i]
                            R_i = self.cal_return(self.rewards[i+1:i+b])
                            if self.batch_n< self.args.batch_size*self.args.n_steps:
                                if value is not None:
                                    self.buffer.add([ob_i, action_i, logp_i, r_i, value])   # List['obs', 'action', 'logprb', 'rewards', 'values']
                                else:
                                    self.buffer.add([ob_i, action_i, logp_i, r_i, R_i])   # List['obs', 'action', 'logprb', 'rewards', 'values']
                                # self.obs_batch[self.batch_n, :] = ob_i
                                # self.return_batch[self.batch_n, :] = R_i
                                # self.action_batch[self.batch_n, :] = action_i
                                self.batch_n+=1
                            else:
                                self.batch_n=0
                                if value is not None:
                                    self.buffer.cal_R_adv(self.args.batch_size)
                                p_loss_i, v_loss_i = self.algo.train(self.buffer)
                                self.buffer.reset()  # dxl: can update to be able to store somme history info
                                self.p_loss_list.append(p_loss_i)
                                self.v_loss_list.append(v_loss_i)
                self.sensor_index+=1
    def setup(self, algo):
        self.algo = algo
        self.agent = self.algo.policy
        self.ready_to_train = True
        
    def run(self, agent = None, train = True):
        self.train = train
        self.sensor_index = 0
        # if agent is not None:
        #     self.agent = agent
        # asyncio.run(energyplus_running(self.simulator, self.idf_file, self.epw_file))
        self.world.start().wait()
    
    @abstractmethod
    def control_fun(self, actions):
        pass
        # self.com +=  (actions.cpu().item()-1)*0.5
        # self.com = max(min(self.com, 27), 23)
        # self.ep_para.action.value = {
        # 'Thermostat': self.com,
        # }     
    
    # def normalize_input(self, data=None):
    #     nor_min = np.array([22.8, 22, 0, 0, 0])
    #     nor_mean = np.array([28.7, 26, 0.77, 0.57, 0.9])
    #     # nor_min = np.array([0, 0, 0, 0, 0])
    #     nor_max = np.array([33.3, 27, 1, 1, 1])
    #     std = np.array([2, 0.5, 0.4, 0.26, 0.26])
    #     # nor_max = np.array([1, 1, 1, 1, 1])
    #     if data == None:
    #         data = self.sensor_dic[self.inter_obs_var]
    #     # nor_input = (data - nor_min)/(nor_max - nor_min)
    #     nor_input = (data - nor_mean)/std
    #     # nor_input = (data - np.array([27, 25, 0.5, 0.5, 0.5]))/np.array([3, 1, 0.2, 0.2, 0.2])
    #     j = 0
    #     for i in self.inter_obs_var:
    #         col_i =  i + "_nor"
    #         self.sensor_dic[col_i] = nor_input.iloc[:, j]
    #         j+=1

    # def normalize_input_i(self, state):
    #     # nor_min = np.array([22.8, 22, 0, 0, 0])
    #     if self.ext_obs_bool:
    #         nor_mean = np.array([28.7, 26, 0.78, 0.58, 0.89, 0])
    #         std = np.array([2.17, 0.5, 0.39, 0.26, 0.26, 1])
    #     else:
    #         nor_mean = np.array([28.7, 26, 0.78, 0.58, 0.89])
    #         std = np.array([2.17, 0.5, 0.39, 0.26, 0.26])
            
    #     # nor_mean = np.array([28.7, 26, 0.78, 0.58, 0.89])

    #     # nor_mean = np.array([28.7, 26, 0.78, 0.58, 0.89])
    #     # std = np.array([2.17, 0.5, 0.39, 0.26, 0.26, 1])
    #     # std = np.array([2.17, 0.5, 0.39, 0.26, 0.26])

    #     # std = np.array([2.17, 0.5, 0.39, 0.26, 0.26])


    #     # nor_min = np.array([0, 0, 0, 0, 0])
    #     # nor_max = np.array([33.3, 27, 1, 1, 1])
    #     # nor_max = np.array([1, 1, 1, 1, 1])
    #     return (state- nor_mean)/std

    
    # def label_working_time(self):
    #     start = pd.to_datetime(self.args.work_time_start, format='%H:%M')
    #     end = pd.to_datetime(self.args.work_time_end, format='%H:%M')
    #     # remove data without enough outlook step
    #     dt = int(60/self.args.n_time_step)
    #     dt = pd.to_timedelta(dt, unit='min')
    #     # end -= dt
    #     wt = [] # wt: working time label
    #     terminations = [] # terminations: end of working time
    #     for i in range(int(self.sensor_dic.shape[0])):
    #         h = self.sensor_dic['Time'].iloc[i].hour
    #         m = self.sensor_dic['Time'].iloc[i].minute
    #         t = pd.to_datetime(str(h)+':'+str(m), format='%H:%M')
    #         if t >= start and t < end:
    #             wt.append(True)
    #         else:
    #             wt.append(False)
    #         if t >= end - dt:
    #             terminations.append(True)
    #         else:
    #             terminations.append(False)
    #     self.sensor_dic['Working_time'] = wt
    #     self.sensor_dic['Terminations'] = terminations    

    def label_working_time_i(self, t):
        start = pd.to_datetime(self.args.work_time_start, format='%H:%M')
        end = pd.to_datetime(self.args.work_time_end, format='%H:%M')
        # remove data without enough outlook step
        dt = int(60/self.args.n_time_step)
        dt = pd.to_timedelta(dt, unit='min')
        # end -= dt
        day_of_week = t.weekday()
        h = t.hour
        m = t.minute
        t = pd.to_datetime(str(h)+':'+str(m), format='%H:%M')
        if t >= start and t < end and day_of_week<5:
            wt = True
        else:
            wt = False
        if t >= end - dt:
            terminations = True
        else:
            terminations = False
        return wt
        # self.sensor_dic['Terminations'] = terminations            

    # def cal_r(self):
    #     baseline = pd.read_csv('Data\Day_mean.csv')
    #     reward = []
    #     result = []
    #     # Realtime reward function
    #     for j in range(self.sensor_dic.shape[0]):
    #         energy_i = self.sensor_dic['Chiller Electricity Rate'].iloc[j]
    #         k = j % (24*self.args.n_time_step)
    #         baseline_i = baseline['Day_mean'].iloc[k]
    #         reward_i = max(round(0.3 - abs(energy_i ** 2 - baseline_i ** 2)/baseline_i ** 2,2),-0.4)*5
    #         result_i = round(1 - abs(energy_i - baseline_i)/baseline_i,2)
    #         # reward_i = result_i
    #         # if reward_i<0.8:
    #         #     reward_i = reward_i**2
    #         # else:
    #         #     reward_i+=reward_i*5
    #         reward.append(reward_i)
    #         result.append(result_i)          
        
    #     reward = reward[1:]
    #     result = result[1:]
    #     self.actions = self.actions[0:-1]
    #     self.logprobs = self.logprobs[0:-1]
    #     self.sensor_dic =  self.sensor_dic[0:-1]
    #     self.sensor_dic['rewards'] = reward
    #     self.sensor_dic['results'] = result

    @abstractmethod
    def cal_r_i(self, obs, state, time):
        pass
        # ##____important: must define the rewards as the reward at each time step
        # cooling_energy =  obs['Energy_1'].item() + obs['Energy_2'].item() + obs['Energy_3'].item() + obs['Energy_4'].item() + obs['Energy_5'].item()
        # if self.ext_obs_bool:
        #     signal = state[-1]
        # else:
        #     signal = 0.5               
        # # baseline = pd.read_csv('Data\Day_mean.csv')
        # hour = time.hour
        # min = time.minute
        # idx = int(hour*6+int(min/10))
        # # baseline_i = self.baseline['Day_mean'].iloc[idx]
        # baseline_i = 20000
        # # reward_i = max(round(0.3 - abs(data ** 2 - baseline_i ** 2)/baseline_i ** 2,2),-0.4)*5
        # # result_i = round(1 - abs(data - baseline_i)/baseline_i,2)
        # # return reward_i, result_i, baseline_i
        # # baseline_energy = self.baseline['cooling_energy'].iloc[idx]
        # actual_reduction = (baseline_i - cooling_energy) / baseline_i
        
        # # Target reduction percentage
        # target_reduction = 0.3 * signal
        
        # # if abs(actual_reduction-target_reduction) < 0.05:
        # #     self.reward_i = 5
        # # elif abs(actual_reduction-target_reduction) < 0.15:
        # #     self.reward_i = 2
        # # else:
        # #     self.reward_i = -1
        # self.reward_i = 1.5 - abs(actual_reduction - target_reduction) * 10
        # # if self.reward_i<-5:
        # #     self.reward_i = -5
        # return {'rewards':self.reward_i, 'actual_reduction': actual_reduction, 'baseline_i': baseline_i, 'signal': signal, 'cooling_energy': cooling_energy}
        
    
    def cal_return(self, reward_list):
        R = 0
        for r in reward_list[::-1]:
            R = r + R * self.args.gamma
        return R
    
    # def get_ext_var(self, t=None):
    #     ext_obs_var = {}
    #     if t.hour >=11 and t.hour<=13:
    #         for i in self.ext_obs_var:
    #             ext_obs_var[i] = random.choice([1])
    #     elif t.hour >=14 and t.hour<=16:
    #         for i in self.ext_obs_var:
    #             ext_obs_var[i] = random.choice([0.5])     
    #     elif t.hour >=17 and t.hour<=19:
    #         for i in self.ext_obs_var:
    #             ext_obs_var[i] = random.choice([1])                    
    #     else:
    #         for i in self.ext_obs_var:
    #             ext_obs_var[i] = random.choice([0])                        
    #     return ext_obs_var
    
    # def __init_subclass__(cls, **kwargs):
    #     super().__init_subclass__(**kwargs)

    #     # If subclass provides its own 'control_fun', wrap it to enforce the rule.
    #     if "control_fun" in cls.__dict__:
    #         original = cls.__dict__["control_fun"]

    #         @functools.wraps(original)
    #         def _wrapped(self, *args, **kw):
    #             result = original(self, *args, **kw)
    #             if result is not self.com:  # identity, not just equality
    #                 raise ValueError(
    #                     f"{cls.__name__}.control_fun() must return self.com (same object)."
    #                 )
    #             return result

    #         setattr(cls, "control_fun", _wrapped)

    #     if "cal_r_i" in cls.__dict__:
    #         original = cls.__dict__["cal_r_i"]

    #         @functools.wraps(original)
    #         def _wrapped(self, *args, **kw):
    #             result = original(self, *args, **kw)
    #             if result is not self.reward_i:  # identity, not just equality
    #                 raise ValueError(
    #                     f"{cls.__name__}.cal_r_i() must return self.reward_i (same object)."
    #                 )
    #             return result

    #         setattr(cls, "cal_r_i", _wrapped)      

    # def handler(self, __event):
    #     global thinenv

    #     # from energyplus.ooep import TemporaryUnavailableError
    #     # try:
    #     #     print(self.simulator.variables.getdefault(
    #     #         ooep.WallClock.Ref()
    #     #     ).value)
    #     #     a = 1
    #     # except TemporaryUnavailableError:
    #     #     pass

    #     try:
    #         obs = thinenv.observe()
    #         t = self.simulator.variables.getdefault(
    #             ooep.WallClock.Ref()
    #         ).value
    #         warm_up = False
    #     except:
    #         warm_up = True

    #     if not warm_up:
    #         state = [float(obs[i]) for i in self.inter_obs_var]
    #         if self.ext_obs_bool:
    #             if t.hour == 0 or t.hour>self.t_index:
    #                 self.ext_obs_var = self.get_ext_var(t)
    #                 self.t_index = t.hour
    #             for _, value in self.ext_obs_var.items():
    #                 state.append(value)
    #         cooling_energy =  obs['Energy_1'].item() + obs['Energy_2'].item() + obs['Energy_3'].item() + obs['Energy_4'].item() + obs['Energy_5'].item()
    #         state = self.normalize_input_i(state)
    #         if self.ext_obs_bool:
    #             signal = state[-1]
    #         else:
    #             signal = 0.5
    #         state = torch.Tensor(state).cuda() if torch.cuda.is_available() and self.args.cuda else torch.Tensor(state).cpu()
    #         with torch.no_grad():
    #             actions, value, logprob = self.agent(state)
    #             # actions = torch.argmax(q_values, dim=0).cpu().item()
    #         self.com +=  (actions.cpu().item()-1)*0.5
    #         self.com = max(min(self.com, 27), 23)
    #         # self.com = 26
    #         obs = pd.DataFrame(obs, index = [self.sensor_index])                
    #         obs.insert(0, 'Time', t)
    #         obs.insert(0, 'day_of_week', t.weekday())
    #         obs.insert(1, 'Working time', self.label_working_time_i(t))            
    #         obs.insert(obs.columns.get_loc("t_in") + 1, 'Thermostat', self.com)
    #         reward_i, result_i, baseline_i = self.cal_r_i(cooling_energy, t, signal)
    #         obs['cooling_energy'] = cooling_energy
    #         obs['results'] = result_i
    #         obs['rewards'] = reward_i
    #         obs['baseline'] = baseline_i
    #         obs['Signal'] = signal
    #         obs['Target'] = 20000 * (1-0.3*signal)
    #         obs.insert(obs.columns.get_loc("t_in") + 1, 'actions', actions.cpu().item())
    #         obs.insert(obs.columns.get_loc("t_in") + 1, 'logprobs', logprob.cpu().item())
    #         if value is not None:
    #             obs.insert(obs.columns.get_loc("t_in") + 1, 'values', value.cpu().item())


    #         if self.sensor_index == 0:
    #             self.sensor_dic = pd.DataFrame({})
    #             self.sensor_dic = obs
    #             self.logprobs = [logprob]
    #             # self.values = [value]
    #             self.actions = [actions]
    #             self.states = [state]
    #             self.values = [value]
    #             self.rewards = [reward_i]
    #         else:
    #             self.sensor_dic = pd.concat([self.sensor_dic, obs])           
    #             self.logprobs.append(logprob) 
    #             # self.values.append(value) 
    #             self.actions.append(actions)
    #             self.states.append(state)
    #             self.values.append(value)
    #             self.rewards.append(reward_i)
    #         actions = actions.cpu().item()
    #         # com = 25. + actions * 2
    #         act = thinenv.act({'Thermostat': self.com})
    #         # act = thinenv.act({'Thermostat': 26})

    #         b  = self.args.outlook_steps + 1
    #         if self.sensor_index > b:
    #             i = self.sensor_index-b
    #             if i % self.args.step_size == 0:
    #                 if np.sum(self.sensor_dic['Working time'].iloc[i:(self.sensor_index)]) == b:
    #                     ob_i = self.states[i]
    #                     r_i = self.rewards[i+1]
    #                     logp_i = self.logprobs[i]
    #                     action_i = self.actions[i]
    #                     R_i = self.cal_return(self.rewards[i+1:i+b])
    #                     if self.batch_n< self.args.batch_size*self.args.n_steps:
    #                         if value is not None:
    #                             self.buffer.add([ob_i, action_i, logp_i, r_i, value])   # List['obs', 'action', 'logprb', 'rewards', 'values']
    #                         else:
    #                             self.buffer.add([ob_i, action_i, logp_i, r_i, R_i])   # List['obs', 'action', 'logprb', 'rewards', 'values']
    #                         # self.obs_batch[self.batch_n, :] = ob_i
    #                         # self.return_batch[self.batch_n, :] = R_i
    #                         # self.action_batch[self.batch_n, :] = action_i
    #                         self.batch_n+=1
    #                     else:
    #                         self.batch_n=0
    #                         if value is not None:
    #                             self.buffer.cal_R_adv(self.args.batch_size)
    #                         p_loss_i, v_loss_i = self.algo.train(self.buffer)
    #                         self.buffer.reset()  # dxl: can update to be able to store somme history info
    #                         self.p_loss_list.append(p_loss_i)
    #                         self.v_loss_list.append(v_loss_i)
                            
    #         self.sensor_index+=1