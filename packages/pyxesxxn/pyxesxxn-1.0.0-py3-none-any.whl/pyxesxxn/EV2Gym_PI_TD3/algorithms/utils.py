import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

def td_lambda_forward_view(
    rewards, dones, states, actions, critic, gamma=0.99, lambda_=0.95, horizon = -1, avg=True,
):
    """
    Implements TD(lambda) as in the formula:
        G_t^λ = (1-λ) * sum_{n=1}^{T-t-1} λ^{n-1} G_t^{(n)} + λ^{T-t-1} G_t^{(T-t)}
    for all t in [0, horizon-1], batched.
    - rewards: [B, H]
    - dones:   [B, H] (1 if done, 0 otherwise)
    - states:  [B, H+1, state_dim]
    - actions: [B, H+1, action_dim]
    - critic: function(states, actions) -> Q-values [B, H+1]
    Returns:
    - td_lambda: [B, H]
    """
    B, H = rewards.shape
    device = rewards.device
    

    assert horizon < H and horizon > 1, "Horizon must be less than or equal to H and greater than 0."
    rewards = rewards[:, :horizon]
    dones = dones[:, :horizon]
    states = states[:, :horizon + 1, :]
    actions = actions[:, :horizon + 1, :]
    H = horizon
    
    # print(f"\n\nTD(lambda) forward view: B={B}, H={H}, device={device}")
    # print(f"Rewards shape: {rewards.shape}\nDones shape: {dones.shape}"
    #       f"\nStates shape: {states.shape}\nActions shape: {actions.shape}")

    # Compute all Q(s_{t+n}, a_{t+n}) up to H (for bootstrapping)
    with torch.no_grad():                
        
        if avg:
            q1, q2 = critic(states.reshape(-1, states.shape[-1]),
                                actions.reshape(-1, actions.shape[-1])
                                )
            q_bootstrap = (q1 + q2) / 2.0  # Average Q-values from both critics
        else:
            q_bootstrap = critic.Q1(states.reshape(-1, states.shape[-1]),
                             actions.reshape(-1, actions.shape[-1])
                             )
        
        q_bootstrap = q_bootstrap.view(B, H+1)  # Reshape to [B, H+1]    
        
        # print(f"Q-values computed: {q_bootstrap.shape}, first 5 values: {q_bootstrap[:, :5]}")
        # print(f"Q-values shape: {q_bootstrap.shape}")

    td_lambda = torch.zeros(B, H, device=device)

    # For each t, compute all G_t^{(n)}
    for t in range(H):
        # Maximum possible n: until end or done
        G_lambda = torch.zeros(B, device=device)
        # print(f"Processing t={t}: G_lambda shape: {G_lambda.shape}")
        # For each possible n-step return
        for n in range(1, H-t+1):
            # Rewards from t to t+n-1
            idxs = torch.arange(t, t+n, device=device)
            gammas = gamma ** torch.arange(n, device=device)
            reward_slice = rewards[:, idxs]    # [B, n]
            done_slice = dones[:, idxs]        # [B, n]
            # print(f"Processing n={n}: reward_slice shape: {reward_slice.shape}, done_slice shape: {done_slice.shape}")
            # Cumulative done mask: 1 until first done (0 after)
            mask = torch.cumprod(1 - done_slice, dim=1)
            # For correct bootstrapping, only up to first done
            mask = torch.cat([torch.ones(B,1,device=device), mask[:,:-1]], dim=1)
            reward_sum = torch.sum(reward_slice * gammas * mask, dim=1)   # [B]

            # Bootstrap Q for G_t^{(n)}
            q_val = torch.zeros(B, device=device)
            # If not done before t+n, add Q
            not_done_n = torch.prod(1 - done_slice, dim=1)
            if t + n < H + 1:            
                q_val = not_done_n * (gamma ** n) * q_bootstrap[:, t+n]
            G_n = reward_sum + q_val

            # λ weights
            if n < H-t:
                weight = (1 - lambda_) * (lambda_ ** (n-1))
            else:
                weight = lambda_ ** (n-1)
            G_lambda = G_lambda + weight * G_n

        td_lambda[:, t] = G_lambda

    # print(f"Final TD(lambda) shape: {td_lambda.shape}, first 5 values: {td_lambda[:, :5]}")
    return td_lambda[:,0]

@torch.no_grad()
def compute_target_values(rewards, next_values, dones, gamma=0.99, lam=0.95, device='cpu'):
    batch_size, horizon = rewards.shape
    target_values = torch.zeros_like(rewards).to(device)

    Ai = torch.zeros(batch_size).to(device)
    Bi = torch.zeros(batch_size).to(device)
    lam_tensor = torch.ones(batch_size).to(device)

    for t in reversed(range(horizon)):
        done_mask = 1. - dones[:, t]
        lam_tensor = lam_tensor * lam * done_mask + (1. - done_mask)

        Ai = done_mask * (lam * gamma * Ai + gamma *
                            next_values[:, t] + (1. - lam_tensor) / (1. - lam) * rewards[:, t])
        Bi = gamma * \
            (next_values[:, t] * (1. - done_mask) +
                Bi * done_mask) + rewards[:, t]

        target_values[:, t] = (1. - lam) * Ai + lam_tensor * Bi

    return target_values


class ActorWithEntropy(nn.Module):
    def __init__(self, state_dim, action_dim, max_action, mlp_hidden_dim):
        super(ActorWithEntropy, self).__init__()
        self.l1 = nn.Linear(state_dim, mlp_hidden_dim)
        self.ln1 = nn.LayerNorm(mlp_hidden_dim)
        self.l2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim)
        self.ln2 = nn.LayerNorm(mlp_hidden_dim)
        self.l3_mu = nn.Linear(mlp_hidden_dim, action_dim)
        self.l3_log_std = nn.Linear(mlp_hidden_dim, action_dim)
        self.max_action = max_action

    def forward(self, state):
        x = F.silu(self.ln1(self.l1(state)))
        x = F.silu(self.ln2(self.l2(x)))
        mu = self.l3_mu(x)
        log_std = self.l3_log_std(x).clamp(-5, 2)
        std = log_std.exp()
        
        # create Gaussian
        normal = Normal(mu, std)
        # sample using reparameterization
        y = normal.rsample()
        # action via tanh
        action = torch.tanh(y) * self.max_action
        
        # log probability with tanh correction
        log_prob = normal.log_prob(y).sum(dim=-1, keepdim=True)
        # correction: derivative of tanh(y)*max_action is max_action*(1-tanh(y)^2)
        log_prob -= torch.log(self.max_action * (1 - torch.tanh(y)**2) + 1e-6).sum(dim=-1, keepdim=True)
        
        # entropy of base gaussian
        entropy = normal.entropy().sum(dim=-1, keepdim=True)
        
        return action, log_prob, entropy
