"""Verify WelQrate dataset outputs generated by welqrate_preparation.py.

This script validates:
- All required Parquet files exist for all targets
- Parquet files have correct structure (SMILES + target columns)
- SMILES are canonical
- Target values are binary (0/1)
- All split files exist for all targets and seeds
- Split indices are valid (within data range, no overlaps, etc.)
- Data matches original WelQrate dataset (if --original-dir provided)

Usage example:
python -m lbvslitmus.scripts.data_preperation.verify_welqrate --output-dir ./data --splits-dir ./data/splits --original-dir F:\\WelQrate2\\WelQrate
"""

from __future__ import annotations

import logging
import sys
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import torch
from rdkit import Chem
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# Import constants from welqrate_preparation
AVAILABLE_TARGETS = [
    "AID1798",
    "AID1843",
    "AID2258",
    "AID2689",
    "AID435008",
    "AID435034",
    "AID463087",
    "AID485290",
    "AID488997",
]

DATASET_PREFIX = "WELQRATE"
SEEDS = tuple(range(1, 6))


def canonicalize_smiles(smiles: str) -> str | None:
    """Return canonical SMILES or None if the input is invalid."""
    if pd.isna(smiles) or not isinstance(smiles, str) or not smiles.strip():
        return None
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        return Chem.MolToSmiles(mol, canonical=True)
    except Exception:
        return None


def verify_smiles_canonical(smiles: str) -> bool:
    """Check if SMILES string is canonical by re-canonicalizing and comparing."""
    canonical = canonicalize_smiles(smiles)
    return canonical is not None and canonical == smiles


def verify_parquet_file(parquet_path: Path, target: str) -> tuple[bool, list[str]]:
    """Verify a single Parquet file for a target.

    Returns
    -------
        (is_valid, list_of_errors)
    """
    errors = []

    # Check file exists
    if not parquet_path.exists():
        errors.append(f"File does not exist: {parquet_path}")
        return False, errors

    # Try to read
    try:
        df = pd.read_parquet(parquet_path)
    except Exception as e:
        errors.append(f"Failed to read Parquet file: {e}")
        return False, errors

    # Check required columns
    required_cols = {"SMILES", target}
    missing_cols = required_cols - set(df.columns)
    if missing_cols:
        errors.append(f"Missing required columns: {missing_cols}")

    # Check SMILES column
    if "SMILES" in df.columns:
        # Check for nulls
        null_count = df["SMILES"].isna().sum()
        if null_count > 0:
            errors.append(f"Found {null_count} null SMILES values")

        # Check if SMILES are canonical (sample check)
        sample_size = min(100, len(df))
        sample_df = (
            df.sample(n=sample_size, random_state=42) if len(df) > sample_size else df
        )
        non_canonical = []
        for idx, smiles in enumerate(sample_df["SMILES"]):
            if not verify_smiles_canonical(smiles):
                non_canonical.append(idx)
                if len(non_canonical) >= 10:  # Report first 10
                    break

        if non_canonical:
            errors.append(
                f"Found {len(non_canonical)} non-canonical SMILES in sample of {sample_size} "
                f"(first indices: {non_canonical[:5]})"
            )

    # Check target column
    if target in df.columns:
        # Check for nulls
        null_count = df[target].isna().sum()
        if null_count > 0:
            errors.append(f"Found {null_count} null values in target column")

        # Check binary values
        unique_values = set(df[target].unique())
        expected_values = {0, 1}
        if not unique_values.issubset(expected_values):
            errors.append(
                f"Target column contains non-binary values: {unique_values - expected_values}"
            )

        # Check class balance (informational)
        class_counts = df[target].value_counts()
        if len(class_counts) < 2:
            errors.append(f"Target has only one class: {class_counts.to_dict()}")

    # Check for empty dataframe
    if len(df) == 0:
        errors.append("DataFrame is empty")

    is_valid = len(errors) == 0
    return is_valid, errors


def verify_split_file(
    split_path: Path, data_size: int, split_name: str
) -> tuple[bool, list[str]]:
    """Verify a single split file.

    Args:
    ----
        split_path: Path to .npy file
        data_size: Total number of rows in the dataset
        split_name: Name of the split (train/test) for error messages

    Returns:
    -------
        (is_valid, list_of_errors)
    """
    errors = []

    # Check file exists
    if not split_path.exists():
        errors.append(f"Split file does not exist: {split_path}")
        return False, errors

    # Try to load
    try:
        indices = np.load(split_path)
    except Exception as e:
        errors.append(f"Failed to load split file: {e}")
        return False, errors

    # Check it's a numpy array
    if not isinstance(indices, np.ndarray):
        errors.append(f"Split file does not contain a numpy array, got {type(indices)}")
        return False, errors

    # Check dtype
    if not np.issubdtype(indices.dtype, np.integer):
        errors.append(f"Split indices should be integers, got {indices.dtype}")

    # Convert to list for easier checking
    indices_list = indices.tolist()

    # Check for empty split
    if len(indices_list) == 0:
        errors.append(f"{split_name} split is empty")

    # Check indices are within valid range
    if len(indices_list) > 0:
        min_idx = min(indices_list)
        max_idx = max(indices_list)
        if min_idx < 0:
            errors.append(f"{split_name} contains negative indices (min: {min_idx})")
        if max_idx >= data_size:
            errors.append(
                f"{split_name} contains indices out of range: max={max_idx}, data_size={data_size}"
            )

        # Check for duplicates
        if len(indices_list) != len(set(indices_list)):
            duplicates = len(indices_list) - len(set(indices_list))
            errors.append(f"{split_name} contains {duplicates} duplicate indices")

    is_valid = len(errors) == 0
    return is_valid, errors


def verify_target_splits(
    splits_dir: Path, target: str, data_size: int, scaffold: Optional[str] = None
) -> tuple[bool, list[str]]:
    """Verify all split files for a single target.

    Returns
    -------
        (is_valid, list_of_errors)
    """
    errors = []

    for seed in SEEDS:
        train_path = splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.train_idx.npy"
        test_path = splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.test_idx.npy"

        # Verify individual split files
        train_valid, train_errors = verify_split_file(
            train_path, data_size, f"train (seed {seed})"
        )
        test_valid, test_errors = verify_split_file(
            test_path, data_size, f"test (seed {seed})"
        )

        if not train_valid:
            errors.extend([f"Seed {seed}: {e}" for e in train_errors])
        if not test_valid:
            errors.extend([f"Seed {seed}: {e}" for e in test_errors])

        # If both files exist and are valid, check for overlaps
        if train_valid and test_valid:
            try:
                train_idx = set(np.load(train_path).tolist())
                test_idx = set(np.load(test_path).tolist())

                # Check for overlap
                overlap = train_idx & test_idx
                if overlap:
                    errors.append(
                        f"Seed {seed}: train and test splits overlap on {len(overlap)} indices "
                        f"(sample: {sorted(list(overlap))[:5]})"
                    )

                # Check coverage (train + test should be <= data_size, but may be less if some were removed)
                total_covered = len(train_idx | test_idx)
                if total_covered > data_size:
                    errors.append(
                        f"Seed {seed}: train+test cover {total_covered} indices, but data has only {data_size} rows"
                    )

                # Warn if coverage is very low (might indicate mapping issues)
                coverage_pct = 100.0 * total_covered / data_size if data_size > 0 else 0
                if coverage_pct < 50:
                    errors.append(
                        f"Seed {seed}: train+test only cover {coverage_pct:.1f}% of data "
                        f"({total_covered}/{data_size}) - this might indicate mapping issues"
                    )

            except Exception as e:
                errors.append(f"Seed {seed}: Failed to check split overlap: {e}")

    is_valid = len(errors) == 0
    return is_valid, errors


def locate_welqrate_dirs(root_dir: Path) -> tuple[Path, Path]:
    """Locate standard_formats and split/scaffold directories under root.

    Handles archives that add an extra top-level folder (e.g., WelQrate/...).
    """
    # 1) Direct paths
    std_direct = root_dir / "standard_formats"
    sca_direct = root_dir / "split" / "scaffold"
    if std_direct.is_dir() and sca_direct.is_dir():
        return std_direct, sca_direct

    # 2) Nested path: look for a single subdir that contains both
    for sub in root_dir.iterdir():
        if sub.is_dir():
            std = sub / "standard_formats"
            sca = sub / "split" / "scaffold"
            if std.is_dir() and sca.is_dir():
                return std, sca

    raise FileNotFoundError(
        "Could not locate required directories under original root. "
        f"Looked for 'standard_formats' and 'split/scaffold' under: {root_dir}"
    )


def read_original_target_data(standard_dir: Path, target: str) -> pd.DataFrame:
    """Read original actives and inactives CSVs for a target."""
    actives_path = standard_dir / f"{target}_actives.csv"
    inactives_path = standard_dir / f"{target}_inactives.csv"

    if not actives_path.exists() or not inactives_path.exists():
        raise FileNotFoundError(
            f"Missing CSVs for {target}: {actives_path} or {inactives_path}"
        )

    df_a = pd.read_csv(actives_path)
    df_i = pd.read_csv(inactives_path)

    # Normalize column names (strip whitespace)
    df_a.columns = [c.strip() for c in df_a.columns]
    df_i.columns = [c.strip() for c in df_i.columns]

    # Add target label
    df_a[target] = 1
    df_i[target] = 0

    df = pd.concat([df_a, df_i], ignore_index=True, sort=False)

    if "SMILES" not in df.columns:
        raise ValueError(f"CSV files for {target} must include a 'SMILES' column")

    return df


def load_original_scaffold_split(
    scaffold_dir: Path, target: str, scaffold: str, seed: int
) -> dict[str, list[int]]:
    """Load original scaffold split for a given target, scaffold type (2d/3d) and seed (1..5)."""
    if torch is None:
        raise RuntimeError(
            "torch is required to load WelQrate .pt split files. Please install torch."
        )

    fname = f"{target}_{scaffold}_scaffold_seed{seed}.pt"
    pt_path = scaffold_dir / fname
    if not pt_path.exists():
        raise FileNotFoundError(f"Original split file not found: {pt_path}")

    obj = torch.load(pt_path, map_location="cpu")

    # Normalize split object
    if not isinstance(obj, dict):
        raise TypeError(
            "Expected split file to load as a dict with train/valid/test keys"
        )

    def _as_list(value) -> list[int]:
        if hasattr(value, "tolist"):
            value = value.tolist()
        return list(value)

    normalized: dict[str, list[int]] = {}
    for key, alias in (
        ("train", "train"),
        ("valid", "valid"),
        ("validation", "valid"),
        ("val", "valid"),
        ("test", "test"),
    ):
        if key in obj:
            normalized[alias] = _as_list(obj[key])

    if "train" not in normalized or "test" not in normalized:
        raise ValueError("Split file must contain 'train' and 'test' entries")

    return normalized


def verify_against_original(
    processed_df: pd.DataFrame,
    original_df: pd.DataFrame,
    target: str,
    original_dir: Path,
    scaffold: Optional[str],
) -> tuple[bool, list[str]]:
    """Verify that processed data matches original data.

    Checks:
    - All original SMILES can be canonicalized to match processed SMILES
    - Class counts match (after removing invalid SMILES)
    - No data loss beyond invalid SMILES removal
    """
    errors = []

    logger.info("  Comparing with original WelQrate data...")

    # 1. Check that all processed SMILES come from original
    processed_smiles_set = set(processed_df["SMILES"].tolist())
    original_smiles_list = original_df["SMILES"].tolist()

    # Canonicalize all original SMILES
    original_canonical_map = {}
    invalid_count = 0
    for idx, orig_smiles in enumerate(original_smiles_list):
        canonical = canonicalize_smiles(orig_smiles)
        if canonical is not None:
            original_canonical_map[idx] = canonical
        else:
            invalid_count += 1

    original_canonical_set = set(original_canonical_map.values())

    # Check if processed SMILES are subset of canonicalized original
    extra_smiles = processed_smiles_set - original_canonical_set
    if extra_smiles:
        errors.append(
            f"Found {len(extra_smiles)} SMILES in processed data that don't match any canonicalized original SMILES"
        )
        logger.debug(f"Sample extra SMILES: {list(extra_smiles)[:5]}")

    # 2. Check class distribution
    original_actives = len(original_df[original_df[target] == 1])
    original_inactives = len(original_df[original_df[target] == 0])

    # Count how many actives/inactives were removed due to invalid SMILES
    removed_actives = 0
    removed_inactives = 0
    for orig_idx in range(len(original_df)):
        row = original_df.iloc[orig_idx]
        if canonicalize_smiles(row["SMILES"]) is None:
            if row[target] == 1:
                removed_actives += 1
            else:
                removed_inactives += 1

    expected_actives = original_actives - removed_actives
    expected_inactives = original_inactives - removed_inactives

    processed_actives = len(processed_df[processed_df[target] == 1])
    processed_inactives = len(processed_df[processed_df[target] == 0])

    if processed_actives != expected_actives:
        errors.append(
            f"Active count mismatch: processed={processed_actives}, "
            f"expected={expected_actives} (original={original_actives}, removed={removed_actives})"
        )

    if processed_inactives != expected_inactives:
        errors.append(
            f"Inactive count mismatch: processed={processed_inactives}, "
            f"expected={expected_inactives} (original={original_inactives}, removed={removed_inactives})"
        )

    # 3. Check total count
    expected_total = expected_actives + expected_inactives
    processed_total = len(processed_df)
    if processed_total != expected_total:
        errors.append(
            f"Total count mismatch: processed={processed_total}, expected={expected_total}"
        )

    # Log statistics
    logger.info(
        f"    Original: {original_actives} actives, {original_inactives} inactives"
    )
    logger.info(
        f"    Invalid SMILES removed: {removed_actives} actives, {removed_inactives} inactives"
    )
    logger.info(
        f"    Processed: {processed_actives} actives, {processed_inactives} inactives"
    )

    is_valid = len(errors) == 0
    return is_valid, errors


def verify_splits_against_original(
    processed_df: pd.DataFrame,
    splits_dir: Path,
    original_dir: Path,
    target: str,
    scaffold: str,
) -> tuple[bool, list[str]]:
    """Verify that processed splits correctly map to original splits.

    This function recreates the same mapping logic as welqrate_preparation.py:
    1. Loads original data
    2. Adds _original_index column (0, 1, 2, ...)
    3. Canonicalizes SMILES
    4. Drops invalid SMILES
    5. Maps original indices to processed indices using _original_index
    """
    errors = []

    if scaffold is None:
        return True, []  # Skip if scaffold not specified

    logger.info("  Verifying splits against original WelQrate splits...")

    try:
        standard_dir, scaffold_dir = locate_welqrate_dirs(original_dir)
    except FileNotFoundError as e:
        errors.append(f"Could not locate original WelQrate directories: {e}")
        return False, errors

    # Recreate the same processing as welqrate_preparation.py
    # This ensures we use the exact same mapping logic
    original_df = read_original_target_data(standard_dir, target)

    # Add _original_index column (same as in welqrate_preparation.py)
    original_df = original_df.copy()
    original_df["_original_index"] = range(len(original_df))

    # Canonicalize SMILES (same as in welqrate_preparation.py)
    canonical_smiles = []
    for smiles in original_df["SMILES"]:
        canonical_smiles.append(canonicalize_smiles(smiles))

    original_df["SMILES"] = canonical_smiles

    # Drop invalid SMILES (same as in welqrate_preparation.py)
    processed_recreated = original_df.dropna(subset=["SMILES"]).reset_index(drop=True)

    # Create mapping: original_index -> processed_index
    # This is the same logic as map_ids_to_processed_indices in welqrate_preparation.py
    original_to_processed = dict(
        zip(processed_recreated["_original_index"], processed_recreated.index)
    )

    # Verify that recreated processed data matches actual processed data
    if len(processed_recreated) != len(processed_df):
        errors.append(
            f"Recreated processed data has {len(processed_recreated)} rows, "
            f"but actual processed data has {len(processed_df)} rows"
        )
        return False, errors  # Can't continue if sizes don't match

    # Check SMILES match (they should be identical sets)
    recreated_smiles = set(processed_recreated["SMILES"].tolist())
    actual_smiles = set(processed_df["SMILES"].tolist())
    if recreated_smiles != actual_smiles:
        diff = recreated_smiles - actual_smiles
        if diff:
            errors.append(
                f"Recreated SMILES don't match actual processed SMILES: "
                f"{len(diff)} SMILES differ (sample: {list(diff)[:5]})"
            )
            return False, errors  # Can't continue if SMILES don't match

    # Verify that the order matches between recreated and actual processed data
    # If order matches, we can use direct index mapping (same as welqrate_preparation.py)
    order_matches = True
    for i in range(min(len(processed_recreated), len(processed_df))):
        if processed_recreated.iloc[i]["SMILES"] != processed_df.iloc[i]["SMILES"]:
            order_matches = False
            break

    if order_matches:
        # Order matches - use direct index mapping (same as welqrate_preparation.py)
        original_to_processed = dict(
            zip(processed_recreated["_original_index"], processed_recreated.index)
        )
        logger.info("    Order of processed data matches - using direct index mapping")
    else:
        # Order doesn't match - use SMILES-based mapping
        logger.warning(
            "    Order of processed data doesn't match - using SMILES-based mapping"
        )
        smiles_to_processed_idx = {}
        for idx, smiles in enumerate(processed_df["SMILES"]):
            if smiles not in smiles_to_processed_idx:
                smiles_to_processed_idx[smiles] = idx

        original_to_processed_via_smiles = {}
        for orig_idx in processed_recreated["_original_index"]:
            orig_row = processed_recreated[
                processed_recreated["_original_index"] == orig_idx
            ].iloc[0]
            smiles = orig_row["SMILES"]
            if smiles in smiles_to_processed_idx:
                original_to_processed_via_smiles[orig_idx] = smiles_to_processed_idx[
                    smiles
                ]

        original_to_processed = original_to_processed_via_smiles

    for seed in SEEDS:
        try:
            # Load original split
            original_split = load_original_scaffold_split(
                scaffold_dir, target, scaffold, seed
            )
            original_train = set(original_split.get("train", []))
            if "valid" in original_split:
                original_train |= set(original_split["valid"])
            original_test = set(original_split.get("test", []))

            # Load processed split
            train_path = (
                splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.train_idx.npy"
            )
            test_path = (
                splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.test_idx.npy"
            )

            if not train_path.exists() or not test_path.exists():
                errors.append(f"Seed {seed}: Processed split files not found")
                continue

            processed_train = set(np.load(train_path).tolist())
            processed_test = set(np.load(test_path).tolist())

            # Map original indices to processed indices
            mapped_train = set()
            mapped_test = set()
            unmapped_train = []
            unmapped_test = []

            for orig_idx in original_train:
                if orig_idx in original_to_processed:
                    mapped_train.add(original_to_processed[orig_idx])
                else:
                    unmapped_train.append(orig_idx)

            for orig_idx in original_test:
                if orig_idx in original_to_processed:
                    mapped_test.add(original_to_processed[orig_idx])
                else:
                    unmapped_test.append(orig_idx)

            # Check if mapped splits match processed splits
            # Note: welqrate_preparation.py sorts the indices, so we compare sets
            train_diff = (mapped_train - processed_train) | (
                processed_train - mapped_train
            )
            test_diff = (mapped_test - processed_test) | (processed_test - mapped_test)

            if train_diff:
                # Debug info
                logger.info(
                    f"    Seed {seed}: Train mismatch details - "
                    f"mapped_train size: {len(mapped_train)}, processed_train size: {len(processed_train)}, "
                    f"intersection: {len(mapped_train & processed_train)}, "
                    f"only_in_mapped: {len(mapped_train - processed_train)}, "
                    f"only_in_processed: {len(processed_train - mapped_train)}"
                )
                # Check if train and test are swapped
                if len(mapped_train & processed_test) > len(
                    mapped_train & processed_train
                ):
                    errors.append(
                        f"Seed {seed}: Train and test splits appear to be SWAPPED! "
                        f"Train indices match test set better than train set."
                    )
                else:
                    errors.append(
                        f"Seed {seed}: Train split mismatch - {len(train_diff)} indices differ "
                        f"(sample: {sorted(list(train_diff))[:5]})"
                    )

            if test_diff:
                # Debug info
                logger.info(
                    f"    Seed {seed}: Test mismatch details - "
                    f"mapped_test size: {len(mapped_test)}, processed_test size: {len(processed_test)}, "
                    f"intersection: {len(mapped_test & processed_test)}, "
                    f"only_in_mapped: {len(mapped_test - processed_test)}, "
                    f"only_in_processed: {len(processed_test - mapped_test)}"
                )
                errors.append(
                    f"Seed {seed}: Test split mismatch - {len(test_diff)} indices differ "
                    f"(sample: {sorted(list(test_diff))[:5]})"
                )

            # Warn about unmapped indices (these are likely invalid SMILES that were removed)
            if unmapped_train or unmapped_test:
                logger.info(
                    f"    Seed {seed}: {len(unmapped_train)} train and {len(unmapped_test)} test "
                    f"original indices were removed (invalid SMILES)"
                )

        except Exception as e:
            errors.append(f"Seed {seed}: Failed to verify split: {e}")

    is_valid = len(errors) == 0
    return is_valid, errors


def verify_welqrate_outputs(
    output_dir: Path,
    splits_dir: Optional[Path] = None,
    scaffold: Optional[str] = None,
    original_dir: Optional[Path] = None,
) -> bool:
    """Main verification function.

    Returns
    -------
        True if all checks pass, False otherwise
    """
    output_dir = Path(output_dir)
    if splits_dir is None:
        splits_dir = output_dir / "splits"
    else:
        splits_dir = Path(splits_dir)

    logger.info("=" * 60)
    logger.info("WelQrate Output Verification")
    logger.info("=" * 60)
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"Splits directory: {splits_dir}")
    if original_dir:
        logger.info(f"Original WelQrate directory: {original_dir}")
    logger.info("")

    all_valid = True
    summary = {
        "targets_checked": 0,
        "targets_valid": 0,
        "parquet_errors": 0,
        "split_errors": 0,
        "original_comparison_errors": 0,
    }

    # Verify each target
    for target in tqdm(AVAILABLE_TARGETS, desc="Verifying targets"):
        logger.info(f"\n{'=' * 60}")
        logger.info(f"Verifying target: {target}")
        logger.info(f"{'=' * 60}")

        summary["targets_checked"] += 1
        target_valid = True

        # 1. Verify Parquet file
        parquet_path = output_dir / f"{DATASET_PREFIX}_{target}.parquet"
        logger.info(f"Checking Parquet file: {parquet_path.name}")

        data_size = 0
        parquet_valid, parquet_errors = verify_parquet_file(parquet_path, target)
        if not parquet_valid:
            target_valid = False
            summary["parquet_errors"] += 1
            logger.error("  ✗ Parquet file validation failed:")
            for error in parquet_errors:
                logger.error(f"    - {error}")
        else:
            logger.info("  ✓ Parquet file is valid")
            # Get data size for split verification
            try:
                df = pd.read_parquet(parquet_path)
                data_size = len(df)
                logger.info(f"  Data size: {data_size} rows")
                logger.info(
                    f"  Class distribution: {df[target].value_counts().to_dict()}"
                )
            except Exception as e:
                logger.error(f"  Failed to read Parquet for size check: {e}")
                data_size = 0

        # 2. Verify split files
        logger.info(f"Checking split files in: {splits_dir}")
        splits_valid, splits_errors = verify_target_splits(
            splits_dir, target, data_size, scaffold
        )
        if not splits_valid:
            target_valid = False
            summary["split_errors"] += 1
            logger.error("  ✗ Split files validation failed:")
            for error in splits_errors:
                logger.error(f"    - {error}")
        else:
            logger.info("  ✓ All split files are valid")
            # Show split statistics
            for seed in SEEDS:
                train_path = (
                    splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.train_idx.npy"
                )
                test_path = (
                    splits_dir / f"seed{seed}_{DATASET_PREFIX}_{target}.test_idx.npy"
                )
                if train_path.exists() and test_path.exists():
                    train_size = len(np.load(train_path))
                    test_size = len(np.load(test_path))
                    logger.info(
                        f"    Seed {seed}: train={train_size}, test={test_size}"
                    )

        # 3. Verify against original data (if provided)
        if original_dir and parquet_valid:
            original_dir_path = Path(original_dir)
            try:
                standard_dir, _ = locate_welqrate_dirs(original_dir_path)
                original_df = read_original_target_data(standard_dir, target)
                processed_df = pd.read_parquet(parquet_path)

                original_valid, original_errors = verify_against_original(
                    processed_df, original_df, target, original_dir_path, scaffold
                )
                if not original_valid:
                    target_valid = False
                    summary["original_comparison_errors"] += 1
                    logger.error("  ✗ Original data comparison failed:")
                    for error in original_errors:
                        logger.error(f"    - {error}")
                else:
                    logger.info("  ✓ Data matches original WelQrate")

                # Verify splits against original
                if scaffold and splits_valid:
                    (
                        splits_original_valid,
                        splits_original_errors,
                    ) = verify_splits_against_original(
                        processed_df, splits_dir, original_dir_path, target, scaffold
                    )
                    if not splits_original_valid:
                        target_valid = False
                        summary["split_errors"] += 1
                        logger.error("  ✗ Split comparison with original failed:")
                        for error in splits_original_errors:
                            logger.error(f"    - {error}")
                    else:
                        logger.info("  ✓ Splits match original WelQrate splits")

            except Exception as e:
                logger.warning(f"  ⚠ Could not verify against original data: {e}")

        if target_valid:
            summary["targets_valid"] += 1
            logger.info(f"\n✓ Target {target} passed all checks")
        else:
            all_valid = False
            logger.error(f"\n✗ Target {target} failed validation")

    # Print summary
    logger.info("\n" + "=" * 60)
    logger.info("Verification Summary")
    logger.info("=" * 60)
    logger.info(f"Targets checked: {summary['targets_checked']}")
    logger.info(f"Targets valid: {summary['targets_valid']}")
    logger.info(f"Targets with Parquet errors: {summary['parquet_errors']}")
    logger.info(f"Targets with split errors: {summary['split_errors']}")
    if original_dir:
        logger.info(
            f"Targets with original comparison errors: {summary['original_comparison_errors']}"
        )
    logger.info("")

    if all_valid:
        logger.info("✓ All verifications passed!")
    else:
        logger.error("✗ Some verifications failed. See errors above.")

    logger.info("=" * 60)

    return all_valid


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Verify WelQrate dataset outputs generated by welqrate_preparation.py"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        required=True,
        help="Directory containing processed Parquet files",
    )
    parser.add_argument(
        "--splits-dir",
        type=Path,
        help="Directory containing split files (default: output_dir / 'splits')",
    )
    parser.add_argument(
        "--scaffold",
        type=str,
        choices=["2d", "3d"],
        help="Scaffold type used (required for original data comparison)",
    )
    parser.add_argument(
        "--original-dir",
        type=Path,
        help="Path to original WelQrate directory (e.g., F:\\WelQrate2\\WelQrate) for comparison",
    )
    args = parser.parse_args()

    success = verify_welqrate_outputs(
        output_dir=args.output_dir,
        splits_dir=args.splits_dir,
        scaffold=args.scaffold,
        original_dir=args.original_dir,
    )

    sys.exit(0 if success else 1)
