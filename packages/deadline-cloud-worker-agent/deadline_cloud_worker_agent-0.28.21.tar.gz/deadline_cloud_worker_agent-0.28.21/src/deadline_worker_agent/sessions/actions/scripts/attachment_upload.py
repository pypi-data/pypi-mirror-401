# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.

"""
Job Attachment Upload Script

This script handles the upload of job output attachments to S3 storage.
It processes worker manifest properties, merges manifests, creates snapshots
of output files, and uploads the assets to the configured S3 location.

The script is typically executed as part of a session action in the worker agent
to handle job output file uploads after task completion.
"""

#! /usr/bin/env python3
import argparse
import dataclasses
import functools
import sys
import time
import os
import json
from typing import cast, Any, Callable, Optional, TypeVar
from pathlib import Path
from dataclasses import asdict
import glob

from deadline.client.config import config_file
from deadline.job_attachments.api import human_readable_file_size
from deadline.job_attachments.asset_manifests.decode import decode_manifest
from deadline.job_attachments.progress_tracker import ProgressTracker, ProgressStatus

from deadline.job_attachments.api.manifest import _manifest_snapshot, _manifest_merge
from deadline.job_attachments.upload import S3AssetUploader
from deadline.job_attachments.models import (
    ManifestSnapshot,
    ManifestMerge,
    JobAttachmentS3Settings,
    UploadManifestInfo,
)
from deadline_worker_agent.aws.deadline import (
    record_attachment_upload_fail_telemetry_event,
    record_attachment_upload_latencies_telemetry_event,
    record_attachment_upload_telemetry_event,
    record_success_fail_telemetry_event,
)
from deadline_worker_agent.sessions.attachment_models import (
    WorkerManifestProperties,
)

_queue_id = os.environ.get("DEADLINE_QUEUE_ID", "queue-unknown")  # Just for telemetry

F = TypeVar("F", bound=Callable[..., Any])


def failure_telemetry(function: F) -> F:
    """Decorator to record failure telemetry on a function"""

    @functools.wraps(function)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        try:
            return function(*args, **kwargs)
        except Exception as e:
            record_attachment_upload_fail_telemetry_event(
                queue_id=_queue_id,
                failure_reason=f"{function.__name__}: {type(e).__name__}",
            )
            raise e

    return cast(F, wrapper)


@dataclasses.dataclass
class AttachmentUploadLatences:
    """Stores metrics for function latencies in this script"""

    merge: int = 0
    snapshot: int = 0
    parse_worker_manifest_properties: int = 0
    upload_output_assets: int = 0
    total: int = 0


@failure_telemetry
def merge(worker_manifest_properties: list[WorkerManifestProperties]) -> dict[str, Optional[str]]:
    """
    Merge multiple manifest files for each worker manifest property.

    This function takes a list of worker manifest properties, each represents all manifests within a root,
    and merges their associated manifest files into consolidated manifests. This is typically
    done to combine input manifests before creating output snapshots.

    Args:
        worker_manifest_properties: List of WorkerManifestProperties containing
                                  manifest paths and configuration

    Returns:
        Dictionary mapping root paths to their merged manifest file paths.
        None values indicate no manifests were available to merge.
    """
    # Create directory for merged manifest files
    manifest_path = "manifest_merge"
    root_path_to_local_manifest: dict[str, Optional[str]] = {}

    # Process each worker manifest property
    for manifest_props in worker_manifest_properties:
        # Merge manifest files using the job attachments API
        output: Optional[ManifestMerge] = _manifest_merge(
            # Use source path for correct path hash correspondence
            root=manifest_props.to_path_mapping_rule().source_path,
            # paths to manifest files to be merged
            manifest_files=manifest_props.local_manifest_paths,
            # directory to put the generated merged manifests
            destination=manifest_path,
            name="merge",
            print_function_callback=print,
        )

        if root_path_to_local_manifest.get(manifest_props.root_path) is not None:
            print(
                f"Duplicate root path for {manifest_props.root_path}, replacing with latest manifest"
            )

        if output:
            # Store the path to the merged manifest
            root_path_to_local_manifest[manifest_props.root_path] = output.local_manifest_path
        else:
            # No manifests were generated by merge
            root_path_to_local_manifest[manifest_props.root_path] = None

    return root_path_to_local_manifest


@failure_telemetry
def snapshot(
    worker_manifest_properties: list[WorkerManifestProperties],
    root_path_to_base_manifest: dict[str, Optional[str]],
) -> dict[str, str]:
    """
    Create snapshots of output files by comparing against base manifests.

    This function generates manifest snapshots that capture the differences
    between the current state of output directories and their base manifests.
    Only files that have changed or are new will be included in the snapshot.

    Args:
        worker_manifest_properties: List of WorkerManifestProperties containing
                                  output directory configuration
        root_path_to_base_manifest: Dictionary mapping root paths to their
                                  base manifest file paths for comparison

    Returns:
        Dictionary mapping root paths to their output manifest file paths.
        Only includes entries where changes were detected.
    """
    # Create directory for snapshot manifest files
    output_path = "manifest_snapshot"
    root_path_to_output_manifest = {}

    # Process each worker manifest property
    for manifest_props in worker_manifest_properties:
        local_root_path = manifest_props.local_root_path
        output_relative_directories = manifest_props.local_output_relative_directories()

        # Skip when output relative directories is None or []
        if not output_relative_directories:
            print(
                f"No output directories specified for {manifest_props.root_path}, skipping upload"
            )
            continue

        # Create a snapshot of the output files, comparing against the base manifest
        output_manifest: Optional[ManifestSnapshot] = _manifest_snapshot(
            root=local_root_path,
            # directory to put the generated diff manifests
            destination=output_path,
            # base manifest to compare against (for detecting changes)
            # if the base is None, meaning all changes are output
            diff=root_path_to_base_manifest.get(manifest_props.root_path),
            # include patterns for output directories (with recursive wildcard)
            # when the code reaches here, it's guaranteed output_relative_directories contains value
            include=[glob.escape(subdir) + "/**" for subdir in output_relative_directories],
            name="output",
        )

        if output_manifest:
            # Log the snapshot information for worker agent processing
            # ja_snapshot: is a keyword detected by the worker agent log filter
            print(
                f"ja_snapshot: {json.dumps({'root': local_root_path, 'manifest': output_manifest.manifest})}"
            )
            root_path_to_output_manifest[manifest_props.root_path] = output_manifest.manifest
        else:
            # No changes detected - no files to upload
            print(f"No change detected for root {manifest_props.root_path}")

    return root_path_to_output_manifest


@failure_telemetry
def parse_worker_manifest_properties(file_path: str) -> list[WorkerManifestProperties]:
    """
    Parse WorkerManifestProperties from a JSON file.

    Reads and deserializes a JSON file containing worker manifest properties
    configuration. Each item in the JSON array is converted to a
    WorkerManifestProperties object.

    Args:
        file_path: Path to the JSON file containing worker manifest properties

    Returns:
        List of WorkerManifestProperties objects parsed from the file

    Raises:
        ValueError: If the file cannot be read or contains invalid JSON
    """
    try:
        with open(file_path, "r") as f:
            data = json.load(f)
    except Exception as e:
        raise ValueError(
            f"Error reading worker properties file '{file_path}': {type(e).__name__} - {e}"
        )

    # Convert each JSON object to WorkerManifestProperties
    return [WorkerManifestProperties.from_dict(item) for item in data]


def parse_args(args):
    """
    Parse command line arguments for the attachment upload script.

    Args:
        args: List of command line arguments to parse

    Returns:
        Parsed arguments namespace containing s3_uri and worker_properties
    """
    parser = argparse.ArgumentParser(description="Upload job output attachments to S3 storage")
    parser.add_argument(
        "-s3",
        "--s3-uri",
        type=str,
        help="S3 root URI where attachments will be uploaded",
        required=True,
    )
    parser.add_argument(
        "-wp",
        "--worker-properties",
        type=str,
        help="Path to JSON file containing worker manifest properties configuration",
        required=True,
    )
    return parser.parse_args(args)


@failure_telemetry
def upload_output_assets(
    s3_uri: str,
    worker_manifest_properties: list[WorkerManifestProperties],
    root_path_to_output_manifest: dict[str, str],
) -> list[UploadManifestInfo]:
    """
    Upload output assets to S3 storage based on manifest snapshots.

    This function handles the actual upload of job output files to S3. It processes
    each worker manifest property, reads the corresponding output manifest, and
    uploads the assets using the S3AssetUploader.

    Args:
        s3_uri: S3 root URI where assets will be uploaded
        worker_manifest_properties: List of WorkerManifestProperties containing
                                   upload configuration and path mappings
        root_path_to_output_manifest: Dictionary mapping root paths to their
                                    output manifest file paths

    Returns:
        List of UploadManifestInfo objects containing upload results and metadata

    Raises:
        ValueError: If required environment variables are missing
    """

    # Generate S3 upload path using session context from environment variables
    s3_upload_path = build_s3_manifest_path()

    # Create S3 settings from the provided URI
    s3_settings = JobAttachmentS3Settings.from_s3_root_uri(s3_uri)

    # Initialize the S3 asset uploader
    asset_uploader: S3AssetUploader = S3AssetUploader()
    output_manifest_info_list = []

    all_upload_summaries = []

    # Process each worker manifest property for upload
    for manifest_props in worker_manifest_properties:
        output_manifest_path = root_path_to_output_manifest.get(manifest_props.root_path)

        if output_manifest_path:
            # Only upload when there is an output manifest (changes detected)
            try:
                # Read and decode the output manifest
                with open(output_manifest_path, "r") as manifest_file:
                    output_manifest = decode_manifest(manifest_file.read())
            except (IOError, OSError) as e:
                print(f"Error reading output manifest: {e}")
                continue

            total_file_count = len(output_manifest.paths)
            total_file_size = sum([path.size for path in output_manifest.paths])

            print(
                f"Found {total_file_count} file{'' if total_file_count == 1 else 's'}"
                f" totaling {human_readable_file_size(total_file_size)}"
                f" in output directory: {manifest_props.local_root_path}"
            )
            print(
                f"Uploading {total_file_count} output file{'' if total_file_count == 1 else 's'}"
                f" to S3: {s3_settings.s3BucketName}/{s3_settings.full_cas_prefix()}"
            )

            progress_tracker_per_root = ProgressTracker(
                status=ProgressStatus.UPLOAD_IN_PROGRESS,
                total_files=total_file_count,
                total_bytes=total_file_size,
            )
            key, data = asset_uploader.upload_assets(
                job_attachment_settings=s3_settings,
                manifest=output_manifest,
                partial_manifest_prefix=s3_upload_path,
                # Create unique manifest filename using hashed source path
                manifest_file_name=f"{manifest_props.get_hashed_source_path()}_output",
                manifest_metadata=manifest_props.as_output_metadata(),
                source_root=Path(manifest_props.root_path),
                asset_root=Path(manifest_props.local_root_path),
                s3_check_cache_dir=config_file.get_cache_directory(),
                progress_tracker=progress_tracker_per_root,
            )

            print(f"Uploaded output manifest to {key}")
            print(
                f"Summary Statistics for file uploads:\n{progress_tracker_per_root.get_summary_statistics()}"
            )
            all_upload_summaries.append(progress_tracker_per_root.get_summary_statistics())

            output_manifest_info_list.append(
                UploadManifestInfo(
                    output_manifest_path=key,
                    output_manifest_hash=data,
                    source_path=manifest_props.root_path,
                )
            )

    record_attachment_upload_telemetry_event(
        queue_id=_queue_id,
        upload_summaries=all_upload_summaries,
    )

    return output_manifest_info_list


def should_use_task_chunking_format() -> bool:
    """Determine if task chunking format (without task_id) should be used."""
    return not os.environ.get("DEADLINE_TASK_ID")


def build_s3_manifest_path() -> str:
    """Build S3 path for manifest upload based on current configuration."""

    farm_id = os.environ["DEADLINE_FARM_ID"]
    queue_id = os.environ["DEADLINE_QUEUE_ID"]
    job_id = os.environ["DEADLINE_JOB_ID"]
    step_id = os.environ["DEADLINE_STEP_ID"]
    session_action_id = os.environ["DEADLINE_SESSIONACTION_ID"]
    time = float(os.environ["DEADLINE_SESSIONACTION_START_TIME"])

    if should_use_task_chunking_format():
        return JobAttachmentS3Settings.partial_session_action_manifest_prefix_without_task(  # type: ignore[attr-defined]
            farm_id=farm_id,
            queue_id=queue_id,
            job_id=job_id,
            step_id=step_id,
            session_action_id=session_action_id,
            time=time,
        )
    else:
        task_id = os.environ["DEADLINE_TASK_ID"]
        return JobAttachmentS3Settings.partial_session_action_manifest_prefix(
            farm_id=farm_id,
            queue_id=queue_id,
            job_id=job_id,
            step_id=step_id,
            task_id=task_id,
            session_action_id=session_action_id,
            time=time,
        )


@record_success_fail_telemetry_event(metric_name="attachment_upload")
def main(args=None):
    """
    Main entry point for the attachment upload script.

    Orchestrates the complete upload process:
    1. Parse command line arguments
    2. Load worker manifest properties
    3. Merge input manifests
    4. Create snapshots of output files
    5. Upload changed assets to S3

    Args:
        args: Optional list of command line arguments. If None, uses sys.argv[1:]
    """
    total_start_time = time.perf_counter_ns()
    latencies = AttachmentUploadLatences()

    # Use command line arguments if not provided
    if args is None:
        args = sys.argv[1:]

    parsed_args = parse_args(args)

    # Load and parse worker manifest properties configuration
    start_t = time.perf_counter_ns()
    worker_manifest_properties = parse_worker_manifest_properties(parsed_args.worker_properties)
    latencies.parse_worker_manifest_properties = time.perf_counter_ns() - start_t

    # Step 1: Merge input manifests to create base manifests for comparison
    start_t = time.perf_counter_ns()
    root_path_to_base_manifest = merge(worker_manifest_properties=worker_manifest_properties)
    latencies.merge = time.perf_counter_ns() - start_t

    # Step 2: Create snapshots of output files by comparing against base manifests
    start_t = time.perf_counter_ns()
    root_path_to_output_manifest = snapshot(
        worker_manifest_properties=worker_manifest_properties,
        root_path_to_base_manifest=root_path_to_base_manifest,
    )
    latencies.snapshot = time.perf_counter_ns() - start_t

    # Step 3: Upload assets if there are any changes to upload
    if not root_path_to_output_manifest:
        print("\nNo output to upload")

    else:
        print("\nStarting upload...")
        start_t = time.perf_counter_ns()
        manifest_infos = upload_output_assets(
            s3_uri=parsed_args.s3_uri,
            worker_manifest_properties=worker_manifest_properties,
            root_path_to_output_manifest=root_path_to_output_manifest,
        )
        latencies.upload_output_assets = time.perf_counter_ns() - start_t

        # ja_upload: is a key word that is detected in the worker agent log filter
        # We're printing the manifest info to the logs so that we can re-load it as a manifest info in the worker agent process
        print(f"ja_upload: {json.dumps([asdict(info) for info in manifest_infos])}")

    # Always record latencies telemetry regardless of whether upload occurred
    total = time.perf_counter_ns() - total_start_time
    print(f"Finished after {round(total * 10**-9, 2)} seconds")
    latencies.total = total

    record_attachment_upload_latencies_telemetry_event(
        queue_id=_queue_id,
        latencies=dataclasses.asdict(latencies),
    )


if __name__ == "__main__":
    main()
