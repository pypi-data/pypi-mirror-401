{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec65c075-6a5a-4379-9c69-8420430dd982",
   "metadata": {},
   "source": [
    "# Building Environments in Aviary\n",
    "\n",
    "In this tutorial we'll focus on constructing a language agent environment where the agent employs a calculator **tool** to answer questions from the Grade School Mathematics 8K (GSM8K) dataset introduced in [1]. GSM8K consists of linguistically diverse grade school math word problems designed to assess multi-step mathematical reasoning. The GSM8K dataset comprises a training set of 7,473 questions and a test set of 1,319 questions. The language agent is equipped with the following tools:\n",
    "\n",
    "1. **Calculator(Expression)**: Return the result of a numerical expression.\n",
    "2. **Check(Answer)**: Check if the answer is correct.\n",
    "\n",
    "The GSM8K environment can be represented as a Markov Decision Process $(\\mathcal{V}, \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, R, \\gamma)$ as\n",
    "\n",
    "\n",
    "- $\\mathcal{V}:$ The vocabulary - the English alphabet, together with punctuation symbols.\n",
    "- $\\mathcal{S}:$ The state - GSM8K question, current step in the reasoning process.\n",
    "- $\\mathcal{A}:$ The action - \\{Calculator, Check Answer\\}.\n",
    "- $\\mathcal{T}:$ The deterministic transition function.\n",
    "- $R:$ The reward function:\n",
    "\n",
    "    \\begin{cases} \n",
    "    1 & \\text{if the action submits a correct answer}, \\\\\n",
    "    -1 & \\text{if the action is an invalid tool call}, \\\\\n",
    "    0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\n",
    "- $\\gamma:$ The discount factor\n",
    "\n",
    "Below we define the GSM8K environment in our language agent library called **Aviary**. The framework for defining an environment losely follows the template for reinforcement learning environments from Spinning up in Deep RL [2]. Custom implementations of Aviary environments should inherit from the `Environment` class and should implement the **reset**, and **step** methods in addition to defining functions for the tools the agent has access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c65bac-0b9b-45a3-bde6-d9e825e24873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_API_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcb60e-1b4b-48d4-9c78-a56794e75aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "from aviary.envs.gsm8k.env import SafeMathEvaluator\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from aviary.core import (\n",
    "    Environment,\n",
    "    Message,\n",
    "    TaskDataset,\n",
    "    Tool,\n",
    "    ToolRequestMessage,\n",
    "    ToolResponseMessage,\n",
    ")\n",
    "\n",
    "\n",
    "class CalculatorEnvConfig(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    correct_reward: float = 1.0\n",
    "    incorrect_reward: float = 0.0\n",
    "    tool_failure_reward: float = -1.0\n",
    "    tool_success_reward: float = 0.0\n",
    "    rel_tol: float = 1e-4\n",
    "\n",
    "    done_on_failure: bool = True\n",
    "\n",
    "\n",
    "class CalculatorEnv(Environment[None]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem_id: str,\n",
    "        problem: str,\n",
    "        answer: float,\n",
    "        config: CalculatorEnvConfig | None = None,\n",
    "    ):\n",
    "        \"\"\"An environment for solving simple math problems using a calculator tool and a check answer tool.\n",
    "\n",
    "        Arguments:\n",
    "            problem_id: The unique identifier for the problem (index in the GSM8K dataset)\n",
    "            problem: The calculation problem in string format.\n",
    "            answer: The ground truth (correct) answer to the calculation problem.\n",
    "            config: Configuration for environment behavior and rewards.\n",
    "        \"\"\"\n",
    "        self.problem_id = problem_id\n",
    "        self.problem = problem\n",
    "        self.answer = float(answer)\n",
    "        self.config = config or CalculatorEnvConfig()\n",
    "\n",
    "        self.calc_tool = Tool.from_function(self.calculator)\n",
    "        self.check_tool = Tool.from_function(self.check_answer)\n",
    "        self.tools = [self.calc_tool, self.check_tool]\n",
    "\n",
    "    async def reset(self) -> tuple[list[Message], list[Tool]]:\n",
    "        \"\"\"Resets the environment, returning the initial problem statement and available tools.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - A list with the problem message for the agent to view.\n",
    "                - A list of tools available for the agent to interact with.\n",
    "        \"\"\"\n",
    "        self.state = None  # this environment is effectively stateless\n",
    "        return [Message(content=self.problem)], self.tools\n",
    "\n",
    "    async def step(\n",
    "        self, action: ToolRequestMessage\n",
    "    ) -> tuple[list[Message], float, bool, bool]:\n",
    "        \"\"\"Processes an action (tool request) and returns the result.\n",
    "\n",
    "        Args:\n",
    "            action: The action message containing tool requests.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - A list of response messages from the tools e.g. the output of the calculator\n",
    "                - The total reward accumulated from the tool calls.\n",
    "                - A flag indicating if the episode is done (e.g., answer is correct or a tool failure).\n",
    "                - Always False, as this environment does not use truncation.\n",
    "        \"\"\"\n",
    "        # We must use a tool at each step\n",
    "        if not action.tool_calls:\n",
    "            return (\n",
    "                [\n",
    "                    Message(\n",
    "                        content=\"Must call one of the provided tools (calculator or check_answer).\"\n",
    "                    )\n",
    "                ],\n",
    "                self.config.tool_failure_reward,\n",
    "                self.config.done_on_failure,\n",
    "                False,\n",
    "            )\n",
    "\n",
    "        # Detect invalid tool calls\n",
    "        valid_action, invalid_action = self.filter_invalid_tool_calls(action)\n",
    "\n",
    "        invalid_response_msgs = [\n",
    "            ToolResponseMessage.from_call(tool_call, content=\"\")\n",
    "            for tool_call in invalid_action.tool_calls\n",
    "        ]\n",
    "\n",
    "        # Execute tool calls\n",
    "        if valid_action.tool_calls:\n",
    "            results = await self.exec_tool_calls(valid_action)\n",
    "            response_msgs = []\n",
    "            total_reward = 0.0\n",
    "            any_done = False\n",
    "\n",
    "            for tool_call, result in zip(valid_action.tool_calls, results, strict=True):\n",
    "                response, reward, done = json.loads(result.content)\n",
    "\n",
    "                response_msgs.append(\n",
    "                    ToolResponseMessage.from_call(tool_call, content=str(response))\n",
    "                )\n",
    "\n",
    "                total_reward += reward\n",
    "                any_done |= done\n",
    "\n",
    "            return response_msgs + invalid_response_msgs, total_reward, any_done, False\n",
    "\n",
    "        return (\n",
    "            invalid_response_msgs,\n",
    "            self.config.tool_failure_reward * len(invalid_response_msgs),\n",
    "            self.config.done_on_failure,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "    # We define our tools, check_answer and calculator, below\n",
    "\n",
    "    def check_answer(self, answer: str) -> tuple[bool, float, Literal[True]]:\n",
    "        \"\"\"Check if the proposed answer is correct.\n",
    "\n",
    "        Args:\n",
    "            answer: Proposed answer.\n",
    "\n",
    "        Returns:\n",
    "            Three-tuple of if correct, associated reward (correct_reward if correct,\n",
    "                tool_failure_reward if tool failure, otherwise incorrect_reward), and\n",
    "                True indicating done.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            correct: bool = (\n",
    "                abs(float(answer) - self.answer)\n",
    "                / (abs(self.answer) + self.config.rel_tol)\n",
    "                < self.config.rel_tol\n",
    "            )\n",
    "            reward = (\n",
    "                self.config.correct_reward if correct else self.config.incorrect_reward\n",
    "            )\n",
    "        except ValueError:\n",
    "            return False, self.config.tool_failure_reward, True\n",
    "        else:\n",
    "            return correct, reward, True\n",
    "\n",
    "    def calculator(self, expr: str) -> tuple[float | str, float, bool]:\n",
    "        \"\"\"Calculate a mathematical expression using a secure evaluator.\n",
    "\n",
    "        Args:\n",
    "            expr: A valid mathematical expression.\n",
    "\n",
    "        Returns:\n",
    "            A three-tuple where the first element is the float evaluation if successful,\n",
    "                or a string containing the failure cause if unsuccessful, the second\n",
    "                element is the reward associated with success or failure, and the third\n",
    "                element is a boolean indicating if this action is terminal.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            expr = expr.strip()\n",
    "            result = SafeMathEvaluator.evaluate(expr)\n",
    "            with contextlib.suppress(ValueError):  # If possible, downcast float to int\n",
    "                if int(result) == result:\n",
    "                    result = int(result)\n",
    "        except Exception as exc:\n",
    "            return (\n",
    "                f\"Error using calculator: {exc!r}.\",\n",
    "                self.config.tool_failure_reward,\n",
    "                self.config.done_on_failure,\n",
    "            )\n",
    "        return result, self.config.tool_success_reward, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dbf60-9c9c-46e4-bc80-785debabf54d",
   "metadata": {},
   "source": [
    "We next define an instance of `TaskDataset` to load the GSM8K dataset from Hugging Face and provide convenience methods for accessing specific questions, namely `get_new_env_by_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817f9d3-78e7-47c6-9ba9-1b18bb30edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# SEE: https://huggingface.co/datasets/openai/gsm8k\n",
    "GSM8K_PUBLIC_SOURCE = \"openai/gsm8k\"\n",
    "\n",
    "\n",
    "class GSM8kDataset(TaskDataset):\n",
    "    \"\"\"A dataset class for GSM8K.\n",
    "\n",
    "    Attributes:\n",
    "        config: Configuration for the Calculator environment.\n",
    "        src_df: DataFrame containing dataset problems, processed to include\n",
    "                numerical answers and unique problem IDs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    class Split(StrEnum):\n",
    "        train_full = \"train_full\"  # full training set from OpenAI\n",
    "        train = \"train\"  # 80% of train_full\n",
    "        val = \"val\"  # 20% of train_full\n",
    "        test = \"test\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: Split | str,\n",
    "        config: CalculatorEnvConfig | None = None,\n",
    "        hf_source: str = GSM8K_PUBLIC_SOURCE,\n",
    "    ):\n",
    "        self.config = config\n",
    "\n",
    "        if isinstance(split, str):\n",
    "            split = self.Split(split)\n",
    "\n",
    "        src_df = self._get_df_from_hf(hf_source, split)\n",
    "\n",
    "        # Assign problem ID for the env\n",
    "        src_df[\"problem_id\"] = split.value + \"_\" + src_df.index.astype(str)\n",
    "\n",
    "        # attempt to extract a numerical answer\n",
    "        try:\n",
    "            src_df[\"answer_num\"] = src_df[\"answer\"].apply(\n",
    "                # answer is formatted as: <some text>\\n#### <answer_num>\n",
    "                lambda a: float(a.split(\"#### \")[1].replace(\",\", \"\"))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to extract numerical answer from 'answer' column\"\n",
    "            ) from e\n",
    "\n",
    "        self.src_df = src_df\n",
    "\n",
    "    def _get_df_from_hf(self, hf_source: str, split: Split) -> \"pd.DataFrame\":\n",
    "        \"\"\"Loads the GSM8K dataset from the Hugging Face hub and processes it based on the specified split.\n",
    "\n",
    "        Args:\n",
    "            hf_source: The Hugging Face source identifier for the GSM8K dataset.\n",
    "            split: The specified dataset split.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing the GSM8K problems, filtered and split according to the specification.\n",
    "        \"\"\"\n",
    "        # All non-test splits are derived from train\n",
    "        hf_split = \"test\" if split == self.Split.test else \"train\"\n",
    "\n",
    "        kw = {}\n",
    "        if hf_source == GSM8K_PUBLIC_SOURCE:\n",
    "            kw[\"name\"] = \"main\"  # as opposed to \"socratic\"\n",
    "\n",
    "        src_df = (\n",
    "            datasets\n",
    "            .load_dataset(hf_source, split=hf_split, **kw)\n",
    "            .to_pandas()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        if split == self.Split.train:\n",
    "            src_df = src_df[src_df.index % 5 != 0]\n",
    "        elif split == self.Split.val:\n",
    "            src_df = src_df[src_df.index % 5 == 0]\n",
    "        return src_df\n",
    "\n",
    "    def get_new_env_by_idx(self, idx: int) -> CalculatorEnv:\n",
    "        \"\"\"Creates a new Calculator environment instance for a specific problem in the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the problem in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            A new Calculator environment initialized with the problem ID,\n",
    "            question, answer, and configuration.\n",
    "        \"\"\"\n",
    "        row = self.src_df.iloc[idx]\n",
    "        return CalculatorEnv(\n",
    "            problem_id=row[\"problem_id\"],\n",
    "            problem=row[\"question\"],\n",
    "            answer=row[\"answer_num\"],\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of problems in the dataset.\"\"\"\n",
    "        return len(self.src_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6827098-c012-4328-9c93-cb3d2b565a02",
   "metadata": {},
   "source": [
    "In the next cell, we perform a **rollout** using the `ToolSelector` agent using GPT-4o as the base LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302b166-8be3-45b5-a434-b612e4a56639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating problem number 1:\n",
      "\n",
      "Observation is [Message(role='user', content='Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='(50/60) * 12') [id=call_AmfsBdqTgU3jV9Ni3dWWDi7t]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='10', name='calculator', tool_call_id='call_AmfsBdqTgU3jV9Ni3dWWDi7t')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: check_answer(answer='10') [id=call_1HKFkh8BhOs7BBPFAojuoAyE]\n",
      "\n",
      "\n",
      "Evaluating problem number 2:\n",
      "\n",
      "Observation is [Message(role='user', content='Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='100 / 2') [id=call_Ts9iE2ZZSjBVOTEFlqbJgBiP]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='50', name='calculator', tool_call_id='call_Ts9iE2ZZSjBVOTEFlqbJgBiP')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='15 * 2') [id=call_ItKmh0TKIvaIHMPlcfwFJdXb]; calculator(expr='50 + 15') [id=call_4NY9WHLyGVQ010aZITQ06J6C]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='30', name='calculator', tool_call_id='call_ItKmh0TKIvaIHMPlcfwFJdXb'), ToolResponseMessage(role='tool', content='65', name='calculator', tool_call_id='call_4NY9WHLyGVQ010aZITQ06J6C')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='65 + 30') [id=call_EXhCQ1uYqrBuAnAGYviichyz]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='95', name='calculator', tool_call_id='call_EXhCQ1uYqrBuAnAGYviichyz')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='100 - 95') [id=call_fAODL0FzwXbevnMJCFWAumZ1]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='5', name='calculator', tool_call_id='call_fAODL0FzwXbevnMJCFWAumZ1')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: check_answer(answer='5') [id=call_8epdwS5j4Nxw8UGDB05e2wRu]\n",
      "\n",
      "\n",
      "Evaluating problem number 3:\n",
      "\n",
      "Observation is [Message(role='user', content='Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='12 * 2') [id=call_MgQgHT6e9DpDCdzhb93YFGP7]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='24', name='calculator', tool_call_id='call_MgQgHT6e9DpDCdzhb93YFGP7')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='120 - (12 + 24)') [id=call_9Lv65NHAw9oWcASlZwVXdkeM]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='84', name='calculator', tool_call_id='call_9Lv65NHAw9oWcASlZwVXdkeM')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: calculator(expr='84 / 2') [id=call_55MiSCqyuNQ2jbHiFE9cyazA]\n",
      "\n",
      "\n",
      "Observation is [ToolResponseMessage(role='tool', content='42', name='calculator', tool_call_id='call_55MiSCqyuNQ2jbHiFE9cyazA')]\n",
      "\n",
      "Action is Tool request message '' for tool calls: check_answer(answer='42') [id=call_bNdPTB23BD6fEQRnyaG5U6N0]\n",
      "\n",
      "\n",
      "Accuracy is 100.0%\n"
     ]
    }
   ],
   "source": [
    "# ToolSelector is a simple language agent that directly prompts a language model to call a tool.\n",
    "from aviary.core import ToolSelector\n",
    "\n",
    "dataset = GSM8kDataset(split=\"train\")\n",
    "total_reward = 0\n",
    "verbose = True  # Whether to explicitly print the environment obserations and actions.\n",
    "n_questions = 3\n",
    "max_steps = 5  # Maximum number of actions the agent can take.\n",
    "\n",
    "for i in range(n_questions):\n",
    "    print(f\"Evaluating problem number {i + 1}:\\n\")\n",
    "    env = dataset.get_new_env_by_idx(i)\n",
    "    obs, tools = await env.reset()\n",
    "    tool_selector = ToolSelector(model_name=\"gpt-4o\", accum_messages=True)\n",
    "    for _ in range(max_steps):\n",
    "        action = await tool_selector(obs, tools)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Observation is {obs}\\n\")\n",
    "            print(f\"Action is {action}\\n\")\n",
    "\n",
    "        obs, reward, done, _ = await env.step(action)\n",
    "        total_reward += reward\n",
    "        print()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(f\"Accuracy is {total_reward / n_questions * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bad9c-bdb3-4854-8549-7d6474dfe186",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R. and Hesse, C., 2021. [Training verifiers to solve math word problems](https://arxiv.org/abs/2110.14168). arXiv preprint arXiv:2110.14168.\n",
    "\n",
    "[2] Achiam, J., [Spinning up in Deep Reinforcement Learning](https://spinningup.openai.com/en/latest/). 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
