"""
AI-Powered Log Analyzer
S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch log CI/CD v√† ƒë∆∞a ra g·ª£i √Ω s·ª≠a l·ªói

Kh√¥ng d√πng regex patterns c·ªë ƒë·ªãnh - AI t·ª± ƒë·ªông:
1. ƒê·ªçc v√† hi·ªÉu log
2. X√°c ƒë·ªãnh l·ªói v√† nguy√™n nh√¢n g·ªëc
3. ƒê∆∞a ra g·ª£i √Ω s·ª≠a l·ªói c·ª• th·ªÉ

H·ªó tr·ª£:
- Ollama (local, mi·ªÖn ph√≠)
- OpenAI API
- Fallback v·ªÅ basic analysis n·∫øu kh√¥ng c√≥ AI
"""
import os
import json
import re
import subprocess
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum


class AIProvider(Enum):
    OLLAMA = "ollama"
    OPENAI = "openai"
    NONE = "none"


@dataclass
class AnalysisResult:
    """K·∫øt qu·∫£ ph√¢n t√≠ch t·ª´ AI"""
    success: bool
    error_summary: str
    root_cause: str
    root_cause_explanation: str
    layer: str  # CI, Build, App, Infra, Platform, Data, Security
    severity: str  # low, medium, high, critical
    fix_suggestions: List[Dict[str, Any]]
    raw_errors: List[str]
    confidence: float  # 0.0 - 1.0
    ai_provider: str
    

class AILogAnalyzer:
    """
    AI-Powered Log Analyzer
    
    S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch log thay v√¨ regex patterns
    """
    
    # System prompt cho AI - h∆∞·ªõng d·∫´n c√°ch ph√¢n t√≠ch log
    SYSTEM_PROMPT = """B·∫°n l√† chuy√™n gia ph√¢n t√≠ch CI/CD logs. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

1. ƒê·ªåC LOG v√† x√°c ƒë·ªãnh l·ªói ch√≠nh
2. PH√ÇN T√çCH nguy√™n nh√¢n g·ªëc (root cause) - kh√¥ng ph·∫£i tri·ªáu ch·ª©ng
3. ƒê∆ØA RA g·ª£i √Ω s·ª≠a l·ªói c·ª• th·ªÉ, c√≥ th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c

QUAN TR·ªåNG:
- T·∫≠p trung v√†o L·ªñI CH√çNH, kh√¥ng li·ªát k√™ t·∫•t c·∫£ warnings
- Gi·∫£i th√≠ch T·∫†I SAO l·ªói x·∫£y ra
- ƒê∆∞a ra COMMANDS c·ª• th·ªÉ ƒë·ªÉ s·ª≠a
- C·∫£nh b√°o n·∫øu action c√≥ r·ªßi ro cao

Output JSON format:
{
    "error_found": true/false,
    "error_summary": "T√≥m t·∫Øt ng·∫Øn g·ªçn l·ªói ch√≠nh",
    "root_cause": "Nguy√™n nh√¢n g·ªëc c·ªßa l·ªói",
    "root_cause_explanation": "Gi·∫£i th√≠ch chi ti·∫øt t·∫°i sao l·ªói x·∫£y ra",
    "layer": "CI|Build|App|Infra|Platform|Data|Security",
    "severity": "low|medium|high|critical",
    "raw_errors": ["d√≤ng l·ªói 1", "d√≤ng l·ªói 2"],
    "fix_suggestions": [
        {
            "title": "Ti√™u ƒë·ªÅ g·ª£i √Ω",
            "description": "M√¥ t·∫£ chi ti·∫øt",
            "commands": ["command 1", "command 2"],
            "risk_level": "safe|low|medium|high|critical",
            "conditions": ["ƒëi·ªÅu ki·ªán 1", "ƒëi·ªÅu ki·ªán 2"],
            "why": "T·∫°i sao g·ª£i √Ω n√†y gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ"
        }
    ],
    "confidence": 0.95
}"""

    def __init__(self, provider: Optional[AIProvider] = None, model: Optional[str] = None):
        """
        Kh·ªüi t·∫°o AI Analyzer
        
        Args:
            provider: AIProvider.OLLAMA ho·∫∑c AIProvider.OPENAI
            model: T√™n model (e.g., "llama3.2", "gpt-4")
        """
        self.provider = provider or self._detect_provider()
        self.model = model or self._get_default_model()
        
    def _detect_provider(self) -> AIProvider:
        """Auto-detect available AI provider"""
        # Check Ollama first (local, free)
        if self._check_ollama():
            return AIProvider.OLLAMA
        
        # Check OpenAI
        if os.getenv("OPENAI_API_KEY"):
            return AIProvider.OPENAI
        
        return AIProvider.NONE
    
    def _check_ollama(self) -> bool:
        """Check if Ollama is running"""
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def _get_default_model(self) -> str:
        """Get default model for provider"""
        if self.provider == AIProvider.OLLAMA:
            # Check available models
            try:
                result = subprocess.run(
                    ["ollama", "list"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    if len(lines) > 1:
                        # Get first model name
                        first_model = lines[1].split()[0]
                        return first_model
            except:
                pass
            return "llama3.2"  # Default
        elif self.provider == AIProvider.OPENAI:
            return "gpt-4o-mini"
        return ""
    
    def analyze(self, log_content: str) -> AnalysisResult:
        """
        Ph√¢n t√≠ch log s·ª≠ d·ª•ng AI
        
        Args:
            log_content: N·ªôi dung log
            
        Returns:
            AnalysisResult v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
        """
        if self.provider == AIProvider.OLLAMA:
            return self._analyze_with_ollama(log_content)
        elif self.provider == AIProvider.OPENAI:
            return self._analyze_with_openai(log_content)
        else:
            return self._analyze_basic(log_content)
    
    def _analyze_with_ollama(self, log_content: str) -> AnalysisResult:
        """Ph√¢n t√≠ch s·ª≠ d·ª•ng Ollama local"""
        try:
            # Truncate log if too long
            max_chars = 8000
            if len(log_content) > max_chars:
                # Keep first and last parts
                half = max_chars // 2
                log_content = log_content[:half] + "\n...[TRUNCATED]...\n" + log_content[-half:]
            
            prompt = f"""Ph√¢n t√≠ch CI/CD log sau v√† tr·∫£ v·ªÅ JSON:

```
{log_content}
```

Tr·∫£ v·ªÅ JSON theo format ƒë√£ h∆∞·ªõng d·∫´n. CH·ªà tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text kh√°c."""

            # Call Ollama
            result = subprocess.run(
                ["ollama", "run", self.model, prompt],
                capture_output=True,
                text=True,
                timeout=60,
                env={**os.environ, "OLLAMA_SYSTEM": self.SYSTEM_PROMPT}
            )
            
            if result.returncode != 0:
                return self._analyze_basic(log_content)
            
            # Parse JSON response
            response = result.stdout.strip()
            return self._parse_ai_response(response, "ollama")
            
        except Exception as e:
            print(f"Ollama error: {e}")
            return self._analyze_basic(log_content)
    
    def _analyze_with_openai(self, log_content: str) -> AnalysisResult:
        """Ph√¢n t√≠ch s·ª≠ d·ª•ng OpenAI API"""
        try:
            import openai
            
            client = openai.OpenAI()
            
            # Truncate log if too long
            max_chars = 12000
            if len(log_content) > max_chars:
                half = max_chars // 2
                log_content = log_content[:half] + "\n...[TRUNCATED]...\n" + log_content[-half:]
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": f"Ph√¢n t√≠ch CI/CD log sau:\n\n```\n{log_content}\n```\n\nTr·∫£ v·ªÅ JSON theo format ƒë√£ h∆∞·ªõng d·∫´n."}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            return self._parse_ai_response(response.choices[0].message.content, "openai")
            
        except Exception as e:
            print(f"OpenAI error: {e}")
            return self._analyze_basic(log_content)
    
    def _parse_ai_response(self, response: str, provider: str) -> AnalysisResult:
        """Parse AI response th√†nh AnalysisResult"""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
            else:
                raise ValueError("No JSON found in response")
            
            return AnalysisResult(
                success=data.get("error_found", True),
                error_summary=data.get("error_summary", "Unknown error"),
                root_cause=data.get("root_cause", "Unknown"),
                root_cause_explanation=data.get("root_cause_explanation", ""),
                layer=data.get("layer", "Unknown"),
                severity=data.get("severity", "medium"),
                fix_suggestions=data.get("fix_suggestions", []),
                raw_errors=data.get("raw_errors", []),
                confidence=data.get("confidence", 0.8),
                ai_provider=provider
            )
        except Exception as e:
            print(f"Parse error: {e}")
            return self._create_fallback_result(response, provider)
    
    def _create_fallback_result(self, response: str, provider: str) -> AnalysisResult:
        """T·∫°o result khi kh√¥ng parse ƒë∆∞·ª£c JSON"""
        return AnalysisResult(
            success=True,
            error_summary="AI analysis completed but response parsing failed",
            root_cause="See raw response",
            root_cause_explanation=response[:500],
            layer="Unknown",
            severity="medium",
            fix_suggestions=[{
                "title": "Review AI Response",
                "description": response[:1000],
                "commands": [],
                "risk_level": "safe",
                "conditions": [],
                "why": "AI response c·∫ßn ƒë∆∞·ª£c review th·ªß c√¥ng"
            }],
            raw_errors=[],
            confidence=0.5,
            ai_provider=provider
        )
    
    def _analyze_basic(self, log_content: str) -> AnalysisResult:
        """
        Basic analysis khi kh√¥ng c√≥ AI
        S·ª≠ d·ª•ng simple heuristics thay v√¨ regex patterns ph·ª©c t·∫°p
        """
        lines = log_content.split('\n')
        
        # T√¨m d√≤ng c√≥ keywords l·ªói
        error_keywords = ['error', 'failed', 'fatal', 'exception', 'panic', 'denied', 'not found', 'cannot', 'unable']
        success_keywords = ['success', 'passed', 'completed', 'done']
        
        error_lines = []
        last_error_line = ""
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            
            # Skip success lines
            if any(kw in line_lower for kw in success_keywords):
                continue
            
            # Detect error lines
            if any(kw in line_lower for kw in error_keywords):
                error_lines.append(line.strip())
                last_error_line = line.strip()
        
        # X√°c ƒë·ªãnh l·ªói cu·ªëi c√πng (th∆∞·ªùng l√† root cause indicator)
        if error_lines:
            # L·∫•y context xung quanh error cu·ªëi
            root_cause = error_lines[-1] if error_lines else "Unknown error"
            
            # Detect layer t·ª´ keywords
            layer = self._detect_layer_basic(log_content)
            
            # T·∫°o g·ª£i √Ω c∆° b·∫£n
            fix_suggestions = self._generate_basic_suggestions(root_cause, layer, log_content)
            
            return AnalysisResult(
                success=True,
                error_summary=root_cause[:200],
                root_cause=root_cause,
                root_cause_explanation="Ph√¢n t√≠ch c∆° b·∫£n d·ª±a tr√™n keywords. C√†i ƒë·∫∑t Ollama ƒë·ªÉ c√≥ ph√¢n t√≠ch AI chi ti·∫øt h∆°n.",
                layer=layer,
                severity="medium",
                fix_suggestions=fix_suggestions,
                raw_errors=error_lines[:5],
                confidence=0.6,
                ai_provider="basic"
            )
        
        return AnalysisResult(
            success=False,
            error_summary="No errors detected",
            root_cause="",
            root_cause_explanation="",
            layer="Unknown",
            severity="low",
            fix_suggestions=[],
            raw_errors=[],
            confidence=0.9,
            ai_provider="basic"
        )
    
    def _detect_layer_basic(self, log_content: str) -> str:
        """Detect layer t·ª´ log content"""
        log_lower = log_content.lower()
        
        layer_keywords = {
            "CI": ["runner", "gitlab-runner", "jenkins", "github actions", "executor"],
            "Build": ["build", "compile", "npm", "pip", "docker build", "webpack"],
            "App": ["test", "assert", "exception", "traceback", "unittest", "pytest"],
            "Infra": ["kubernetes", "k8s", "kubectl", "docker", "container", "pod"],
            "Platform": ["1c", "designer", "infobase", "repository", "extension"],
            "Data": ["database", "migration", "sql", "schema", "table"],
            "Security": ["vulnerability", "cve", "security", "permission", "auth"],
        }
        
        scores = {layer: 0 for layer in layer_keywords}
        for layer, keywords in layer_keywords.items():
            for kw in keywords:
                if kw in log_lower:
                    scores[layer] += 1
        
        max_layer = max(scores, key=scores.get)
        return max_layer if scores[max_layer] > 0 else "Unknown"
    
    def _generate_basic_suggestions(self, error: str, layer: str, log_content: str) -> List[Dict]:
        """T·∫°o g·ª£i √Ω c∆° b·∫£n d·ª±a tr√™n error v√† layer"""
        suggestions = []
        
        error_lower = error.lower()
        
        # Common patterns
        if "not found" in error_lower or "cannot find" in error_lower:
            suggestions.append({
                "title": "üîç Check Missing Resource",
                "description": f"M·ªôt resource kh√¥ng t√¨m th·∫•y: {error[:100]}",
                "commands": [
                    "# Ki·ªÉm tra resource c√≥ t·ªìn t·∫°i kh√¥ng",
                    "# Ki·ªÉm tra path/t√™n c√≥ ƒë√∫ng kh√¥ng",
                    "# Ki·ªÉm tra permissions",
                ],
                "risk_level": "safe",
                "conditions": ["C√≥ th·ªÉ reproduce l·ªói locally"],
                "why": "Resource b·ªã thi·∫øu ho·∫∑c path kh√¥ng ƒë√∫ng"
            })
        
        if "permission" in error_lower or "denied" in error_lower or "access" in error_lower:
            suggestions.append({
                "title": "üîê Check Permissions",
                "description": "L·ªói li√™n quan ƒë·∫øn quy·ªÅn truy c·∫≠p",
                "commands": [
                    "# Ki·ªÉm tra user/role c√≥ quy·ªÅn kh√¥ng",
                    "# Ki·ªÉm tra file permissions: ls -la <path>",
                    "# Ki·ªÉm tra credentials",
                ],
                "risk_level": "low",
                "conditions": ["C√≥ quy·ªÅn admin ƒë·ªÉ ki·ªÉm tra"],
                "why": "User/process kh√¥ng c√≥ quy·ªÅn th·ª±c hi·ªán action"
            })
        
        if "exit code 1" in error_lower or "failed" in error_lower:
            suggestions.append({
                "title": "‚ö†Ô∏è Debug Command Failure",
                "description": "Command th·ª±c thi th·∫•t b·∫°i",
                "commands": [
                    "# Ch·∫°y l·∫°i command v·ªõi verbose mode",
                    "# Ki·ªÉm tra dependencies",
                    "# Xem full error output",
                ],
                "risk_level": "safe",
                "conditions": ["C√≥ th·ªÉ ch·∫°y locally"],
                "why": "Command exit v·ªõi error code"
            })
        
        if "is not a" in error_lower and "command" in error_lower:
            suggestions.append({
                "title": "üîß Fix Invalid Command",
                "description": "Command kh√¥ng h·ª£p l·ªá ho·∫∑c script syntax sai",
                "commands": [
                    "# Ki·ªÉm tra script syntax",
                    "# ƒê·∫£m b·∫£o d√πng ƒë√∫ng shell (bash vs sh)",
                    "# Ki·ªÉm tra .gitlab-ci.yml ho·∫∑c Jenkinsfile",
                ],
                "risk_level": "safe",
                "conditions": ["C√≥ access v√†o CI config"],
                "why": "Command ƒë∆∞·ª£c g·ªçi sai ho·∫∑c script format kh√¥ng ƒë√∫ng"
            })
        
        # Layer-specific suggestions
        if layer == "CI":
            suggestions.append({
                "title": "üèÉ Check CI Configuration",
                "description": "Ki·ªÉm tra CI/CD configuration",
                "commands": [
                    "# GitLab: Ki·ªÉm tra .gitlab-ci.yml",
                    "# Jenkins: Ki·ªÉm tra Jenkinsfile",
                    "# GitHub: Ki·ªÉm tra .github/workflows/",
                ],
                "risk_level": "safe",
                "conditions": [],
                "why": "L·ªói ·ªü t·∫ßng CI th∆∞·ªùng do config sai"
            })
        elif layer == "Build":
            suggestions.append({
                "title": "üì¶ Fix Build Issues",
                "description": "Debug build problems",
                "commands": [
                    "# Build locally v·ªõi verbose",
                    "# Clear cache v√† rebuild",
                    "# Ki·ªÉm tra dependencies versions",
                ],
                "risk_level": "low",
                "conditions": ["C√≥ th·ªÉ build locally"],
                "why": "Build errors th∆∞·ªùng do dependencies ho·∫∑c config"
            })
        
        if not suggestions:
            suggestions.append({
                "title": "üîç General Debug Steps",
                "description": f"L·ªói: {error[:150]}",
                "commands": [
                    "# 1. Reproduce locally",
                    "# 2. Check full logs",
                    "# 3. Search error message online",
                    "# 4. Check recent changes (git diff)",
                ],
                "risk_level": "safe",
                "conditions": [],
                "why": "C·∫ßn debug th√™m ƒë·ªÉ x√°c ƒë·ªãnh nguy√™n nh√¢n c·ª• th·ªÉ"
            })
        
        return suggestions


# Singleton instance
_analyzer_instance = None

def get_ai_analyzer(provider: Optional[AIProvider] = None, model: Optional[str] = None) -> AILogAnalyzer:
    """Get or create AI analyzer instance"""
    global _analyzer_instance
    if _analyzer_instance is None or provider is not None:
        _analyzer_instance = AILogAnalyzer(provider, model)
    return _analyzer_instance


def analyze_log_with_ai(log_content: str) -> AnalysisResult:
    """
    Ph√¢n t√≠ch log s·ª≠ d·ª•ng AI
    
    Args:
        log_content: N·ªôi dung log
        
    Returns:
        AnalysisResult
    """
    analyzer = get_ai_analyzer()
    return analyzer.analyze(log_content)
