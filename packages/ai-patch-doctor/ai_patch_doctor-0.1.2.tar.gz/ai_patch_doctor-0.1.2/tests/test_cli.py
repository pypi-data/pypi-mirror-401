"""
Test suite for AI Patch Python CLI
"""

import sys
import os
from pathlib import Path

# Add python directory to path to import config, report, and checks
python_dir = Path(__file__).parent.parent
if str(python_dir) not in sys.path:
    sys.path.append(str(python_dir))

from config import Config
from report import ReportGenerator


def test_config_auto_detect():
    """Test configuration auto-detection"""
    print("Testing config auto-detection...")
    
    # Test OpenAI-compatible
    os.environ['OPENAI_API_KEY'] = 'test-key'
    os.environ['OPENAI_BASE_URL'] = 'https://api.openai.com'
    
    config = Config.auto_detect('openai-compatible')
    
    assert config.base_url == 'https://api.openai.com', "Base URL should match"
    assert config.api_key == 'test-key', "API key should match"
    assert config.provider == 'openai-compatible', "Provider should match"
    
    print("✅ Config auto-detection test passed")


def test_report_generation():
    """Test report generation"""
    print("Testing report generation...")
    
    report_gen = ReportGenerator()
    
    # Create sample check results
    checks = {
        'streaming': {
            'status': 'pass',
            'findings': [],
            'metrics': {'ttfb_ms': 245.5, 'total_time_s': 3.2}
        },
        'retries': {
            'status': 'pass',
            'findings': [
                {'severity': 'info', 'message': 'Recommended: Use exponential backoff'}
            ],
            'metrics': {}
        }
    }
    
    report = report_gen.create_report(
        target='all',
        provider='openai-compatible',
        base_url='https://api.openai.com',
        checks=checks,
        duration=2.5
    )
    
    assert report['version'] == '1.0.0', "Version should be 1.0.0"
    assert report['target'] == 'all', "Target should match"
    assert report['summary']['status'] == 'success', "Status should be success"
    assert 'streaming' in report['checks'], "Should have streaming check"
    
    # Test markdown generation
    md = report_gen.generate_markdown(report)
    assert '# AI Patch Report' in md, "Should have title"
    assert 'Streaming' in md, "Should have streaming section"
    assert 'Generated by AI Patch' in md, "Should have footer"
    
    print("✅ Report generation test passed")


def test_check_modules_import():
    """Test that all check modules can be imported"""
    print("Testing check module imports...")
    
    try:
        from checks import streaming, retries, cost, trace
        
        assert hasattr(streaming, 'check'), "streaming should have check function"
        assert hasattr(retries, 'check'), "retries should have check function"
        assert hasattr(cost, 'check'), "cost should have check function"
        assert hasattr(trace, 'check'), "trace should have check function"
        
        print("✅ Check module imports test passed")
    except ImportError as e:
        # If httpx or other dependencies are missing, just check files exist
        print(f"⚠️  Import error (missing dependencies): {e}")
        print("   Checking if check files exist instead...")
        
        checks_dir = Path(__file__).parent.parent / 'checks'
        check_files = ['streaming.py', 'retries.py', 'cost.py', 'trace.py']
        
        for check_file in check_files:
            assert (checks_dir / check_file).exists(), f"{check_file} should exist"
        
        print("✅ Check module files exist (dependencies not installed)")


def test_config_validation():
    """Test configuration validation"""
    print("Testing config validation...")
    
    # Valid config
    config = Config(
        base_url='https://api.openai.com',
        api_key='test-key',
        provider='openai-compatible',
        model='gpt-3.5-turbo'
    )
    
    assert config.is_valid(), "Valid config should pass validation"
    
    # Invalid config (missing API key)
    config_invalid = Config(
        base_url='https://api.openai.com',
        api_key='',
        provider='openai-compatible'
    )
    
    assert not config_invalid.is_valid(), "Invalid config should fail validation"
    assert 'OPENAI_API_KEY' in config_invalid.get_missing_vars(), "Should identify missing var"
    
    print("✅ Config validation test passed")


def run_all_tests():
    """Run all tests"""
    print("=" * 80)
    print("AI Patch Python CLI - Test Suite")
    print("=" * 80)
    print()
    
    tests = [
        test_config_auto_detect,
        test_report_generation,
        test_check_modules_import,
        test_config_validation
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
            print()
        except AssertionError as e:
            print(f"❌ Test failed: {e}")
            failed += 1
            print()
        except Exception as e:
            print(f"❌ Test error: {e}")
            failed += 1
            print()
    
    print("=" * 80)
    print(f"Results: {passed} passed, {failed} failed out of {len(tests)} tests")
    print("=" * 80)
    
    return failed == 0


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
