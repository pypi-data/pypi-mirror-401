Metadata-Version: 2.4
Name: datacheck-cli
Version: 0.3.1
Summary: Enterprise-grade data quality validation platform with CLI
License: MIT
License-File: LICENSE
Keywords: data-quality,validation,cli,data-engineering,enterprise
Author: datacheck
Author-email: hello@datacheck.com
Requires-Python: >=3.10,<4.0
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Provides-Extra: all
Provides-Extra: api
Provides-Extra: cloud
Provides-Extra: duckdb
Provides-Extra: ml
Provides-Extra: mssql
Provides-Extra: mysql
Provides-Extra: postgres
Provides-Extra: postgresql
Provides-Extra: streaming
Requires-Dist: azure-storage-blob (>=12.19.0,<13.0.0) ; extra == "cloud" or extra == "all"
Requires-Dist: boto3 (>=1.34.0,<2.0.0) ; extra == "cloud" or extra == "all"
Requires-Dist: click (>=8.1.0,<8.2.0)
Requires-Dist: confluent-kafka (>=2.3.0,<3.0.0) ; extra == "streaming" or extra == "all"
Requires-Dist: duckdb (>=0.8.1,<0.9.0) ; (platform_system != "Windows") and (extra == "duckdb" or extra == "all")
Requires-Dist: fastapi (>=0.115.0,<1.0.0) ; extra == "api" or extra == "all"
Requires-Dist: google-cloud-storage (>=2.14.0,<3.0.0) ; extra == "cloud" or extra == "all"
Requires-Dist: kafka-python (>=2.0.2,<3.0.0) ; extra == "streaming" or extra == "all"
Requires-Dist: mysql-connector-python (>=8.2.0,<9.0.0) ; extra == "mysql" or extra == "all"
Requires-Dist: pandas (>=2.0.0,<3.0.0)
Requires-Dist: psycopg2-binary (>=2.9.9,<3.0.0) ; extra == "postgresql" or extra == "postgres" or extra == "all"
Requires-Dist: pyarrow (>=22.0.0,<23.0.0)
Requires-Dist: pyodbc (>=5.0.1,<6.0.0) ; extra == "mssql" or extra == "all"
Requires-Dist: pyyaml (>=6.0,<7.0)
Requires-Dist: rich (>=13,<15)
Requires-Dist: scikit-learn (>=1.4.0,<2.0.0) ; extra == "ml" or extra == "all"
Requires-Dist: scipy (>=1.11.0,<2.0.0) ; extra == "ml" or extra == "all"
Requires-Dist: sqlalchemy (>=2.0.23,<3.0.0)
Requires-Dist: typer (>=0.12,<0.22)
Requires-Dist: uvicorn (>=0.25.0,<1.0.0) ; extra == "api" or extra == "all"
Project-URL: Homepage, https://github.com/yash-chauhan-dev/datacheck
Project-URL: Repository, https://github.com/yash-chauhan-dev/datacheck
Description-Content-Type: text/markdown

# DataCheck

**Lightweight data quality validation CLI tool with enterprise features**

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

DataCheck is a fast, CLI-first data validation tool for data engineers who need to validate data quality without heavy frameworks.

## Quick Start

```bash
# Install
pip install datacheck-cli

# Create validation rules (validation.yaml)
checks:
  - name: age_validation
    column: age
    rules:
      not_null: true
      min: 18
      max: 120

# Validate your data
datacheck validate data.csv --config validation.yaml
```

## Key Features

### Core Validation
- **Multiple formats**: CSV, Parquet, PostgreSQL, MySQL, SQL Server
- **Simple YAML config**: Easy to write and version control
- **Rich terminal output**: Beautiful, colorful validation results
- **CI/CD ready**: Proper exit codes (0=pass, 1=fail, 2+=errors)

### Enterprise Features (v0.2.0)
- **Database connectors**: Direct validation of PostgreSQL, MySQL, SQL Server tables
- **Custom plugins**: Write validation rules in Python, load dynamically
- **Row sampling**: Random, stratified, top-N sampling for large datasets
- **Parallel execution**: Multi-core processing for datasets with 10,000+ rows
- **Data profiling**: Comprehensive quality analysis with `datacheck profile`
- **Slack notifications**: Real-time validation results to Slack webhooks

### Validation Rules
- `not_null`: No missing values
- `min` / `max`: Numeric range validation
- `unique`: Detect duplicates
- `regex`: Pattern matching
- `allowed_values`: Whitelist validation

## Installation

```bash
# Basic installation
pip install datacheck-cli

# Install all features (recommended)
pip install datacheck-cli[all]

# Individual features
pip install datacheck-cli[postgresql]  # PostgreSQL
pip install datacheck-cli[mysql]       # MySQL
pip install datacheck-cli[mssql]       # SQL Server
```

**`[all]` includes**: PostgreSQL, MySQL, SQL Server, DuckDB (Linux/macOS), and all enterprise features.

## Usage Examples

### File Validation
```bash
# Basic validation
datacheck validate data.csv --config rules.yaml

# JSON output
datacheck validate data.csv --format json --output results.json

# Parallel execution for large files
datacheck validate large_data.csv --parallel --workers 4
```

### Database Validation
```bash
# PostgreSQL
datacheck validate "postgresql://user:pass@localhost/db" --table users

# MySQL with WHERE clause
datacheck validate "mysql://user:pass@localhost/db" --table orders \
  --where "created_at > '2024-01-01'"

# Custom SQL query
datacheck validate "postgresql://localhost/db" \
  --query "SELECT * FROM users WHERE active = true"
```

### Data Profiling
```bash
# Terminal output
datacheck profile data.csv

# JSON export
datacheck profile data.csv --output profile.json
```

### Row Sampling
```bash
# Random 10% sample
datacheck validate data.csv --sample-rate 0.1

# First 1000 rows
datacheck validate data.csv --top 1000

# Stratified sampling
datacheck validate data.csv --sample-count 5000 --stratify category
```

### Slack Notifications
```bash
datacheck validate data.csv \
  --slack-webhook https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

## Configuration Example

```yaml
# .datacheck.yaml
checks:
  - name: user_id_validation
    column: user_id
    rules:
      not_null: true
      unique: true

  - name: email_format
    column: email
    rules:
      not_null: true
      regex: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

  - name: age_range
    column: age
    rules:
      min: 18
      max: 120

  - name: status_values
    column: status
    rules:
      allowed_values: ["active", "inactive", "pending"]

# Optional: Custom plugins
plugins:
  - ./custom_rules.py

# Optional: Sampling
sampling:
  strategy: random
  rate: 0.1
  seed: 42
```

## Exit Codes

DataCheck uses standard exit codes for automation:

| Code | Meaning | Description |
|------|---------|-------------|
| `0` | Success | All validation rules passed |
| `1` | Failed | Some validation rules failed |
| `2` | Config Error | Configuration file error |
| `3` | Data Error | Data loading error |
| `4` | Runtime Error | Unexpected error |

## CI/CD Integration

### GitHub Actions
```yaml
- name: Validate Data
  run: |
    pip install datacheck-cli[all]
    datacheck validate data/export.csv --config validation.yaml
```

### GitLab CI
```yaml
validate:
  script:
    - pip install datacheck-cli[all]
    - datacheck validate data.csv --config rules.yaml
```

### Pre-commit Hook
```bash
#!/bin/bash
for file in $(git diff --cached --name-only | grep '\.csv$'); do
  datacheck validate "$file" || exit 1
done
```

## Custom Validation Rules

Create custom rules in Python:

```python
# custom_rules.py
from datacheck.plugins import rule

@rule("custom_email_domain")
def validate_email_domain(df, column, allowed_domains):
    """Validate email addresses are from allowed domains."""
    emails = df[column].dropna()
    domains = emails.str.split('@').str[1]
    invalid = ~domains.isin(allowed_domains)

    if invalid.any():
        failed_indices = df[invalid].index.tolist()
        return False, failed_indices
    return True, []
```

Use in config:
```yaml
plugins:
  - ./custom_rules.py

checks:
  - name: corporate_email
    column: email
    rules:
      custom_email_domain:
        allowed_domains: ["company.com", "corp.com"]
```

## Performance

- **Fast**: Validates 1M rows with 10 rules in ~2-3 seconds
- **Scalable**: Parallel processing for large datasets (10,000+ rows)
- **Efficient**: Uses pandas for optimized data operations

## Requirements

- Python 3.10+
- pandas 2.0+
- typer, rich, pyyaml, pyarrow

Optional:
- psycopg2 (PostgreSQL)
- mysql-connector-python (MySQL)
- pyodbc (SQL Server)

## Links

- **Documentation**: [https://yash-chauhan-dev.github.io/datacheck/](https://yash-chauhan-dev.github.io/datacheck/)
- **Source Code**: [https://github.com/yash-chauhan-dev/datacheck](https://github.com/yash-chauhan-dev/datacheck)
- **Issue Tracker**: [https://github.com/yash-chauhan-dev/datacheck/issues](https://github.com/yash-chauhan-dev/datacheck/issues)

## License

MIT License - see [LICENSE](https://github.com/yash-chauhan-dev/datacheck/blob/main/LICENSE) for details.

---

**Made for data engineers who value simplicity and speed.**

