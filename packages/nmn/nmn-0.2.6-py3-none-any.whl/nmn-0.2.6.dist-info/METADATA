Metadata-Version: 2.4
Name: nmn
Version: 0.2.6
Summary: Neural-Matter Network (NMN) - Advanced neural network layers with attention mechanisms
Project-URL: Homepage, https://github.com/mlnomadpy/nmn
Project-URL: Bug Tracker, https://github.com/mlnomadpy/nmn/issues
Author-email: Taha Bouhsine <taha@azetta.ai>
License-File: LICENSE
Classifier: License :: OSI Approved :: GNU Affero General Public License v3
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.8
Provides-Extra: all
Requires-Dist: flax>=0.7.0; extra == 'all'
Requires-Dist: jax>=0.4.0; extra == 'all'
Requires-Dist: tensorflow>=2.10.0; extra == 'all'
Requires-Dist: torch>=1.11.0; extra == 'all'
Requires-Dist: torchvision>=0.12.0; extra == 'all'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: flake8>=6.0.0; extra == 'dev'
Requires-Dist: isort>=5.12.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Provides-Extra: keras
Requires-Dist: tensorflow>=2.10.0; extra == 'keras'
Provides-Extra: linen
Requires-Dist: flax>=0.7.0; extra == 'linen'
Requires-Dist: jax>=0.4.0; extra == 'linen'
Provides-Extra: nnx
Requires-Dist: flax>=0.7.0; extra == 'nnx'
Requires-Dist: jax>=0.4.0; extra == 'nnx'
Provides-Extra: test
Requires-Dist: black>=23.0.0; extra == 'test'
Requires-Dist: flake8>=6.0.0; extra == 'test'
Requires-Dist: flax>=0.7.0; extra == 'test'
Requires-Dist: isort>=5.12.0; extra == 'test'
Requires-Dist: jax>=0.4.0; extra == 'test'
Requires-Dist: matplotlib>=3.5.0; extra == 'test'
Requires-Dist: mypy>=1.0.0; extra == 'test'
Requires-Dist: optax>=0.1.4; extra == 'test'
Requires-Dist: pytest-cov>=4.0.0; extra == 'test'
Requires-Dist: pytest>=7.0.0; extra == 'test'
Requires-Dist: scikit-learn>=1.1.0; extra == 'test'
Requires-Dist: seaborn>=0.11.0; extra == 'test'
Requires-Dist: tensorflow-datasets>=4.8.0; extra == 'test'
Requires-Dist: tensorflow>=2.10.0; extra == 'test'
Requires-Dist: torch>=1.11.0; extra == 'test'
Requires-Dist: torchvision>=0.12.0; extra == 'test'
Provides-Extra: tf
Requires-Dist: tensorflow>=2.10.0; extra == 'tf'
Provides-Extra: torch
Requires-Dist: torch>=1.11.0; extra == 'torch'
Requires-Dist: torchvision>=0.12.0; extra == 'torch'
Description-Content-Type: text/markdown

<p align="center">
  <img src="https://raw.githubusercontent.com/mlnomadpy/nmn/master/assets/logo.png" alt="NMN Logo" width="200" height="200" onerror="this.style.display='none'">
</p>

<h1 align="center">âš›ï¸ NMN â€” Neural Matter Networks</h1>

<p align="center">
  <strong>Not the neurons we want, but the neurons we need</strong>
</p>

<p align="center">
  <em>Activation-free neural layers that learn non-linearity through geometric operations</em>
</p>

<p align="center">
  <a href="https://pypi.org/project/nmn/"><img src="https://img.shields.io/pypi/v/nmn.svg?style=flat-square&color=blue" alt="PyPI version"></a>
  <a href="https://pepy.tech/project/nmn"><img src="https://static.pepy.tech/badge/nmn?style=flat-square" alt="Downloads"></a>
  <a href="https://github.com/mlnomadpy/nmn"><img src="https://img.shields.io/github/stars/mlnomadpy/nmn?style=flat-square&color=yellow" alt="GitHub stars"></a>
  <a href="https://github.com/mlnomadpy/nmn/actions/workflows/test.yml"><img src="https://img.shields.io/github/actions/workflow/status/mlnomadpy/nmn/test.yml?style=flat-square&label=tests" alt="Tests"></a>
  <a href="https://codecov.io/gh/mlnomadpy/nmn"><img src="https://img.shields.io/codecov/c/github/mlnomadpy/nmn?style=flat-square" alt="Coverage"></a>
  <a href="https://pypi.org/project/nmn/"><img src="https://img.shields.io/pypi/pyversions/nmn?style=flat-square" alt="Python"></a>
  <a href="https://pypi.org/project/nmn/"><img src="https://img.shields.io/pypi/l/nmn?style=flat-square&color=green" alt="License"></a>
</p>

---

## ğŸ¯ TL;DR

**NMN** replaces traditional `Linear + ReLU` with a single geometric operation that learns non-linearity without activation functions:

```python
# Traditional approach
y = relu(linear(x))  # dot product â†’ activation

# NMN approach  
y = yat(x)  # geometric operation with built-in non-linearity
```

The **Yat-Product** (âµŸ) balances *similarity* and *distance* to create inherently non-linear transformationsâ€”no activations needed.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”¥ **Activation-Free** | Learn complex non-linear relationships without ReLU, sigmoid, or tanh |
| ğŸŒ **Multi-Framework** | PyTorch, TensorFlow, Keras, Flax (Linen & NNX) |
| ğŸ§® **Geometric Foundation** | Based on distance-similarity tradeoff, not just correlations |
| âœ… **Cross-Framework Consistency** | Verified numerical equivalence across all frameworks |
| ğŸ§  **Complete Layer Suite** | Dense, Conv1D/2D/3D, ConvTranspose, Attention, RNN cells |
| âš¡ **Production Ready** | Comprehensive tests, CI/CD, high code coverage |

---

## ğŸ“ The Mathematics

### Yat-Product (âµŸ)

The core operation that powers NMN:

$$
âµŸ(\mathbf{w}, \mathbf{x}) = \frac{\langle \mathbf{w}, \mathbf{x} \rangle^2}{\|\mathbf{w} - \mathbf{x}\|^2 + \epsilon}
$$

<details>
<summary><strong>ğŸ” Geometric Interpretation (click to expand)</strong></summary>

Rewriting in terms of norms and angles:

$$
âµŸ(\mathbf{w}, \mathbf{x}) = \frac{\|\mathbf{w}\|^2 \|\mathbf{x}\|^2 \cos^2\theta}{\|\mathbf{w}\|^2 - 2\langle\mathbf{w}, \mathbf{x}\rangle + \|\mathbf{x}\|^2 + \epsilon}
$$

**Output is maximized when:**
- âœ… Vectors are **aligned** (small Î¸ â†’ large cosÂ²Î¸)
- âœ… Vectors are **close** (small Euclidean distance)
- âœ… Vectors have **large magnitude** (amplifies the signal)

**This creates a fundamentally different learning dynamic:**

| Traditional Neuron | Yat Neuron |
|-------------------|------------|
| Measures correlation only | Balances similarity AND proximity |
| Requires activation for non-linearity | Non-linearity is intrinsic |
| Can fire for distant but aligned vectors | Penalizes distance between w and x |

</details>

### Yat-Convolution (âµŸ*)

The same principle applied to local patches:

$$
âµŸ^*(\mathbf{W}, \mathbf{X}) = \frac{(\sum_{i,j} w_{ij} \cdot x_{ij})^2}{\sum_{i,j}(w_{ij} - x_{ij})^2 + \epsilon}
$$

Where **W** is the kernel and **X** is the input patch.

---

## ğŸš€ Quick Start

### Installation

```bash
pip install nmn

# Framework-specific installations
pip install "nmn[torch]"    # PyTorch
pip install "nmn[keras]"    # Keras/TensorFlow  
pip install "nmn[nnx]"      # Flax NNX (JAX)
pip install "nmn[all]"      # Everything
```

### Basic Usage

<table>
<tr>
<td width="50%">

**PyTorch**
```python
import torch
from nmn.torch.nmn import YatNMN

# Replace nn.Linear + activation
layer = YatNMN(
    in_features=128,
    out_features=64,
    epsilon=1e-5
)

x = torch.randn(32, 128)
y = layer(x)  # (32, 64) â€” non-linear output!
```

</td>
<td width="50%">

**Keras**
```python
import keras
from nmn.keras.nmn import YatNMN

# Drop-in replacement for Dense
layer = YatNMN(
    features=64,
    epsilon=1e-5
)

x = keras.ops.zeros((32, 128))
y = layer(x)  # (32, 64)
```

</td>
</tr>
<tr>
<td>

**Flax NNX**
```python
from flax import nnx
from nmn.nnx.nmn import YatNMN

layer = YatNMN(
    in_features=128,
    out_features=64,
    rngs=nnx.Rngs(0)
)

x = jax.numpy.zeros((32, 128))
y = layer(x)  # (32, 64)
```

</td>
<td>

**TensorFlow**
```python
import tensorflow as tf
from nmn.tf.nmn import YatNMN

layer = YatNMN(features=64)

x = tf.zeros((32, 128))
y = layer(x)  # (32, 64)
```

</td>
</tr>
</table>

---

## ğŸ“¦ Layer Support Matrix

### Core Layers

| Layer | PyTorch | TensorFlow | Keras | Flax NNX | Flax Linen |
|-------|:-------:|:----------:|:-----:|:--------:|:----------:|
| **YatNMN** (Dense) | âœ… | âœ… | âœ… | âœ… | âœ… |
| **YatConv1D** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **YatConv2D** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **YatConv3D** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **YatConvTranspose1D** | âœ… | âœ… | âœ… | âœ… | âŒ |
| **YatConvTranspose2D** | âœ… | âœ… | âœ… | âœ… | âŒ |
| **YatConvTranspose3D** | âœ… | âœ… | âŒ | âœ… | âŒ |

### Advanced Layers (Flax NNX)

| Layer | Status | Description |
|-------|:------:|-------------|
| **MultiHeadAttention** | âœ… | Yat-based attention mechanism |
| **YatSimpleCell** | âœ… | Simple RNN cell |
| **YatLSTMCell** | âœ… | LSTM with Yat operations |
| **YatGRUCell** | âœ… | GRU with Yat operations |
| **softermax** | âœ… | Generalized softmax: $\frac{x_k^n}{\epsilon + \sum_i x_i^n}$ |
| **softer_sigmoid** | âœ… | Smooth sigmoid variant |
| **soft_tanh** | âœ… | Smooth tanh variant |
| **DropConnect** | âœ… | Weight-level dropout regularization |

---

## ğŸ”¬ Cross-Framework Consistency

All implementations are verified to produce **numerically equivalent outputs** given identical inputs and weights:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cross-Framework Consistency Test               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Framework Pair          â”‚ Max Error    â”‚ Status            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PyTorch â†” TensorFlow    â”‚ < 1e-6       â”‚ âœ… PASS           â”‚
â”‚  PyTorch â†” Keras         â”‚ < 1e-6       â”‚ âœ… PASS           â”‚
â”‚  PyTorch â†” Flax NNX      â”‚ < 1e-6       â”‚ âœ… PASS           â”‚
â”‚  PyTorch â†” Flax Linen    â”‚ < 1e-6       â”‚ âœ… PASS           â”‚
â”‚  TensorFlow â†” Keras      â”‚ < 1e-7       â”‚ âœ… PASS           â”‚
â”‚  Flax NNX â†” Flax Linen   â”‚ < 1e-7       â”‚ âœ… PASS           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This demonstrates the **robustness of the geometric YAT formulation** across different numerical backends.

---

## ğŸ“š Examples

See **[EXAMPLES.md](EXAMPLES.md)** for comprehensive usage guides including:
- Framework-specific quick starts (PyTorch, Keras, TensorFlow, Flax)
- Architecture examples (CNN, Transformer, RNN)
- Advanced features (DropConnect, custom squashers, attention)

**Quick run:**

```bash
python examples/torch/yat_cifar10.py      # PyTorch CIFAR-10
python examples/keras/language_imdb.py    # Keras sentiment
python examples/nnx/language/mingpt.py    # JAX GPT
```

---

## ğŸ§ª Testing

Comprehensive test suite with **cross-framework validation**:

```bash
# Install test dependencies
pip install "nmn[test]"

# Run all tests
pytest tests/ -v

# Run specific framework
pytest tests/test_torch/ -v
pytest tests/test_keras/ -v
pytest tests/test_nnx/ -v

# Run cross-framework consistency tests
pytest tests/integration/test_cross_framework_consistency.py -v

# With coverage
pytest tests/ --cov=nmn --cov-report=html
```

### Test Structure

```
tests/
â”œâ”€â”€ test_torch/          # PyTorch layer tests + math validation
â”œâ”€â”€ test_keras/          # Keras layer tests
â”œâ”€â”€ test_tf/             # TensorFlow layer tests
â”œâ”€â”€ test_nnx/            # Flax NNX tests (attention, RNN, etc.)
â”œâ”€â”€ test_linen/          # Flax Linen tests
â””â”€â”€ integration/
    â”œâ”€â”€ test_cross_framework_consistency.py  # Numerical equivalence
    â””â”€â”€ test_compatibility.py                # API compatibility
```

---

## ğŸ“š Theoretical Foundation

Based on the research papers:

> **Deep Learning 2.0**: *Artificial Neurons that Matter â€” Reject Correlation, Embrace Orthogonality*
>
> **Deep Learning 2.1**: *Mind and Cosmos â€” Towards Cosmos-Inspired Interpretable Neural Networks*

### Why Yat-Product?

Traditional neurons compute: $y = \sigma(\mathbf{w}^\top \mathbf{x} + b)$

This has limitations:
- **Correlation-based**: Only measures alignment, ignores proximity
- **Requires activation**: Non-linearity is external
- **Spurious activations**: Can fire strongly for distant but aligned vectors

The Yat-Product addresses these by combining:
1. **Squared dot product** (similarity) in the numerator
2. **Squared distance** (proximity) in the denominator
3. **Epsilon** for numerical stability

The result is a neuron that responds **geometrically** â€” activated when inputs are both similar AND close to weights.

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Development setup
git clone https://github.com/mlnomadpy/nmn.git
cd nmn
pip install -e ".[dev,test]"

# Run tests
pytest tests/ -v

# Format code
black src/ tests/
isort src/ tests/
```

**Areas for contribution:**
- ğŸ› Bug fixes ([open issues](https://github.com/mlnomadpy/nmn/issues))
- âœ¨ New layer types (normalization, graph, etc.)
- ğŸ“š Documentation and tutorials
- âš¡ Performance optimizations
- ğŸ¨ Example applications

---

## ğŸ“– API Reference

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `in_features` | int | Input dimension (Dense) or channels (Conv) |
| `out_features` | int | Output dimension or filters |
| `kernel_size` | int \| tuple | Convolution kernel size |
| `epsilon` | float | Numerical stability (default: 1e-5) |
| `use_bias` | bool | Include bias term (default: True) |
| `use_alpha` | bool | Learnable output scaling (default: True) |

### Quick Imports

```python
# PyTorch
from nmn.torch.nmn import YatNMN
from nmn.torch.layers import YatConv2d, YatConvTranspose2d

# Keras / TensorFlow
from nmn.keras.nmn import YatNMN
from nmn.keras.conv import YatConv2D

# Flax NNX (most complete)
from nmn.nnx.nmn import YatNMN
from nmn.nnx.yatconv import YatConv
from nmn.nnx.yatattention import MultiHeadAttention
from nmn.nnx.rnn import YatLSTMCell
```

ğŸ“‹ Full import reference â†’ **[EXAMPLES.md](EXAMPLES.md#framework-imports-reference)**

---

## ğŸ“„ Citation

If you use NMN in your research, please cite:

```bibtex
@software{nmn2024,
  author = {Bouhsine, Taha},
  title = {NMN: Neural Matter Networks},
  year = {2024},
  url = {https://github.com/mlnomadpy/nmn}
}

@article{bouhsine2024dl2,
  author = {Taha Bouhsine},
  title = {Deep Learning 2.0: Artificial Neurons that Matter},
  year = {2024}
}

@article{bouhsine2025dl21,
  author = {Taha Bouhsine},
  title = {Deep Learning 2.1: Mind and Cosmos},
  year = {2025}
}

@article{bouhsine2025nomoredelulu,
  author = {Taha Bouhsine},
  title = {No More DeLuLu: A Kernel-Based Activation-Free Neural Networks},
  year = {2025}
}
```

---

## ğŸ“¬ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/mlnomadpy/nmn/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/mlnomadpy/nmn/discussions)
- ğŸ“§ **Contact**: taha@azetta.ai

---

## ğŸ“œ License

**AGPL-3.0** â€” Free for personal, academic, and commercial use with attribution.

If you modify and deploy on a network, you must share the source code.

For alternative licensing, contact us.

---

<p align="center">
  <sub>Built with â¤ï¸ by <a href="https://github.com/mlnomadpy">azetta.ai</a></sub>
</p>
