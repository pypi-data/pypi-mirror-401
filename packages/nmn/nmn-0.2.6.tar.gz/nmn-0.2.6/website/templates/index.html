<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Matter Networks ‚Äî Activation-Free Neural Computation</title>
    <meta name="description"
        content="The ‚µü-product: A kernel-based activation-free neural network architecture that unifies alignment and proximity for geometrically-aware computation.">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=JetBrains+Mono:wght@400;500;600&family=Space+Grotesk:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Tifinagh&display=swap" rel="stylesheet">

    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Three.js for 3D -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <!-- Navigation -->
    <nav class="main-nav">
        <div class="nav-container">
            <a href="#" class="nav-logo">
                <span class="yat-symbol">‚µü</span>
                <span>NMN</span>
            </a>
            <div class="nav-links">
                <a href="#introduction">Introduction</a>
                <a href="#yat-product">‚µü-Product</a>
                <a href="#visualizations">Visualizations</a>
                <a href="#results">Results</a>
                <a href="#theory" class="nav-blog">Blog</a>
                <a href="#code">Code</a>
                <a href="https://github.com/mlnomadpy/nmn" target="_blank" class="nav-github">
                    <svg viewBox="0 0 24 24" width="20" height="20" fill="currentColor">
                        <path
                            d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z" />
                    </svg>
                </a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-particles" id="particles"></div>
        <div class="hero-content">
            <h1 class="hero-title">
                <span class="title-line">No More DeLuLu</span>
                <span class="title-subtitle">A Kernel-Based Activation-Free Neural Networks</span>
            </h1>
            <div class="hero-equation">
                <div class="equation-box">
                    <span class="yat-symbol-large">‚µü</span>
                    <span class="equation-text">(<b>w</b>, <b>x</b>) = </span>
                    <span class="equation-frac">
                        <span class="frac-num">‚ü®<b>w</b>, <b>x</b>‚ü©¬≤</span>
                        <span class="frac-line"></span>
                        <span class="frac-den">‚Äñ<b>w</b> ‚àí <b>x</b>‚Äñ¬≤ + Œµ</span>
                    </span>
                </div>
            </div>
            <p class="hero-abstract">
                The <span class="yat-symbol">‚µü</span>-product unifies <em>alignment</em> and <em>proximity</em> in a
                single geometric operator,
                enabling neural networks without activation functions while maintaining universal approximation
                capabilities.
            </p>
            <div class="hero-actions">
                <a href="#visualizations" class="btn btn-primary">Explore Visualizations</a>
                <a href="https://github.com/mlnomadpy/nmn" class="btn btn-secondary" target="_blank">
                    <svg viewBox="0 0 24 24" width="18" height="18" fill="currentColor">
                        <path
                            d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z" />
                    </svg>
                    View on GitHub
                </a>
            </div>
            <div class="hero-stats">
                <div class="stat">
                    <span class="stat-value">15-25%</span>
                    <span class="stat-label">Memory Reduction</span>
                </div>
                <div class="stat">
                    <span class="stat-value">11.2%</span>
                    <span class="stat-label">Loss Improvement</span>
                </div>
                <div class="stat">
                    <span class="stat-value">0</span>
                    <span class="stat-label">Activation Functions</span>
                </div>
            </div>
        </div>
        <div class="scroll-indicator">
            <span>Scroll to explore</span>
            <div class="scroll-arrow"></div>
        </div>
    </header>

    <!-- Introduction Section -->
    <section id="introduction" class="section">
        <div class="container">
            <h2 class="section-title">The Problem with Traditional Neural Networks</h2>
            <div class="intro-grid">
                <div class="intro-text">
                    <p class="lead">
                        Modern neural networks rely on a paradigm that <strong>separates geometry from
                            non-linearity</strong>:
                        linear transformations (dot products) followed by element-wise activation functions (ReLU,
                        sigmoid).
                    </p>
                    <p>
                        Consider ReLU: it maps the entire spectrum of negative pre-activations‚Äîrepresenting varying
                        degrees
                        of dissimilarity‚Äîto a uniform zero, <em>discarding nuanced geometric relationships</em>.
                    </p>
                    <div class="problem-cards">
                        <div class="problem-card">
                            <div class="problem-icon">üìê</div>
                            <h4>Dot Product Limitation</h4>
                            <p>Captures alignment but ignores spatial proximity. Vectors can be aligned yet arbitrarily
                                far apart.</p>
                        </div>
                        <div class="problem-card">
                            <div class="problem-icon">üéØ</div>
                            <h4>Activation Information Loss</h4>
                            <p>ReLU collapses half-spaces to zero. Sigmoid saturates extremes. Geometric structure is
                                lost.</p>
                        </div>
                        <div class="problem-card">
                            <div class="problem-icon">üîÑ</div>
                            <h4>Architectural Complexity</h4>
                            <p>Requires normalization layers, attention mechanisms, and regularization to stabilize
                                training.</p>
                        </div>
                    </div>
                </div>
                <div class="intro-visual">
                    <div class="manifold-viz-container">
                        <h4>Topological Distortion by Activations</h4>
                        <img src="assets/images/manifold_distortion.png"
                            alt="Manifold distortion by activation functions" class="manifold-img">
                        <p class="caption">Activation functions can distort the geometric structure of the input data
                            manifold, leading to information loss.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- YAT Product Section -->
    <section id="yat-product" class="section section-dark">
        <div class="container">
            <h2 class="section-title">The <span class="yat-symbol">‚µü</span>-Product: Unifying Alignment & Proximity</h2>
            <div class="yat-intro">
                <p class="lead">
                    Inspired by <strong>inverse-square laws in physics</strong>, the <span
                        class="yat-symbol">‚µü</span>-product
                    creates a unified operator that captures both directional alignment and spatial proximity.
                </p>
            </div>

            <div class="equation-showcase">
                <div class="equation-main" id="yat-equation">
                    $$\text{‚µü}(\mathbf{w}, \mathbf{x}) = \frac{\langle \mathbf{w}, \mathbf{x} \rangle^2}{\|\mathbf{w} -
                    \mathbf{x}\|^2 + \epsilon}$$
                </div>
                <div class="equation-breakdown">
                    <div class="breakdown-item">
                        <div class="breakdown-formula">‚ü®<b>w</b>, <b>x</b>‚ü©¬≤</div>
                        <div class="breakdown-label">Numerator</div>
                        <div class="breakdown-desc">Squared dot product captures <strong>alignment</strong> between
                            vectors</div>
                    </div>
                    <div class="breakdown-divider">√∑</div>
                    <div class="breakdown-item">
                        <div class="breakdown-formula">‚Äñ<b>w</b> ‚àí <b>x</b>‚Äñ¬≤ + Œµ</div>
                        <div class="breakdown-label">Denominator</div>
                        <div class="breakdown-desc">Squared distance captures <strong>proximity</strong> with
                            inverse-square decay</div>
                    </div>
                </div>
            </div>

            <div class="properties-grid">
                <div class="property-card">
                    <div class="property-header">
                        <span class="property-icon">üéì</span>
                        <h4>Mercer Kernel</h4>
                    </div>
                    <p>Symmetric and positive semidefinite, connecting to established kernel theory.</p>
                    <div class="theorem-ref">Theorem 2.1</div>
                </div>
                <div class="property-card">
                    <div class="property-header">
                        <span class="property-icon">‚àû</span>
                        <h4>Universal Approximation</h4>
                    </div>
                    <p>NMNs are dense in C(ùí≥) under uniform norm on compact domains.</p>
                    <div class="theorem-ref">Theorem 2.4</div>
                </div>
                <div class="property-card">
                    <div class="property-header">
                        <span class="property-icon">üìâ</span>
                        <h4>Self-Regularization</h4>
                    </div>
                    <p>Responses remain bounded and gradients decay at infinity without explicit normalization.</p>
                    <div class="theorem-ref">Proposition 3.1</div>
                </div>
                <div class="property-card">
                    <div class="property-header">
                        <span class="property-icon">‚àá</span>
                        <h4>Stable Gradients</h4>
                    </div>
                    <p>Gradients vanish for distant inputs, providing natural localization during training.</p>
                    <div class="theorem-ref">Proposition 3.2</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Visualizations Section -->
    <!-- VISUALIZATIONS_PLACEHOLDER -->


    <!-- Architecture Section -->
    <section id="architecture" class="section section-dark">
        <div class="container">
            <h2 class="section-title">Neural Matter Network Architectures</h2>
            <div class="arch-intro">
                <p class="lead">
                    NMN layers serve as drop-in replacements for Linear + Activation, providing intrinsic non-linearity
                    through geometry.
                </p>
            </div>

            <div class="arch-cards">
                <div class="arch-card">
                    <div class="arch-header">
                        <h4>NMN Layer</h4>
                        <span class="arch-badge">Dense</span>
                    </div>
                    <div class="arch-equation">
                        $$h(\mathbf{x}) = s \cdot \sum_{i=1}^n \frac{(\mathbf{w}_i^\top\mathbf{x} +
                        b_i)^2}{\|\mathbf{w}_i - \mathbf{x}\|^2 + \epsilon}$$
                    </div>
                    <p>Replaces Linear + ReLU with a single geometric operation.</p>
                </div>
                <div class="arch-card">
                    <div class="arch-header">
                        <h4><span class="yat-symbol">‚µü</span>-Conv</h4>
                        <span class="arch-badge">Convolution</span>
                    </div>
                    <div class="arch-equation">
                        $$(\text{‚µü-Conv}(K, I))_{i,j} = \frac{\langle K, I_{i,j} \rangle^2}{\|K - I_{i,j}\|^2 +
                        \epsilon}$$
                    </div>
                    <p>Geometrically-aware feature extraction for spatial data.</p>
                </div>
                <div class="arch-card">
                    <div class="arch-header">
                        <h4><span class="yat-symbol">‚µü</span>-Attention</h4>
                        <span class="arch-badge">Transformer</span>
                    </div>
                    <div class="arch-equation">
                        $$\text{‚µü-Attn}(Q,K,V) = \text{softmax}(s \cdot Q \text{‚µü} K^T) V$$
                    </div>
                    <p>Query-key similarity through geometric alignment and proximity.</p>
                </div>
            </div>

            <div class="impl-table">
                <h3>Implemented Architectures</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Architecture</th>
                            <th>Base Model</th>
                            <th>Design</th>
                            <th>Key Change</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>AetherResNet</strong></td>
                            <td>ResNet</td>
                            <td><span class="yat-symbol">‚µü</span>-Conv ‚Üí Linear Conv per block</td>
                            <td>No activation functions</td>
                        </tr>
                        <tr>
                            <td><strong>AetherGPT</strong></td>
                            <td>GPT-2</td>
                            <td>MHA + NMN ‚Üí Linear</td>
                            <td>No normalization layers</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results" class="section">
        <div class="container">
            <h2 class="section-title">Experimental Results</h2>

            <div class="results-grid">
                <div class="results-block">
                    <h3>Vision Benchmarks</h3>
                    <table class="results-table">
                        <thead>
                            <tr>
                                <th>Architecture</th>
                                <th>CIFAR-10</th>
                                <th>CIFAR-100</th>
                                <th>STL-10</th>
                                <th>Tiny-ImageNet</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>ResNet-18</td>
                                <td><strong>94.23%</strong></td>
                                <td>72.15%</td>
                                <td>78.42%</td>
                                <td>56.89%</td>
                            </tr>
                            <tr class="highlight-row">
                                <td>Aether-ResNet-18</td>
                                <td>92.37%</td>
                                <td><strong>74.83%</strong></td>
                                <td><strong>80.91%</strong></td>
                                <td><strong>59.34%</strong></td>
                            </tr>
                            <tr>
                                <td>ViT-Small</td>
                                <td>91.78%</td>
                                <td>69.91%</td>
                                <td>75.13%</td>
                                <td><strong>52.76%</strong></td>
                            </tr>
                            <tr class="highlight-row">
                                <td>Aether-ViT-Small</td>
                                <td><strong>92.45%</strong></td>
                                <td><strong>70.58%</strong></td>
                                <td><strong>78.89%</strong></td>
                                <td>51.42%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="results-block">
                    <h3>Language Modeling (Fineweb 2.5B tokens)</h3>
                    <div class="lang-results">
                        <div class="lang-card">
                            <h4>GPT-2 Baseline</h4>
                            <div class="lang-stats">
                                <div class="lang-stat">
                                    <span class="stat-label">FP32 Loss</span>
                                    <span class="stat-value">2.43</span>
                                </div>
                                <div class="lang-stat">
                                    <span class="stat-label">BF16 Loss</span>
                                    <span class="stat-value">3.03</span>
                                </div>
                            </div>
                        </div>
                        <div class="lang-card highlight-card">
                            <h4>Aether-GPT2</h4>
                            <div class="lang-stats">
                                <div class="lang-stat">
                                    <span class="stat-label">FP32 Loss</span>
                                    <span class="stat-value better">2.29</span>
                                </div>
                                <div class="lang-stat">
                                    <span class="stat-label">BF16 Loss</span>
                                    <span class="stat-value better">2.69</span>
                                </div>
                            </div>
                            <div class="improvement">11.2% improvement in BF16</div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="training-curves">
                <h3>Training Dynamics</h3>
                <img src="assets/images/training_curves.png" alt="Training curves comparing Aether vs Linear baseline"
                    class="training-img">
                <p class="caption">Validation and training loss over 35k steps. Aether-GPT2 (blue) consistently
                    outperforms the linear baseline (green).</p>
            </div>

            <div class="mnist-prototypes">
                <h3>Learned Prototypes: Linear vs <span class="yat-symbol">‚µü</span>-Product</h3>
                <img src="assets/images/mnist_prototypes.png" alt="MNIST prototype comparison" class="prototypes-img">
                <p class="caption">Linear models produce diffuse, blurry prototypes. <span
                        class="yat-symbol">‚µü</span>-product neurons learn sharp, geometrically coherent digit
                    representations.</p>
            </div>
        </div>
    </section>

    <!-- Theory Blog Section -->
    <section id="theory" class="section section-dark">
        <div class="container">
            <h2 class="section-title">Mathematical Foundations</h2>
            <p class="section-subtitle">
                Explore the rigorous theoretical guarantees behind Neural Matter Networks.
                Each theorem builds upon the previous, creating a complete mathematical framework.
            </p>

            <!-- Blog Cards -->
            <div class="blog-cards">
                <div class="blog-card" data-modal="modal-mercer">
                    <span class="blog-card-num">1</span>
                    <h3 class="blog-card-title">The <span class="yat-symbol">‚µü</span>-Product is a Mercer Kernel</h3>
                    <p class="blog-card-preview">Discover why the ‚µü-product satisfies Mercer's condition, connecting it
                        to 50+ years of kernel method research.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">Kernel Theory</span>
                        <span class="blog-card-tag">PSD</span>
                        <span class="blog-card-tag">Foundation</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>

                <div class="blog-card" data-modal="modal-uat">
                    <span class="blog-card-num">2</span>
                    <h3 class="blog-card-title">Universal Approximation Theorem</h3>
                    <p class="blog-card-preview">Learn how NMNs can approximate any continuous function without losing
                        expressive power.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">UAT</span>
                        <span class="blog-card-tag">Expressiveness</span>
                        <span class="blog-card-tag">Density</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>

                <div class="blog-card" data-modal="modal-regulation">
                    <span class="blog-card-num">3</span>
                    <h3 class="blog-card-title">Self-Regulation & Bounded Outputs</h3>
                    <p class="blog-card-preview">Understand how the ‚µü-product naturally self-regulates without explicit
                        normalization.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">Stability</span>
                        <span class="blog-card-tag">Normalization</span>
                        <span class="blog-card-tag">ICS</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>

                <div class="blog-card" data-modal="modal-gradient">
                    <span class="blog-card-num">4</span>
                    <h3 class="blog-card-title">Stable Learning & Gradient Localization</h3>
                    <p class="blog-card-preview">Explore how gradients vanish for distant inputs, providing natural
                        localization during training.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">Gradients</span>
                        <span class="blog-card-tag">Lipschitz</span>
                        <span class="blog-card-tag">C‚àû</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>

                <div class="blog-card" data-modal="modal-info">
                    <span class="blog-card-num">5</span>
                    <h3 class="blog-card-title">Information-Geometric Foundations</h3>
                    <p class="blog-card-preview">See how the ‚µü-product bridges Euclidean geometry and information
                        theory.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">KL Divergence</span>
                        <span class="blog-card-tag">Entropy</span>
                        <span class="blog-card-tag">Duality</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>

                <div class="blog-card" data-modal="modal-topology">
                    <span class="blog-card-num">6</span>
                    <h3 class="blog-card-title">Topological Organization: Neural Fiber Bundles</h3>
                    <p class="blog-card-preview">Discover the vortex-like territorial fields created by ‚µü-product
                        classification.</p>
                    <div class="blog-card-tags">
                        <span class="blog-card-tag">Topology</span>
                        <span class="blog-card-tag">Fiber Bundles</span>
                        <span class="blog-card-tag">Vortex</span>
                    </div>
                    <div class="blog-card-cta">
                        <span>Read more</span>
                        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                            stroke-width="2">
                            <path d="M5 12h14M12 5l7 7-7 7" />
                        </svg>
                    </div>
                </div>
            </div>

            <!-- Theorem 1: Mercer Kernel -->
            <article id="thm-mercer" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Theorem 1</span>
                    <h3 class="theorem-title">The <span class="yat-symbol">‚µü</span>-Product is a Mercer Kernel</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Theorem (Mercer's Condition):</strong> The kernel
                        $k_{\text{‚µü}}(\mathbf{x}, \mathbf{w}) = \frac{(\mathbf{x} \cdot \mathbf{w})^2}{\|\mathbf{x} -
                        \mathbf{w}\|^2 + \varepsilon}$
                        is symmetric and positive semi-definite, hence a valid Mercer kernel on $\mathbb{R}^d$.
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        A <strong>Mercer kernel</strong> is a special type of similarity function that has two critical
                        properties:
                    </p>
                    <ul class="theorem-list">
                        <li><strong>Symmetry:</strong> $k(\mathbf{x}, \mathbf{w}) = k(\mathbf{w}, \mathbf{x})$ ‚Äî the
                            similarity between x and w is the same as between w and x.</li>
                        <li><strong>Positive Semi-Definiteness (PSD):</strong> For any set of points, the kernel matrix
                            has all non-negative eigenvalues.</li>
                    </ul>

                    <h4>Why Is This Important?</h4>
                    <p>
                        Being a Mercer kernel means the <span class="yat-symbol">‚µü</span>-product implicitly computes an
                        inner product
                        in a <strong>high-dimensional feature space</strong> without ever explicitly computing that
                        space. This is the famous
                        <em>"kernel trick"</em> from machine learning theory.
                    </p>

                    <div class="insight-box">
                        <span class="insight-icon">üí°</span>
                        <div>
                            <strong>Key Insight:</strong> The <span class="yat-symbol">‚µü</span>-product is a <em>product
                                of two PSD kernels</em>:
                            the squared dot product $(\mathbf{x} \cdot \mathbf{w})^2$ (a polynomial kernel) and the
                            inverse multiquadric
                            $\frac{1}{\|\mathbf{x} - \mathbf{w}\|^2 + \varepsilon}$ (an RBF-like kernel). By the Schur
                            product theorem,
                            their product is also PSD.
                        </div>
                    </div>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>Theoretical Foundation:</strong> Connects NMNs to 50+ years of kernel method
                            research (SVMs, Gaussian Processes, etc.)</li>
                        <li><strong>Reproducing Kernel Hilbert Space (RKHS):</strong> Guarantees existence of a rich
                            function space for learning</li>
                        <li><strong>Optimization Guarantees:</strong> Many kernel-based optimization results apply
                            directly</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <span class="nav-label">Foundation for:</span>
                    <a href="#thm-uat" class="nav-next">Universal Approximation ‚Üí</a>
                </div>
            </article>

            <!-- Theorem 2: Universal Approximation -->
            <article id="thm-uat" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Theorem 2</span>
                    <h3 class="theorem-title">Universal Approximation Theorem</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Theorem:</strong> Let $\mathcal{X} \subset \mathbb{R}^d$ be compact. The class of
                        single-hidden-layer
                        <span class="yat-symbol">‚µü</span>-product networks
                        $f(\mathbf{x}) = \sum_{i=1}^n \alpha_i \cdot g(\mathbf{x}; \mathbf{w}_i, b_i) + c$
                        is <em>dense</em> in $C(\mathcal{X})$ under the uniform norm. That is, NMNs can approximate any
                        continuous function
                        to arbitrary precision.
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        The Universal Approximation Theorem (UAT) is the <strong>fundamental existence theorem</strong>
                        for neural networks.
                        It guarantees that given enough neurons, a network can approximate any continuous function as
                        closely as desired.
                    </p>
                    <p>
                        This theorem proves that <strong>NMNs lose no expressive power</strong> by removing activation
                        functions.
                        The <span class="yat-symbol">‚µü</span>-product's inherent non-linearity is sufficient.
                    </p>

                    <h4>The Proof Strategy</h4>
                    <div class="proof-sketch">
                        <p>The proof is elegant and leverages the kernel structure:</p>
                        <ol>
                            <li><strong>Recover IMQ kernel:</strong> By differentiating $g(\mathbf{x}; \mathbf{w}, b)$
                                twice with respect to bias $b$,
                                we recover the inverse multiquadric (IMQ) kernel: $\partial_b^2 g =
                                \frac{2}{\|\mathbf{x} - \mathbf{w}\|^2 + \varepsilon}$</li>
                            <li><strong>Fourier analysis:</strong> The IMQ kernel has a strictly positive Fourier
                                transform (Bessel function)</li>
                            <li><strong>Uniqueness:</strong> Any measure orthogonal to all IMQ translates must be zero
                            </li>
                            <li><strong>Density:</strong> By Hahn-Banach/Riesz duality, the span is dense in
                                $C(\mathcal{X})$</li>
                        </ol>
                    </div>

                    <div class="insight-box">
                        <span class="insight-icon">üî¨</span>
                        <div>
                            <strong>Key Insight:</strong> The bias term $b$ is crucial ‚Äî it allows the network to
                            "shift" response fields
                            and span the entire function space through differentiation. This is why we use
                            $(\mathbf{w}^\top\mathbf{x} + b)^2$ rather than just $(\mathbf{w}^\top\mathbf{x})^2$.
                        </div>
                    </div>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>No Power Loss:</strong> NMNs are as expressive as ReLU/Sigmoid networks</li>
                        <li><strong>Geometric Localization:</strong> Unlike ReLU (unbounded growth), the <span
                                class="yat-symbol">‚µü</span>-product achieves density through <em>localized</em>
                            geometric units</li>
                        <li><strong>Practical Corollary:</strong> Single hidden layer is sufficient in theory ‚Äî though
                            deeper networks may learn more efficiently</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <a href="#thm-mercer" class="nav-prev">‚Üê Mercer Kernel</a>
                    <span class="nav-label">Enables training via:</span>
                    <a href="#thm-self-regulation" class="nav-next">Self-Regulation ‚Üí</a>
                </div>
            </article>

            <!-- Theorem 3: Self-Regulation -->
            <article id="thm-self-regulation" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Proposition 3</span>
                    <h3 class="theorem-title">Self-Regulation & Bounded Outputs</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Proposition:</strong> For any fixed weight vector $\mathbf{w}$, the <span
                            class="yat-symbol">‚µü</span>-product output
                        remains bounded and converges as $\|\mathbf{x}\| \to \infty$:
                        $$\lim_{\|\mathbf{x}\| \to \infty} \text{‚µü}(\mathbf{w}, \mathbf{x}) = \|\mathbf{w}\|^2
                        \cos^2\theta$$
                        where $\theta$ is the angle between $\mathbf{w}$ and the direction of $\mathbf{x}$.
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        Unlike ReLU which can grow unboundedly with input magnitude, or dot products which scale
                        linearly,
                        the <span class="yat-symbol">‚µü</span>-product <strong>naturally self-regulates</strong>. As
                        inputs get very large,
                        the output converges to a finite value that depends only on the <em>direction</em>, not
                        magnitude.
                    </p>

                    <h4>Corollary: Dimensional Self-Normalization</h4>
                    <div class="corollary-box">
                        <strong>Corollary:</strong> At initialization with random i.i.d. weights, both the numerator
                        $(\mathbf{w}^\top\mathbf{x})^2$ and denominator $\|\mathbf{w} - \mathbf{x}\|^2$ scale as
                        $\mathcal{O}(d)$
                        with dimension $d$, so their ratio remains $\mathcal{O}(1)$.
                    </div>
                    <p>
                        This means NMNs don't need careful initialization schemes like Xavier or He ‚Äî they're
                        <strong>dimensionally stable</strong> by design!
                    </p>

                    <h4>Corollary: Mitigating Internal Covariate Shift</h4>
                    <div class="corollary-box">
                        <strong>Corollary:</strong> As input magnitudes grow large, the mean and variance of neuron
                        activations
                        across a batch become <em>independent of input magnitudes</em>, depending only on angular
                        distribution.
                    </div>
                    <p>
                        This explains why NMNs can often operate <strong>without batch normalization</strong> ‚Äî
                        the "covariate shift" problem that plagues deep networks is naturally mitigated.
                    </p>

                    <div class="insight-box">
                        <span class="insight-icon">‚ö°</span>
                        <div>
                            <strong>Practical Impact:</strong> No gradient explosion from large inputs.
                            No need for gradient clipping in most cases. Simpler, more stable training dynamics.
                        </div>
                    </div>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>No Exploding Activations:</strong> Outliers don't cause numerical instabilities</li>
                        <li><strong>Reduced Normalization Needs:</strong> Less reliance on BatchNorm, LayerNorm</li>
                        <li><strong>Memory Efficiency:</strong> 15-25% reduction from eliminating normalization layers
                        </li>
                        <li><strong>Robust Training:</strong> Stable even with varying input distributions</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <a href="#thm-uat" class="nav-prev">‚Üê Universal Approximation</a>
                    <span class="nav-label">Gradient behavior:</span>
                    <a href="#thm-stable-gradient" class="nav-next">Stable Gradients ‚Üí</a>
                </div>
            </article>

            <!-- Theorem 4: Stable Gradients -->
            <article id="thm-stable-gradient" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Proposition 4</span>
                    <h3 class="theorem-title">Stable Learning & Gradient Localization</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Proposition:</strong> The gradient of the <span class="yat-symbol">‚µü</span>-product with
                        respect to input vanishes for distant inputs:
                        $$\lim_{\|\mathbf{x}\| \to \infty} \|\nabla_{\mathbf{x}} \text{‚µü}(\mathbf{w}, \mathbf{x})\| =
                        0$$
                        Specifically, $\|\nabla_{\mathbf{x}} \text{‚µü}\| \sim \mathcal{O}(1/k)$ as $\|\mathbf{x}\| = k
                        \to \infty$.
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        Each <span class="yat-symbol">‚µü</span>-product neuron creates a <strong>localized learning
                            region</strong>.
                        Points far from the weight vector contribute vanishingly small gradients. This is fundamentally
                        different from:
                    </p>
                    <ul class="theorem-list">
                        <li><strong>Linear neurons:</strong> Gradients are constant regardless of distance (no
                            localization)</li>
                        <li><strong>ReLU neurons:</strong> Gradients are either constant (positive side) or zero
                            (negative side)</li>
                    </ul>

                    <h4>Regularity Properties</h4>
                    <div class="dual-box">
                        <div class="property-mini">
                            <h5>Analyticity (Lemma)</h5>
                            <p>The <span class="yat-symbol">‚µü</span>-product is $C^\infty$ ‚Äî infinitely differentiable.
                                Perfect for physics-informed neural networks (PINNs) where you need higher-order
                                derivatives.</p>
                        </div>
                        <div class="property-mini">
                            <h5>Lipschitz Continuity (Proposition)</h5>
                            <p>The kernel is globally Lipschitz continuous:
                                $|K(\mathbf{w}, \mathbf{x}) - K(\mathbf{w}, \mathbf{y})| \leq L \|\mathbf{x} -
                                \mathbf{y}\|$.
                                This bounds how fast outputs can change.</p>
                        </div>
                    </div>

                    <div class="insight-box">
                        <span class="insight-icon">üéØ</span>
                        <div>
                            <strong>Geometric Intuition:</strong> Each neuron is like a "gravity well" ‚Äî it strongly
                            attracts nearby
                            points but has negligible influence on distant ones. During training, each neuron learns to
                            "own"
                            a local region of the input space.
                        </div>
                    </div>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>Outlier Robustness:</strong> Distant outliers don't cause large, destabilizing
                            gradient updates</li>
                        <li><strong>Local Learning:</strong> Each neuron specializes in its region without interfering
                            with others</li>
                        <li><strong>Smoother Optimization:</strong> Lipschitz bounds provide theoretical guarantees on
                            loss landscape</li>
                        <li><strong>PINN-Friendly:</strong> $C^\infty$ smoothness enables any-order derivative
                            computation</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <a href="#thm-self-regulation" class="nav-prev">‚Üê Self-Regulation</a>
                    <span class="nav-label">Connects to probability:</span>
                    <a href="#thm-information" class="nav-next">Information Theory ‚Üí</a>
                </div>
            </article>

            <!-- Theorem 5: Information Theory -->
            <article id="thm-information" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Theorems 5-6</span>
                    <h3 class="theorem-title">Information-Geometric Foundations</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Theorem (Minimal Similarity):</strong> For probability distributions $\mathbf{p},
                        \mathbf{q} \in \Delta^{n-1}$:
                        $$\text{‚µü}(\mathbf{p}, \mathbf{q}) = 0 \iff \text{supp}(\mathbf{p}) \cap \text{supp}(\mathbf{q})
                        = \emptyset$$
                        Furthermore, disjoint support implies $\text{KL}(\mathbf{p} \| \mathbf{q}) = \infty$.
                    </div>
                    <div class="theorem-box" style="margin-top: 16px;">
                        <strong>Theorem (Maximal Similarity):</strong> $\text{‚µü}(\mathbf{p}, \mathbf{q}) \to \infty \iff
                        \mathbf{p} = \mathbf{q}$,
                        and this corresponds to $\text{KL}(\mathbf{p} \| \mathbf{q}) = 0$.
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        When the <span class="yat-symbol">‚µü</span>-product is applied to <strong>probability
                            distributions</strong>
                        (like softmax outputs), it exhibits deep connections to information theory. The extremes of the
                        <span class="yat-symbol">‚µü</span>-product correspond precisely to extremes in KL divergence!
                    </p>

                    <div class="connection-diagram">
                        <div class="connection-item">
                            <div class="connection-value">‚µü = 0</div>
                            <div class="connection-arrow">‚ü∫</div>
                            <div class="connection-meaning">
                                <strong>Disjoint Support</strong><br>
                                Distributions never overlap<br>
                                KL divergence = ‚àû
                            </div>
                        </div>
                        <div class="connection-item">
                            <div class="connection-value">‚µü ‚Üí ‚àû</div>
                            <div class="connection-arrow">‚ü∫</div>
                            <div class="connection-meaning">
                                <strong>Identical Distributions</strong><br>
                                p = q everywhere<br>
                                KL divergence = 0
                            </div>
                        </div>
                    </div>

                    <h4>Duality of Orthogonality Concepts</h4>
                    <p>
                        The <span class="yat-symbol">‚µü</span>-product unifies three distinct notions of "orthogonality":
                    </p>
                    <table class="orthogonality-table">
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Condition</th>
                                <th>Interpretation</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Euclidean</td>
                                <td>$\mathbf{p} \cdot \mathbf{q} = 0$</td>
                                <td>Vectors perpendicular in space</td>
                            </tr>
                            <tr>
                                <td>Combinatorial</td>
                                <td>$\text{supp}(\mathbf{p}) \cap \text{supp}(\mathbf{q}) = \emptyset$</td>
                                <td>No shared non-zero entries</td>
                            </tr>
                            <tr>
                                <td>Information-Theoretic</td>
                                <td>$\text{KL}(\mathbf{p} \| \mathbf{q}) = \infty$</td>
                                <td>Infinite surprise/divergence</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>
                        <strong>All three are equivalent</strong> when $\text{‚µü}(\mathbf{p}, \mathbf{q}) = 0$!
                    </p>

                    <div class="insight-box">
                        <span class="insight-icon">üåâ</span>
                        <div>
                            <strong>Bridge Between Worlds:</strong> The <span class="yat-symbol">‚µü</span>-product acts
                            as a bridge
                            connecting Euclidean geometry (dot products, distances) with probabilistic reasoning (KL
                            divergence, entropy).
                            This explains why NMNs work well with cross-entropy loss ‚Äî there's deep mathematical
                            harmony.
                        </div>
                    </div>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>Natural Compatibility:</strong> Works harmoniously with entropy-based loss functions
                        </li>
                        <li><strong>Interpretable Outputs:</strong> Similarity scores have information-theoretic meaning
                        </li>
                        <li><strong>Theoretical Unity:</strong> Unifies geometric and probabilistic perspectives in one
                            framework</li>
                        <li><strong>Distribution Modeling:</strong> Natural fit for generative models and density
                            estimation</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <a href="#thm-stable-gradient" class="nav-prev">‚Üê Stable Gradients</a>
                    <span class="nav-label">Space structure:</span>
                    <a href="#thm-topology" class="nav-next">Topology ‚Üí</a>
                </div>
            </article>

            <!-- Theorem 6: Topology -->
            <article id="thm-topology" class="theorem-post">
                <div class="theorem-header">
                    <span class="theorem-badge">Theorem 7</span>
                    <h3 class="theorem-title">Topological Organization: Neural Fiber Bundles</h3>
                </div>

                <div class="theorem-statement">
                    <div class="theorem-box">
                        <strong>Theorem:</strong> Let $\mathcal{M} \subset \mathbb{R}^d$ be a smooth compact data
                        manifold
                        and $\{w_c\}_{c=1}^C$ be class prototypes. The classification rule
                        $\hat{c}(x) = \arg\max_c \text{‚µü}(w_c, x)$ partitions $\mathcal{M}$ into decision regions.

                        <br><br><strong>Separation Property:</strong> If prototypes are orthogonal ($\langle w_i, w_j
                        \rangle = 0$ for $i \neq j$), then:
                        <ol>
                            <li>Prototypes are maximally dissimilar: $\text{‚µü}(w_i, w_j) = 0$</li>
                            <li>Decision boundaries are spatially separated from prototype cores</li>
                        </ol>
                    </div>
                </div>

                <div class="theorem-content">
                    <h4>What Does This Mean?</h4>
                    <p>
                        Traditional linear classifiers create <strong>hyperplane decision boundaries</strong> ‚Äî flat
                        cuts through space
                        that extend infinitely. The <span class="yat-symbol">‚µü</span>-product creates something
                        fundamentally different:
                        <strong>vortex-like territorial fields</strong> around each prototype.
                    </p>

                    <div class="visual-comparison">
                        <div class="visual-item">
                            <div class="visual-diagram linear-diagram">
                                <div class="half-space left">Class A</div>
                                <div class="boundary-line"></div>
                                <div class="half-space right">Class B</div>
                            </div>
                            <p><strong>Linear:</strong> Unbounded half-spaces</p>
                        </div>
                        <div class="visual-item">
                            <div class="visual-diagram yat-diagram">
                                <div class="vortex v1">A</div>
                                <div class="vortex v2">B</div>
                                <div class="vortex v3">C</div>
                            </div>
                            <p><strong>‚µü-Product:</strong> Localized vortex territories</p>
                        </div>
                    </div>

                    <h4>Why Orthogonal Prototypes?</h4>
                    <p>
                        When class prototypes are orthogonal, something magical happens:
                    </p>
                    <ul class="theorem-list">
                        <li>Each prototype's "core" (where response is maximal) is <strong>completely separate</strong>
                            from other prototypes</li>
                        <li>At point $x = w_i$: the response to class $i$ is $\|w_i\|^4/\varepsilon$ (very large), while
                            response to class $j$ is <strong>exactly 0</strong></li>
                        <li>Decision boundaries are forced into "neutral zones" between prototypes</li>
                    </ul>

                    <div class="insight-box">
                        <span class="insight-icon">üåÄ</span>
                        <div>
                            <strong>Vortex Intuition:</strong> Imagine each prototype as a whirlpool. Points are "sucked
                            in" toward their
                            nearest prototype with strength proportional to alignment and inverse-square of distance.
                            The decision boundary
                            is where the "pull" from two prototypes exactly balances.
                        </div>
                    </div>

                    <h4>Fiber Bundle Structure</h4>
                    <p>
                        For those familiar with differential geometry: the classification can be viewed through the lens
                        of
                        <strong>fiber bundles</strong>. The data manifold $\mathcal{M}$ is the base space, each point
                        $x$ has a
                        "fiber" of response values $[\text{‚µü}(w_1, x), ..., \text{‚µü}(w_C, x)]$, and classification is
                        projection
                        to the dominant fiber component.
                    </p>

                    <h4>What Does This Offer?</h4>
                    <ul class="benefits-list">
                        <li><strong>Geometric Interpretability:</strong> Decision regions have intuitive spatial meaning
                        </li>
                        <li><strong>Natural Clustering:</strong> Each class "owns" a territory, not just a half-space
                        </li>
                        <li><strong>Robust Boundaries:</strong> Orthogonal prototypes guarantee maximum class separation
                        </li>
                        <li><strong>Manifold-Aware:</strong> Works naturally with curved data manifolds, not just flat
                            spaces</li>
                    </ul>
                </div>

                <div class="theorem-nav-links">
                    <a href="#thm-information" class="nav-prev">‚Üê Information Theory</a>
                    <span class="nav-label">Full circle to:</span>
                    <a href="#thm-mercer" class="nav-next">Mercer Kernel (start) ‚Üí</a>
                </div>
            </article>

            <!-- Summary Card -->
            <div class="theory-summary">
                <h3>The Complete Picture</h3>
                <p>These six theorem groups form a coherent mathematical framework:</p>
                <div class="summary-flow">
                    <div class="summary-item">
                        <span class="summary-num">1</span>
                        <span class="summary-text"><strong>Mercer:</strong> Valid kernel ‚üπ rich function space</span>
                    </div>
                    <div class="summary-arrow">‚Üì</div>
                    <div class="summary-item">
                        <span class="summary-num">2</span>
                        <span class="summary-text"><strong>UAT:</strong> Can approximate any function</span>
                    </div>
                    <div class="summary-arrow">‚Üì</div>
                    <div class="summary-item">
                        <span class="summary-num">3</span>
                        <span class="summary-text"><strong>Self-Regulation:</strong> Bounded, stable outputs</span>
                    </div>
                    <div class="summary-arrow">‚Üì</div>
                    <div class="summary-item">
                        <span class="summary-num">4</span>
                        <span class="summary-text"><strong>Stable Gradients:</strong> Trainable via localized
                            learning</span>
                    </div>
                    <div class="summary-arrow">‚Üì</div>
                    <div class="summary-item">
                        <span class="summary-num">5</span>
                        <span class="summary-text"><strong>Info Theory:</strong> Works with entropy losses</span>
                    </div>
                    <div class="summary-arrow">‚Üì</div>
                    <div class="summary-item">
                        <span class="summary-num">6</span>
                        <span class="summary-text"><strong>Topology:</strong> Geometric decision boundaries</span>
                    </div>
                </div>
                <p class="summary-conclusion">
                    Together, these guarantee that NMNs are <em>theoretically sound</em>, <em>practically
                        trainable</em>,
                    and <em>geometrically interpretable</em> ‚Äî without a single activation function.
                </p>
            </div>
        </div>
    </section>

    <!-- Code Section -->
    <section id="code" class="section section-dark">
        <div class="container">
            <h2 class="section-title">Quick Start</h2>

            <div class="install-block">
                <h3>Installation</h3>
                <div class="code-block">
                    <pre><code class="language-bash">pip install nmn

# Framework-specific
pip install "nmn[torch]"    # PyTorch
pip install "nmn[keras]"    # Keras/TensorFlow
pip install "nmn[nnx]"      # Flax NNX (JAX)
pip install "nmn[all]"      # Everything</code></pre>
                </div>
            </div>

            <div class="code-examples">
                <div class="code-tabs">
                    <button class="code-tab active" data-tab="pytorch">PyTorch</button>
                    <button class="code-tab" data-tab="keras">Keras</button>
                    <button class="code-tab" data-tab="jax">JAX/Flax</button>
                </div>
                <div class="code-panels">
                    <div class="code-panel active" id="panel-pytorch">
                        <pre><code class="language-python">import torch
from nmn.torch.nmn import YatNMN

# Replace nn.Linear + activation with single layer
layer = YatNMN(
    in_features=128,
    out_features=64,
    epsilon=1e-5
)

x = torch.randn(32, 128)
y = layer(x)  # (32, 64) ‚Äî inherently non-linear!</code></pre>
                    </div>
                    <div class="code-panel" id="panel-keras">
                        <pre><code class="language-python">import keras
from nmn.keras.nmn import YatNMN

# Drop-in replacement for Dense + activation
layer = YatNMN(
    features=64,
    epsilon=1e-5
)

x = keras.ops.zeros((32, 128))
y = layer(x)  # (32, 64)</code></pre>
                    </div>
                    <div class="code-panel" id="panel-jax">
                        <pre><code class="language-python">import jax.numpy as jnp
from flax import nnx
from nmn.nnx.nmn import YatNMN

layer = YatNMN(
    in_features=128,
    out_features=64,
    rngs=nnx.Rngs(0)
)

x = jnp.zeros((32, 128))
y = layer(x)  # (32, 64)</code></pre>
                    </div>
                </div>
            </div>

            <div class="layer-support">
                <h3>Available Layers</h3>
                <div class="layers-grid">
                    <div class="layer-item">
                        <span class="layer-name">YatNMN</span>
                        <span class="layer-type">Dense</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatConv1D</span>
                        <span class="layer-type">Conv</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatConv2D</span>
                        <span class="layer-type">Conv</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatConv3D</span>
                        <span class="layer-type">Conv</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatConvTranspose</span>
                        <span class="layer-type">Conv</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">MultiHeadAttention</span>
                        <span class="layer-type">Attention</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatLSTMCell</span>
                        <span class="layer-type">RNN</span>
                    </div>
                    <div class="layer-item">
                        <span class="layer-name">YatGRUCell</span>
                        <span class="layer-type">RNN</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation" class="section">
        <div class="container">
            <h2 class="section-title">Citation</h2>
            <div class="citation-block">
                <pre><code>@article{bouhsine2025nomoredelulu,
  author = {Taha Bouhsine},
  title = {No More DeLuLu: A Kernel-Based Activation-Free Neural Networks},
  year = {2025},
  url = {https://github.com/mlnomadpy/nmn}
}</code></pre>
                <button class="copy-btn" onclick="copyBibtex()">
                    <svg viewBox="0 0 24 24" width="18" height="18" fill="currentColor">
                        <path
                            d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z" />
                    </svg>
                    Copy
                </button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <span class="yat-symbol">‚µü</span>
                    <span>Neural Matter Networks</span>
                </div>
                <div class="footer-links">
                    <a href="https://github.com/mlnomadpy/nmn" target="_blank">GitHub</a>
                    <a href="https://pypi.org/project/nmn/" target="_blank">PyPI</a>
                    <a href="mailto:taha@azetta.ai">Contact</a>
                </div>
                <div class="footer-copy">
                    <p>Built with ‚ù§Ô∏è by <a href="https://azetta.ai" target="_blank">azetta.ai</a></p>
                    <p>AGPL-3.0 License</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- BLOG_MODALS_PLACEHOLDER -->
    <!-- Scripts -->
    <script src="js/math-utils.js"></script>
    <script src="js/heatmap-viz.js"></script>
    <script src="js/gradient-viz.js"></script>
    <script src="js/xor-demo.js"></script>
    <script src="js/decision-boundary.js"></script>
    <script src="js/loss-landscape.js"></script>
    <script src="js/main.js"></script>
</body>

</html>