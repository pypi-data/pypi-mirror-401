{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/nmn/docs/intro","label":"Introduction to Neural Matter Networks","docId":"intro","unlisted":false},{"type":"category","label":"Getting Started","items":[{"type":"link","href":"/nmn/docs/installation","label":"Installation","docId":"installation","unlisted":false},{"type":"link","href":"/nmn/docs/quick-start","label":"Quick Start","docId":"quick-start","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Layers","items":[{"type":"link","href":"/nmn/docs/layers/yat-nmn","label":"YatNMN","docId":"layers/yat-nmn","unlisted":false},{"type":"link","href":"/nmn/docs/layers/yat-conv","label":"YatConv","docId":"layers/yat-conv","unlisted":false},{"type":"link","href":"/nmn/docs/layers/yat-conv-transpose","label":"YatConvTranspose","docId":"layers/yat-conv-transpose","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Attention","items":[{"type":"link","href":"/nmn/docs/attention/yat-attention","label":"YatAttention","docId":"attention/yat-attention","unlisted":false},{"type":"link","href":"/nmn/docs/attention/multi-head","label":"MultiHeadAttention","docId":"attention/multi-head","unlisted":false},{"type":"link","href":"/nmn/docs/attention/rotary","label":"Rotary Position Embeddings","docId":"attention/rotary","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"RNN","items":[{"type":"link","href":"/nmn/docs/rnn/lstm","label":"YatLSTM","docId":"rnn/lstm","unlisted":false},{"type":"link","href":"/nmn/docs/rnn/gru","label":"YatGRU","docId":"rnn/gru","unlisted":false},{"type":"link","href":"/nmn/docs/rnn/simple","label":"SimpleRNN","docId":"rnn/simple","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Examples","items":[{"type":"link","href":"/nmn/docs/examples/mnist","label":"MNIST Classification","docId":"examples/mnist","unlisted":false},{"type":"link","href":"/nmn/docs/examples/cifar10","label":"CIFAR-10 Classification","docId":"examples/cifar10","unlisted":false},{"type":"link","href":"/nmn/docs/examples/transformer","label":"Transformer Example","docId":"examples/transformer","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"attention/multi-head":{"id":"attention/multi-head","title":"MultiHeadAttention","description":"Multi-head attention with ⵟ-product query-key similarity.","sidebar":"tutorialSidebar"},"attention/rotary":{"id":"attention/rotary","title":"Rotary Position Embeddings","description":"Rotary position embeddings (RoPE) with ⵟ-attention for positional encoding.","sidebar":"tutorialSidebar"},"attention/yat-attention":{"id":"attention/yat-attention","title":"YatAttention","description":"Self-attention mechanism using the ⵟ-product for query-key similarity.","sidebar":"tutorialSidebar"},"examples/cifar10":{"id":"examples/cifar10","title":"CIFAR-10 Classification","description":"Image classification with YatConv on CIFAR-10.","sidebar":"tutorialSidebar"},"examples/mnist":{"id":"examples/mnist","title":"MNIST Classification","description":"A complete example of training an NMN classifier on MNIST.","sidebar":"tutorialSidebar"},"examples/transformer":{"id":"examples/transformer","title":"Transformer Example","description":"Building a transformer with YatAttention.","sidebar":"tutorialSidebar"},"installation":{"id":"installation","title":"Installation","description":"Install Neural Matter Networks using pip:","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to Neural Matter Networks","description":"Neural Matter Networks (NMN) introduce a fundamentally new approach to neural computation through the ⵟ-product (pronounced \"Yat\") — a geometric operator that replaces both dot products and activation functions with a single unified operation.","sidebar":"tutorialSidebar"},"layers/yat-conv":{"id":"layers/yat-conv","title":"YatConv","description":"Convolutional layer using the ⵟ-product for geometrically-aware feature extraction.","sidebar":"tutorialSidebar"},"layers/yat-conv-transpose":{"id":"layers/yat-conv-transpose","title":"YatConvTranspose","description":"Transposed convolution (deconvolution) using the ⵟ-product for upsampling operations.","sidebar":"tutorialSidebar"},"layers/yat-nmn":{"id":"layers/yat-nmn","title":"YatNMN","description":"A YAT linear transformation applied over the last dimension of the input.","sidebar":"tutorialSidebar"},"quick-start":{"id":"quick-start","title":"Quick Start","description":"Build your first Neural Matter Network in minutes.","sidebar":"tutorialSidebar"},"rnn/gru":{"id":"rnn/gru","title":"YatGRU","description":"Gated Recurrent Unit with ⵟ-product gates.","sidebar":"tutorialSidebar"},"rnn/lstm":{"id":"rnn/lstm","title":"YatLSTM","description":"Long Short-Term Memory (LSTM) with ⵟ-product gates.","sidebar":"tutorialSidebar"},"rnn/simple":{"id":"rnn/simple","title":"SimpleRNN","description":"Basic recurrent neural network with ⵟ-product.","sidebar":"tutorialSidebar"}}}}