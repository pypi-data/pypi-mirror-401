    <div class="blog-modal" id="modal-uat">
        <div class="blog-modal-header">
            <div class="blog-modal-header-content">
                <span class="blog-modal-badge">Theorem 2</span>
                <h2 class="blog-modal-title">Universal Approximation Theorem</h2>
            </div>
            <button class="blog-modal-close" onclick="closeModal()">
                <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M18 6L6 18M6 6l12 12"/>
                </svg>
            </button>
        </div>
        <div class="blog-modal-body">
            <!-- ELI5 Section -->
            <div class="eli5-section">
                <div class="eli5-header">
                    <span class="eli5-icon">üßí</span>
                    <h4>Explain Like I'm 5</h4>
                </div>
                <div class="eli5-content">
                    <p>
                        Imagine you have a magic box that can draw any picture you want. You just need to tell it 
                        "draw a cat" or "draw a house" and it will draw it perfectly!
                    </p>
                    <p>
                        The Universal Approximation Theorem says our <span class="yat-symbol">‚µü</span>-product networks 
                        are like that magic box ‚Äî they can <strong>learn to do anything</strong> (well, any continuous function) 
                        if we give them enough "drawing tools" (neurons).
                    </p>
                    <p>
                        Even though we removed the activation functions (like ReLU), we didn't lose any power! 
                        The <span class="yat-symbol">‚µü</span>-product is already "magical" enough on its own.
                    </p>
                </div>
            </div>

            <!-- Formal Statement -->
            <div class="theorem-statement">
                <div class="theorem-box">
                    <strong>Theorem:</strong> Let $\mathcal{X} \subset \mathbb{R}^d$ be compact. The class of single-hidden-layer 
                    <span class="yat-symbol">‚µü</span>-product networks 
                    $f(\mathbf{x}) = \sum_{i=1}^n \alpha_i \cdot g(\mathbf{x}; \mathbf{w}_i, b_i) + c$
                    is <em>dense</em> in $C(\mathcal{X})$ under the uniform norm. That is, NMNs can approximate any continuous function 
                    to arbitrary precision.
                </div>
            </div>

            <div class="theorem-content">
                <!-- The Problem It Solves -->
                <h4>üéØ The Problem This Solves</h4>
                <p>
                    When we removed activation functions, critics asked: <em>"Can you still learn complex functions?"</em>
                </p>
                <p>
                    Traditional neural networks rely on activation functions (ReLU, sigmoid) to create non-linearity. 
                    Without them, a network would just be a series of linear transformations ‚Äî which can only learn linear functions.
                </p>
                <p>
                    This theorem proves that the <span class="yat-symbol">‚µü</span>-product's <strong>inherent geometric non-linearity</strong> 
                    is sufficient. We don't need separate activation functions because the <span class="yat-symbol">‚µü</span>-product 
                    itself is non-linear.
                </p>

                <!-- Deep Mathematical Explanation -->
                <h4>üìê The Mathematics In Depth</h4>
                <p>
                    The proof is elegant and leverages the kernel structure established by Theorem 1:
                </p>

                <div class="proof-step">
                    <strong>Step 1: Recover IMQ Kernel</strong>
                    <p>
                        Consider the <span class="yat-symbol">‚µü</span>-product with bias: 
                        $g(\mathbf{x}; \mathbf{w}, b) = \frac{(\mathbf{w}^\top\mathbf{x} + b)^2}{\|\mathbf{w} - \mathbf{x}\|^2 + \varepsilon}$
                    </p>
                    <p>
                        By differentiating twice with respect to $b$:
                        $$\partial_b^2 g(\mathbf{x}; \mathbf{w}, b) = \frac{2}{\|\mathbf{x} - \mathbf{w}\|^2 + \varepsilon}$$
                    </p>
                    <p>
                        This is the <strong>inverse multiquadric (IMQ) kernel</strong> ‚Äî a well-studied kernel in approximation theory.
                    </p>
                </div>

                <div class="proof-step">
                    <strong>Step 2: Fourier Analysis</strong>
                    <p>
                        The IMQ kernel has a <strong>strictly positive Fourier transform</strong> (related to the modified Bessel function $K_0$). 
                        This is a key property for density results.
                    </p>
                </div>

                <div class="proof-step">
                    <strong>Step 3: Uniqueness via Orthogonality</strong>
                    <p>
                        If a measure $\mu$ is orthogonal to all IMQ translates (i.e., $\int k(\mathbf{x}, \mathbf{w}) d\mu(\mathbf{x}) = 0$ 
                        for all $\mathbf{w}$), then by the positivity of the Fourier transform, $\mu$ must be the zero measure.
                    </p>
                </div>

                <div class="proof-step">
                    <strong>Step 4: Density via Hahn-Banach/Riesz Duality</strong>
                    <p>
                        By the Hahn-Banach theorem and Riesz representation theorem, if the span of $\{g(\cdot; \mathbf{w}, b)\}$ 
                        is not dense, there exists a non-zero continuous linear functional that vanishes on the span. 
                        This functional corresponds to a measure, which by Step 3 must be zero ‚Äî a contradiction.
                    </p>
                    <p>
                        Therefore, the span is dense in $C(\mathcal{X})$. ‚àé
                    </p>
                </div>

                <div class="insight-box">
                    <span class="insight-icon">üî¨</span>
                    <div>
                        <strong>Key Insight:</strong> The bias term $b$ is crucial! It allows the network to "shift" response fields 
                        and span the entire function space through differentiation. This is why we use 
                        $(\mathbf{w}^\top\mathbf{x} + b)^2$ rather than just $(\mathbf{w}^\top\mathbf{x})^2$.
                    </div>
                </div>

                <!-- The Consequences -->
                <h4>üí• The Consequences</h4>
                <div class="consequences-grid">
                    <div class="consequence-item">
                        <span class="consequence-icon">‚àû</span>
                        <h5>No Expressive Power Loss</h5>
                        <p>
                            NMNs are as expressive as ReLU/Sigmoid networks. Single hidden layer is sufficient in theory 
                            (though deeper networks may learn more efficiently).
                        </p>
                    </div>
                    <div class="consequence-item">
                        <span class="consequence-icon">üéØ</span>
                        <h5>Geometric Localization</h5>
                        <p>
                            Unlike ReLU (unbounded growth), the <span class="yat-symbol">‚µü</span>-product achieves density 
                            through <em>localized</em> geometric units ‚Äî creating "vortex-like" territorial fields.
                        </p>
                    </div>
                    <div class="consequence-item">
                        <span class="consequence-icon">üîß</span>
                        <h5>Simpler Architecture</h5>
                        <p>
                            No need for complex activation functions. The geometric operator itself provides the non-linearity, 
                            leading to simpler, more interpretable networks.
                        </p>
                    </div>
                    <div class="consequence-item">
                        <span class="consequence-icon">üìä</span>
                        <h5>Kernel Method Connection</h5>
                        <p>
                            The proof connects NMNs to kernel methods, opening the door to using kernel-based optimization 
                            techniques and theoretical guarantees.
                        </p>
                    </div>
                </div>

                <!-- What This Really Means -->
                <h4>üéì What This Really Means</h4>
                <p>
                    This is the <strong>fundamental existence theorem</strong> for NMNs. It answers the question: 
                    "Can we really learn complex functions without activation functions?"
                </p>
                <p>
                    The answer is a resounding <strong>yes</strong>. The <span class="yat-symbol">‚µü</span>-product's 
                    geometric structure provides sufficient non-linearity to approximate any continuous function.
                </p>
                <p>
                    This theorem bridges the gap between <em>theoretical possibility</em> and <em>practical feasibility</em>, 
                    showing that activation-free networks are not just a curiosity ‚Äî they're a viable alternative 
                    with the same expressive power.
                </p>

                <!-- Historical Context -->
                <h4>üìú Historical Context</h4>
                <p>
                    Universal approximation theorems date back to the 1980s, with seminal work by Cybenko (1989) and 
                    Hornik et al. (1989) showing that single-hidden-layer networks with sigmoidal activations are universal approximators.
                </p>
                <p>
                    Our theorem extends this tradition, showing that <em>geometric non-linearity</em> (via the <span class="yat-symbol">‚µü</span>-product) 
                    can replace <em>functional non-linearity</em> (via activations) without losing approximation power.
                </p>
            </div>
        </div>
        <div class="blog-modal-nav">
            <button class="blog-modal-nav-btn" onclick="navigateModal('modal-mercer')">‚Üê Previous: Mercer Kernel</button>
            <button class="blog-modal-nav-btn" onclick="navigateModal('modal-regulation')">Next: Self-Regulation ‚Üí</button>
        </div>
    </div>
