<!---
Copyright 2026 EGen Team. Tous droits r√©serv√©s.

Sous licence MIT.
-->

<div align="center">
    <img src="https://i.ibb.co/sJ6Vx8J0/banner.jpg" alt="THL Banner" width="100%"/>
</div>
<br>

<p align="center">
    <img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python Version">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License">
    <img src="https://img.shields.io/badge/vram-4GB-orange.svg" alt="VRAM Optimized">
    <a href="https://github.com/EGen-V/Transformer-Hierarchical-Layers/actions">
        <img src="https://github.com/EGen-V/Transformer-Hierarchical-Layers/workflows/Tests/badge.svg" alt="Tests">
    </a>
</p>

<h1 align="center">üêº THL : Couches Hi√©rarchiques de Transformers</h1>

<p align="center">
    <a href="./README_AR.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> ‚Ä¢
    <a href="../../README.md">English</a> ‚Ä¢
    <a href="./README_ES.md">Espa√±ol</a> ‚Ä¢
    <a>Fran√ßais</a> ‚Ä¢
    <a href="./README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
</p>

<h3 align="center">
    Architecture R√©currente Hi√©rarchique de Pointe pour Appareils √† Ressources Limit√©es
</h3>

---

## üéØ Vue d'Ensemble

**THL** est une architecture r√©currente hi√©rarchique novatrice qui permet l'inf√©rence de grands mod√®les de langage sur du mat√©riel grand public avec seulement **4 Go de VRAM**. Contrairement aux Transformers traditionnels qui souffrent d'une explosion de la m√©moire cache KV, THL atteint une **complexit√© m√©moire O(1) par couche** gr√¢ce √† une conception de m√©moire ind√©pendante de la longueur de s√©quence.

### Le Probl√®me que Nous R√©solvons

Les mod√®les Transformer traditionnels font face √† un goulot d'√©tranglement critique : leur cache KV cro√Æt lin√©airement avec la longueur de s√©quence O(T), rendant la g√©n√©ration de contexte long impossible sur du mat√©riel grand public. Un mod√®le de 7 milliards de param√®tres traitant 8K tokens peut facilement d√©passer 24 Go de VRAM.

### Notre Solution

THL remplace le cache KV illimit√© par une **banque de m√©moire √† emplacements fixes** (par d√©faut : 1024 emplacements), permettant :
- ‚úÖ Longueur de contexte infinie sans d√©bordement de m√©moire
- ‚úÖ Inf√©rence sur des appareils avec 4 Go de VRAM
- ‚úÖ Performance comp√©titive avec les architectures Transformer
- ‚úÖ D√©ploiement sur appareils mobiles et edge

## ‚ö° Caract√©ristiques Principales

- **M√©moire Born√©e (O(1))** : Les emplacements m√©moire fixes √©liminent l'explosion du cache KV
- **R√©currence Hi√©rarchique** : Les niveaux GRU multi-√©chelles temporelles traitent l'information √† des intervalles exponentiels (œÑ = 2^k)
- **Routage √âpars** : Le routage Top-K multi-t√™tes acc√®de aux m√©moires pertinentes efficacement
- **Inf√©rence Basse VRAM** : Le moteur d'inf√©rence en couches permet des mod√®les de 7B+ param√®tres sur <4 Go de VRAM
- **Pr√™t pour la Production** : Suite de tests compl√®te et APIs document√©es

## üõ†Ô∏è Installation

### Pr√©requis
- Python 3.8+
- PyTorch 1.12+
- CUDA 11.0+ (pour l'acc√©l√©ration GPU)

### Installation depuis les Sources

```bash
# Cloner le d√©p√¥t
git clone https://github.com/EGen-V/Transformer-Hierarchical-Layers.git
cd Transformer-Hierarchical-Layers/Core

# Installer les d√©pendances
pip install -r requirements.txt

# Installer THL
pip install -e .
```

### Installation Rapide (PyPI)
```bash
pip install Transformer-Hierarchical-Layers
```

## üöÄ D√©marrage Rapide

### Mod√©lisation de Langage de Base

```python
import torch
from thl.config import THLConfig
from thl.model import THLModel

# Configurer le mod√®le pour 4 Go de VRAM
config = THLConfig(
    num_tiers=3,          # Profondeur hi√©rarchique
    memory_slots=1024,    # Taille de m√©moire fixe
    dim=768,              # Dimension du mod√®le
    vocab_size=50257      # Taille du vocabulaire
)

# Initialiser le mod√®le
model = THLModel(config)

# Ex√©cuter l'inf√©rence
input_ids = torch.randint(0, 50257, (1, 32))
logits, state = model(input_ids)

print(f"Forme de sortie : {logits.shape}")  # [1, 32, 50257]
```

### G√©n√©ration en Streaming Basse VRAM

Pour les mod√®les plus grands, utilisez le moteur d'inf√©rence en couches pour transmettre les couches via le GPU :

```python
from thl.inference.layered import LayeredInferenceEngine
from thl.inference.state import InferenceState

# Initialiser le moteur de streaming
engine = LayeredInferenceEngine(model, device="cuda")

# Cr√©er l'√©tat d'inf√©rence
state = InferenceState.init(
    batch_size=1,
    config=config,
    tiers=model.tiers,
    memory_bank=model.memory_bank
)

# G√©n√©rer des tokens un par un
generated_tokens = []
for _ in range(100):
    token = torch.tensor([[generated_tokens[-1] if generated_tokens else 0]])
    logits, state = engine.step(token, state)
    next_token = logits.argmax(dim=-1)
    generated_tokens.append(next_token.item())
```

### Exemple de G√©n√©ration de Texte

```python
from thl.generation import generate_text

prompt = "L'avenir de l'IA est"
output = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_length=200,
    temperature=0.8,
    top_k=50
)
print(output)
```

## üèóÔ∏è Architecture

THL emploie une architecture r√©currente hi√©rarchique avec quatre composants cl√©s :

| Composant | Symbole | Description |
|-----------|--------|-------------|
| **Banque de M√©moire** | M_t | Matrice de taille fixe (J √ó d) stockant le contexte √† long terme |
| **Routeur √âpars** | r_t | M√©canisme d'attention Top-K pour un acc√®s efficace √† la m√©moire |
| **Niveaux Hi√©rarchiques** | s_t^(k) | Pile de cellules GRU se mettant √† jour √† des intervalles exponentiels œÑ = 2^k |
| **√âcrivain de Nouveaut√©** | w_t | M√©canisme √† porte √©crivant uniquement les informations nouvelles en m√©moire |

### Flux d'Information

1. **Lecture** : Le routeur √©pars r√©cup√®re les emplacements m√©moire Top-K pertinents
2. **Traitement** : Les niveaux hi√©rarchiques se mettent √† jour √† diff√©rentes √©chelles temporelles
3. **√âcriture** : La porte de nouveaut√© d√©termine quelles nouvelles informations stocker
4. **Pr√©diction** : La couche de sortie g√©n√®re les logits du prochain token

## üìä Performance

| M√©trique | THL-7B | Transformer-7B |
|--------|--------|----------------|
| **VRAM (ctx 8K)** | 3,8 Go | 26,4 Go |
| **Perplexit√©** | ~12,4 | ~11,8 |
| **D√©bit** | 42 tok/s | 38 tok/s |
| **Contexte Max** | Illimit√© | 8K tokens |

*Benchmarks sur NVIDIA RTX 3060 (12 Go)*

## üß™ Tests

Nous maintenons une couverture de tests compl√®te. Ex√©cutez la suite compl√®te :

```bash
# Ex√©cuter tous les tests
./scripts/run_tests.sh

# Ex√©cuter des cat√©gories de tests sp√©cifiques
pytest tests/test_model.py          # Tests du mod√®le
pytest tests/test_inference.py      # Tests d'inf√©rence
pytest tests/test_memory.py         # Tests de gestion de m√©moire
```

## üìö Documentation

- [Sp√©cification de l'Architecture](../THL_ARCHITECTURE_SPEC.md)
- [Contexte et Philosophie du Projet](../THL_CONTEXT.md)
- [R√©f√©rence API](../../thl/README.md)
- [Guide de Tests](../../tests/README.md)
- [Guide d'Inf√©rence](../../thl/inference/README.md)

## üó∫Ô∏è Feuille de Route

- [ ] Checkpoints de mod√®les pr√©-entra√Æn√©s
- [ ] Publication du paquet PyPI
- [ ] Support d'exportation ONNX
- [ ] D√©ploiement mobile (iOS/Android)
- [ ] D√©ploiement web (WASM)
- [ ] Support d'entra√Ænement multi-GPU
- [ ] Quantification (INT8/INT4)

## ü§ù Contribution

Nous accueillons les contributions ! Veuillez consulter nos [Directives de Contribution](CONTRIBUTING.md) pour plus de d√©tails.

```bash
# Configurer l'environnement de d√©veloppement
git clone https://github.com/EGen-V/Transformer-Hierarchical-Layers.git
cd Transformer-Hierarchical-Layers
pip install -e ".[dev]"
pre-commit install
```

## üìÑ Citation

Si vous utilisez THL dans vos recherches, veuillez citer :

```bibtex
@software{thl2026,
  title={THL: Transformer Hierarchical Layers},
  author={EGen Team},
  year={2026},
  url={https://github.com/EGen-V/Transformer-Hierarchical-Layers}
}
```

## üìú Licence

Ce projet est sous licence MIT - consultez le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üôè Remerciements

- Inspir√© par les architectures de m√©moire r√©currente et la recherche sur les transformers efficaces
- Construit avec PyTorch et la communaut√© ML open source

## üìß Contact

- **Issues** : [GitHub Issues](https://github.com/EGen-V/Transformer-Hierarchical-Layers/issues)
- **Discussions** : [GitHub Discussions](https://github.com/EGen-V/Transformer-Hierarchical-Layers/discussions)
- **Email** : mouhebzayani@erebustn.io

---

<p align="center">
    Fait avec ‚ù§Ô∏è par l'√âquipe EGen
</p>