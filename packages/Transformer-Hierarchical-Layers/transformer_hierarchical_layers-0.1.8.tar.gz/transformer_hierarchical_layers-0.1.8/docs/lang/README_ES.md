<!---
Copyright 2026 EGen Team. Todos los derechos reservados.

Licenciado bajo la Licencia MIT.
-->

<div align="center">
    <img src="https://i.ibb.co/sJ6Vx8J0/banner.jpg" alt="THL Banner" width="100%"/>
</div>
<br>

<p align="center">
    <img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python Version">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License">
    <img src="https://img.shields.io/badge/vram-4GB-orange.svg" alt="VRAM Optimized">
    <a href="https://github.com/EGen-V/Transformer-Hierarchical-Layers/actions">
        <img src="https://github.com/EGen-V/Transformer-Hierarchical-Layers/workflows/Tests/badge.svg" alt="Tests">
    </a>
</p>

<h1 align="center">üêº THL: Capas Jer√°rquicas de Transformers</h1>

<p align="center">
    <a href="./README_AR.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> ‚Ä¢
    <a href="../../README.md">English</a> ‚Ä¢
    <a>Espa√±ol</a> ‚Ä¢
    <a href="./README_FR.md">Fran√ßais</a> ‚Ä¢
    <a href="./README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
</p>

<h3 align="center">
    Arquitectura Recurrente Jer√°rquica de Vanguardia para Dispositivos con Recursos Limitados
</h3>

---

## üéØ Descripci√≥n General

**THL** es una arquitectura recurrente jer√°rquica novedosa que permite la inferencia de modelos de lenguaje grandes en hardware de consumo con tan solo **4GB de VRAM**. A diferencia de los Transformers tradicionales que sufren de explosi√≥n de memoria de cach√© KV, THL logra **complejidad de memoria O(1) por capa** mediante un dise√±o de memoria independiente de la longitud de secuencia.

### El Problema que Resolvemos

Los modelos Transformer tradicionales enfrentan un cuello de botella cr√≠tico: su cach√© KV crece linealmente con la longitud de secuencia O(T), haciendo imposible la generaci√≥n de contexto largo en hardware de consumo. Un modelo de 7B par√°metros procesando 8K tokens puede exceder f√°cilmente los 24GB de VRAM.

### Nuestra Soluci√≥n

THL reemplaza la cach√© KV ilimitada con un **banco de memoria de ranuras fijas** (predeterminado: 1024 ranuras), permitiendo:
- ‚úÖ Longitud de contexto infinita sin desbordamiento de memoria
- ‚úÖ Inferencia en dispositivos con 4GB de VRAM
- ‚úÖ Rendimiento competitivo con arquitecturas Transformer
- ‚úÖ Implementaci√≥n en dispositivos m√≥viles y edge

## ‚ö° Caracter√≠sticas Principales

- **Memoria Acotada (O(1))**: Las ranuras de memoria fijas eliminan la explosi√≥n de cach√© KV
- **Recurrencia Jer√°rquica**: Niveles GRU multi-escala temporal procesan informaci√≥n en intervalos exponenciales (œÑ = 2^k)
- **Enrutamiento Disperso**: Enrutamiento Top-K multi-cabeza accede a memorias relevantes eficientemente
- **Inferencia de Baja VRAM**: Motor de inferencia por capas permite modelos de 7B+ par√°metros en <4GB VRAM
- **Listo para Producci√≥n**: Suite de pruebas completa y APIs documentadas

## üõ†Ô∏è Instalaci√≥n

### Requisitos
- Python 3.8+
- PyTorch 1.12+
- CUDA 11.0+ (para aceleraci√≥n GPU)

### Instalar desde el C√≥digo Fuente

```bash
# Clonar el repositorio
git clone https://github.com/EGen-V/Transformer-Hierarchical-Layers.git
cd Transformer-Hierarchical-Layers/Core

# Instalar dependencias
pip install -r requirements.txt

# Instalar THL
pip install -e .
```

### Instalaci√≥n R√°pida (PyPI)
```bash
pip install Transformer-Hierarchical-Layers
```

## üöÄ Inicio R√°pido

### Modelado B√°sico de Lenguaje

```python
import torch
from thl.config import THLConfig
from thl.model import THLModel

# Configurar modelo para 4GB VRAM
config = THLConfig(
    num_tiers=3,          # Profundidad jer√°rquica
    memory_slots=1024,    # Tama√±o de memoria fijo
    dim=768,              # Dimensi√≥n del modelo
    vocab_size=50257      # Tama√±o del vocabulario
)

# Inicializar modelo
model = THLModel(config)

# Ejecutar inferencia
input_ids = torch.randint(0, 50257, (1, 32))
logits, state = model(input_ids)

print(f"Forma de salida: {logits.shape}")  # [1, 32, 50257]
```

### Generaci√≥n por Streaming de Baja VRAM

Para modelos m√°s grandes, usa el motor de inferencia por capas para transmitir capas a trav√©s de la GPU:

```python
from thl.inference.layered import LayeredInferenceEngine
from thl.inference.state import InferenceState

# Inicializar motor de streaming
engine = LayeredInferenceEngine(model, device="cuda")

# Crear estado de inferencia
state = InferenceState.init(
    batch_size=1,
    config=config,
    tiers=model.tiers,
    memory_bank=model.memory_bank
)

# Generar tokens uno a la vez
generated_tokens = []
for _ in range(100):
    token = torch.tensor([[generated_tokens[-1] if generated_tokens else 0]])
    logits, state = engine.step(token, state)
    next_token = logits.argmax(dim=-1)
    generated_tokens.append(next_token.item())
```

### Ejemplo de Generaci√≥n de Texto

```python
from thl.generation import generate_text

prompt = "El futuro de la IA es"
output = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_length=200,
    temperature=0.8,
    top_k=50
)
print(output)
```

## üèóÔ∏è Arquitectura

THL emplea una arquitectura recurrente jer√°rquica con cuatro componentes clave:

| Componente | S√≠mbolo | Descripci√≥n |
|-----------|--------|-------------|
| **Banco de Memoria** | M_t | Matriz de tama√±o fijo (J √ó d) que almacena contexto a largo plazo |
| **Enrutador Disperso** | r_t | Mecanismo de atenci√≥n Top-K para acceso eficiente a la memoria |
| **Niveles Jer√°rquicos** | s_t^(k) | Pila de celdas GRU que se actualizan en intervalos exponenciales œÑ = 2^k |
| **Escritor de Novedad** | w_t | Mecanismo con compuerta que escribe solo informaci√≥n novedosa en memoria |

### Flujo de Informaci√≥n

1. **Lectura**: El enrutador disperso recupera las ranuras de memoria Top-K relevantes
2. **Procesamiento**: Los niveles jer√°rquicos se actualizan en diferentes escalas temporales
3. **Escritura**: La compuerta de novedad determina qu√© informaci√≥n nueva almacenar
4. **Predicci√≥n**: La capa de salida genera logits del siguiente token

## üìä Rendimiento

| M√©trica | THL-7B | Transformer-7B |
|--------|--------|----------------|
| **VRAM (ctx 8K)** | 3.8 GB | 26.4 GB |
| **Perplejidad** | ~12.4 | ~11.8 |
| **Rendimiento** | 42 tok/s | 38 tok/s |
| **Contexto M√°x** | Ilimitado | 8K tokens |

*Benchmarks en NVIDIA RTX 3060 (12GB)*

## üß™ Pruebas

Mantenemos cobertura de pruebas completa. Ejecuta la suite completa:

```bash
# Ejecutar todas las pruebas
./scripts/run_tests.sh

# Ejecutar categor√≠as espec√≠ficas de pruebas
pytest tests/test_model.py          # Pruebas del modelo
pytest tests/test_inference.py      # Pruebas de inferencia
pytest tests/test_memory.py         # Pruebas de gesti√≥n de memoria
```

## üìö Documentaci√≥n

- [Especificaci√≥n de Arquitectura](../THL_ARCHITECTURE_SPEC.md)
- [Contexto y Filosof√≠a del Proyecto](../THL_CONTEXT.md)
- [Referencia de API](../../thl/README.md)
- [Gu√≠a de Pruebas](../../tests/README.md)
- [Gu√≠a de Inferencia](../../thl/inference/README.md)

## üó∫Ô∏è Hoja de Ruta

- [ ] Checkpoints de modelos pre-entrenados
- [ ] Lanzamiento de paquete PyPI
- [ ] Soporte de exportaci√≥n ONNX
- [ ] Implementaci√≥n m√≥vil (iOS/Android)
- [ ] Implementaci√≥n web (WASM)
- [ ] Soporte de entrenamiento multi-GPU
- [ ] Cuantizaci√≥n (INT8/INT4)

## ü§ù Contribuci√≥n

¬°Damos la bienvenida a contribuciones! Por favor consulta nuestras [Directrices de Contribuci√≥n](CONTRIBUTING.md) para m√°s detalles.

```bash
# Configurar entorno de desarrollo
git clone https://github.com/EGen-V/Transformer-Hierarchical-Layers.git
cd Transformer-Hierarchical-Layers
pip install -e ".[dev]"
pre-commit install
```

## üìÑ Cita

Si usas THL en tu investigaci√≥n, por favor cita:

```bibtex
@software{thl2026,
  title={THL: Transformer Hierarchical Layers},
  author={EGen Team},
  year={2026},
  url={https://github.com/EGen-V/Transformer-Hierarchical-Layers}
}
```

## üìú Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para m√°s detalles.

## üôè Agradecimientos

- Inspirado por arquitecturas de memoria recurrente e investigaci√≥n de transformers eficientes
- Construido con PyTorch y la comunidad de ML de c√≥digo abierto

## üìß Contacto

- **Issues**: [GitHub Issues](https://github.com/EGen-V/Transformer-Hierarchical-Layers/issues)
- **Discusiones**: [GitHub Discussions](https://github.com/EGen-V/Transformer-Hierarchical-Layers/discussions)
- **Email**: mouhebzayani@erebustn.io

---

<p align="center">
    Hecho con ‚ù§Ô∏è por el Equipo EGen
</p>