# coding: utf-8

"""
    Arthur GenAI Engine

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: 2.1.294
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from arthur_observability_sdk._generated.models.prompt_experiment_detail import PromptExperimentDetail

class TestPromptExperimentDetail(unittest.TestCase):
    """PromptExperimentDetail unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> PromptExperimentDetail:
        """Test PromptExperimentDetail
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `PromptExperimentDetail`
        """
        model = PromptExperimentDetail()
        if include_optional:
            return PromptExperimentDetail(
                id = '',
                name = '',
                description = '',
                created_at = '',
                finished_at = '',
                status = 'queued',
                total_rows = 56,
                completed_rows = 56,
                failed_rows = 56,
                total_cost = '',
                dataset_ref = arthur_observability_sdk._generated.models.dataset_ref.DatasetRef(
                    id = '', 
                    name = '', 
                    version = 56, ),
                eval_list = [
                    arthur_observability_sdk._generated.models.eval_ref.EvalRef(
                        name = '', 
                        version = 56, 
                        variable_mapping = [
                            arthur_observability_sdk._generated.models.eval_variable_mapping.EvalVariableMapping(
                                variable_name = '', 
                                source = null, )
                            ], )
                    ],
                dataset_row_filter = [
                    arthur_observability_sdk._generated.models.new_dataset_version_row_column_item_request.NewDatasetVersionRowColumnItemRequest(
                        column_name = '', 
                        column_value = '', )
                    ],
                notebook_id = '',
                prompt_configs = [
                    null
                    ],
                prompt_variable_mapping = [
                    arthur_observability_sdk._generated.models.prompt_variable_mapping.PromptVariableMapping(
                        variable_name = '', 
                        source = arthur_observability_sdk._generated.models.dataset_column_variable_source.DatasetColumnVariableSource(
                            type = 'dataset_column', 
                            dataset_column = arthur_observability_sdk._generated.models.dataset_column_source.DatasetColumnSource(
                                name = '', ), ), )
                    ],
                summary_results = arthur_observability_sdk._generated.models.summary_results.SummaryResults(
                    prompt_eval_summaries = [
                        arthur_observability_sdk._generated.models.prompt_eval_result_summaries.PromptEvalResultSummaries(
                            prompt_key = '', 
                            prompt_type = '', 
                            prompt_name = '', 
                            prompt_version = '', 
                            eval_results = [
                                arthur_observability_sdk._generated.models.eval_result_summary.EvalResultSummary(
                                    eval_name = '', 
                                    eval_version = '', 
                                    pass_count = 56, 
                                    total_count = 56, )
                                ], )
                        ], )
            )
        else:
            return PromptExperimentDetail(
                id = '',
                name = '',
                created_at = '',
                status = 'queued',
                total_rows = 56,
                completed_rows = 56,
                failed_rows = 56,
                dataset_ref = arthur_observability_sdk._generated.models.dataset_ref.DatasetRef(
                    id = '', 
                    name = '', 
                    version = 56, ),
                eval_list = [
                    arthur_observability_sdk._generated.models.eval_ref.EvalRef(
                        name = '', 
                        version = 56, 
                        variable_mapping = [
                            arthur_observability_sdk._generated.models.eval_variable_mapping.EvalVariableMapping(
                                variable_name = '', 
                                source = null, )
                            ], )
                    ],
                prompt_configs = [
                    null
                    ],
                prompt_variable_mapping = [
                    arthur_observability_sdk._generated.models.prompt_variable_mapping.PromptVariableMapping(
                        variable_name = '', 
                        source = arthur_observability_sdk._generated.models.dataset_column_variable_source.DatasetColumnVariableSource(
                            type = 'dataset_column', 
                            dataset_column = arthur_observability_sdk._generated.models.dataset_column_source.DatasetColumnSource(
                                name = '', ), ), )
                    ],
                summary_results = arthur_observability_sdk._generated.models.summary_results.SummaryResults(
                    prompt_eval_summaries = [
                        arthur_observability_sdk._generated.models.prompt_eval_result_summaries.PromptEvalResultSummaries(
                            prompt_key = '', 
                            prompt_type = '', 
                            prompt_name = '', 
                            prompt_version = '', 
                            eval_results = [
                                arthur_observability_sdk._generated.models.eval_result_summary.EvalResultSummary(
                                    eval_name = '', 
                                    eval_version = '', 
                                    pass_count = 56, 
                                    total_count = 56, )
                                ], )
                        ], ),
        )
        """

    def testPromptExperimentDetail(self):
        """Test PromptExperimentDetail"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
