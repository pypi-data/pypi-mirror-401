# coding: utf-8

"""
    Arthur GenAI Engine

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: 2.1.294
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from _generated.models.prompt_version_result_list_response import PromptVersionResultListResponse

class TestPromptVersionResultListResponse(unittest.TestCase):
    """PromptVersionResultListResponse unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> PromptVersionResultListResponse:
        """Test PromptVersionResultListResponse
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `PromptVersionResultListResponse`
        """
        model = PromptVersionResultListResponse()
        if include_optional:
            return PromptVersionResultListResponse(
                page = 56,
                page_size = 56,
                total_pages = 56,
                total_count = 56,
                data = [
                    _generated.models.prompt_version_result.PromptVersionResult(
                        status = 'queued', 
                        dataset_row_id = '', 
                        evals = [
                            _generated.models.eval_execution.EvalExecution(
                                eval_name = '', 
                                eval_version = '', 
                                eval_input_variables = [
                                    _generated.models.input_variable.InputVariable(
                                        variable_name = '', 
                                        value = '', )
                                    ], 
                                eval_results = _generated.models.eval_execution_result.EvalExecutionResult(
                                    score = 1.337, 
                                    explanation = '', 
                                    cost = '', ), )
                            ], 
                        total_cost = '', 
                        prompt_input_variables = [
                            _generated.models.input_variable.InputVariable(
                                variable_name = '', 
                                value = '', )
                            ], 
                        rendered_prompt = '', 
                        output = _generated.models.prompt_output.PromptOutput(
                            content = '', 
                            tool_calls = [
                                null
                                ], 
                            cost = '', ), )
                    ]
            )
        else:
            return PromptVersionResultListResponse(
                page = 56,
                page_size = 56,
                total_pages = 56,
                total_count = 56,
                data = [
                    _generated.models.prompt_version_result.PromptVersionResult(
                        status = 'queued', 
                        dataset_row_id = '', 
                        evals = [
                            _generated.models.eval_execution.EvalExecution(
                                eval_name = '', 
                                eval_version = '', 
                                eval_input_variables = [
                                    _generated.models.input_variable.InputVariable(
                                        variable_name = '', 
                                        value = '', )
                                    ], 
                                eval_results = _generated.models.eval_execution_result.EvalExecutionResult(
                                    score = 1.337, 
                                    explanation = '', 
                                    cost = '', ), )
                            ], 
                        total_cost = '', 
                        prompt_input_variables = [
                            _generated.models.input_variable.InputVariable(
                                variable_name = '', 
                                value = '', )
                            ], 
                        rendered_prompt = '', 
                        output = _generated.models.prompt_output.PromptOutput(
                            content = '', 
                            tool_calls = [
                                null
                                ], 
                            cost = '', ), )
                    ],
        )
        """

    def testPromptVersionResultListResponse(self):
        """Test PromptVersionResultListResponse"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
