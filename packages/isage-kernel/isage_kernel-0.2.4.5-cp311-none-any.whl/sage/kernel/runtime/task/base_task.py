import threading
import time
from abc import ABC
from queue import Empty as QueueEmpty
from typing import TYPE_CHECKING

try:
    from ray.util.queue import Empty as RayQueueEmpty  # type: ignore
except ImportError:
    RayQueueEmpty = QueueEmpty  # type: ignore
from sage.kernel.runtime.communication.packet import Packet, StopSignal
from sage.kernel.runtime.context.task_context import TaskContext
from sage.kernel.runtime.monitoring import (
    RESOURCE_MONITOR_AVAILABLE,
    MetricsCollector,
    MetricsReporter,
    ResourceMonitor,
    TaskPerformanceMetrics,
)

if TYPE_CHECKING:
    from sage.kernel.api.operator.base_operator import BaseOperator
    from sage.kernel.runtime.factory.operator_factory import OperatorFactory


QUEUE_EMPTY_EXCEPTIONS = (
    (QueueEmpty,) if RayQueueEmpty is QueueEmpty else (QueueEmpty, RayQueueEmpty)
)


class BaseTask(ABC):  # noqa: B024
    def __init__(self, ctx: "TaskContext", operator_factory: "OperatorFactory") -> None:
        self.ctx = ctx

        # ä½¿ç”¨ä»ä¸Šä¸‹æ–‡ä¼ å…¥çš„é˜Ÿåˆ—æè¿°ç¬¦
        self.input_qd = self.ctx.input_qd

        if self.input_qd:
            self.logger.info(
                f"ğŸ¯ Task: Using queue descriptor for input buffer: {self.input_qd.queue_id}"
            )
        else:
            self.logger.info("ğŸ¯ Task: No input queue (source/spout node)")

        # === çº¿ç¨‹æ§åˆ¶ ===
        self._worker_thread: threading.Thread | None = None
        self.is_running = False

        # === æ€§èƒ½ç›‘æ§ ===
        self._processed_count = 0
        self._error_count = 0

        # âœ… æ·»åŠ  checkpoint ç›¸å…³å±æ€§
        self._checkpoint_counter = 0
        self._last_checkpoint_time = 0.0

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
        self._enable_monitoring = getattr(ctx, "enable_monitoring", False)
        self.metrics_collector: MetricsCollector | None = None
        self.resource_monitor: ResourceMonitor | None = None
        self.metrics_reporter: MetricsReporter | None = None

        self.fault_handler = None  # Will be set by dispatcher if applicable

        if self._enable_monitoring:
            try:
                self.metrics_collector = MetricsCollector(
                    name=self.ctx.name,
                    window_size=getattr(ctx, "metrics_window_size", 10000),
                    enable_detailed_tracking=getattr(ctx, "enable_detailed_tracking", True),
                )

                # å°è¯•å¯åŠ¨èµ„æºç›‘æ§ï¼ˆéœ€è¦psutilï¼‰
                if RESOURCE_MONITOR_AVAILABLE:
                    try:
                        self.resource_monitor = ResourceMonitor(
                            sampling_interval=getattr(ctx, "resource_sampling_interval", 1.0),
                            enable_auto_start=True,
                        )
                    except Exception as e:
                        self.logger.warning(
                            f"Failed to start resource monitoring for task {self.name}: {e}"
                        )
                else:
                    self.logger.debug(
                        f"psutil not available, resource monitoring disabled for task {self.name}"
                    )

                # å¯é€‰ï¼šå¯åŠ¨æ€§èƒ½æ±‡æŠ¥å™¨
                if getattr(ctx, "enable_auto_report", False):
                    self.metrics_reporter = MetricsReporter(
                        metrics_collector=self.metrics_collector,
                        resource_monitor=self.resource_monitor,
                        report_interval=getattr(ctx, "report_interval", 60),
                        enable_auto_report=True,
                        report_callback=lambda report: self.logger.info(f"\n{report}"),
                    )

                self.logger.info(f"Performance monitoring enabled for task {self.name}")
            except Exception as e:
                self.logger.warning(f"Failed to initialize monitoring for task {self.name}: {e}")
                self._enable_monitoring = False

        try:
            self.operator: BaseOperator = operator_factory.create_operator(self.ctx)
            if hasattr(self.operator, "task"):
                self.operator.task = self  # type: ignore
        except Exception as e:
            self.logger.error(f"Failed to initialize node {self.name}: {e}", exc_info=True)
            raise

    def get_state(self) -> dict:
        """
        è·å–ä»»åŠ¡å®Œæ•´çŠ¶æ€ç”¨äº checkpoint

        åŒ…æ‹¬ï¼š
        1. Task å±‚çš„çŠ¶æ€ï¼ˆprocessed_count, error_count ç­‰ï¼‰
        2. Operator å±‚çš„çŠ¶æ€ï¼ˆé€šè¿‡ operator.get_state()ï¼‰
        3. Function å±‚çš„çŠ¶æ€ï¼ˆé€šè¿‡ function.get_state()ï¼Œå·²åŒ…å«åœ¨ operator ä¸­ï¼‰

        Returns:
            ä»»åŠ¡å®Œæ•´çŠ¶æ€å­—å…¸
        """
        state = {
            # === Task å…ƒæ•°æ® ===
            "task_id": self.name,
            "task_type": self.__class__.__name__,
            "is_spout": self.is_spout,
            "timestamp": time.time(),
            # === Task æ€§èƒ½æŒ‡æ ‡ ===
            "processed_count": self._processed_count,
            "error_count": self._error_count,
            "checkpoint_counter": self._checkpoint_counter,
            "last_checkpoint_time": self._last_checkpoint_time,
            # === Task é…ç½® ===
            "delay": self.delay,
        }

        # === Operator å’Œ Function çŠ¶æ€ ===
        if hasattr(self.operator, "get_state"):
            try:
                operator_state = self.operator.get_state()
                state["operator_state"] = operator_state

                self.logger.debug(
                    f"Captured operator state for {self.name}: {list(operator_state.keys())}"
                )

                # å¦‚æœ operator_state åŒ…å« function_stateï¼Œä¹Ÿè®°å½•
                if "function_state" in operator_state:
                    function_attrs = list(operator_state["function_state"].keys())
                    self.logger.debug(f"Function state includes: {function_attrs}")

            except Exception as e:
                self.logger.warning(
                    f"Failed to get operator state for {self.name}: {e}", exc_info=True
                )
                state["operator_state"] = None
        else:
            self.logger.warning(
                f"Operator {self.operator.__class__.__name__} does not support get_state()"
            )
            state["operator_state"] = None

        # === Context é…ç½®ä¿¡æ¯ï¼ˆåªä¿å­˜é…ç½®ï¼Œä¸ä¿å­˜è¿è¡Œæ—¶å¯¹è±¡ï¼‰===
        try:
            state["context_config"] = {
                "name": self.ctx.name,
                "is_spout": self.ctx.is_spout,
                "delay": self.ctx.delay,
                # ä¸ä¿å­˜ queue, router ç­‰è¿è¡Œæ—¶å¯¹è±¡
            }
        except Exception as e:
            self.logger.warning(f"Failed to capture context config: {e}")

        # è®°å½•çŠ¶æ€å¤§å°ï¼ˆç”¨äºç›‘æ§ï¼‰
        try:
            import sys

            state_size = sys.getsizeof(str(state))
            self.logger.debug(f"Checkpoint state size for {self.name}: {state_size} bytes")
        except Exception:
            pass

        return state

    def restore_state(self, state: dict):
        """
        ä» checkpoint å®Œæ•´æ¢å¤ä»»åŠ¡çŠ¶æ€

        æ¢å¤é¡ºåºï¼š
        1. Task å±‚çŠ¶æ€
        2. Operator å±‚çŠ¶æ€
        3. Function å±‚çŠ¶æ€ï¼ˆé€šè¿‡ operator.restore_stateï¼‰

        Args:
            state: ä¿å­˜çš„çŠ¶æ€å­—å…¸
        """
        self.logger.info(f"â®ï¸ Restoring state for task {self.name}")

        try:
            # === æ¢å¤ Task å±‚çŠ¶æ€ ===
            self._processed_count = state.get("processed_count", 0)
            self._error_count = state.get("error_count", 0)
            self._checkpoint_counter = state.get("checkpoint_counter", 0)
            self._last_checkpoint_time = state.get("last_checkpoint_time", 0.0)

            self.logger.info(
                f"âœ… Task state restored: "
                f"processed={self._processed_count}, "
                f"errors={self._error_count}, "
                f"checkpoints={self._checkpoint_counter}"
            )

            # === æ¢å¤ Operator å’Œ Function çŠ¶æ€ ===
            operator_state = state.get("operator_state")
            if operator_state and hasattr(self.operator, "restore_state"):
                try:
                    self.operator.restore_state(operator_state)
                    self.logger.info(f"âœ… Operator state restored for {self.name}")

                    # éªŒè¯ function çŠ¶æ€æ˜¯å¦æ¢å¤
                    if hasattr(self.operator, "function"):
                        function = self.operator.function

                        # è®°å½•æ¢å¤çš„ function å±æ€§
                        restored_attrs = []
                        if "function_state" in operator_state:
                            for attr_name in operator_state["function_state"].keys():
                                if hasattr(function, attr_name):
                                    value = getattr(function, attr_name)
                                    restored_attrs.append(f"{attr_name}={value}")

                        if restored_attrs:
                            self.logger.info(
                                f"âœ… Function attributes restored: {', '.join(restored_attrs)}"
                            )

                except Exception as e:
                    self.logger.error(f"âŒ Failed to restore operator state: {e}", exc_info=True)
            else:
                if not operator_state:
                    self.logger.warning(f"âš ï¸ No operator state found in checkpoint for {self.name}")
                elif not hasattr(self.operator, "restore_state"):
                    self.logger.warning(
                        f"âš ï¸ Operator {self.operator.__class__.__name__} does not support restore_state()"
                    )

            self.logger.info(f"ğŸ‰ Complete state restoration finished for task {self.name}")

        except Exception as e:
            self.logger.error(
                f"âŒ Critical error during state restoration for {self.name}: {e}",
                exc_info=True,
            )
            raise

    def save_checkpoint_if_needed(self, fault_handler) -> bool:
        """
        å¦‚æœéœ€è¦ï¼Œä¿å­˜ checkpoint

        Args:
            fault_handler: å®¹é”™å¤„ç†å™¨

        Returns:
            True å¦‚æœä¿å­˜äº† checkpoint
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ CheckpointBasedRecovery
        from sage.kernel.fault_tolerance.impl.checkpoint_recovery import (
            CheckpointBasedRecovery,
        )

        if not isinstance(fault_handler, CheckpointBasedRecovery):
            return False

        current_time = time.time()
        interval = fault_handler.checkpoint_interval

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¿å­˜ checkpoint
        if (current_time - self._last_checkpoint_time) >= interval:
            state = self.get_state()
            success = fault_handler.save_checkpoint(self.name, state)

            if success:
                self._last_checkpoint_time = current_time
                self._checkpoint_counter += 1
                self.logger.debug(
                    f"Checkpoint #{self._checkpoint_counter} saved for task {self.name}"
                )

            return success

        return False

    @property
    def router(self):
        return self.ctx.router

    def start_running(self):
        """å¯åŠ¨ä»»åŠ¡çš„å·¥ä½œå¾ªç¯"""
        if self.is_running:
            self.logger.warning(f"Task {self.name} is already running")
            return

        self.logger.info(f"Starting task {self.name}")

        # è®¾ç½®è¿è¡ŒçŠ¶æ€
        self.is_running = True
        self.ctx.clear_stop_signal()

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self._worker_thread = threading.Thread(
            target=self._worker_loop, name=f"{self.name}_worker", daemon=True
        )
        self._worker_thread.start()

        self.logger.info(f"Task {self.name} started with worker thread")

    # è¿æ¥ç®¡ç†ç°åœ¨ç”±TaskContextåœ¨æ„é€ æ—¶å®Œæˆï¼Œä¸å†éœ€è¦åŠ¨æ€æ·»åŠ è¿æ¥

    def trigger(self, input_tag: str | None = None, packet: "Packet | None" = None) -> None:
        try:
            self.logger.debug(f"Received data in node {self.name}, channel {input_tag}")
            if packet is not None:
                self.operator.process_packet(packet)  # type: ignore
        except Exception as e:
            self.logger.error(f"Error processing data in node {self.name}: {e}", exc_info=True)
            raise

    def stop(self) -> None:
        """Signal the worker loop to stop."""
        if not self.ctx.is_stop_requested():
            self.ctx.set_stop_signal()
            self.logger.info(f"Node '{self.name}' received stop signal.")
            # ç«‹å³æ ‡è®°ä»»åŠ¡ä¸ºå·²åœæ­¢ï¼Œè¿™æ ·dispatcherå°±èƒ½æ­£ç¡®æ£€æµ‹åˆ°
            self.is_running = False

    def get_object(self):
        return self

    def get_input_buffer(self):
        """
        è·å–è¾“å…¥ç¼“å†²åŒº
        :return: è¾“å…¥ç¼“å†²åŒºå¯¹è±¡
        """
        # é€šè¿‡æè¿°ç¬¦è·å–é˜Ÿåˆ—å®ä¾‹
        return self.input_qd.queue_instance

    def _worker_loop(self) -> None:
        """
        Main worker loop that executes continuously until stop is signaled.
        """
        # è·å– fault_handlerï¼ˆå¦‚æœæœ‰ï¼‰
        fault_handler = None
        if (
            hasattr(self.ctx, "dispatcher")
            and self.ctx.dispatcher
            and hasattr(self.ctx.dispatcher, "fault_handler")
        ):
            fault_handler = self.ctx.dispatcher.fault_handler
            self.logger.debug(f"Task {self.name} has fault_handler: {type(fault_handler).__name__}")

        # Main execution loop
        while not self.ctx.is_stop_requested():
            try:
                # âœ… å®šæœŸä¿å­˜ checkpoint
                if fault_handler:
                    self.save_checkpoint_if_needed(fault_handler)

                if self.is_spout:
                    self.logger.debug(f"Running spout node '{self.name}'")
                    if hasattr(self.operator, "receive_packet"):
                        self.operator.receive_packet(None)  # type: ignore

                    # å¢åŠ å¤„ç†è®¡æ•°
                    self._processed_count += 1

                    # æ£€æŸ¥æ˜¯å¦åœ¨æ‰§è¡Œåæ”¶åˆ°äº†åœæ­¢ä¿¡å·
                    if self.ctx.is_stop_requested():
                        break

                    self.logger.debug(f"self.delay: {self.delay}")
                    if self.delay > 0.002:
                        time.sleep(self.delay)
                else:
                    # For non-spout nodes, fetch input and process
                    try:
                        data_packet = self.input_qd.get(timeout=5.0)
                    except QUEUE_EMPTY_EXCEPTIONS:
                        if self.delay > 0.002:
                            time.sleep(self.delay)
                        continue
                    except Exception as e:
                        self.logger.error(
                            f"Unexpected error fetching data for task {self.name}: {e}",
                            exc_info=True,
                        )
                        if self.delay > 0.002:
                            time.sleep(self.delay)
                        continue

                    self.logger.debug(
                        f"Node '{self.name}' received data packet: {data_packet}, type: {type(data_packet)}"
                    )

                    if data_packet is None:
                        self.logger.info(f"Task {self.name}: Received None packet, continuing loop")
                        if self.delay > 0.002:
                            time.sleep(self.delay)
                        continue

                    # Check if received packet is a StopSignal
                    if isinstance(data_packet, StopSignal):
                        self.logger.info(f"Node '{self.name}' received stop signal: {data_packet}")

                        from sage.kernel.api.operator.join_operator import JoinOperator
                        from sage.kernel.api.operator.sink_operator import SinkOperator

                        if isinstance(self.operator, SinkOperator):
                            self.logger.info(
                                f"SinkOperator {self.name} starting graceful shutdown after stop signal"
                            )
                            self._handle_sink_stop_signal(data_packet)
                            break
                        elif isinstance(self.operator, (JoinOperator)):
                            self.logger.info(
                                f"Calling handle_stop_signal for {type(self.operator).__name__} {self.name}"
                            )
                            input_index = getattr(data_packet, "input_index", None)
                            self.operator.handle_stop_signal(
                                stop_signal_name=data_packet.source,
                                input_index=input_index,
                            )
                            continue

                        # åœæ­¢å½“å‰taskçš„worker loop
                        from sage.kernel.api.operator.filter_operator import (
                            FilterOperator,
                        )
                        from sage.kernel.api.operator.keyby_operator import (
                            KeyByOperator,
                        )
                        from sage.kernel.api.operator.map_operator import MapOperator

                        if isinstance(self.operator, (KeyByOperator, MapOperator, FilterOperator)):
                            self.logger.info(
                                f"Intermediate operator {self.name} received stop signal, draining remaining data first"
                            )
                            drained = self._drain_and_process_remaining(data_packet)
                            self.logger.info(
                                f"Intermediate operator {self.name} drained {drained} packets before forwarding stop signal"
                            )
                            self.router.send_stop_signal(data_packet)
                            try:
                                stop_packet = Packet(payload=data_packet)
                                self.operator.receive_packet(stop_packet)
                            except Exception as e:
                                self.logger.error(
                                    f"Error processing StopSignal in {self.name}: {e}"
                                )
                            self.ctx.send_stop_signal_back(self.name)
                            self.ctx.set_stop_signal()
                            break
                        else:
                            self.router.send_stop_signal(data_packet)
                            should_stop_pipeline = self.ctx.handle_stop_signal(data_packet)
                            if should_stop_pipeline:
                                self.ctx.set_stop_signal()
                                break

                        continue

                    # è®°å½•åŒ…å¤„ç†å¼€å§‹ï¼ˆå¦‚æœå¯ç”¨ç›‘æ§ï¼‰
                    packet_id = None
                    if self._enable_monitoring and self.metrics_collector:
                        packet_id = self.metrics_collector.record_packet_start(
                            packet_id=getattr(data_packet, "packet_id", None),
                            packet_size=getattr(data_packet, "size", 0),
                        )

                    # å¤„ç†æ•°æ®åŒ…
                    try:
                        self.operator.receive_packet(data_packet)

                        # è®°å½•åŒ…å¤„ç†æˆåŠŸ
                        if self._enable_monitoring and self.metrics_collector and packet_id:
                            self.metrics_collector.record_packet_end(
                                packet_id=packet_id,
                                success=True,
                            )
                        self._processed_count += 1

                    except Exception as process_error:
                        # è®°å½•åŒ…å¤„ç†å¤±è´¥
                        if self._enable_monitoring and self.metrics_collector and packet_id:
                            self.metrics_collector.record_packet_end(
                                packet_id=packet_id,
                                success=False,
                                error_type=type(process_error).__name__,
                            )
                        self._error_count += 1
                        raise

            except Exception as e:
                if fault_handler:
                    try:
                        current_state = self.get_state()
                        saved = fault_handler.save_checkpoint(
                            task_id=self.name,
                            state=current_state,
                            force=True,  # å¼ºåˆ¶ä¿å­˜ï¼Œå¿½ç•¥æ—¶é—´é—´éš”
                        )
                        if saved:
                            self.logger.info(
                                f"ğŸ’¾ Checkpoint saved on exception for task {self.name} "
                                f"(processed={self._processed_count}, errors={self._error_count})"
                            )
                    except Exception as checkpoint_error:
                        self.logger.warning(
                            f"Failed to save checkpoint on exception: {checkpoint_error}"
                        )
                # âœ… æ•è·å¼‚å¸¸å¹¶ä½¿ç”¨å®¹é”™å¤„ç†å™¨
                self.logger.error(f"Critical error in node '{self.name}': {str(e)}", exc_info=True)
                self._error_count += 1

                # é€šçŸ¥ dispatcher å¤„ç†å¤±è´¥
                if fault_handler:
                    handled = fault_handler.handle_failure(self.name, e)
                    if handled:
                        self.logger.info(
                            f"Task {self.name} failure was handled by fault tolerance, "
                            f"task will be restarted"
                        )
                        # ä»»åŠ¡å°†è¢«é‡å¯ï¼Œé€€å‡ºå½“å‰ worker loop
                        break
                    else:
                        self.logger.error(
                            f"Task {self.name} failure could not be handled, stopping..."
                        )
                        break
                else:
                    # æ²¡æœ‰ dispatcher æˆ–å®¹é”™å¤„ç†å™¨ï¼Œç›´æ¥åœæ­¢
                    self.logger.error(
                        f"No dispatcher available for fault handling, task {self.name} stopping"
                    )
                    break

        self.is_running = False
        self.logger.info(f"Task {self.name} worker loop exited")

    @property
    def is_spout(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º spout èŠ‚ç‚¹"""
        return self.ctx.is_spout

    @property
    def delay(self) -> float:
        """è·å–ä»»åŠ¡çš„å»¶è¿Ÿæ—¶é—´"""
        return self.ctx.delay

    @property
    def logger(self):
        """è·å–å½“å‰ä»»åŠ¡çš„æ—¥å¿—è®°å½•å™¨"""
        return self.ctx.logger

    @property
    def name(self) -> str:
        """è·å–ä»»åŠ¡åç§°"""
        return self.ctx.name

    def cleanup(self):
        """æ¸…ç†ä»»åŠ¡èµ„æº"""
        self.logger.info(f"Cleaning up task {self.name}")

        try:
            # åœæ­¢ä»»åŠ¡
            if self.is_running:
                self.stop()

            # åœæ­¢ç›‘æ§ç»„ä»¶
            if self._enable_monitoring:
                if self.metrics_reporter:
                    self.metrics_reporter.stop_reporting()
                if self.resource_monitor:
                    self.resource_monitor.stop_monitoring()
                self.logger.debug(f"Stopped monitoring for task {self.name}")

            # # æ¸…ç†ç®—å­èµ„æº
            # if hasattr(self.operator, 'cleanup'):
            #     self.operator.cleanup()
            # è¿™äº›å†…å®¹åº”è¯¥ä¼šè‡ªå·±æ¸…ç†æ‰
            # # æ¸…ç†è·¯ç”±å™¨
            # if hasattr(self.router, 'cleanup'):
            #     self.router.cleanup()

            # æ¸…ç†è¾“å…¥é˜Ÿåˆ—æè¿°ç¬¦
            if self.input_qd:
                if hasattr(self.input_qd, "cleanup"):
                    self.input_qd.cleanup()  # type: ignore
                elif hasattr(self.input_qd, "close"):
                    self.input_qd.close()  # type: ignore

            # æ¸…ç†è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬service_managerï¼‰
            if hasattr(self.ctx, "cleanup"):
                self.ctx.cleanup()

            self.logger.debug(f"Task {self.name} cleanup completed")

        except Exception as e:
            self.logger.error(f"Error during cleanup of task {self.name}: {e}")

    def _handle_sink_stop_signal(self, stop_signal: "StopSignal"):
        """Gracefully drain in-flight data before finalizing a sink task."""
        drain_timeout = getattr(self.operator, "drain_timeout", 10.0)
        quiet_period = getattr(self.operator, "drain_quiet_period", 0.3)
        drained = self._drain_inflight_messages(
            timeout=drain_timeout,
            quiet_period=quiet_period,
        )

        if drained == -1:
            self.logger.warning(f"Sink task {self.name} timed out while draining in-flight data")
        else:
            self.logger.info(
                f"Sink task {self.name} drained {drained} in-flight packets before shutdown"
            )

        # å®Œæˆæœ€ç»ˆçš„å…³é—­é€»è¾‘
        try:
            if hasattr(self.operator, "handle_stop_signal"):
                self.operator.handle_stop_signal()  # type: ignore
        except Exception as e:
            self.logger.error(
                f"Error during sink operator finalization for {self.name}: {e}",
                exc_info=True,
            )

        # é€šè¿‡ä¸Šä¸‹æ–‡é€šçŸ¥JobManagerå¹¶ä¼ æ’­åœæ­¢ä¿¡å·
        try:
            self.ctx.handle_stop_signal(stop_signal)
        finally:
            self.ctx.set_stop_signal()

    def _drain_and_process_remaining(self, stop_signal: "StopSignal") -> int:
        """Drain and process remaining packets before forwarding stop signal."""
        if not self.input_qd:
            return 0
        drained_packets = 0
        timeout = 5.0
        quiet_period = 0.5
        poll_interval = 0.1
        start_time = time.time()
        last_packet_time = start_time
        self.logger.debug(f"Intermediate task {self.name} draining remaining packets")
        while True:
            elapsed = time.time() - start_time
            if elapsed >= timeout:
                self.logger.warning(f"Intermediate task {self.name} timed out while draining")
                break
            try:
                packet = self.input_qd.get(timeout=poll_interval)
            except QUEUE_EMPTY_EXCEPTIONS:
                if time.time() - last_packet_time >= quiet_period:
                    break
                continue
            if isinstance(packet, StopSignal):
                continue
            try:
                self.operator.receive_packet(packet)
                drained_packets += 1
                last_packet_time = time.time()
            except Exception as e:
                self.logger.error(f"Failed to process drained packet in {self.name}: {e}")
        return drained_packets

    def _drain_inflight_messages(
        self,
        timeout: float,
        quiet_period: float,
    ) -> int:
        """Drain packets that arrived before the stop signal reached the sink."""
        if not self.input_qd:
            return 0

        start_time = time.time()
        last_packet_time = start_time
        drained_packets = 0
        poll_interval = min(quiet_period, 0.1)

        self.logger.debug(
            f"Sink task {self.name} draining queues with timeout={timeout}s and quiet_period={quiet_period}s"
        )

        while True:
            elapsed = time.time() - start_time
            if elapsed >= timeout:
                return -1

            try:
                packet = self.input_qd.get(timeout=poll_interval)
            except QUEUE_EMPTY_EXCEPTIONS:
                if time.time() - last_packet_time >= quiet_period:
                    break
                continue

            if isinstance(packet, StopSignal):
                # å¦‚æœè¿˜æœ‰å…¶ä»–åœæ­¢ä¿¡å·ï¼Œç»§ç»­ç­‰å¾…æ•°æ®æ’ç©º
                continue

            try:
                self.operator.receive_packet(packet)
                drained_packets += 1
                last_packet_time = time.time()
            except Exception as e:
                self.logger.error(
                    f"Failed to process in-flight packet during draining for {self.name}: {e}",
                    exc_info=True,
                )

        return drained_packets

    # === Performance Monitoring API ===

    def get_current_metrics(self) -> TaskPerformanceMetrics | None:
        """
        è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡

        Returns:
            TaskPerformanceMetrics å®ä¾‹ï¼Œå¦‚æœç›‘æ§æœªå¯ç”¨åˆ™è¿”å› None
        """
        if not self._enable_monitoring or not self.metrics_collector:
            return None

        metrics = self.metrics_collector.get_real_time_metrics()

        # æ·»åŠ èµ„æºç›‘æ§æ•°æ®
        if self.resource_monitor:
            cpu, memory = self.resource_monitor.get_current_usage()
            metrics.cpu_usage_percent = cpu
            metrics.memory_usage_mb = memory

        # æ·»åŠ é˜Ÿåˆ—æ·±åº¦
        if self.input_qd:
            try:
                queue_instance = self.input_qd.queue_instance
                if queue_instance and hasattr(queue_instance, "qsize"):
                    metrics.input_queue_depth = queue_instance.qsize()
            except Exception:
                pass

        return metrics

    def reset_metrics(self) -> None:
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        if self.metrics_collector:
            self.metrics_collector.reset_metrics()

    def export_metrics(self, format: str = "json") -> str | None:
        """
        å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡

        Args:
            format: å¯¼å‡ºæ ¼å¼ ("json", "prometheus", "csv", "human")

        Returns:
            æ ¼å¼åŒ–çš„æŒ‡æ ‡å­—ç¬¦ä¸²ï¼Œå¦‚æœç›‘æ§æœªå¯ç”¨åˆ™è¿”å› None
        """
        if not self._enable_monitoring or not self.metrics_reporter:
            return None

        return self.metrics_reporter.generate_report(format=format)
