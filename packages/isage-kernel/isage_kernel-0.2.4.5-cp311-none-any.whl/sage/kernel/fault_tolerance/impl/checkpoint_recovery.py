"""
Checkpoint-based Fault Tolerance Strategy

åŸºäºæ£€æŸ¥ç‚¹çš„å®¹é”™æ¢å¤ç­–ç•¥ï¼Œå‘¨æœŸæ€§ä¿å­˜ä»»åŠ¡çŠ¶æ€ï¼Œå¤±è´¥æ—¶ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹æ¢å¤ã€‚
"""

import time
from typing import TYPE_CHECKING, Any

from sage.common.core import TaskID
from sage.kernel.fault_tolerance.base import BaseFaultHandler
from sage.kernel.fault_tolerance.impl.checkpoint_impl import CheckpointManagerImpl

if TYPE_CHECKING:
    from sage.kernel.runtime.dispatcher import Dispatcher


class CheckpointBasedRecovery(BaseFaultHandler):
    """
    åŸºäº Checkpoint çš„å®¹é”™æ¢å¤ç­–ç•¥

    å®šæœŸä¿å­˜ä»»åŠ¡çŠ¶æ€ï¼Œå¤±è´¥æ—¶ä»æœ€è¿‘çš„ checkpoint æ¢å¤ã€‚
    é€‚ç”¨äºé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿå‡å°‘é‡æ–°è®¡ç®—çš„å¼€é”€ã€‚
    """

    def __init__(
        self,
        checkpoint_manager: CheckpointManagerImpl | None = None,
        checkpoint_interval: float = 60.0,
        max_recovery_attempts: int = 3,
        checkpoint_dir: str = ".sage/checkpoints",
    ):
        """
        åˆå§‹åŒ– Checkpoint å®¹é”™ç­–ç•¥

        Args:
            checkpoint_manager: Checkpoint ç®¡ç†å™¨
            checkpoint_interval: Checkpoint ä¿å­˜é—´éš”ï¼ˆç§’ï¼‰
            max_recovery_attempts: æœ€å¤§æ¢å¤å°è¯•æ¬¡æ•°
            checkpoint_dir: Checkpoint å­˜å‚¨ç›®å½•
        """
        self.checkpoint_manager = checkpoint_manager or CheckpointManagerImpl(checkpoint_dir)
        self.checkpoint_interval = checkpoint_interval
        self.max_recovery_attempts = max_recovery_attempts

        # è®°å½•å¤±è´¥ä¿¡æ¯
        self.failure_counts: dict[TaskID, int] = {}
        self.last_checkpoint_time: dict[TaskID, float] = {}

        self.logger = None  # å¯ä»¥åç»­æ³¨å…¥
        self.dispatcher: Dispatcher | None = None  # å¯ä»¥åç»­æ³¨å…¥

    def handle_failure(self, task_id: TaskID, error: Exception) -> bool:
        """
        å¤„ç†ä»»åŠ¡å¤±è´¥

        Args:
            task_id: å¤±è´¥çš„ä»»åŠ¡ ID
            error: å¤±è´¥çš„å¼‚å¸¸ä¿¡æ¯

        Returns:
            True å¦‚æœå¤„ç†æˆåŠŸ
        """
        # è®°å½•å¤±è´¥
        self.failure_counts[task_id] = self.failure_counts.get(task_id, 0) + 1

        if self.logger:
            self.logger.warning(
                f"Task {task_id} failed (attempt #{self.failure_counts[task_id]}): {error}"
            )

        # è°ƒç”¨å›è°ƒ
        self.on_failure_detected(task_id, error)

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
        if self.can_recover(task_id):
            return self.recover(task_id)
        else:
            if self.logger:
                self.logger.error(f"Task {task_id} cannot be recovered (max attempts reached)")
            return False

    def can_recover(self, task_id: TaskID) -> bool:
        """
        æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯ä»¥æ¢å¤

        Args:
            task_id: ä»»åŠ¡ ID

        Returns:
            True å¦‚æœä»»åŠ¡å¯ä»¥æ¢å¤
        """
        failure_count = self.failure_counts.get(task_id, 0)
        has_checkpoint = len(self.checkpoint_manager.list_checkpoints(task_id)) > 0

        return failure_count < self.max_recovery_attempts and has_checkpoint

    def _is_remote_task(self, task_id: TaskID) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¿œç¨‹ä»»åŠ¡"""
        if not hasattr(self, "dispatcher") or not self.dispatcher:
            return False
        task = self.dispatcher.tasks.get(task_id)
        from sage.kernel.utils.ray.actor import ActorWrapper

        return isinstance(task, ActorWrapper)

    def recover(self, task_id: TaskID) -> bool:
        """
        ä» Checkpoint æ¢å¤ä»»åŠ¡ï¼ˆæœ¬åœ°æˆ–è¿œç¨‹ï¼‰
        """
        self.on_recovery_started(task_id)
        try:
            state = self.checkpoint_manager.load_checkpoint(task_id)
            if state is None:
                if self.logger:
                    self.logger.error(f"No checkpoint found for task {task_id}")
                self.on_recovery_completed(task_id, False)
                return False

            if self.logger:
                self.logger.info(
                    f"Loaded checkpoint for task {task_id}, "
                    f"processed_count={state.get('processed_count', 0)}, "
                    f"checkpoint_counter={state.get('checkpoint_counter', 0)}"
                )

            if not hasattr(self, "dispatcher") or not self.dispatcher:
                if self.logger:
                    self.logger.error("No dispatcher available for recovery")
                self.on_recovery_completed(task_id, False)
                return False

            success = self.dispatcher.restart_task_with_state(task_id, state)

            if success and self.logger:
                self.logger.info(f"Task {task_id} restarted and state restored")
            elif not success and self.logger:
                self.logger.error(f"Failed to restart task {task_id}")

            self.on_recovery_completed(task_id, success)
            return success

        except Exception as e:
            if self.logger:
                self.logger.error(f"Recover task {task_id} failed: {e}", exc_info=True)
            self.on_recovery_completed(task_id, False)
            return False

    def on_recovery_started(self, task_id: TaskID):
        """æ¢å¤å¼€å§‹æ—¶çš„å›è°ƒ"""
        if self.logger:
            self.logger.info(f"ğŸ”„ Starting recovery for task {task_id}")

    def on_recovery_completed(self, task_id: TaskID, success: bool):
        """æ¢å¤å®Œæˆæ—¶çš„å›è°ƒ"""
        if self.logger:
            if success:
                self.logger.info(f"âœ… Recovery completed successfully for task {task_id}")
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘ï¼Œå¦‚ï¼š
                # - å‘é€é€šçŸ¥
                # - è®°å½•æŒ‡æ ‡
                # - è§¦å‘å‘Šè­¦è§£é™¤
            else:
                self.logger.error(f"âŒ Recovery failed for task {task_id}")
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¤±è´¥å¤„ç†é€»è¾‘ï¼Œå¦‚ï¼š
                # - å‘é€å‘Šè­¦
                # - è®°å½•å¤±è´¥åŸå› 
                # - è§¦å‘å¤‡ç”¨æ–¹æ¡ˆ

    def on_failure_detected(self, task_id: TaskID, error: Exception):
        """æ£€æµ‹åˆ°å¤±è´¥æ—¶çš„å›è°ƒ"""
        if self.logger:
            self.logger.warning(f"âš ï¸ Failure detected for task {task_id}: {error}")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘ï¼Œå¦‚ï¼š
            # - å‘é€å‘Šè­¦é€šçŸ¥
            # - è®°å½•å¤±è´¥æ¨¡å¼
            # - æ›´æ–°ç›‘æ§é¢æ¿

    def save_checkpoint(self, task_id: TaskID, state: dict[str, Any], force: bool = False) -> bool:
        """
        ä¿å­˜ä»»åŠ¡ checkpoint

        Args:
            task_id: ä»»åŠ¡ ID
            state: ä»»åŠ¡çŠ¶æ€
            force: æ˜¯å¦å¼ºåˆ¶ä¿å­˜ï¼ˆå¿½ç•¥æ—¶é—´é—´éš”ï¼‰

        Returns:
            True å¦‚æœä¿å­˜æˆåŠŸ
        """
        current_time = time.time()
        last_time = self.last_checkpoint_time.get(task_id, 0)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
        if not force and (current_time - last_time) < self.checkpoint_interval:
            return False

        try:
            self.checkpoint_manager.save_checkpoint(task_id, state)
            self.last_checkpoint_time[task_id] = current_time

            if self.logger:
                self.logger.debug(f"Saved checkpoint for task {task_id}")

            return True

        except Exception as e:
            if self.logger:
                self.logger.error(f"Failed to save checkpoint for {task_id}: {e}")
            return False

    def cleanup_checkpoints(self, task_id: TaskID):
        """
        æ¸…ç†ä»»åŠ¡çš„æ‰€æœ‰ checkpoint

        Args:
            task_id: ä»»åŠ¡ ID
        """
        try:
            self.checkpoint_manager.delete_checkpoint(task_id)

            if task_id in self.failure_counts:
                del self.failure_counts[task_id]
            if task_id in self.last_checkpoint_time:
                del self.last_checkpoint_time[task_id]

        except Exception as e:
            if self.logger:
                self.logger.error(f"Failed to cleanup checkpoints for {task_id}: {e}")


__all__ = ["CheckpointBasedRecovery"]
