"""----------------------------------------------------------------------------
help_anomaly
----------------------------------------------------------------------------

For a nice overview of the anomaly pipeline run the following lines of code:

>> from anomaly_pipeline.helpers.help_anomaly import help_anomaly
>> help_anomaly()

You can see information about specific models used in the anomaly pipeline with any of the following commands:

>> help_anomaly('percentile')
>> help_anomaly('iqr')
>> help_anomaly('mad')
>> help_anomaly('std')
>> help_anomaly('ewma')
>> help_anomaly('prophet')
>> help_anomaly('dbscan')
>> help_anomaly('iso') # For information on isolation forest


----------------------------------------------------------------------------
Functional Overview
----------------------------------------------------------------------------
The pipeline takes raw master data, partitions it into groups by unique ID, applies a suite of 8 different anomaly detection methods, and then flags observations as anomalies where at least half of the models consider the observation an anomaly.

The master data DataFrame that you pass into the anomaly detection pipeline needs to have at least 3 columns - unique ID, date, and a target variable. The unique ID can be defined by multiple columns.

Core Execution Stages
 1. Preprocessing & Interpolation
    Before modeling, the function interpolates target variable values for missing dates
    Fill gaps in the variable column to prevent model crashes.

 2. Statistical Baseline Models (Local Execution)
    The pipeline first runs four computationally light models sequentially on each group:
    - Percentile & IQR: Non-parametric bounds detection.
    - SD (Standard Deviation) & MAD (Median Absolute Deviation): Variance-based detection.

 3. Parallel Machine Learning Suite (process_group)
    To maximize performance, the pipeline uses joblib.Parallel to run intensive models across all available CPU cores. The process_group utility acts as a router, sending data to the correct engine based on the model key:
    - FB (Prophet): Walk-forward time-series forecasting.
    - EWMA: Exponentially weighted moving averages.
    - ISF (Isolation Forest): Unsupervised isolation of anomalies.
    - DBSCAN: Density-based spatial clustering.

 4. Majority Voting (Ensemble Logic)
    The power of this pipeline lies in its Consensus Model. After all models finish, the pipeline calculates:
    - Anomaly_Votes: The sum of flags across all 8 methods.
    - is_Anomaly: A final boolean set to True only if at least 4 models agree that the point is an outlier.

Key Output Columns
   refresh_date: The timestamp of when the pipeline was executed.
   Anomaly_Votes: Total count of models that flagged the row.
   is_Anomaly: The final "Gold Standard" anomaly flag.
   Individual Model Flags: Columns like is_FB_anomaly, is_IQR_anomaly, etc., for granular auditing.

Usage Context
   Use run_pipeline when you need a highly reliable, automated output. By combining statistical, forecasting, and clustering models, the pipeline reduces "false positives" often generated by single-model approaches.
"""

from .main import timeseries_anomaly_detection
from .helpers import (help_anomaly, anomaly_eval_plot, 
    anomaly_percentile_plot,
    anomaly_sd_plot, 
    anomaly_mad_plot, 
    anomaly_iqr_plot, 
    anomaly_ewma_plot, 
    anomaly_fb_plot, 
    anomaly_dbscan_plot, 
    anomaly_isolation_forest_plot)