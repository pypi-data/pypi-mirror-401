"""AWS Bedrock model profiles.

Defines capabilities and configuration for Bedrock models (January 2026).

Supported model families:
- Anthropic Claude 4.5/4.1/4 (opus, sonnet, haiku)
- Anthropic Claude 3.7/3.5/3 (legacy)
- Amazon Nova (premier, pro, lite, micro, nova-2-lite)
- Meta Llama 4 (scout) and Llama 3.x
- Mistral (large-3, pixtral-large, ministral-3)
- Cohere (command-r-plus, command-r)
- DeepSeek (r1)
"""

from __future__ import annotations

from . import ModelProfile

PROFILES: dict[str, ModelProfile] = {
    # Anthropic Claude 4.5 on Bedrock
    "anthropic.claude-opus-4-5-20251101-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,  # Supports extended thinking
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=64000,
    ),
    "anthropic.claude-sonnet-4-5-20250929-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=64000,
    ),
    "anthropic.claude-haiku-4-5-20251001-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=64000,
    ),
    # Anthropic Claude 4.1/4.0 on Bedrock
    "anthropic.claude-opus-4-1-20250805-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=32000,
    ),
    "anthropic.claude-sonnet-4-20250514-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=64000,
    ),
    "anthropic.claude-opus-4-20250514-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=32000,
    ),
    # Anthropic Claude 3.7 on Bedrock
    "anthropic.claude-3-7-sonnet-20250219-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=64000,
    ),
    # Anthropic Claude 3.5 on Bedrock (legacy)
    "anthropic.claude-3-5-sonnet": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=8192,
    ),
    "anthropic.claude-3-5-sonnet-20241022-v2:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=8192,
    ),
    "anthropic.claude-3-5-haiku": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=8192,
    ),
    "anthropic.claude-3-5-haiku-20241022-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=8192,
    ),
    # Anthropic Claude 3 on Bedrock (legacy)
    "anthropic.claude-3-opus": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=4096,
    ),
    "anthropic.claude-3-sonnet": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=4096,
    ),
    "anthropic.claude-3-haiku": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=4096,
    ),
    "anthropic.claude-3-haiku-20240307-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=False,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=200000,
        max_output_tokens=4096,
    ),
    # Amazon Nova models
    "amazon.nova-premier-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    "amazon.nova-pro-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    "amazon.nova-pro": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    "amazon.nova-lite-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    "amazon.nova-lite": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    "amazon.nova-micro-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=5000,
    ),
    "amazon.nova-micro": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=5000,
    ),
    # Meta Llama 4 on Bedrock
    "meta.llama4-maverick-17b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=4096,
    ),
    "meta.llama4-scout-17b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=4096,
    ),
    # Meta Llama 3.x on Bedrock
    "meta.llama3-3-70b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-3-70b-instruct": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-2-90b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-2-11b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-1-405b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=4096,
    ),
    "meta.llama3-1-70b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-1-70b-instruct": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-1-8b-instruct-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    "meta.llama3-1-8b-instruct": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=128000,
        max_output_tokens=2048,
    ),
    # Amazon Nova 2 models
    "amazon.nova-2-lite-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=300000,
        max_output_tokens=5000,
    ),
    # Mistral models on Bedrock
    "mistral.mistral-large-3-675b-instruct": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=8192,
    ),
    "mistral.mistral-large-2407-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=8192,
    ),
    "mistral.pixtral-large-2502-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=8192,
    ),
    "mistral.ministral-3-8b-instruct": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="tools",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=32000,
        max_output_tokens=4096,
    ),
    # Cohere models on Bedrock
    "cohere.command-r-plus-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=4096,
    ),
    "cohere.command-r-v1:0": ModelProfile(
        supports_schema_guided_output=True,
        supports_json_only_output=True,
        supports_tools=True,
        supports_reasoning=False,
        supports_streaming=True,
        default_output_mode="native",
        native_structured_kind="bedrock_tool_use",
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=True,
        max_context_tokens=128000,
        max_output_tokens=4096,
    ),
    # DeepSeek models on Bedrock
    "deepseek.r1-v1:0": ModelProfile(
        supports_schema_guided_output=False,
        supports_json_only_output=True,
        supports_tools=False,  # R1 focuses on reasoning, limited tool support
        supports_reasoning=True,
        supports_streaming=True,
        default_output_mode="prompted",  # Uses prompted JSON output
        native_structured_kind="bedrock_tool_use",  # Falls back to prompted mode
        schema_transformer_name="BedrockJsonSchemaTransformer",
        strict_mode_default=False,
        max_context_tokens=64000,
        max_output_tokens=8192,
    ),
}
