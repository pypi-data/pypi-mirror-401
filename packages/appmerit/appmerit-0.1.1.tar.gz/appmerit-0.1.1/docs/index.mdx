---
title: "Welcome to Merit"
description: "Merit is a Python testing framework for AI projects. It follows pytest syntax and culture while introducing components essential for testing AI software: metrics, datasets, and traces."
---

### Why Merit exists

You can assess AI agent and workflow quality using two major approaches. 

- One is to treat them as AI models and measure quality using metrics and datasets. 
- The other is to treat them as software and write automated tests case-by-case.

Merit was built for developers who want **both worlds in one framework**.

### Why developers choose Merit

<Columns cols={2}>
  <Card title="Merit vs evals" href="/get-started/merit-vs-evals">
    - native Python syntax instead of custom DSL
    - small granular predicates instead of bloated evaluators
    - cases, assertions, metrics are defined in code, not abstracted away
    - fully composable, no need for going all-in from the beginning
  </Card>
  <Card title="Merit vs pytest" href="/concepts/merit">
    - access OTEL traces for assertions within the test
    - collect failed assertions into metrics
    - use semantic predicates to assert natural language
    - aggregate results for thousands of iterated cases
  </Card>
</Columns>

### Get started

<CardGroup cols={3}>
  <Card title="Quick Start" icon="rocket" href="/get-started/quick-start">
    Write your first merit in 5 minutes
  </Card>
  <Card title="Merit Functions" icon="code" href="/concepts/merit">
    Discovery, parametrization, and dependency injection
  </Card>
  <Card title="Semantic Predicates" icon="brain" href="/concepts/semantic-predicates">
    LLM-powered assertions for natural language
  </Card>
</CardGroup>
