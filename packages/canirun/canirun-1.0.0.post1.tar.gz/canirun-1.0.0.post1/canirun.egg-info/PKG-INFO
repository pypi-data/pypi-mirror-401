Metadata-Version: 2.4
Name: canirun
Version: 1.0.0.post1
Summary: Check if you can run a Hugging Face model locally.
Author-email: Varun Agnihotri <hello@pythonicvarun.me>
Project-URL: Homepage, https://github.com/PythonicVarun/canirun
Project-URL: Repository, https://github.com/PythonicVarun/canirun.git
Project-URL: Issues, https://github.com/PythonicVarun/canirun/issues
Keywords: huggingface,hardware,quantization,llm,cli,torch
Classifier: Development Status :: 3 - Alpha
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: System :: Hardware
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: Operating System :: OS Independent
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: huggingface-hub~=1.3.1
Requires-Dist: psutil~=7.2.1
Requires-Dist: torch~=2.9.1
Requires-Dist: tabulate~=0.9.0
Requires-Dist: click~=8.3.1
Requires-Dist: numpy~=2.2.6
Dynamic: license-file

# canirun

A lightweight CLI to estimate hardware requirements and quantization compatibility for Hugging Face models.

[![PyPI version](https://badge.fury.io/py/canirun.svg)](https://badge.fury.io/py/canirun)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Key Features

- **Hardware Detection**: Automatically detects your CPU/GPU and available VRAM/RAM.
- **Memory Estimation**: Estimates the memory required to run a given Hugging Face model.
- **Quantization Analysis**: Checks compatibility for different quantization levels (e.g., 4-bit, 8-bit, 16-bit).
- **Simple CLI & API**: Easy to use from the command line or integrate into your Python projects.

## Installation

You can install `canirun` using pip:

```bash
pip install canirun
```

## CLI Usage

The `canirun` command allows you to quickly check a model from your terminal.

```bash
canirun <model_id> [OPTIONS]
```

### Example

Let's check if `meta-llama/Meta-Llama-3-8B` can run on the local hardware:

```bash
canirun meta-llama/Meta-Llama-3-8B --ctx 4096
```

This will produce a report like this:

```
 ğŸ” ANALYSIS REPORT: meta-llama/Meta-Llama-3-8B 
 Context Length  : 4096
 Device          : NVIDIA GeForce RTX 3090
 VRAM / RAM      : 24.0 GB / 64.0 GB

â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ Quantization   â”‚   Total Est. â”‚   KV Cache â”‚ Compatibility          â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ FP16           â”‚     16.96 GB â”‚  512.00 MB â”‚ âœ… GPU                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ INT8           â”‚      9.48 GB â”‚  512.00 MB â”‚ âœ… GPU                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4-bit          â”‚      6.30 GB â”‚  512.00 MB â”‚ âœ… GPU                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2-bit          â”‚      4.34 GB â”‚  512.00 MB â”‚ âœ… GPU                 â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›
```

## API Usage

You can also use `canirun` programmatically in your Python code.

```python
from canirun import canirun

model_id = "mistralai/Mistral-7B-v0.1"

# Analyze the model
result = canirun(model_id, context_length=2048)

if result and result.is_supported:
    print(f"'{model_id}' is supported on your hardware!")

    # Get the detailed report
    report = result.report()
    for quant_result in report:
        print(f"- {quant_result['quant']}: {quant_result['status']}")
else:
    print(f"'{model_id}' is not supported on your hardware.")

```

## How It Works

`canirun` works by:
1. Fetching the model's configuration from the Hugging Face Hub.
2. Calculating the memory required for the model's parameters.
3. Estimating the size of the KV cache based on the context length and model architecture.
4. Comparing the estimated memory requirements with your system's available VRAM (if a GPU is detected) or RAM.

The tool checks for different levels of quantization to see if a smaller, quantized version of the model could fit.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
