"""
    Alchemite

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)  # noqa: E501
    Contact: support@intellegens.com
    Generated by: https://openapi-generator.tech
"""


import re  # noqa: F401
import sys  # noqa: F401

from alchemite_apiclient.model_utils import (  # noqa: F401
    ApiTypeError,
    ModelComposed,
    ModelNormal,
    ModelSimple,
    cached_property,
    change_keys_js_to_python,
    convert_js_args_to_python_args,
    date,
    datetime,
    file_type,
    none_type,
    validate_get_composed_info,
    OpenApiModel
)
from alchemite_apiclient.exceptions import ApiAttributeError


def lazy_import():
    from alchemite_apiclient.model.hyperopt_target_function import HyperoptTargetFunction
    from alchemite_apiclient.model.models_id_train_permitted_column_relationships import ModelsIdTrainPermittedColumnRelationships
    from alchemite_apiclient.model.validation_split import ValidationSplit
    globals()['HyperoptTargetFunction'] = HyperoptTargetFunction
    globals()['ModelsIdTrainPermittedColumnRelationships'] = ModelsIdTrainPermittedColumnRelationships
    globals()['ValidationSplit'] = ValidationSplit


class TrainRequest(ModelNormal):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Attributes:
      allowed_values (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          with a capitalized key describing the allowed value and an allowed
          value. These dicts store the allowed enum values.
      attribute_map (dict): The key is attribute name
          and the value is json key in definition.
      discriminator_value_class_map (dict): A dict to go from the discriminator
          variable value to the discriminator class name.
      validations (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          that stores validations for max_length, min_length, max_items,
          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,
          inclusive_minimum, and regex.
      additional_properties_type (tuple): A tuple of classes accepted
          as additional properties values.
    """

    allowed_values = {
        ('validation',): {
            'NONE': "none",
            '80/20': "80/20",
            '5-FOLD': "5-fold",
            'CUSTOM': "custom",
        },
        ('hyperparameter_optimization',): {
            'NONE': "none",
            'RANDOM': "random",
            'TPE': "TPE",
        },
    }

    validations = {
        ('validation_target_columns',): {
            'min_items': 1,
        },
        ('validation_splits',): {
            'max_items': 10,
            'min_items': 1,
        },
        ('permitted_column_relationships',): {
            'min_items': 1,
        },
        ('max_number_samples',): {
            'inclusive_maximum': 2000,
            'inclusive_minimum': 1,
        },
        ('exploration_exploitation',): {
            'inclusive_maximum': 1,
            'inclusive_minimum': 0,
        },
    }

    @cached_property
    def additional_properties_type():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded
        """
        return (bool, date, datetime, dict, float, int, list, str, none_type,)  # noqa: E501

    _nullable = True

    @cached_property
    def openapi_types():  # noqa
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded

        Returns
            openapi_types (dict): The key is attribute name
                and the value is attribute type.
        """
        lazy_import()
        return {
            'validation': (str,),  # noqa: E501
            'validation_target_columns': ([str], none_type,),  # noqa: E501
            'validation_splits': ([ValidationSplit],),  # noqa: E501
            'hyperparameter_optimization': (str,),  # noqa: E501
            'bespoke_column_hyperparameters': (bool,),  # noqa: E501
            'hyperparameters': ({str: (bool, date, datetime, dict, float, int, list, str, none_type)},),  # noqa: E501
            'fraction_data_present': ([float],),  # noqa: E501
            'virtual_experiment_validation': (bool,),  # noqa: E501
            'virtual_training': (bool,),  # noqa: E501
            'permitted_column_relationships': ([ModelsIdTrainPermittedColumnRelationships],),  # noqa: E501
            'enable_training_dataset_outliers': (bool, none_type,),  # noqa: E501
            'max_number_samples': (int,),  # noqa: E501
            'target_function': (HyperoptTargetFunction,),  # noqa: E501
            'exploration_exploitation': (float,),  # noqa: E501
        }

    @cached_property
    def discriminator():  # noqa
        return None


    attribute_map = {
        'validation': 'validation',  # noqa: E501
        'validation_target_columns': 'validationTargetColumns',  # noqa: E501
        'validation_splits': 'validationSplits',  # noqa: E501
        'hyperparameter_optimization': 'hyperparameterOptimization',  # noqa: E501
        'bespoke_column_hyperparameters': 'bespokeColumnHyperparameters',  # noqa: E501
        'hyperparameters': 'hyperparameters',  # noqa: E501
        'fraction_data_present': 'fractionDataPresent',  # noqa: E501
        'virtual_experiment_validation': 'virtualExperimentValidation',  # noqa: E501
        'virtual_training': 'virtualTraining',  # noqa: E501
        'permitted_column_relationships': 'permittedColumnRelationships',  # noqa: E501
        'enable_training_dataset_outliers': 'enableTrainingDatasetOutliers',  # noqa: E501
        'max_number_samples': 'maxNumberSamples',  # noqa: E501
        'target_function': 'targetFunction',  # noqa: E501
        'exploration_exploitation': 'explorationExploitation',  # noqa: E501
    }

    read_only_vars = {
    }

    _composed_schemas = {}

    @classmethod
    @convert_js_args_to_python_args
    def _from_openapi_data(cls, *args, **kwargs):  # noqa: E501
        """TrainRequest - a model defined in OpenAPI

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            validation (str): Methods for validating the model using the provided dataset. If set to \"custom\", `validationSplits` must also be provided. [optional] if omitted the server will use the default value of "none"  # noqa: E501
            validation_target_columns ([str], none_type): A list of the training dataset's column names that the model's validation metric will be the median average over.  Cannot include descriptor columns.  If not provided then the model's validation metric will be the median average over all non-descriptor columns.. [optional]  # noqa: E501
            validation_splits ([ValidationSplit]): A list of test-train pairs, describing the row IDs of the training dataset to include in each validation split. Must be provided exactly when `validation` is 'custom'. . [optional]  # noqa: E501
            hyperparameter_optimization (str): Search method for finding the optimal parameters to use when training the model.  If 'none' then the parameters as specified by the 'hyperparameters' argument will be used to train the model.  If 'hyperparameterOptimization' is not 'none' then 'validation' must also be not 'none'.. [optional] if omitted the server will use the default value of "none"  # noqa: E501
            bespoke_column_hyperparameters (bool): Whether to use bespoke hyperparameters for each target column. If false, hyperparameters are shared between columns. Defaults to true.. [optional] if omitted the server will use the default value of True  # noqa: E501
            hyperparameters ({str: (bool, date, datetime, dict, float, int, list, str, none_type)}): The hyperparameters which Alchemite will use when training.  If 'hyperparameterOptimization' is 'none', these hyperparameters will be used for training, otherwise the hyperparameters will be used as a starting point for optimization.  If no argument is provided for this parameter and the model was previously trained then the existing hyperparameters will be used to train the model, otherwise Alchemite's default hyperparameters will be used instead.. [optional]  # noqa: E501
            fraction_data_present ([float]): The fraction of data expected to be in each column of the datasets which the model will make predictions from.  The list 'fractionDataPresent' should be ordered corresponding to the training dataset's 'columnHeaders' parameter.. [optional]  # noqa: E501
            virtual_experiment_validation (bool): If true then only the descriptor columns will be used to make predictions when computing the validation metric.. [optional] if omitted the server will use the default value of False  # noqa: E501
            virtual_training (bool): If true then only the descriptor columns will be used as input in the first iteration of training. [optional] if omitted the server will use the default value of False  # noqa: E501
            permitted_column_relationships ([ModelsIdTrainPermittedColumnRelationships]): An array of objects defining which columns the ML model is able to use or not use as inputs when modelling specific columns.  The \"allow\" and \"disallow\" arrays must contain distinct columns. They do not need to contain all columns in the dataset.  If columns are not allowed in either \"allow\" nor \"disallow\", the model will use default behaviors:   - use all descriptors for all targets when virtualTraining is true.   - use all descriptors + targets when virtualTraining is false for all targets (except for the same target -> target).  if virtualTraining is false:   This is equivalent to passing \"allow\": list_of_all_columns for every column in the dataset.   Therefore, passing allow when virtualTraining is false has no effect on the model.   However, columns passed within \"disallow\" will have an effect.  if virtualTraining is true:   This is equivalent to passing \"allow\": list_of_all_descriptors and passing \"disallow\" for all non descriptors.   Therefore, passing descriptor columns in the \"allow\" list has no effect on the model.   Similarly, passing non descriptor columns in the \"disallow\" list has no effect on the model.   However, columns passed within \"allow\" for non descriptors, and \"disallow\" for descriptors will have an effect.  Interaction with Measurement Groups:   If measurement groups are specified for the training dataset that are incompatible, a 400 response is returned.   This happens when a column defined in \"name\" and one or more columns defined in \"allow\" are part of the same measurement group. . [optional]  # noqa: E501
            enable_training_dataset_outliers (bool, none_type): If true then compute the outliers in the training dataset using the validation sub-models. If `validation` is '5-fold', each will be trained on 80% of the full dataset to identify the outliers in the remaining 20%. If `validation` is 'custom', each row in a test set will be checked against a model trained on the matching train set. Please note, if `validation` is '80/20', only the validation 20% will report outliers, and is thus not recommended.  If null then will be set to true if `validation` is not equal to 'none'. Otherwise will be false.  Once `trainingDatasetOutliersJobStatus` in the model metadata has the value 'done' then the results will be available via `/models/id/training-dataset-outliers`. Computing the outliers this way is generally expected to give better results than using the model trained on the full dataset to identify the outliers in its own training dataset. . [optional]  # noqa: E501
            max_number_samples (int): The maximum number of hyperparameter optimization samples to use for training the model.  Training may stop before the specified amount of samples if an ideal set of hyperparameters if found early.  If 'hyperparameterOptimization' is 'none' then 'maxNumberSamples' will be ignored. . [optional] if omitted the server will use the default value of 200  # noqa: E501
            target_function (HyperoptTargetFunction): [optional]  # noqa: E501
            exploration_exploitation (float): The desired tradeoff between 'exploration', at 0, or 'exploitation' at 1: * 'exploration': build a general model equally focused on all regions of this column * 'exploitation': focused purely on achieving the specified TargetFunction for this column . [optional] if omitted the server will use the default value of 1  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        self = super(OpenApiModel, cls).__new__(cls)

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        for var_name, var_value in kwargs.items():
            if var_name not in self.attribute_map and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self.additional_properties_type is None:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
        return self

    required_properties = set([
        '_data_store',
        '_check_type',
        '_spec_property_naming',
        '_path_to_item',
        '_configuration',
        '_visited_composed_classes',
    ])

    @convert_js_args_to_python_args
    def __init__(self, *args, **kwargs):  # noqa: E501
        """TrainRequest - a model defined in OpenAPI

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            validation (str): Methods for validating the model using the provided dataset. If set to \"custom\", `validationSplits` must also be provided. [optional] if omitted the server will use the default value of "none"  # noqa: E501
            validation_target_columns ([str], none_type): A list of the training dataset's column names that the model's validation metric will be the median average over.  Cannot include descriptor columns.  If not provided then the model's validation metric will be the median average over all non-descriptor columns.. [optional]  # noqa: E501
            validation_splits ([ValidationSplit]): A list of test-train pairs, describing the row IDs of the training dataset to include in each validation split. Must be provided exactly when `validation` is 'custom'. . [optional]  # noqa: E501
            hyperparameter_optimization (str): Search method for finding the optimal parameters to use when training the model.  If 'none' then the parameters as specified by the 'hyperparameters' argument will be used to train the model.  If 'hyperparameterOptimization' is not 'none' then 'validation' must also be not 'none'.. [optional] if omitted the server will use the default value of "none"  # noqa: E501
            bespoke_column_hyperparameters (bool): Whether to use bespoke hyperparameters for each target column. If false, hyperparameters are shared between columns. Defaults to true.. [optional] if omitted the server will use the default value of True  # noqa: E501
            hyperparameters ({str: (bool, date, datetime, dict, float, int, list, str, none_type)}): The hyperparameters which Alchemite will use when training.  If 'hyperparameterOptimization' is 'none', these hyperparameters will be used for training, otherwise the hyperparameters will be used as a starting point for optimization.  If no argument is provided for this parameter and the model was previously trained then the existing hyperparameters will be used to train the model, otherwise Alchemite's default hyperparameters will be used instead.. [optional]  # noqa: E501
            fraction_data_present ([float]): The fraction of data expected to be in each column of the datasets which the model will make predictions from.  The list 'fractionDataPresent' should be ordered corresponding to the training dataset's 'columnHeaders' parameter.. [optional]  # noqa: E501
            virtual_experiment_validation (bool): If true then only the descriptor columns will be used to make predictions when computing the validation metric.. [optional] if omitted the server will use the default value of False  # noqa: E501
            virtual_training (bool): If true then only the descriptor columns will be used as input in the first iteration of training. [optional] if omitted the server will use the default value of False  # noqa: E501
            permitted_column_relationships ([ModelsIdTrainPermittedColumnRelationships]): An array of objects defining which columns the ML model is able to use or not use as inputs when modelling specific columns.  The \"allow\" and \"disallow\" arrays must contain distinct columns. They do not need to contain all columns in the dataset.  If columns are not allowed in either \"allow\" nor \"disallow\", the model will use default behaviors:   - use all descriptors for all targets when virtualTraining is true.   - use all descriptors + targets when virtualTraining is false for all targets (except for the same target -> target).  if virtualTraining is false:   This is equivalent to passing \"allow\": list_of_all_columns for every column in the dataset.   Therefore, passing allow when virtualTraining is false has no effect on the model.   However, columns passed within \"disallow\" will have an effect.  if virtualTraining is true:   This is equivalent to passing \"allow\": list_of_all_descriptors and passing \"disallow\" for all non descriptors.   Therefore, passing descriptor columns in the \"allow\" list has no effect on the model.   Similarly, passing non descriptor columns in the \"disallow\" list has no effect on the model.   However, columns passed within \"allow\" for non descriptors, and \"disallow\" for descriptors will have an effect.  Interaction with Measurement Groups:   If measurement groups are specified for the training dataset that are incompatible, a 400 response is returned.   This happens when a column defined in \"name\" and one or more columns defined in \"allow\" are part of the same measurement group. . [optional]  # noqa: E501
            enable_training_dataset_outliers (bool, none_type): If true then compute the outliers in the training dataset using the validation sub-models. If `validation` is '5-fold', each will be trained on 80% of the full dataset to identify the outliers in the remaining 20%. If `validation` is 'custom', each row in a test set will be checked against a model trained on the matching train set. Please note, if `validation` is '80/20', only the validation 20% will report outliers, and is thus not recommended.  If null then will be set to true if `validation` is not equal to 'none'. Otherwise will be false.  Once `trainingDatasetOutliersJobStatus` in the model metadata has the value 'done' then the results will be available via `/models/id/training-dataset-outliers`. Computing the outliers this way is generally expected to give better results than using the model trained on the full dataset to identify the outliers in its own training dataset. . [optional]  # noqa: E501
            max_number_samples (int): The maximum number of hyperparameter optimization samples to use for training the model.  Training may stop before the specified amount of samples if an ideal set of hyperparameters if found early.  If 'hyperparameterOptimization' is 'none' then 'maxNumberSamples' will be ignored. . [optional] if omitted the server will use the default value of 200  # noqa: E501
            target_function (HyperoptTargetFunction): [optional]  # noqa: E501
            exploration_exploitation (float): The desired tradeoff between 'exploration', at 0, or 'exploitation' at 1: * 'exploration': build a general model equally focused on all regions of this column * 'exploitation': focused purely on achieving the specified TargetFunction for this column . [optional] if omitted the server will use the default value of 1  # noqa: E501
        """

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        for var_name, var_value in kwargs.items():
            if var_name not in self.attribute_map and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self.additional_properties_type is None:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
            if var_name in self.read_only_vars:
                raise ApiAttributeError(f"`{var_name}` is a read-only attribute. Use `from_openapi_data` to instantiate "
                                     f"class with read only attributes.")
