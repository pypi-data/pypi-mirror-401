import os
from concurrent.futures import ThreadPoolExecutor
from decimal import Decimal
from functools import reduce
from typing import List
import re
from hestia_earth.schema import TermTermType, SiteSiteType
from hestia_earth.utils.api import download_hestia
from hestia_earth.utils.model import filter_list_term_type, find_term_match
from hestia_earth.utils.lookup import (
    download_lookup,
    get_table_value,
    column_name,
    extract_grouped_data,
)
from hestia_earth.utils.tools import (
    flatten,
    list_sum,
    safe_parse_float,
    safe_parse_date,
    get_dict_key,
    to_precision,
    non_empty_list,
)

from hestia_earth.validation.gee import (
    MAX_AREA_SIZE,
    is_enabled as gee_is_enabled,
    id_to_level,
    get_region_id,
    get_region_distance,
)
from hestia_earth.validation.models import (
    is_enabled as models_is_enabled,
    value_from_model,
    method_tier_from_model,
    run_model,
    run_model_from_node,
)
from hestia_earth.validation.utils import (
    update_error_path,
    _filter_list_errors,
    _next_error,
    _value_average,
    is_number,
    match_value_type,
    find_linked_node,
    _is_before_today,
    _list_except_item,
    _dict_without_key,
    hash_dict,
    group_blank_nodes,
    term_valueType,
    get_lookup_value,
)

_VALIDATE_PRIVATE_SOURCE = os.getenv("VALIDATE_PRIVATE_SOURCE", "true") == "true"
CROP_SITE_TYPE = [
    SiteSiteType.CROPLAND.value,
    SiteSiteType.GLASS_OR_HIGH_ACCESSIBLE_COVER.value,
]
OTHER_MODEL_ID = "otherModel"


def validate_date_lt_today(node: dict, key: str):
    """
    Validate that a certain date must be before the current date
    """
    date = get_dict_key(node, key)
    return (
        date is None
        or _is_before_today(date)
        or {"level": "error", "dataPath": f".{key}", "message": "must be before today"}
    )


def validate_list_date_lt_today(node: dict, list_key: str, node_keys: list):
    def validate(values: tuple):
        index, value = values
        errors = list(
            map(
                lambda key: {"key": key, "error": validate_date_lt_today(value, key)},
                node_keys,
            )
        )
        return _filter_list_errors(
            [
                update_error_path(error["error"], list_key, index)
                for error in errors
                if error["error"] is not True
            ]
        )

    return _filter_list_errors(
        flatten(map(validate, enumerate(node.get(list_key, []))))
    )


def is_date_after(min_date: str, date: str, strict: bool = True):
    return (
        min_date is None
        or date is None
        or (len(min_date) <= 7 and len(date) <= 7 and date >= min_date)
        or (date > min_date if strict else date >= min_date)
    )


def is_date_equal(date1: str, date2: str, validate_year_only: bool = False):
    date1 = safe_parse_date(date1)
    date2 = safe_parse_date(date2)
    return (
        (date1.year == date2.year if validate_year_only else date1 == date2)
        if all([date1, date2])
        else False
    )


def validate_list_dates_after(
    node: dict, node_key: str, list_key: str, list_key_fields: list
):
    min_date = node.get(node_key)

    def validate_field_list(blank_node: dict, index: int, field: str, field_index: int):
        date = blank_node.get(field)[field_index]
        return is_date_after(min_date, date, False) or {
            "level": "warning",
            "dataPath": f".{list_key}[{index}].{field}[{field_index}]",
            "message": f"must be greater than {node.get('type', node.get('@type'))} {node_key}",
        }

    def validate_field(blank_node: dict, index: int, field: str):
        date = blank_node.get(field)
        return (
            [
                validate_field_list(blank_node, index, field, field_index)
                for field_index in range(0, len(date))
            ]
            if isinstance(date, list)
            else (
                is_date_after(min_date, date, False)
                or {
                    "level": "warning",
                    "dataPath": f".{list_key}[{index}].{field}",
                    "message": f"must be greater than {node.get('type', node.get('@type'))} {node_key}",
                }
            )
        )

    def validate(values: tuple):
        index, blank_node = values
        return _filter_list_errors(
            flatten(
                [validate_field(blank_node, index, field) for field in list_key_fields]
            )
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def _validate_dates_format(node: dict):
    return any(
        [
            not node.get("startDate"),
            not node.get("endDate"),
            len(node.get("startDate", "")) == len(node.get("endDate", "")),
        ]
    )


def validate_node_dates(node: dict):
    """
    Validate Dates

    This validation ensures two things:
    1. The `endDate` must be later than `startDate`;
    2. The date format of `endDate` must be the same as `startDate`.
    """
    return _filter_list_errors(
        [
            is_date_after(node.get("startDate"), node.get("endDate"))
            or {
                "level": "error",
                "dataPath": ".endDate",
                "message": "must be greater than startDate",
            },
            _validate_dates_format(node)
            or {
                "level": "error",
                "dataPath": ".startDate",
                "message": "must be in the same format as endDate",
            },
        ]
    )


def validate_list_dates(node: dict, list_key: str):
    def validate(values: tuple):
        index, value = values
        return is_date_after(value.get("startDate"), value.get("endDate")) or {
            "level": "error",
            "dataPath": f".{list_key}[{index}].endDate",
            "message": "must be greater than startDate",
        }

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_list_dates_format(node: dict, list_key: str):
    """
    Validate the format of `endDate` and `startDate`

    This validation is to ensure that the `startDate` and `endDate` formats match, for any blank node.
    """

    def validate(values: tuple):
        index, blank_node = values
        value_len = len(blank_node.get("endDate", ""))
        invalid_prop_key = next(
            (
                key
                for key in ["startDate"]
                if blank_node.get(key) and len(blank_node.get(key)) != value_len
            ),
            None,
        )
        return (
            value_len == 0
            or invalid_prop_key is None
            or {
                "level": "error",
                "dataPath": f".{list_key}[{index}].{invalid_prop_key}",
                "message": "must have the same length as endDate",
            }
        )

    return _filter_list_errors(
        flatten(map(validate, enumerate(node.get(list_key, []))))
    )


def validate_list_dates_length(node: dict, list_key: str):
    """
    Validate dates and value items

    This validation ensures that the number of items in `value` and `dates` are identical.
    """

    def validate(values: tuple):
        index, blank_node = values
        value = blank_node.get("value")
        dates = blank_node.get("dates")
        return (
            value is None
            or dates is None
            or len(dates) == len(value)
            or {
                "level": "error",
                "dataPath": f".{list_key}[{index}].dates",
                "message": "must contain as many items as values",
                "params": {"expected": len(value), "current": len(dates)},
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def is_value_below(value1, value2):
    compare_lists = isinstance(value1, list) and isinstance(value2, list)
    return any([value1 is None, value2 is None]) or (
        _is_list_value_below(value1, value2)
        if compare_lists
        else any(
            [
                # allow 1% of rounding error
                value1 <= value2 * 1.01,
                Decimal(str(value1)) <= Decimal(str(value2)),
            ]
        )
    )


def _is_list_value_below(list1: list, list2: list):
    def compare_enum(index: int):
        return is_value_below(list1[index], list2[index])

    return (
        len(list1) != len(list2)
        or next(
            (x for x in list(map(compare_enum, range(len(list1)))) if x is not True),
            True,
        )
        is True
    )


def validate_list_value_between_min_max(node: dict, list_key: str):
    """
    Validate blank node `value` between provided `min` and `max`

    For any blank node, if the `value` is not between the `min` and `max` fields, an error will be given.
    """

    def validate(values: tuple):
        index, blank_node = values
        min = blank_node.get("min")
        max = blank_node.get("max")
        value = blank_node.get("value")

        return all([is_value_below(value, max), is_value_below(min, value)]) or {
            "level": "error",
            "dataPath": f".{list_key}[{index}].value",
            "message": "must be between min and max",
            "params": {"min": min, "max": max},
        }

    return _next_error(list(map(validate, enumerate(node.get(list_key, [])))))


def validate_list_min_below_max(node: dict, list_key: str):
    """
    Validate blank node `max` is below provided `min`

    For any blank node, if the `max` is not greater than the `min` field, an error will be given.
    """

    def validate(values: tuple):
        index, blank_node = values
        min = blank_node.get("min")
        max = blank_node.get("max")
        return is_value_below(min, max) or {
            "level": "error",
            "dataPath": f".{list_key}[{index}].max",
            "message": "must be greater than min",
        }

    return _next_error(list(map(validate, enumerate(node.get(list_key, [])))))


def _value_range_error(value: int, minimum: int, maximum: int):
    return (
        "minimum"
        if minimum is not None and not is_value_below(minimum, value)
        else (
            "maximum"
            if maximum is not None and not is_value_below(value, maximum)
            else False
        )
    )


def validate_list_min_max_lookup(
    node: dict, list_key: list, list_key_field="value", skip_max_ids: List[str] = []
):
    """
    Validate blank node `value` between the lookup `minimum` and `maximum`

    For some Terms, HESTIA has defined some minimum and maximum value.
    If the specified `value` does not fall within these, an error is raised.
    """

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        term_id = term.get("@id")
        minimum = safe_parse_float(get_lookup_value(term, "minimum"), None)
        maximum = (
            None
            if term_id in skip_max_ids
            else safe_parse_float(get_lookup_value(term, "maximum"), None)
        )
        value = _value_average(blank_node, None, list_key_field)
        error = (
            _value_range_error(value, minimum, maximum) if value is not None else False
        )
        return error is False or (
            {
                "level": "error",
                "dataPath": f".{list_key}[{index}].{list_key_field}",
                "message": "must be above the minimum",
                "params": {"min": minimum, "max": maximum},
            }
            if error == "minimum"
            else {
                "level": "error",
                "dataPath": f".{list_key}[{index}].{list_key_field}",
                "message": "must be below the maximum",
                "params": {"min": minimum, "max": maximum},
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_region_list_value_diff_property_lookup(
    country: dict, node: dict, list_key: list, property_id: str, threshold=0.75
):
    """
    Validate property value based on a lookup value

    This validation checks if the property value provided is consistent with the value stored in a lookup table.
    For example, we can use the lookup `region-liveAnimal-liveweightPerHead.csv` file to determine if the
    `liveweightPerHead` value provided is within 75% of the `high` value contained in the lookup, for the country.
    """

    country_term_id = country.get("@id")

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        term_id = term.get("@id")
        term_type = term.get("termType")
        lookup_name = f"region-{term_type}-{property_id}.csv"
        lookup = download_lookup(lookup_name)
        lookup_value = get_table_value(
            lookup, "term.id", country_term_id, column_name(term_id)
        )
        # handle lookups containing `high`
        expected_value = safe_parse_float(
            (
                extract_grouped_data(lookup_value, "high")
                if "high" in str(lookup_value)
                else lookup_value
            ),
            default=None,
        )
        value = find_term_match(blank_node.get("properties", []), property_id).get(
            "value"
        )
        print(
            lookup_name,
            country_term_id,
            column_name(term_id),
            lookup_value,
            expected_value,
            value,
        )
        delta = value_difference(value, expected_value)
        return (
            value is None
            or delta < threshold
            or {
                "level": "warning",
                "dataPath": f".{list_key}[{index}].value",
                "message": "should be within acceptable range of the lookup value",
                "params": {
                    "current": value,
                    "expected": expected_value,
                    "delta": delta * 100,
                    "threshold": threshold,
                    "term": {
                        "@id": property_id,
                        "termType": TermTermType.PROPERTY.value,
                    },
                },
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_nodes_duplicates(node: dict, node_by_hash: dict):
    """
    Validate duplicated nodes

    This validation gives a warning when 2 identical nodes, with different `id` value`, are detected.
    """
    node_without_id = _dict_without_key(node, "id")
    key = hash_dict(node_without_id)
    duplicates = _list_except_item(node_by_hash.get(key, []), node)
    return (
        [
            next(
                (
                    {
                        "level": "warning",
                        "dataPath": "",
                        "message": f"might be a duplicate of the {dup.get('type')} with id {dup.get('id')}",
                    }
                    for dup in duplicates
                ),
                True,
            )
        ]
        if len(duplicates) > 0
        else []
    )


def validate_list_duplicate_values(node: dict, list_key: str, prop: str, value: str):
    values = node.get(list_key, [])
    duplicates = list(filter(lambda v: get_dict_key(v, prop) == value, values))
    return len(duplicates) < 2 or {
        "level": "error",
        "dataPath": f".{list_key}[{values.index(duplicates[1])}].{prop}",
        "message": f"must have only one entry with the same {prop} = {value}",
    }


def validate_list_term_percent(node: dict, list_key: str):
    """
    Validates the percentage of blank nodes

    For all Terms that are in percentages, the value must be between `0` and `100`.
    """

    def soft_validate(index: int, value):
        return (is_number(value) and 0 < value and value <= 1) and {
            "level": "warning",
            "dataPath": f".{list_key}[{index}].value",
            "message": "may be between 0 and 100",
        }

    def hard_validate(index: int, value):
        return (is_number(value) and 0 <= value and value <= 100) or {
            "level": "error",
            "dataPath": f".{list_key}[{index}].value",
            "message": "should be between 0 and 100 (percentage)",
        }

    def validate(values: tuple):
        index, blank_node = values
        units = blank_node.get("term", {}).get("units", "")
        value = (
            _value_average(blank_node, blank_node.get("value"))
            if units == "%"
            else None
        )
        is_empty = value is None or (isinstance(value, list) and len(value) == 0)
        return is_empty or soft_validate(index, value) or hard_validate(index, value)

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def valid_list_sum(blank_nodes: list):
    values = [_value_average(p, default=None) for p in blank_nodes]
    values_number = list(filter(is_number, values))
    is_valid = len(values) == len(values_number)
    return list_sum(values_number), is_valid


def validate_list_sum_100_percent(node: dict, list_key: str):
    """
    Validate a group of blank nodes must sum to 100%

    Some Terms are part of a group from the lookup `sumIs100Group`, meaning when summed up, their total value must be
    exactly `100%` (with a 5% tolerance).
    """

    def validate(values: list):
        term_ids = [v["node"].get("term", {}).get("@id") for v in values]
        total_value, valid_sum = valid_list_sum([v["node"] for v in values])
        blank_node = values[0]
        min_value = 99.5
        max_value = 100.5
        sum_equal_100 = blank_node.get("sumIs100Group")
        valid = all(
            [total_value <= max_value, not sum_equal_100 or total_value >= min_value]
        )
        return valid or [
            {
                "level": "error",
                "dataPath": f".{list_key}[{value.get('index')}]",
                "message": f"value should sum to {'' if sum_equal_100 else 'maximum '}100 across all values",
                "params": {"termIds": term_ids, "sum": total_value, "max": max_value}
                | ({"min": min_value} if sum_equal_100 else {}),
            }
            for value in values
        ]

    groupped_values = group_blank_nodes(enumerate(node.get(list_key, []))).values()
    return _filter_list_errors(flatten(map(validate, groupped_values)))


def validate_list_percent_requires_value(
    node: dict, list_key: str, term_types: List[TermTermType] = []
):
    """
    Validate the value of some blank nodes when the units is a percentage

    For all Terms that are set as percentages, the `value` must be provided.
    """
    term_types_str = [t.value for t in term_types]

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        validate_value = all(
            [
                term.get("termType") in term_types_str,
                term.get("units", "").startswith("%"),
            ]
        )
        value = blank_node.get("value", [])
        return (
            not validate_value
            or len(value) > 0
            or {
                "level": "error",
                "dataPath": f".{list_key}[{index}]",
                "message": "should have required property 'value'",
                "params": {"term": term, "missingProperty": "value"},
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_list_valueType(node: dict, list_key: str):
    """
    Validate the type of the value of blank nodes

    Using the lookup "valueType", this validation will make sure the provided value is of the correct type.
    Example: a Term that should be either True or False will only accept True/False value.
    """

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        expected_value_type = term_valueType(term)
        value = blank_node.get("value")
        return (
            value is None
            or match_value_type(expected_value_type, value)
            or {
                "level": "error",
                "dataPath": f".{list_key}[{index}].value",
                "message": "the node value type is incorrect",
                "params": {"expected": expected_value_type},
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_list_has_properties(node: dict, list_key: str):
    """
    Validate that some properties are specified for each blank nodes

    Using the lookup `recommendedPropertyTermIds` on a blank node term, this validation will suggest the user to add
    any missing Property. These properties are necessary to run the models, and should be specified to prevent HESTIA
    from using default properties.
    """
    lookup_col = "recommendedPropertyTermIds"

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        properties = blank_node.get("properties", [])
        property_term_ids = non_empty_list(
            (get_lookup_value(term, lookup_col) or "").split(";")
        )
        missing_properties = [
            term_id
            for term_id in property_term_ids
            if not find_term_match(properties, term_id)
        ]
        return not missing_properties or {
            "level": "warning",
            "dataPath": f".{list_key}[{index}]",
            "message": "should add missing properties",
            "params": {
                "type": blank_node.get("@type") or blank_node.get("type"),
                "termType": term.get("termType"),
                "expected": missing_properties,
            },
        }

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_sublist_has_properties(node: dict, list_key: str, sub_list_key: str):
    # same as `validate_list_has_properties` but validating sublists
    def validate(values: tuple):
        index, blank_node = values
        errors = validate_list_has_properties(blank_node, sub_list_key)
        return _filter_list_errors(
            [
                update_error_path(error, list_key, index)
                for error in (errors if isinstance(errors, list) else [errors])
                if error is not True
            ]
        )

    blank_nodes = enumerate(node.get(list_key, []))
    return _filter_list_errors(flatten(map(validate, blank_nodes)))


def validate_is_region(node: dict, region_key="region"):
    """
    Validate that the `region` is not a Country

    When using the `region` field, it must point to GADM Terms that are not countries.
    This can be identified with the `gadmLevel` field: it must be > `0` to be a region.
    """
    region_id = node.get(region_key, {}).get("@id", "")
    level = id_to_level(region_id)
    return (
        region_id == ""
        or level > 0
        or {
            "level": "error",
            "dataPath": f".{region_key}",
            "message": "must not be a country",
        }
    )


def validate_region_in_country(node: dict, region_key="region"):
    """
    Validate that the `region` is within the `country`

    When using both `country` and `region`, the `region` must be within the `country` specified.
    """
    country = node.get("country", {})
    region_id = node.get(region_key, {}).get("@id", "")
    return (
        region_id == ""
        or region_id[0:8] == country.get("@id")
        or {
            "level": "error",
            "dataPath": f".{region_key}",
            "message": "must be within the country",
            "params": {"country": country.get("name")},
        }
    )


def validate_country(node: dict):
    """
    Validate that the `country` is a Country

    When using the `country` field, it must point to GADM Terms that are countries.
    This can be identified with the `gadmLevel` field: it must be == `0` to be a country.
    """
    country_id = node.get("country", {}).get("@id", "")
    # handle additional regions used as country, like region-world
    is_region = country_id.startswith("region-")
    return (
        country_id == ""
        or is_region
        or bool(re.search(r"GADM-[A-Z]{3}$", country_id))
        or {"level": "error", "dataPath": ".country", "message": "must be a country"}
    )


def validate_country_region(node: dict):
    return _filter_list_errors(
        [
            validate_country(node),
            validate_is_region(node),
            validate_region_in_country(node),
        ]
    )


def validate_list_country_region(node: dict, list_key: str):
    def validate(values: tuple):
        index, blank_node = values
        errors = [
            validate_country(blank_node),
            validate_is_region(blank_node),
            validate_region_in_country(blank_node),
        ]
        errors = [
            update_error_path(error, list_key, index)
            for error in errors
            if error is not True
        ]
        return _filter_list_errors(errors)

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def need_validate_coordinates(node: dict):
    return gee_is_enabled() and "latitude" in node and "longitude" in node


def validate_coordinates(node: dict):
    """
    Validate the coordinates are within the `region` or `country`

    When providing `longitude` and `latitude`, the coordinates must fall within the `region` or `country` provided.
    """
    latitude = node.get("latitude")
    longitude = node.get("longitude")
    country = node.get("country", {})
    region = node.get("region")
    gadm_id = region.get("@id") if region else country.get("@id")
    id = get_region_id(node)
    return gadm_id == id or {
        "level": "error",
        "dataPath": ".region" if region else ".country",
        "message": "does not contain latitude and longitude",
        "params": {
            "current": gadm_id,
            "expected": id,
            "distance": get_region_distance(
                gadm_id, latitude=latitude, longitude=longitude
            ),
        },
    }


def need_validate_area(node: dict):
    return all(["area" in node, "boundary" in node, "boundaryArea" in node])


def validate_area(node: dict, threshold=0.05):
    """
    Validate `area` based on `boundary`

    HESTIA will calculate the `boundary` area during upload, and this validation will show a warning when the `area`
    does not match.
    Please verify that the `area` provided is correct and in the correct unit.
    """
    value = round(node.get("area", 0), 1)
    expected_value = round(node.get("boundaryArea", 0), 1)
    delta = value_difference(value, expected_value)
    return delta < threshold or {
        "level": "warning",
        "dataPath": ".area",
        "message": "should be equal to boundary",
        "params": {
            "current": value,
            "expected": expected_value,
            "delta": delta * 100,
            "threshold": threshold,
        },
    }


def validate_boundary_area(node: dict):
    """
    Validate max `boundary` size

    HESTIA will only gap-fill data from Geospatial datasets when the site area is less than 5000km2.
    If you see this warning, it means the calculation will be incomplete, and the measurements will need to be manually
    provided. Please provide a smaller polygon instead.
    """
    area = node.get("boundaryArea", 0) / 100
    return area < MAX_AREA_SIZE or {
        "level": "warning",
        "dataPath": ".boundaryArea",
        "message": "boundaryArea should be lower than max size",
        "params": {"current": area, "expected": MAX_AREA_SIZE},
    }


def need_validate_region_size(node: dict):
    return all(
        [
            gee_is_enabled(),
            not need_validate_coordinates(node),
            "boundaryArea" not in node,
            "region" in node or "country" in node,
        ]
    )


def validate_region_size(node: dict):
    """
    Validate max `region`/`country` size

    HESTIA will only gap-fill data from Geospatial datasets when the site area is less than 5000km2.
    If you see this warning, it means the calculation will be incomplete, and the measurements will need to be manually
    provided. Please provide a smaller `region`/`country` instead.
    """
    region_id = node.get("region", node.get("country", {})).get("@id")
    region = download_hestia(region_id) if region_id else {}
    try:
        from hestia_earth.earth_engine.gadm import get_size_km2

        # get_region_size might throw error is geometry has too many edges
        area = region.get("area", get_size_km2(region_id) if region_id else None) or 0
    except Exception:
        area = 0
    return area < MAX_AREA_SIZE or {
        "level": "warning",
        "dataPath": f".{'region' if node.get('region') else 'country'}",
        "message": "should be lower than max size",
        "params": {"current": area, "expected": MAX_AREA_SIZE},
    }


N_A_VALUES = ["#n/a", "#na", "n/a", "na", "n.a", "nodata", "no data"]


def validate_empty_fields(node: dict):
    """
    Validates empty fields

    This validation returns warnings when fields are added without a value (or using 'no data' for example).
    """
    keys = list(filter(lambda key: isinstance(node.get(key), str), node.keys()))

    def validate(key: str):
        return not node.get(key).lower() in N_A_VALUES or {
            "level": "warning",
            "dataPath": f".{key}",
            "message": "may not be empty",
        }

    return _filter_list_errors(map(validate, keys), False)


def validate_linked_source_privacy(node: dict, key: str, node_map: dict = {}):
    """
    Validate the privacy of 2 linked Nodes

    When adding a Source to a Cycle or an ImpactAssessment, this validation ensures that the same level of privacy
    is used on both.

    Example:
    - if the Cycle is set as `dataPrivate`=`true`, the Source it is linked to must also be `dataPrivate`=`true`;
    - if the Cycle is set as `dataPrivate`=`false`, the Source it is linked to must also be `dataPrivate`=`false`.
    """
    related_source = find_linked_node(node_map, node.get(key, {}))
    node_privacy = node.get("dataPrivate")
    related_source_privacy = (
        related_source.get("dataPrivate") if related_source else None
    )
    return (
        related_source_privacy is None
        or node_privacy == related_source_privacy
        or {
            "level": "error",
            "dataPath": ".dataPrivate",
            "message": "should have the same privacy as the related source",
            "params": {
                "dataPrivate": node_privacy,
                key: {"dataPrivate": related_source_privacy},
            },
        }
    )


def validate_private_has_source(node: dict, key: str):
    """
    Validate private nodes are linked to a Source

    It is recommended to always link a `Cycle`, `Site`, or `ImpactAssessment`, to a `Source`, even when private.
    """
    node_private = node.get("dataPrivate")
    return any(
        [not _VALIDATE_PRIVATE_SOURCE, not node_private, node.get(key) is not None]
    ) or {
        "level": "warning",
        "dataPath": ".dataPrivate",
        "message": "should add a source",
        "params": {"current": key},
    }


def value_difference(value: float, expected_value: float):
    return (
        0
        if any(
            [
                isinstance(expected_value, list),
                expected_value == 0,
                expected_value is None,
                isinstance(value, list),
                value is None,
            ]
        )
        else round(abs(value - expected_value) / expected_value, 4)
    )


def is_value_different(
    value: float, expected_value: float, delta: float = 0.05
) -> bool:
    return value_difference(value, expected_value) > delta


def _parse_node_value(node: dict):
    def parse_list_value(value: list):
        return list_sum(value) if len(value) > 0 else None

    value = node.get("value")
    return (
        None
        if value is None
        else (parse_list_value(value) if isinstance(value, list) else value)
    )


def _get_term_recalculated_max_delta(term: dict):
    col_name = "valueToleranceToHestiaRecalculatedValue"
    return safe_parse_float(get_lookup_value(term, col_name), default=5) / 100


def _validate_list_model(node: dict, list_key: str):
    """
    Validate a blank node value based on HESTIA's model

    This validation will use the models library to recalculate the value of a blank node, and will give an error when
    the `value` too different from the models value.
    Note: the default tolerance is 5%, but we use the lookup `valueToleranceToHestiaRecalculatedValue` when available.
    """

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        max_delta = _get_term_recalculated_max_delta(term)
        try:
            method_tier = blank_node.get("methodTier")
            value = _parse_node_value(blank_node)
            # skip validation if `value` is not set
            result = (
                run_model_from_node(blank_node, node) if value is not None else None
            )
            expected_value = value_from_model(result) if result else 0
            expected_method_tier = method_tier_from_model(result)
            delta = value_difference(value, expected_value)
            return (
                method_tier != expected_method_tier
                or delta < max_delta
                or {
                    "level": "error",
                    "dataPath": f".{list_key}[{index}].value",
                    "message": "the value provided is not consistent with the model result",
                    "params": {
                        "model": blank_node.get("methodModel", {}),
                        "term": term,
                        "current": value,
                        "expected": expected_value,
                        "delta": to_precision(delta * 100, 4),
                        "threshold": max_delta,
                    },
                }
            )
        except Exception:
            return True

    return validate


def validate_list_model(node: dict, list_key: str) -> list:
    nodes = node.get(list_key, []) if models_is_enabled() else []
    with ThreadPoolExecutor() as executor:
        errors = list(
            executor.map(_validate_list_model(node, list_key), enumerate(nodes))
        )
    return _filter_list_errors(errors)


def _reset_completeness(node: dict):
    completeness = node.get("completeness", {})
    completeness = reduce(
        lambda prev, curr: {**prev, curr: False}, completeness.keys(), completeness
    )
    return {**node, "completeness": completeness}


def _get_model_from_result(result: dict):
    return result.get("methodModel", result.get("model")) if result else None


def _validate_list_model_config(node: dict, list_key: str, conf: dict):
    def validate_model(term: dict, value: float, index: int, model_conf: dict):
        node_run = (
            _reset_completeness(node)
            if model_conf.get("resetDataCompleteness", False)
            else node
        )
        expected_result = run_model(model_conf["model"], term.get("@id"), node_run)
        expected_value = value_from_model(expected_result)
        delta = value_difference(value, expected_value)
        return delta < model_conf["delta"] or {
            "level": model_conf.get("level", "error"),
            "dataPath": f".{list_key}[{index}].value",
            "message": "the value provided is not consistent with the model result",
            "params": {
                "model": _get_model_from_result(expected_result[0]),
                "term": term,
                "current": value,
                "expected": expected_value,
                "delta": to_precision(delta * 100, 4),
                "threshold": model_conf["delta"],
            },
        }

    def validate(values: tuple):
        index, blank_node = values
        value = _parse_node_value(blank_node)
        term = blank_node.get("term", {})
        term_id = blank_node.get("term", {}).get("@id")
        # get the configuration for this element
        # if it does not exist or no `value` is set, skip model
        term_conf = conf.get(term_id)
        return (
            validate_model(term, value, index, term_conf)
            if term_conf and value is not None
            else True
        )

    return validate


def validate_list_model_config(node: dict, list_key: str, conf: dict):
    nodes = node.get(list_key, []) if models_is_enabled() else []
    with ThreadPoolExecutor() as executor:
        errors = list(
            executor.map(
                _validate_list_model_config(node, list_key, conf), enumerate(nodes)
            )
        )
    return _filter_list_errors(errors)


def _unique_term_grouping(term_id: str):
    # TODO: use a lookup instead
    return (
        re.split(
            r"(Kg|Liveweight|ColdCarcassWeight|ColdDressedCarcassWeight|ReadyToCookWeight)",
            term_id,
        )[0]
        if term_id
        else None
    )


def validate_duplicated_term_units(
    node: dict, list_key: str, term_types: List[TermTermType]
):
    """
    Validate multiple terms used with different units

    Some of the terms in the Glossary have been added with different units, e.g., `kg` / `kg N` / `kg VS`.
    This validation returns a warning when those terms have been used together in the same Node.
    """

    def term_ids_mapper(prev: dict, curr: dict):
        term = curr.get("term", {})
        term_id = term.get("@id")
        term_id_suffix = _unique_term_grouping(term_id)
        prev[term_id_suffix] = prev.get(term_id_suffix, []) + [term.get("units")]
        return prev

    blank_nodes = node.get(list_key, [])
    term_ids_to_units = reduce(
        term_ids_mapper, filter_list_term_type(blank_nodes, term_types), {}
    )

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("term", {})
        term_id = term.get("@id")
        term_id_suffix = _unique_term_grouping(term_id)
        units = term_ids_to_units.get(term_id_suffix, [])
        return len(set(units)) <= 1 or {
            "level": "warning",
            "dataPath": f".{list_key}[{index}].term",
            "message": "should not use identical terms with different units",
            "params": {"term": term, "units": units},
        }

    return _filter_list_errors(map(validate, enumerate(blank_nodes)))


def validate_other_model(node: dict, list_key: str):
    """
    Validate usage of "Other model"

    The "Other model" Term has been added in the eventuality that HESTIA has not added the required model yet.
    However, to use this Term, the `methodModelDescription` must be set.
    """

    def validate(values: tuple):
        index, blank_node = values
        term = blank_node.get("methodModel", {})
        term_id = term.get("@id")
        return (
            term_id != OTHER_MODEL_ID
            or bool(blank_node.get("methodModelDescription"))
            or {
                "level": "error",
                "dataPath": f".{list_key}[{index}].methodModel",
                "message": "is required when using other model",
            }
        )

    return _filter_list_errors(map(validate, enumerate(node.get(list_key, []))))


def validate_nested_existing_node(node: dict, key: str):
    """
    Validate when a non-indexed node references indexed nodes

    When uploading Cycle, Site, etc., the referenced nodes must also be uploaded.
    Example:
    - if the uploaded Cycle references an existing Site, a warning will be given;
    - if the uploaded Cycle references an uploaded Site, no warning is given.
    """
    is_indexed = "@id" in node
    nested_value = node.get(key)
    return is_indexed or (
        _filter_list_errors(
            [
                "@id" not in value
                or {
                    "level": "warning",
                    "dataPath": f".{key}[{index}].@id",
                    "message": "should not link to an existing node",
                    "params": {"node": value},
                }
                for index, value in enumerate(nested_value)
            ]
        )
        if isinstance(nested_value, list)
        else (
            "@id" not in (nested_value or {})
            or {
                "level": "warning",
                "dataPath": f".{key}.@id",
                "message": "should not link to an existing node",
                "params": {"node": nested_value},
            }
        )
    )
