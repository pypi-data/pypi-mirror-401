# OASIS-SDK

## 1. Concept

OASIS-LLM-PROXY-CLIENTëŠ” **ë‹¤ì–‘í•œ LLM Providerë¥¼ í†µí•©ëœ ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸**ë¡œ ì œê³µí•˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ê³µì‹ SDK ë° LangChainì„ ì–‡ê²Œ wrappingí•˜ì—¬ ì‚¬ë‚´ ê·œì¹™ì— ë§ëŠ” í•„ë“œ ì…ë ¥ê³¼ í”„ë¡ì‹œ ì„œë²„ë¥¼ í†µí•œ í‚¤ ì£¼ì…ì„ ì§€ì›í•©ë‹ˆë‹¤.

**ì§€ì›í•˜ëŠ” LLM Provider:**

- ğŸŸ¢ **OpenAI**: GPT-4o, GPT-4, GPT-3.5, Embedding ëª¨ë¸
- ğŸ”µ **Azure OpenAI**: Azure ë°°í¬ëœ OpenAI ëª¨ë¸ë“¤
- ğŸŸ  **Google**: Gemini 2.0 Pro ë“±
- âš« **XAI**: Grok ëª¨ë¸
- ğŸŸ£ **Anthropic**: Claude Sonnet ì‹œë¦¬ì¦ˆ
- ğŸ¯ **Oasis**: ìì²´ LLM ëª¨ë¸ ë° ì„ë² ë”©

**ì£¼ìš” íŠ¹ì§•:**

- ğŸ”„ **í†µí•© í´ë¼ì´ì–¸íŠ¸**: í•˜ë‚˜ì˜ `Oasis` í´ë¼ì´ì–¸íŠ¸ë¡œ ëª¨ë“  LLM Provider ì‚¬ìš©
- ğŸ¯ **ìë™ í”„ë¡œë°”ì´ë” ì„ íƒ**: ëª¨ë¸ IDë§Œìœ¼ë¡œ ì ì ˆí•œ í”„ë¡œë°”ì´ë” ìë™ ì„ íƒ
- ğŸ”— **ì™„ì „í•œ í˜¸í™˜ì„±**: ì›ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
- ğŸ›¡ï¸ **í†µí•© ì¸ì¦**: í”„ë¡ì‹œ ì„œë²„ë¥¼ í†µí•œ ì•ˆì „í•œ í‚¤ ê´€ë¦¬
- ğŸ§ª **Providerë³„ í…ŒìŠ¤íŠ¸**: ê° Providerë³„ë¡œ ë…ë¦½ì ì¸ í…ŒìŠ¤íŠ¸ ì§€ì›

## 2. Usage

### 2.1 ì„¤ì¹˜

```bash
pip install oasis-sdk
```

### 2.2 í™˜ê²½ ì„¤ì •

í”„ë¡ì‹œ ì„œë²„ URLì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

- ê¸°ë³¸ê°’ì€ ì´ë¯¸ ì„¤ì •ë˜ì–´ìˆìŒ

```bash
export OASIS_PROXY_URL="https://your-proxy-server.com"
```

ë˜ëŠ” í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ì§ì ‘ ì§€ì •:

```python
client = Oasis(
    proxy_url="https://your-proxy-server.com",
    # ... ê¸°íƒ€ ë§¤ê°œë³€ìˆ˜
)
```

### 2.3 ì‚¬ìš© ì˜ˆì‹œ

**client parameters**

[required]

- account_id: ê³„ì • ID
- user_uuid: ì‚¬ìš©ì UUID
- workspace_uuid: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ UUID
- tenant_uuid: í…Œë„ŒíŠ¸ UUID
- plugin_name: í˜¸ì¶œí•œ ì‹œìŠ¤í…œ ëª… (ex: chatbot, mcp1, rag-mcp ë“±)

[optional]

- proxy_url: LLM í”„ë¡ì‹œ ì„œë²„ URL (í™˜ê²½ë³€ìˆ˜ `OASIS_PROXY_URL`ì—ì„œ ìë™ ë¡œë“œ)
- user_ip: ì‚¬ìš©ì IP (ê¸°ë³¸ê°’: 127.0.0.1)

[auto]

- root_id: í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ë°œê¸‰
- req_id: ìš”ì²­ì‹œë§ˆë‹¤ ë°œê¸‰

ğŸ“ **ì£¼ì˜**

- 1ë²ˆì˜ ì—°ì†ì ì¸ ìˆ˜í–‰ì—ì„œ root_idëŠ” ê³ ì •ë˜ì–´ì•¼ í•¨
- ì—°ê³„ë˜ëŠ” ì‹œìŠ¤í…œì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ì´ˆê¸° ë°œê¸‰ëœ root_idë¥¼ ì£¼ì…í•˜ì—¬ ì‚¬ìš©

#### 2.3.1 SDK (í†µí•©ëœ OpenAI í´ë¼ì´ì–¸íŠ¸)

**í†µí•©ëœ ë‹¤ì¤‘ Provider SDK ë˜í¼**

> ğŸ“ **ì¤‘ìš”**: ëª¨ë“  LLM Provider(OpenAI, Azure, Google, XAI, Anthropic, Oasis)ê°€ ë™ì¼í•œ `Oasis` í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë¸ IDë§Œìœ¼ë¡œ ìë™ìœ¼ë¡œ ì ì ˆí•œ í”„ë¡œë°”ì´ë”ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

```python
import oasis
# ë˜ëŠ”
from oasis.compatible.sdk import Oasis, AsyncOasis

# ë™ê¸° í´ë¼ì´ì–¸íŠ¸
with Oasis(
    account_id="your_account_id",
    user_uuid="your_uuid",
    workspace_uuid="your_workspace_uuid",
    tenant_uuid="your_tenant_uuid",
    plugin_name="your_system"
) as client:
    print(f"Client base URL: {client.base_url}")

    # OpenAI ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
    openai_resp = client.chat.completions.create(
        model="your_openai_model_uuid",  # OpenAI ëª¨ë¸ UUID
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of France?"}
        ],
    )
    print("OpenAI Response:", openai_resp.choices[0].message.content)

    # Google Gemini ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
    google_resp = client.chat.completions.create(
        model="your_google_model_uuid",  # Google Gemini UUID
        messages=[
            {"role": "user", "content": "Hello from Google Gemini!"}
        ],
    )
    print("Google Response:", google_resp.choices[0].message.content)

    # Anthropic Claude ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
    anthropic_resp = client.chat.completions.create(
        model="your_anthropic_model_uuid",  # Claude ëª¨ë¸ UUID
        messages=[
            {"role": "user", "content": "Hello from Claude!"}
        ],
    )
    print("Anthropic Response:", anthropic_resp.choices[0].message.content)

    # Azure OpenAI ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ (ë™ì¼í•œ í´ë¼ì´ì–¸íŠ¸ë¡œ!)
    azure_resp = client.chat.completions.create(
        model="your_azure_model_uuid",  # Azure deployment UUID
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello from Azure!"}
        ],
        max_tokens=100,  # Azure ëª¨ë¸ ì‚¬ìš©ì‹œ í† í° ì œí•œ
    )
    print("Azure Response:", azure_resp.choices[0].message.content)

    # ìŠ¤íŠ¸ë¦¬ë° (OpenAI ëª¨ë¸)
    stream = client.chat.completions.create(
        model="your_openai_model_uuid",
        messages=[{"role": "user", "content": "Tell me a short story"}],
        stream=True,
    )
    print("OpenAI Stream:")
    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print("\n")

    # ìŠ¤íŠ¸ë¦¬ë° (Azure ëª¨ë¸)
    azure_stream = client.chat.completions.create(
        model="your_azure_model_uuid",
        messages=[{"role": "user", "content": "Count to 3"}],
        max_tokens=50,
        stream=True,
    )
    print("Azure Stream:")
    for chunk in azure_stream:
        if chunk.choices and chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
    print("\n")

    # ì„ë² ë”© (ë™ì¼í•œ ëª¨ë¸ IDê°€ OpenAI/Azure ëª¨ë‘ ì§€ì›)
    embedding_resp = client.embeddings.create(
        model="your_embedding_uuid",  # ì„ë² ë”© ëª¨ë¸ UUID
        input=["This sentence will be embedded.", "Another test sentence."],
    )
    print(f"Embedding vector dimension: {len(embedding_resp.data[0].embedding)}")

    # Rerank ì‚¬ìš© ì˜ˆì‹œ
    rerank_resp = client.rerank(
        model="your_rerank_model_uuid",  # Rerank ëª¨ë¸ UUID
        query="Python programming",
        documents=[
            "Python is a high-level programming language.",
            "JavaScript is used for web development.",
            "Python is popular for data science and machine learning.",
        ],
        top_n=2,  # ìƒìœ„ 2ê°œ ê²°ê³¼ ë°˜í™˜
    )
    print(f"Rerank results: {rerank_resp['results']}")

# ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ (ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ OpenAI/Azure ëª¨ë‘ ì§€ì›)
import asyncio

async def async_example():
    async with AsyncOasis(
        account_id="your_account_id",
        user_uuid="your_uuid",
        workspace_uuid="your_workspace_uuid",
        tenant_uuid="your_tenant_uuid",
        plugin_name="your_system"
    ) as client:
        # ë¹„ë™ê¸° ì±„íŒ… ì™„ì„± (OpenAI)
        openai_resp = await client.chat.completions.create(
            model="model_uuid",
            messages=[{"role": "user", "content": "Hello OpenAI!"}],
        )
        print("Async OpenAI:", openai_resp.choices[0].message.content)

        # ë¹„ë™ê¸° ì±„íŒ… ì™„ì„± (Azure)
        azure_resp = await client.chat.completions.create(
            model="model_uuid",
            messages=[{"role": "user", "content": "Hello Azure!"}],
            max_tokens=50,
        )
        print("Async Azure:", azure_resp.choices[0].message.content)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
        stream = await client.chat.completions.create(
            model="model_uuid",
            messages=[{"role": "user", "content": "Count to 5"}],
            stream=True,
        )
        print("Async Stream:")
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="", flush=True)
        print("\n")

        # ë¹„ë™ê¸° ì„ë² ë”©
        embedding_resp = await client.embeddings.create(
            model="your_model_uuid",
            input=["Async embedding test"],
        )
        print(f"Async embedding vector: {embedding_resp.data[0].embedding[:5]}...")

        # ë¹„ë™ê¸° Rerank
        rerank_resp = await client.rerank(
            model="your_rerank_model_uuid",
            query="Machine learning",
            documents=[
                "Machine learning is a subset of artificial intelligence.",
                "Deep learning uses neural networks.",
                "Python is a versatile programming language.",
            ],
            top_n=2,
        )
        print(f"Async rerank results: {rerank_resp['results']}")

# ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
asyncio.run(async_example())
```

#### 2.3.2 LangChain (í†µí•©ëœ OpenAI ë˜í¼)

> ğŸ“ **ì¤‘ìš”**: LangChainë„ í†µí•©ëœ `ChatOasis`ì™€ `OasisEmbedding`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë¸ IDë¡œ ëª¨ë“  Providerë¥¼ ìë™ êµ¬ë¶„í•©ë‹ˆë‹¤.

```python
from oasis.compatible.langchain import ChatOasis, OasisEmbedding

# OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì±„íŒ… ì˜ˆì‹œ
openai_llm = ChatOasis(
    account_id="your_account_id",
    user_uuid="your_uuid",
    workspace_uuid="your_workspace_uuid",
    tenant_uuid="your_tenant_uuid",
    model_name="model_uuid",  # OpenAI ëª¨ë¸ UUID
    plugin_name="langchain_openai_test"
)

# Azure ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì±„íŒ… ì˜ˆì‹œ (ë™ì¼í•œ í´ë˜ìŠ¤!)
azure_llm = ChatOasis(
    account_id="your_account_id",
    user_uuid="your_uuid",
    workspace_uuid="your_workspace_uuid",
    tenant_uuid="your_tenant_uuid",
    model_name="model_uuid",  # Azure deployment UUID
    plugin_name="langchain_azure_test"
)

try:
    # OpenAI ëª¨ë¸ í˜¸ì¶œ
    openai_resp = openai_llm.invoke("Hello from OpenAI via LangChain!")
    print("OpenAI Response:", openai_resp.content)

    # Azure ëª¨ë¸ í˜¸ì¶œ (ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤!)
    azure_resp = azure_llm.invoke("Hello from Azure via LangChain!")
    print("Azure Response:", azure_resp.content)

    # OpenAI ìŠ¤íŠ¸ë¦¬ë°
    print("OpenAI Stream:")
    for chunk in openai_llm.stream("Tell me a short story"):
        if chunk.content:
            print(chunk.content, end="", flush=True)
    print("\n")

    # Azure ìŠ¤íŠ¸ë¦¬ë°
    print("Azure Stream:")
    for chunk in azure_llm.stream("Count to 3"):
        if chunk.content:
            print(chunk.content, end="", flush=True)
    print("\n")

    # ë¹„ë™ê¸° í˜¸ì¶œ ì˜ˆì‹œ
    async def async_langchain_example():
        # ë¹„ë™ê¸° OpenAI í˜¸ì¶œ
        openai_async_resp = await openai_llm.ainvoke("Async OpenAI LangChain!")
        print("Async OpenAI:", openai_async_resp.content)

        # ë¹„ë™ê¸° Azure í˜¸ì¶œ
        azure_async_resp = await azure_llm.ainvoke("Async Azure LangChain!")
        print("Async Azure:", azure_async_resp.content)

        # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
        print("Async OpenAI Stream:")
        async for chunk in openai_llm.astream("Async streaming test"):
            if chunk.content:
                print(chunk.content, end="", flush=True)
        print("\n")

        # Rerank ë¹„ë™ê¸° í˜¸ì¶œ
        rerank_resp = await openai_llm.arerank(
            model="your_rerank_model_uuid",
            query="Python data science",
            documents=[
                "Python is widely used in data science.",
                "R is another popular language for statistics.",
                "Python has excellent machine learning libraries.",
            ],
            top_n=2,
        )
        print(f"Async rerank results: {rerank_resp['results']}")

    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    import asyncio
    asyncio.run(async_langchain_example())

finally:
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    openai_llm.close()
    azure_llm.close()

# ì„ë² ë”© ì˜ˆì‹œ (OpenAI/Azure ëª¨ë‘ ë™ì¼í•œ í´ë˜ìŠ¤ ì‚¬ìš©)
openai_embedding = OasisEmbedding(
    account_id="your_account_id",
    user_uuid="your_uuid",
    workspace_uuid="your_workspace_uuid",
    tenant_uuid="your_tenant_uuid",
    model_name="model_uuid",  # ì„ë² ë”© ëª¨ë¸ UUID (OpenAI/Azure ê³µí†µ)
    plugin_name="langchain_embedding_test"
)

try:
    # ë™ê¸° ì„ë² ë”© (ì—¬ëŸ¬ ë¬¸ì„œ)
    vectors = openai_embedding.embed_documents([
        "First document for embedding",
        "Second document for embedding",
        "Third document with different content"
    ])
    print(f"Embedded {len(vectors)} documents, vector dimension: {len(vectors[0])}")

    # ë™ê¸° ì„ë² ë”© (ë‹¨ì¼ ì¿¼ë¦¬)
    query_vector = openai_embedding.embed_query("What was the main topic?")
    print(f"Query vector dimension: {len(query_vector)}")

    # ë¹„ë™ê¸° ì„ë² ë”©
    async def async_embedding_example():
        async_vectors = await openai_embedding.aembed_documents([
            "Async embedding test document"
        ])
        print(f"Async embedded vector dimension: {len(async_vectors[0])}")

    asyncio.run(async_embedding_example())

finally:
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await openai_embedding.aclose()

# Rerank ì˜ˆì‹œ (ChatOasis ë˜ëŠ” OasisEmbedding ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥)
oasis_chat = ChatOasis(
    account_id="your_account_id",
    user_uuid="your_uuid",
    workspace_uuid="your_workspace_uuid",
    tenant_uuid="your_tenant_uuid",
    model_name="your_rerank_model_uuid",
    plugin_name="rerank_test"
)

try:
    # ë™ê¸° Rerank
    rerank_result = oasis_chat.rerank(
        model="your_rerank_model_uuid",
        query="Best practices for Python programming",
        documents=[
            "Python uses indentation for code blocks.",
            "Python is known for its readability.",
            "JavaScript uses curly braces for code blocks.",
            "Python has a large standard library.",
        ],
        top_n=3,
    )
    print(f"Top {len(rerank_result['results'])} reranked documents:")
    for idx, result in enumerate(rerank_result['results'], 1):
        print(f"{idx}. Score: {result.get('relevance_score', 'N/A')} - {result.get('document', 'N/A')}")

    # ë¹„ë™ê¸° Rerank
    async def async_rerank_example():
        async_result = await oasis_chat.arerank(
            model="your_rerank_model_uuid",
            query="Machine learning with Python",
            documents=[
                "Python has powerful ML libraries like scikit-learn.",
                "Java is used in enterprise applications.",
                "TensorFlow and PyTorch are popular Python ML frameworks.",
            ],
            top_n=2,
        )
        print(f"Async rerank results: {async_result['results']}")

    asyncio.run(async_rerank_example())

finally:
    oasis_chat.close()
```

## 3. ëª¨ë²” ì‚¬ë¡€

### 3.1 ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

**ê¶Œì¥: Context Manager ì‚¬ìš©**

```python
# ë™ê¸° í´ë¼ì´ì–¸íŠ¸
with Oasis(...) as client:
    # ì‘ì—… ìˆ˜í–‰
    resp = client.chat.completions.create(...)

# ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸
async with AsyncOasis(...) as client:
    # ì‘ì—… ìˆ˜í–‰
    resp = await client.chat.completions.create(...)
```

**ìˆ˜ë™ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**

```python
# ë™ê¸°
client = Oasis(...)
try:
    # ì‘ì—… ìˆ˜í–‰
    resp = client.chat.completions.create(...)
finally:
    client.close()

# ë¹„ë™ê¸°
client = AsyncOasis(...)
try:
    # ì‘ì—… ìˆ˜í–‰
    resp = await client.chat.completions.create(...)
finally:
    await client.aclose()
```

### 3.2 ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

```python
# ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
with Oasis(...) as client:
    stream = client.chat.completions.create(
        model="model_id",
        messages=[...],
        stream=True
    )
    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
async with AsyncOasis(...) as client:
    stream = await client.chat.completions.create(
        model="model_id",
        messages=[...],
        stream=True
    )
    async for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
```

### 3.3 ì—ëŸ¬ í•¸ë“¤ë§

```python
from oasis.sdk.openai import Oasis
from openai import OpenAIError

with Oasis(...) as client:
    try:
        resp = client.chat.completions.create(
            model="model_id",
            messages=[{"role": "user", "content": "Hello"}]
        )
    except OpenAIError as e:
        print(f"OpenAI API ì—ëŸ¬: {e}")
    except Exception as e:
        print(f"ê¸°íƒ€ ì—ëŸ¬: {e}")
```

## 4. ì˜ì¡´ì„±

- Python 3.11.x
- openai 1.97.0
- langchain-openai 0.3.28

## 6. ê°œë°œì ê°€ì´ë“œ

### 6.1 ìƒˆë¡œìš´ Provider ì¶”ê°€

ìƒˆë¡œìš´ LLM Providerë¥¼ ì¶”ê°€í•˜ë ¤ë©´:

1. **í™˜ê²½ë³€ìˆ˜ ì„¤ì •**: `tests/test_env.py`ì— ìƒˆ Providerì˜ ëª¨ë¸ UUID ì¶”ê°€
2. **í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±**:
   - `tests/test_oasis_{provider}.py` (SDK ë˜í¼ìš©)
   - `tests/test_oasis_lc_{provider}.py` (LangChain ë˜í¼ìš©)
3. **Provider ì‹ë³„**: í™˜ê²½ë³€ìˆ˜ prefixë¡œ Provider êµ¬ë¶„ (ì˜ˆ: `NEWPROVIDER_`)

### 6.2 ì½”ë“œ ê¸°ì—¬

1. **ë¸Œëœì¹˜ ìƒì„±**: `git checkout -b feature/new-feature`
2. **í…ŒìŠ¤íŠ¸ ì‘ì„±**: Providerë³„ í…ŒìŠ¤íŠ¸ íŒŒì¼ì— ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¶”ê°€
3. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**: README.mdì— ìƒˆ ê¸°ëŠ¥ ì„¤ëª… ì¶”ê°€
4. **Pull Request**: ë³€ê²½ì‚¬í•­ì„ ì„¤ëª…í•˜ëŠ” PR ìƒì„±

## 7. ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” [LICENSE](LICENSE) íŒŒì¼ì— ëª…ì‹œëœ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## 8. ë¬¸ì˜ ë° ì§€ì›

- **ì´ìŠˆ ë¦¬í¬íŠ¸**: GitHub Issuesë¥¼ í†µí•´ ë²„ê·¸ë‚˜ ê¸°ëŠ¥ ìš”ì²­ ì œì¶œ
- **ê¸°ìˆ  ë¬¸ì˜**: ê°œë°œíŒ€ì—ê²Œ ì§ì ‘ ì—°ë½
- **ê¸°ì—¬ ê°€ì´ë“œ**: CONTRIBUTING.md ì°¸ì¡° (ìˆëŠ” ê²½ìš°)

## 5. í…ŒìŠ¤íŠ¸

### 5.1 ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/
```

### 5.2 Providerë³„ í…ŒìŠ¤íŠ¸

**SDK ë˜í¼ í…ŒìŠ¤íŠ¸ (Oasis í´ë¼ì´ì–¸íŠ¸)**

```bash
# OpenAI Provider
python -m pytest tests/test_oasis_openai.py

# Azure OpenAI Provider
python -m pytest tests/test_oasis_azure.py

# Google Provider
python -m pytest tests/test_oasis_google.py

# XAI Provider
python -m pytest tests/test_oasis_xai.py

# Anthropic Provider
python -m pytest tests/test_oasis_anthropic.py

# Oasis Provider (ìì²´ ëª¨ë¸)
python -m pytest tests/test_oasis.py
```

**LangChain ë˜í¼ í…ŒìŠ¤íŠ¸**

```bash
# LangChain OpenAI Provider
python -m pytest tests/test_oasis_lc_openai.py

# LangChain Azure Provider
python -m pytest tests/test_oasis_lc_azure.py

# LangChain Google Provider
python -m pytest tests/test_oasis_lc_google.py

# LangChain XAI Provider
python -m pytest tests/test_oasis_lc_xai.py

# LangChain Anthropic Provider
python -m pytest tests/test_oasis_lc_anthropic.py

# LangChain Oasis Provider
python -m pytest tests/test_oasis_lc.py
```

### 5.3 íŠ¹ì • Provider ê·¸ë£¹ í…ŒìŠ¤íŠ¸

```bash
# OpenAI ê´€ë ¨ ëª¨ë“  í…ŒìŠ¤íŠ¸
python -m pytest tests/test_oasis_openai.py tests/test_oasis_lc_openai.py

# Azure ê´€ë ¨ ëª¨ë“  í…ŒìŠ¤íŠ¸
python -m pytest tests/test_oasis_azure.py tests/test_oasis_lc_azure.py

# íŠ¹ì • íŒ¨í„´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/test_*openai*.py  # OpenAI ê´€ë ¨
python -m pytest tests/test_*azure*.py   # Azure ê´€ë ¨
python -m pytest tests/test_*lc*.py      # LangChain ê´€ë ¨
```

### 5.4 Providerë³„ ë¶„ë¦¬ì˜ ì¥ì 

- **ë…ë¦½ì„±**: ê° Providerì˜ ë¬¸ì œê°€ ë‹¤ë¥¸ Providerì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
- **ì„ íƒì  í…ŒìŠ¤íŠ¸**: í•„ìš”í•œ Providerë§Œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì‹œê°„ ë‹¨ì¶•
- **ë””ë²„ê¹… ìš©ì´ì„±**: íŠ¹ì • Providerì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ
- **CI/CD ìµœì í™”**: Providerë³„ë¡œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê°€ëŠ¥

### 5.5 ë…¸íŠ¸ë¶ ì˜ˆì‹œ

ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œëŠ” `tests/notebooks/` ë””ë ‰í† ë¦¬ì˜ Jupyter ë…¸íŠ¸ë¶ì„ ì°¸ê³ í•˜ì„¸ìš”:

- `openai.ipynb`: SDK ë˜í¼ë¥¼ ì‚¬ìš©í•œ OpenAI ëª¨ë¸ ì˜ˆì‹œ
- `azure.ipynb`: SDK ë˜í¼ë¥¼ ì‚¬ìš©í•œ Azure OpenAI ëª¨ë¸ ì˜ˆì‹œ
- `api.ipynb`: ë‹¤ì–‘í•œ Provider API ë ˆë²¨ ì‚¬ìš© ì˜ˆì‹œ

### 5.6 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— `tests/test_env.py`ì—ì„œ í•„ìš”í•œ ëª¨ë¸ UUIDì™€ ì¸ì¦ ì •ë³´ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```python
# tests/test_env.py
def set_env():
    os.environ["PROXY_URL"] = "http://your-proxy-server:port/api/proxy"
    os.environ["ACCOUNT_ID"] = "your_account_id"
    os.environ["USER_UUID"] = "your_user_uuid"
    os.environ["WORKSPACE_UUID"] = "your_workspace_uuid"
    os.environ["TENANT_UUID"] = "your_tenant_uuid"

    # Providerë³„ ëª¨ë¸ UUID ì„¤ì •
    os.environ["OPENAI_GPT_4O"] = "openai_model_uuid"
    os.environ["AOAI_OASIS_GPT_4_1"] = "azure_model_uuid"
    os.environ["GOOGLE_GEMINI_2_5_PRO"] = "google_model_uuid"
    # ... ê¸°íƒ€ Provider UUID
```
