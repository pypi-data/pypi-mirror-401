"""
Qualys integration module for RegScale CLI.
"""

import logging
import os
import pprint
import traceback
from asyncio import sleep
from datetime import datetime, timedelta, timezone
from json import JSONDecodeError
from pathlib import Path
from typing import Any, Optional, Tuple, Union
from urllib.parse import urljoin

import click
import requests
import xmltodict
from requests import Session
from rich.progress import BarColumn, Progress, SpinnerColumn, TaskID, TextColumn, TimeElapsedColumn

from regscale.core.app.utils.app_utils import (
    check_file_path,
    check_license,
    create_progress_object,
    error_and_exit,
    get_current_datetime,
    save_data_to,
)
from regscale.core.app.utils.file_utils import download_from_s3
from regscale.integrations.commercial.qualys.containers import fetch_all_vulnerabilities
from regscale.integrations.commercial.qualys.qualys_error_handler import QualysErrorHandler
from regscale.integrations.commercial.qualys.scanner import QualysTotalCloudJSONLIntegration
from regscale.integrations.commercial.qualys.variables import QualysVariables
from regscale.integrations.commercial.qualys.was_helpers import (
    convert_was_vulns_to_issues,
    extract_was_assets,
    fetch_was_vulnerabilities,
)
from regscale.integrations.scanner_integration import IntegrationAsset, IntegrationFinding
from regscale.integrations.variables import ScannerVariables
from regscale.models import Asset, Issue, Search, regscale_models, IssueStatus, IssueSeverity
from regscale.models.app_models.click import NotRequiredIf, save_output_to, ssp_or_component_id
from regscale.models.integration_models.flat_file_importer import FlatFileImporter
from regscale.models.integration_models.qualys import (
    Qualys,
    QualysContainerScansImporter,
    QualysPolicyScansImporter,
    QualysWasScansImporter,
)
from regscale.validation.record import validate_regscale_object
from regscale.integrations.commercial.qualys.total_cloud_helpers import (
    _extract_container_assets_from_tc,
    extract_total_cloud_assets,
)


# Create logger for this module
logger = logging.getLogger("regscale")
job_progress = create_progress_object()

# Qualys datetime format constant
QUALYS_DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
HEADERS = {"X-Requested-With": "RegScale CLI"}

# Import the Qualys API session object and headers from the main qualys module
QUALYS_API = Session()


@click.group()
def qualys():
    """Performs actions from the Qualys API"""


def _prepare_qualys_params(include_tags: str, exclude_tags: str) -> dict:
    """
    Prepare parameters for Qualys API request.

    :param str include_tags: Tags to include in the filter
    :param str exclude_tags: Tags to exclude in the filter
    :return: Dictionary of parameters for the API request
    :rtype: dict
    """
    params = {
        "action": "list",
        "show_asset_id": "1",
        "show_tags": "1",
    }
    if exclude_tags or include_tags:
        params["use_tags"] = "1"
        params["tag_set_by"] = "name"
        if exclude_tags:
            params["tag_set_exclude"] = exclude_tags
        if include_tags:
            params["tag_set_include"] = include_tags

    return params


def _setup_progress_tracking(integration, progress):
    """
    Set up progress tracking for assets and findings.

    :param integration: Scanner integration instance
    :param progress: Progress bar instance
    :return: Tuple of task IDs and counts for assets and findings
    :rtype: tuple
    """
    # Count assets and findings for progress tracking
    asset_count = sum(1 for _ in open(integration.ASSETS_FILE, "r") if _.strip())
    finding_count = sum(1 for _ in open(integration.FINDINGS_FILE, "r") if _.strip())

    logger.info(f"Found {asset_count} assets and {finding_count} findings in JSONL files")

    # Create tasks
    asset_task = progress.add_task(f"[green]Importing {asset_count} assets to RegScale...", total=asset_count)
    finding_task = progress.add_task(f"[yellow]Importing {finding_count} findings to RegScale...", visible=False)

    return asset_task, finding_task, asset_count, finding_count


def _import_assets(integration, assets_iterator, progress, asset_task):
    """
    Import assets to RegScale with progress tracking.

    :param integration: Scanner integration instance
    :param assets_iterator: Iterator of assets
    :param progress: Progress bar instance to track progress
    :param asset_task: Task ID for asset progress
    :return: Number of assets imported
    :rtype: int
    """
    if not _verify_assets_file_exists(integration):
        return 0

    try:
        wrapped_iterator = _create_asset_progress_tracker(assets_iterator, progress, asset_task)
        assets_imported = integration.update_regscale_assets(wrapped_iterator)
        logger.info(f"Imported {assets_imported} assets to RegScale")
        return assets_imported
    except Exception as e:
        logger.error(f"Error importing assets to RegScale: {str(e)}")
        logger.error(traceback.format_exc())
        return 0


def _verify_assets_file_exists(integration):
    """
    Verify that the assets file exists and is not empty.

    :param integration: Scanner integration instance
    :return: True if the file exists and is not empty, False otherwise
    :rtype: bool
    """
    if not os.path.exists(integration.ASSETS_FILE) or os.path.getsize(integration.ASSETS_FILE) == 0:
        logger.warning(f"Assets file {integration.ASSETS_FILE} is empty or does not exist")
        return False
    return True


def _create_asset_progress_tracker(assets_iter, progress, asset_task):
    """
    Create a generator that tracks progress of asset processing.

    :param assets_iter: Iterator of assets
    :param progress: Progress bar instance
    :param asset_task: Task ID for asset progress
    :return: Generator that yields assets and updates progress
    """
    count = 0
    asset_ids = []

    try:
        for asset in assets_iter:
            count += 1
            if asset and hasattr(asset, "identifier"):
                asset_ids.append(asset.identifier)
            progress.update(asset_task, advance=1)
            yield asset

        _log_asset_results(count, asset_ids)
    except Exception as e:
        logger.error(f"Error while yielding assets: {str(e)}")
        logger.error(traceback.format_exc())


def _log_asset_results(count, asset_ids):
    """
    Log the results of asset processing.

    :param count: Number of assets processed
    :param asset_ids: List of asset IDs
    """
    if count == 0:
        logger.warning("No assets were created/updated from the JSONL file")
    else:
        sample_ids = asset_ids[:5]
        truncation_indicator = ", ..." if len(asset_ids) > 5 else ""
        sample_ids_str = ", ".join(sample_ids)
        logger.debug(f"Created/updated {count} assets to RegScale with IDs: {sample_ids_str}{truncation_indicator}")


def _import_findings(integration, progress, finding_task):
    """
    Import findings to RegScale with progress tracking.

    :param integration: Scanner integration instance
    :param progress: Progress bar instance
    :param finding_task: Task ID for finding progress
    :return: Number of findings imported
    :rtype: int
    """
    progress.update(finding_task, visible=True)
    total_findings = _count_findings_in_file(integration)

    if total_findings > 0:
        progress.update(finding_task, total=total_findings)

    findings_yielded = 0
    try:
        # Create findings iterator directly from the file
        tracked_iterator = _create_finding_progress_tracker(
            integration._yield_items_from_jsonl(integration.FINDINGS_FILE, IntegrationFinding), progress, finding_task
        )

        findings_imported = integration.update_regscale_findings(tracked_iterator)
        logger.info(f"Successfully imported {findings_imported} findings to RegScale")

        # Update final count for nonlocal reference from the tracked_iterator
        findings_yielded = getattr(tracked_iterator, "count", 0)

        # Ensure progress is complete
        if findings_yielded > 0:
            progress.update(finding_task, completed=findings_yielded)

        return findings_imported
    except Exception as e:
        logger.error(f"Error during RegScale findings import: {str(e)}")
        return 0


def _count_findings_in_file(integration):
    """
    Count the findings in the JSONL file.

    :param integration: Scanner integration instance
    :return: Number of findings in the file
    :rtype: int
    """
    try:
        if os.path.exists(integration.FINDINGS_FILE):
            with open(integration.FINDINGS_FILE, "r") as f:
                total_findings = sum(1 for line in f if line.strip())
                logger.info(f"Found {total_findings} findings in JSONL file")
                return total_findings
        else:
            logger.warning(f"Findings file {integration.FINDINGS_FILE} does not exist")
            return 0
    except Exception as e:
        logger.error(f"Error counting findings in JSONL file: {str(e)}")
        return 0


def _create_finding_progress_tracker(findings_iter, progress, finding_task):
    """
    Create a generator function that tracks progress of finding processing.

    :param findings_iter: Iterator of findings
    :param progress: Progress bar instance
    :param finding_task: Task ID for finding progress
    :return: Generator that yields findings and updates progress
    """
    tracker = FindingProgressTracker(findings_iter, progress, finding_task)
    return tracker


class FindingProgressTracker:
    """Class to track progress of finding processing with proper object reference."""

    def __init__(self, findings_iter, progress, finding_task):
        self.findings_iter = findings_iter
        self.progress = progress
        self.finding_task = finding_task
        self.count = 0
        self.finding_ids = []
        self.output_final_log: bool = False

    def __iter__(self):
        return self

    def __next__(self):
        try:
            finding = next(self.findings_iter)
            self.count += 1
            if finding and hasattr(finding, "external_id") and finding.external_id is not None:
                self.finding_ids.append(finding.external_id)
            self.progress.update(self.finding_task, advance=1)
            return finding
        except StopIteration:
            self._log_finding_results()
            return
        except Exception as e:
            logger.debug(f"Findings created/updated before error: {self.count}")
            error_and_exit(f"Error creating/updating findings: {str(e)}")

    def _log_finding_results(self):
        """Log the results of finding processing."""
        if self.count == 0:
            logger.warning("No findings were created/updated from the JSONL file")
        elif not self.output_final_log:
            logger.info(f"Created/Updated {self.count} findings to RegScale")
            sample_ids = self.finding_ids[:5]
            truncation_indicator = ", ..." if len(self.finding_ids) > 5 else ""
            sample_ids_str = ", ".join(sample_ids)
            logger.debug(f"Sample finding IDs: {sample_ids_str}{truncation_indicator}")
            self.output_final_log = True


@click.command(name="import_total_cloud")
@ssp_or_component_id()
@click.option(
    "--include_tags",
    "-t",
    type=click.STRING,
    required=False,
    default=None,
    help="Include tags in the import comma separated string of tag names or ids, defaults to None.",
)
@click.option(
    "--exclude_tags",
    "-e",
    type=click.STRING,
    required=False,
    default=None,
    help="Exclude tags in the import comma separated string of tag names or ids, defaults to None. If used, --include_tags must also be provided.",
)
@click.option(
    "--vulnerability-creation",
    "-v",
    type=click.Choice(["NoIssue", "IssueCreation", "PoamCreation"], case_sensitive=False),
    required=False,
    default=None,
    help="Specify how vulnerabilities are processed: NoIssue, IssueCreation, or PoamCreation.",
)
@click.option(
    "--ssl-verify/--no-ssl-verify",
    default=None,
    required=False,
    help="Enable/disable SSL certificate verification for API calls.",
)
@click.option(
    "--containers",
    type=click.BOOL,
    help="To disable fetching containers, use False. Defaults to True.",
    default=True,
)
def import_total_cloud(
    regscale_ssp_id: int = None,
    component_id: int = None,
    include_tags: str = None,
    exclude_tags: str = None,
    vulnerability_creation: str = None,
    ssl_verify: bool = None,
    containers: bool = True,
):
    """
    Import Qualys Total Cloud Assets and Vulnerabilities using JSONL scanner implementation.

    This command uses the JSONLScannerIntegration class for improved efficiency and memory management.
    """
    # Determine which ID to use and whether it's a component
    if component_id:
        plan_id = component_id
        is_component = True
        if not validate_regscale_object(component_id, "components"):
            logger.warning("Component #%i is not a valid RegScale Component.", component_id)
            return
    elif regscale_ssp_id:
        plan_id = regscale_ssp_id
        is_component = False
        if not validate_regscale_object(regscale_ssp_id, "securityplans"):
            logger.warning("SSP #%i is not a valid RegScale Security Plan.", regscale_ssp_id)
            return
    else:
        error_and_exit(
            "You must provide either a --regscale_ssp_id or a --component_id to import Qualys Total Cloud data."
        )

    # exclude tags must have include_tags
    if exclude_tags and not include_tags:
        error_and_exit("You must provide --include_tags when using --exclude_tags to import Qualys Total Cloud data.")

    # Ensure vulnerability creation is properly set
    if not vulnerability_creation:
        vulnerability_creation = "IssueCreation"  # Default to IssueCreation for Qualys
        logger.info("No vulnerability creation setting provided, defaulting to IssueCreation for Qualys Total Cloud")

    containers_lst = []
    try:
        # Configure scanner variables and fetch data
        _configure_scanner_variables(vulnerability_creation, ssl_verify)
        response_data = _fetch_qualys_api_data(include_tags, exclude_tags)
        if not response_data:
            return

        if containers:
            # Fetch containers and container findings
            params = _prepare_qualys_params(include_tags, exclude_tags)
            containers_lst = fetch_all_vulnerabilities(filters=params)

        # Initialize and run integration
        integration = _initialize_integration(
            plan_id, response_data, vulnerability_creation, ssl_verify, containers_lst, is_component
        )
        _run_integration_import(integration)

        logger.info("Qualys Total Cloud data imported successfully with JSONL scanner.")
    except Exception:
        error_message = traceback.format_exc()
        logger.error("Error occurred while processing Qualys data with JSONL scanner")
        logger.error(error_message)


def _configure_scanner_variables(vulnerability_creation, ssl_verify):
    """Configure scanner variables with appropriate precedence.

    :param str vulnerability_creation: Vulnerability creation mode from command line
    :param bool ssl_verify: SSL verification setting from command line
    """
    # Configure vulnerability creation mode
    _configure_vulnerability_creation(vulnerability_creation)

    # Configure SSL verification
    _configure_ssl_verification(ssl_verify)


def _configure_vulnerability_creation(vulnerability_creation):
    """Configure vulnerability creation mode with appropriate precedence.

    :param str vulnerability_creation: Vulnerability creation mode from command line
    """
    if vulnerability_creation:
        # Command line option takes precedence
        ScannerVariables.vulnerabilityCreation = vulnerability_creation
        logger.info(f"Setting vulnerability creation mode from command line: {vulnerability_creation}")
    elif hasattr(QualysVariables, "vulnerabilityCreation") and ScannerVariables.vulnerabilityCreation:
        # Use Qualys-specific setting if available
        logger.info(f"Using Qualys-specific vulnerability creation mode: {ScannerVariables.vulnerabilityCreation}")
    else:
        # Fall back to global ScannerVariables
        logger.info(f"Using global vulnerability creation mode: {ScannerVariables.vulnerabilityCreation}")


def _configure_ssl_verification(ssl_verify):
    """Configure SSL verification setting with appropriate precedence.

    :param bool ssl_verify: SSL verification setting from command line
    """
    if ssl_verify is not None:
        # Command line option takes precedence
        ScannerVariables.sslVerify = ssl_verify
        logger.info(f"Setting SSL verification from command line: {ssl_verify}")
    elif hasattr(QualysVariables, "sslVerify") and QualysVariables.sslVerify is not None:
        # Use Qualys-specific setting
        logger.info(f"Using Qualys-specific SSL verification setting: {ScannerVariables.sslVerify}")
    else:
        # Fall back to global ScannerVariables
        logger.info(f"Using global SSL verification setting: {ScannerVariables.sslVerify}")


def _fetch_qualys_api_data(include_tags, exclude_tags):
    """Fetch data from Qualys API.

    :param str include_tags: Tags to include in the query
    :param str exclude_tags: Tags to exclude from the query
    :return: Parsed XML data or None if request failed
    """
    from regscale.integrations.commercial.qualys.qualys_error_handler import QualysErrorHandler

    qualys_url, qualys_api = _get_qualys_api()
    params = _prepare_qualys_params(include_tags, exclude_tags)

    logger.info("Fetching Qualys Total Cloud data with JSONL scanner...")

    try:
        response = qualys_api.get(
            url=urljoin(qualys_url, "/api/2.0/fo/asset/host/vm/detection/"),
            headers=HEADERS,
            params=params,
            verify=ScannerVariables.sslVerify,  # Apply SSL verification setting
        )

        # Use the error handler to validate the response
        is_valid, error_message, parsed_data = QualysErrorHandler.validate_response(response)

        if not is_valid:
            logger.error(f"Qualys API request failed: {error_message}")

            # If we have parsed data, extract detailed error information
            if parsed_data:
                error_details = QualysErrorHandler.extract_error_details(parsed_data)
                QualysErrorHandler.log_error_details(error_details)

                # Check if this is a retryable error
                error_code = error_details.get("error_code")
                if error_code and QualysErrorHandler.should_retry(error_code):
                    retry_after = error_details.get("retry_after", 60)
                    logger.warning(f"This error may be retryable. Consider retrying after {retry_after} seconds.")

                # Check if this is a fatal error that should stop processing
                if error_code and QualysErrorHandler.is_fatal_error(error_code):
                    logger.error("Fatal error encountered. Please check your credentials and permissions.")

            return None

        # Process API response
        logger.info("Total cloud data fetched successfully. Processing with JSONL scanner...")
        return parsed_data

    except Exception as e:
        logger.error(f"Unexpected error during Qualys API request: {e}")
        logger.debug(traceback.format_exc())
        return None


def _initialize_integration(plan_id, response_data, vulnerability_creation, ssl_verify, containers, is_component=False):
    integration_kwargs = {
        "plan_id": plan_id,
        "xml_data": response_data,
        "vulnerability_creation": vulnerability_creation or ScannerVariables.vulnerabilityCreation,
        "ssl_verify": ssl_verify if ssl_verify is not None else ScannerVariables.sslVerify,
        "containers": containers,
        "is_component": is_component,
    }
    if hasattr(ScannerVariables, "threadMaxWorkers"):
        integration_kwargs["max_workers"] = ScannerVariables.threadMaxWorkers
        logger.debug(f"Using thread max workers: {ScannerVariables.threadMaxWorkers}")
    integration = QualysTotalCloudJSONLIntegration(**integration_kwargs)
    return integration


def _run_integration_import(integration):
    """Run the integration import process with progress tracking.

    :param QualysTotalCloudJSONLIntegration integration: Initialized integration object
    """
    assets_iterator, _ = integration.fetch_assets_and_findings()
    logger.info("Syncing assets to RegScale...")

    # Set up progress reporting and import data
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("•"),
        TimeElapsedColumn(),
        console=None,
    ) as progress:
        # Set up progress tasks
        asset_task, finding_task, _, _ = _setup_progress_tracking(integration, progress)

        # Import assets and findings
        _import_assets(integration, assets_iterator, progress, asset_task)
        _import_findings(integration, progress, finding_task)


@click.command(name="import_total_cloud_xml")
@ssp_or_component_id()
@click.option(
    "--xml_file",
    "-f",
    type=click.Path(exists=True, file_okay=True, dir_okay=False, readable=True),
    required=True,
    help="Path to Qualys Total Cloud XML file to process.",
)
def import_total_cloud_from_xml(xml_file: str, regscale_ssp_id: int = None, component_id: int = None):
    """
    Import Qualys Total Cloud Assets and Vulnerabilities from an existing XML file using JSONL scanner.

    This command processes an existing XML file instead of making an API call, useful for testing.
    """
    from regscale.integrations.commercial.qualys.qualys_error_handler import QualysErrorHandler

    try:
        logger.info(f"Processing Qualys Total Cloud XML file: {xml_file}")

        # Parse the XML file with error handling
        with open(xml_file, "r") as f:
            xml_content = f.read()

        # Use the error handler to safely parse XML
        success, response_data, error_message = QualysErrorHandler.parse_xml_safely(xml_content)

        if not success:
            logger.error(f"Failed to parse XML file: {error_message}")

            # If we have partial data, try to extract error details
            if response_data:
                error_details = QualysErrorHandler.extract_error_details(response_data)
                QualysErrorHandler.log_error_details(error_details)

            return

        # Check for Qualys-specific errors in the parsed data
        error_details = QualysErrorHandler.extract_error_details(response_data)
        if error_details.get("has_error"):
            logger.error("XML file contains Qualys error response")
            QualysErrorHandler.log_error_details(error_details)
            return

        # Determine which ID to use and whether it's a component
        if component_id:
            plan_id = component_id
            is_component = True
            if not validate_regscale_object(component_id, "components"):
                logger.warning("Component #%i is not a valid RegScale Component.", component_id)
                return
        elif regscale_ssp_id:
            plan_id = regscale_ssp_id
            is_component = False
            if not validate_regscale_object(regscale_ssp_id, "securityplans"):
                logger.warning("SSP #%i is not a valid RegScale Security Plan.", regscale_ssp_id)
                return
        else:
            error_and_exit(
                "You must provide either a --regscale_ssp_id or a --component_id to import Qualys Total Cloud data."
            )

        # Initialize the JSONLScannerIntegration implementation
        integration = QualysTotalCloudJSONLIntegration(
            plan_id=plan_id, xml_data=response_data, file_path=xml_file, is_component=is_component
        )

        # Process data and generate JSONL files
        if not _process_xml_to_jsonl(integration):
            return

        # Count and validate items for progress tracking
        asset_count, finding_count = _count_jsonl_items(integration)
        if asset_count == 0 and finding_count == 0:
            logger.error("No assets or findings found in the processed data")
            return

        # Set up progress tracking and import to RegScale
        _import_to_regscale(integration, asset_count, finding_count)

        logger.info("Qualys Total Cloud XML file imported successfully with JSONL scanner.")
    except Exception:
        error_message = traceback.format_exc()
        logger.error("Error occurred while processing Qualys XML file with JSONL scanner")
        logger.error(error_message)


def _process_xml_to_jsonl(integration):
    """Process XML data and generate JSONL files."""
    try:
        logger.info("Fetching assets and findings...")
        integration.fetch_assets_and_findings()

        # Validate JSONL files
        for file_path, file_type in [(integration.ASSETS_FILE, "Assets"), (integration.FINDINGS_FILE, "Findings")]:
            if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
                logger.error(f"{file_type} file not created or is empty: {file_path}")
                return False

        logger.info("Successfully created JSONL files.")
        logger.info(f"Assets file size: {os.path.getsize(integration.ASSETS_FILE)} bytes")
        logger.info(f"Findings file size: {os.path.getsize(integration.FINDINGS_FILE)} bytes")
        return True
    except Exception as e:
        logger.error(f"Error fetching assets and findings: {str(e)}")
        logger.error(traceback.format_exc())
        return False


def _count_jsonl_items(integration):
    """Count assets and findings in JSONL files."""
    # Count assets
    assets_count = 0
    asset_ids = []
    try:
        assets_iterator = integration._yield_items_from_jsonl(integration.ASSETS_FILE, IntegrationAsset)
        for asset in assets_iterator:
            assets_count += 1
            if asset and hasattr(asset, "identifier"):
                asset_ids.append(asset.identifier)

        logger.debug(f"Asset IDs: {', '.join(asset_ids[:5])}{', ...' if len(asset_ids) > 5 else ''}")
    except Exception as e:
        logger.error(f"Error counting assets: {str(e)}")

    # Count findings
    findings_count = 0
    try:
        findings_iterator = integration._yield_items_from_jsonl(integration.FINDINGS_FILE, IntegrationFinding)
        findings_count = sum(1 for _ in findings_iterator)
    except Exception as e:
        logger.error(f"Error counting findings: {str(e)}")

    logger.info(f"Found {assets_count} assets and {findings_count} findings in JSONL files")
    return assets_count, findings_count


def _create_progress_bar():
    """Create a progress bar that doesn't reset at 100%."""
    from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn, TimeElapsedColumn

    class NonResettingProgress(Progress):
        def update(self, task_id, **fields):
            """Update a task."""
            task = self._tasks[task_id]
            completed_old = task.completed

            # Update the task with the provided fields
            for field_name, value in fields.items():
                if field_name == "completed" and value is True:
                    if task.total is not None:
                        task.completed = task.total
                else:
                    setattr(task, field_name, value)

            # Prevent task from being reset to 0% after reaching 100%
            if completed_old == task.total and task.completed < task.total:
                task.completed = task.total

            self.refresh()

    return NonResettingProgress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("•"),
        TimeElapsedColumn(),
        console=None,
    )


def _track_progress_generator(iterator, progress_bar, task_id, id_attribute=None):
    """Create a generator that tracks progress as items are yielded."""
    processed = 0
    item_ids = []

    for item in iterator:
        processed += 1

        if id_attribute and hasattr(item, id_attribute):
            item_ids.append(getattr(item, id_attribute))

        progress_bar.update(task_id, completed=processed)
        yield item

    # Log debugging information
    item_type = "items"
    if id_attribute == "identifier":
        item_type = "assets"
    elif id_attribute == "external_id":
        item_type = "findings"

    logger.debug(f"Created/updated {processed} {item_type} in RegScale")

    if processed == 0:
        logger.error(f"WARNING: No {item_type} were created/updated in RegScale!")
    elif item_ids:
        logger.debug(f"First 10 {item_type} IDs: {item_ids[:10]}")


def _import_to_regscale(integration, asset_count, finding_count):
    """Import assets and findings to RegScale with progress tracking."""
    progress = _create_progress_bar()

    with progress:
        # Create tasks
        asset_task = progress.add_task(f"[green]Importing {asset_count} assets to RegScale...", total=asset_count)
        finding_task = progress.add_task(
            f"[yellow]Importing {finding_count} findings to RegScale...", visible=False, total=finding_count
        )

        # Import assets
        try:
            assets_iterator = integration._yield_items_from_jsonl(integration.ASSETS_FILE, IntegrationAsset)
            tracked_assets = _track_progress_generator(assets_iterator, progress, asset_task, "identifier")
            assets_imported = integration.update_regscale_assets(tracked_assets)
            logger.info(f"Imported {assets_imported} assets to RegScale")
            # Ensure the progress shows complete
            progress.update(asset_task, completed=asset_count)
        except Exception as e:
            logger.error(f"Error importing assets to RegScale: {str(e)}")
            logger.error(traceback.format_exc())
            # Mark the task as completed even if there was an error
            progress.update(asset_task, completed=asset_count)

        # Import findings
        progress.update(finding_task, visible=True)
        try:
            findings_iterator = integration._yield_items_from_jsonl(integration.FINDINGS_FILE, IntegrationFinding)
            tracked_findings = _track_progress_generator(findings_iterator, progress, finding_task, "external_id")
            findings_imported = integration.update_regscale_findings(tracked_findings)
            logger.info(f"Imported {findings_imported} findings to RegScale")
            # Ensure progress shows complete
            progress.update(finding_task, completed=finding_count)
        except Exception as e:
            logger.error(f"Error importing findings to RegScale: {str(e)}")
            logger.error(traceback.format_exc())
            # Mark as completed even if there was an error
            progress.update(finding_task, completed=finding_count)


@qualys.command(name="export_scans")
@save_output_to()
@click.option(
    "--days",
    type=int,
    default=30,
    help="The number of days to go back for completed scans, default is 30.",
)
@click.option(
    "--export",
    type=click.BOOL,
    help="To disable saving the scans as a .json file, use False. Defaults to True.",
    default=True,
    prompt=False,
    required=False,
)
def export_past_scans(save_output_to: Path, days: int, export: bool = True):
    """Export scans from Qualys Host that were completed
    in the last x days, defaults to last 30 days
    and defaults to save it as a .json file"""
    export_scans(
        save_path=save_output_to,
        days=days,
        export=export,
    )


@qualys.command(name="import_scans")
@FlatFileImporter.common_scanner_options(
    message="File path to the folder containing Qualys .csv or .xlsx files to process to RegScale.",
    prompt="File path for Qualys files",
    import_name="qualys",
)
@click.option(
    "--skip_rows",
    type=click.INT,
    help="The number of rows in the file to skip to get to the column headers, defaults to 129.",
    default=129,
)
def import_scans(
    folder_path: os.PathLike[str],
    regscale_ssp_id: int,
    scan_date: datetime,
    mappings_path: os.PathLike[str],
    disable_mapping: bool,
    skip_rows: int,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    upload_file: bool,
):
    """
    Import vulnerability scans from Qualys CSV or Excel (.xlsx) files.

    This command processes Qualys CSV or Excel export files and imports assets and vulnerabilities
    into RegScale. The files must contain specific required headers.

    TROUBLESHOOTING:
    If you encounter "No columns to parse from file" errors, try:
    1. Run 'regscale qualys validate_csv -f <file_path>' first
    2. Adjust the --skip_rows parameter (default: 129)
    3. Check that your file has the required headers

    REQUIRED HEADERS:
    Severity, Title, Exploitability, CVE ID, Solution, DNS, IP,
    QG Host ID, OS, NetBIOS, FQDN

    For detailed format requirements, see the documentation at:
    regscale/integrations/commercial/qualys/QUALYS_CSV_FORMAT.md
    """
    import_qualys_scans(
        folder_path=folder_path,
        regscale_ssp_id=regscale_ssp_id,
        scan_date=scan_date,
        mappings_path=mappings_path,
        disable_mapping=disable_mapping,
        skip_rows=skip_rows,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        aws_profile=aws_profile,
        upload_file=upload_file,
    )


def import_qualys_scans(
    folder_path: os.PathLike[str],
    regscale_ssp_id: int,
    scan_date: datetime,
    mappings_path: os.PathLike[str],
    disable_mapping: bool,
    skip_rows: int,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    upload_file: Optional[bool] = True,
) -> None:
    """
    Import scans from Qualys

    :param os.PathLike[str] folder_path: File path to the folder containing Qualys .csv or .xlsx files to process to RegScale
    :param int regscale_ssp_id: The RegScale SSP ID
    :param datetime scan_date: The date of the scan
    :param os.PathLike[str] mappings_path: The path to the mappings file
    :param bool disable_mapping: Whether to disable custom mappings
    :param int skip_rows: The number of rows in the file to skip to get to the column headers
    :param str s3_bucket: The S3 bucket to download the files from
    :param str s3_prefix: The S3 prefix to download the files from
    :param str aws_profile: The AWS profile to use for S3 access
    :param Optional[bool] upload_file: Whether to upload the file to RegScale after processing, defaults to True
    :rtype: None
    """
    FlatFileImporter.import_files(
        import_type=Qualys,
        import_name="Qualys",
        file_types=[".csv", ".xlsx"],
        folder_path=folder_path,
        object_id=regscale_ssp_id,
        scan_date=scan_date,
        mappings_path=mappings_path,
        disable_mapping=disable_mapping,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        aws_profile=aws_profile,
        upload_file=upload_file,
        skip_rows=skip_rows,
    )


@qualys.command(name="import_policy_scans")
@FlatFileImporter.common_scanner_options(
    message="File path to the folder containing Qualys policy .csv files to process to RegScale.",
    prompt="File path for Qualys files",
    import_name="qualys_policy_scan",
)
@click.option(
    "--skip_rows",
    type=click.INT,
    help="The number of rows in the file to skip to get to the column headers, defaults to 5.",
    default=5,
)
def import_policy_scans(
    folder_path: os.PathLike[str],
    regscale_ssp_id: int,
    scan_date: datetime,
    mappings_path: Path,
    disable_mapping: bool,
    skip_rows: int,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    upload_file: bool,
):
    """
    Import Qualys policy scans from a CSV file into a RegScale Security Plan as assets and vulnerabilities.
    """
    process_files_with_importer(
        folder_path=str(folder_path),
        importer_class=QualysPolicyScansImporter,
        regscale_ssp_id=regscale_ssp_id,
        importer_args={
            "plan_id": regscale_ssp_id,
            "name": "QualysPolicyScan",
            "parent_id": regscale_ssp_id,
            "parent_module": "securityplans",
            "scan_date": scan_date,
        },
        mappings_path=str(mappings_path),
        disable_mapping=disable_mapping,
        skip_rows=skip_rows,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        aws_profile=aws_profile,
        upload_file=upload_file,
    )


@qualys.command(name="save_results")
@save_output_to()
@click.option(
    "--scan_id",
    type=click.STRING,
    help="Qualys scan reference ID to get results, defaults to all.",
    default="all",
)
def save_results(save_output_to: Path, scan_id: str):
    """Get scan results from Qualys using a scan ID or all scans and save them to a .json file."""
    save_scan_results_by_id(save_path=save_output_to, scan_id=scan_id)


def _resolve_plan_and_component(regscale_ssp_id: int = None, component_id: int = None):
    """
    Utility to resolve plan_id and is_component from regscale_ssp_id and component_id.
    Returns (plan_id, is_component)
    """
    if (regscale_ssp_id is None and component_id is None) or (regscale_ssp_id and component_id):
        error_and_exit("You must provide either --regscale_ssp_id or --component_id, but not both.")
    is_component = component_id is not None
    plan_id = component_id if is_component else regscale_ssp_id
    return plan_id, is_component


@qualys.command(name="sync_qualys")
@ssp_or_component_id(
    ssp_kwargs={"help": "The ID number from RegScale of the System Security Plan."},
    component_kwargs={"help": "The ID number from RegScale of the Component record to sync to."},
)
@click.option(
    "--create_issue",
    type=click.BOOL,
    required=False,
    help="Create Issue in RegScale from vulnerabilities in Qualys.",
    default=False,
)
@click.option(
    "--asset_group_id",
    type=click.INT,
    help="Filter assets from Qualys with an asset group ID.",
    default=None,
    cls=NotRequiredIf,
    not_required_if=["asset_group_name"],
)
@click.option(
    "--asset_group_name",
    type=click.STRING,
    help="Filter assets from Qualys with an asset group name.",
    default=None,
    cls=NotRequiredIf,
    not_required_if=["asset_group_id"],
)
@click.option(
    "--include-containers",
    is_flag=True,
    default=False,
    help="Include Container Security vulnerabilities in sync (uses gateway API endpoint).",
)
@click.option(
    "--include-was",
    is_flag=True,
    default=False,
    help="Include Web Application Scanning results in sync.",
)
@click.option(
    "--include-total-cloud",
    is_flag=True,
    default=False,
    help="Use Total Cloud API as primary data source (includes VMs and containers). Falls back to VMDR if Total Cloud fails or returns no data.",
)
@click.option(
    "--tc-include-tags",
    type=click.STRING,
    required=False,
    default=None,
    help="Include tags for Total Cloud (comma-separated tag names or IDs). Only used with --include-total-cloud.",
)
@click.option(
    "--tc-exclude-tags",
    type=click.STRING,
    required=False,
    default=None,
    help="Exclude tags for Total Cloud (comma-separated tag names or IDs). Requires --tc-include-tags. Only used with --include-total-cloud.",
)
def sync_qualys(
    regscale_ssp_id: int = None,
    component_id: int = None,
    create_issue: bool = False,
    asset_group_id: int = None,
    asset_group_name: str = None,
    include_containers: bool = False,
    include_was: bool = False,
    include_total_cloud: bool = False,
    tc_include_tags: str = None,
    tc_exclude_tags: str = None,
):
    """
    Query Qualys and sync assets & their associated vulnerabilities to a Security Plan or Component in RegScale.

    By default, syncs VMDR (Vulnerability Management) data only.
    Use --include-containers to also sync Container Security vulnerabilities.
    Use --include-was to also sync Web Application Scanning results.
    Use --include-total-cloud to use Total Cloud API as primary source (automatic VMDR fallback).
    """
    # Parameter validation
    if tc_exclude_tags and not tc_include_tags:
        logger.error("--tc-exclude-tags requires --tc-include-tags to be provided")
        return

    # Warn if asset groups used with Total Cloud (they're ignored for TC)
    if include_total_cloud and (asset_group_id or asset_group_name):
        logger.warning("Asset groups are not used with Total Cloud. Use --tc-include-tags / --tc-exclude-tags instead.")
        logger.warning("Asset group filters will be ignored for Total Cloud data.")

    plan_id, is_component = _resolve_plan_and_component(regscale_ssp_id, component_id)
    sync_qualys_to_regscale(
        plan_id=plan_id,
        create_issue=create_issue,
        asset_group_id=asset_group_id,
        asset_group_name=asset_group_name,
        is_component=is_component,
        include_containers=include_containers,
        include_was=include_was,
        include_total_cloud=include_total_cloud,
        tc_include_tags=tc_include_tags,
        tc_exclude_tags=tc_exclude_tags,
    )


@qualys.command(name="get_asset_groups")
@save_output_to()
def get_asset_groups(save_output_to: Path):
    """
    Get all asset groups from Qualys via API and save them to a .json file.
    """
    # see if user has enterprise license
    check_license()

    date = get_current_datetime("%Y%m%d")
    check_file_path(save_output_to)
    asset_groups = get_asset_groups_from_qualys()
    save_data_to(
        file=Path(f"{save_output_to}/qualys_asset_groups_{date}.json"),
        data=asset_groups,
    )


@qualys.command(name="import_container_scans")
@FlatFileImporter.common_scanner_options(
    message="File path to the folder containing container .csv files to process to RegScale.",
    prompt="File path for Qualys files",
    import_name="qualys_container_scan",
)
@click.option(
    "--skip_rows",
    type=click.INT,
    help="The number of rows in the file to skip to get to the column headers, defaults to 5.",
    default=5,
)
def import_container_scans(
    folder_path: os.PathLike[str],
    regscale_ssp_id: int,
    scan_date: datetime,
    mappings_path: Path,
    disable_mapping: bool,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    upload_file: bool,
    skip_rows: int,
):
    """
    Import Qualys container scans from a CSV file into a RegScale Security Plan as assets and vulnerabilities.
    """
    process_files_with_importer(
        folder_path=str(folder_path),
        importer_class=QualysContainerScansImporter,
        regscale_ssp_id=regscale_ssp_id,
        importer_args={
            "plan_id": regscale_ssp_id,
            "name": "QualysContainerScan",
            "parent_id": regscale_ssp_id,
            "parent_module": "securityplans",
            "scan_date": scan_date,
        },
        mappings_path=str(mappings_path),
        disable_mapping=disable_mapping,
        skip_rows=skip_rows,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        aws_profile=aws_profile,
        upload_file=upload_file,
    )


@qualys.command(name="import_was_scans")
@FlatFileImporter.common_scanner_options(
    message="File path to the folder containing was .csv files to process to RegScale.",
    prompt="File path for Qualys files",
    import_name="qualys_was_scan",
)
@click.option(
    "--skip_rows",
    type=click.INT,
    help="The number of rows in the file to skip to get to the column headers, defaults to 5.",
    default=5,
)
def import_was_scans(
    folder_path: os.PathLike[str],
    regscale_ssp_id: int,
    scan_date: datetime,
    mappings_path: Path,
    disable_mapping: bool,
    skip_rows: int,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    upload_file: bool,
):
    """
    Import Qualys was scans from a CSV file into a RegScale Security Plan as assets and vulnerabilities.
    """
    process_files_with_importer(
        folder_path=str(folder_path),
        importer_class=QualysWasScansImporter,
        regscale_ssp_id=regscale_ssp_id,
        importer_args={
            "plan_id": regscale_ssp_id,
            "name": "QualysWASScan",
            "parent_id": regscale_ssp_id,
            "parent_module": "securityplans",
            "scan_date": scan_date,
        },
        mappings_path=str(mappings_path),
        disable_mapping=disable_mapping,
        skip_rows=skip_rows,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        aws_profile=aws_profile,
        upload_file=upload_file,
    )


def process_files_with_importer(
    regscale_ssp_id: int,
    folder_path: str,
    importer_class,
    importer_args: dict,
    s3_bucket: str,
    s3_prefix: str,
    aws_profile: str,
    mappings_path: str = None,
    disable_mapping: bool = False,
    skip_rows: int = 0,
    scan_date: datetime = None,
    upload_file: Optional[bool] = True,
):
    """
    Process files in a folder using a specified importer class.

    :param int regscale_ssp_id: ID of the RegScale Security Plan to import the data into.
    :param str folder_path: Path to the folder containing files.
    :param Any importer_class: The importer class to instantiate for processing.
    :param dict importer_args: Additional arguments to pass to the importer class.
    :param str s3_bucket: S3 bucket to download the files from.
    :param str s3_prefix: S3 prefix to download the files from.
    :param str aws_profile: AWS profile to use for S3 access.
    :param str mappings_path: Path to mapping configurations.
    :param bool disable_mapping: Flag to disable mappings.
    :param int skip_rows: Number of rows to skip in files.
    :param scan_date: Date of the scan. Defaults to current datetime if not provided.
    :param Optional[bool] upload_file: Whether to upload the file to RegScale after processing, defaults to True.
    """
    import csv

    from openpyxl import Workbook

    if s3_bucket:
        download_from_s3(s3_bucket, s3_prefix, folder_path, aws_profile)

    files_lst = list(Path(folder_path).glob("*.csv"))

    # If no files are found in the folder, return a warning
    if len(files_lst) == 0:
        logger.warning("No Qualys files found in the folder path provided.")
        return

    if not scan_date:
        scan_date = datetime.now(timezone.utc)

    for file in files_lst:
        try:
            original_file_name = str(file)
            xlsx_file = (
                f"{file.name}.xlsx" if not file.name.endswith(".csv") else str(file.name).replace(".csv", ".xlsx")
            )

            # Convert CSV to XLSX
            wb = Workbook()
            ws = wb.active
            with open(file, "r") as f:
                for row in csv.reader(f):
                    ws.append(row)

            # Save the Excel file
            full_file_path = Path(f"{file.parent}/{xlsx_file}")
            wb.save(full_file_path)

            # Initialize and use the importer
            importer = importer_class(
                plan_id=regscale_ssp_id,
                name=importer_args.get("name", "QualysFileScan"),
                file_path=str(full_file_path),
                parent_id=regscale_ssp_id,
                parent_module=importer_args.get("parent_module", "securityplans"),
                scan_date=scan_date,
                mappings_path=mappings_path,
                disable_mapping=disable_mapping,
                skip_rows=skip_rows,
                upload_file=upload_file,
            )
            importer.clean_up(file_path=original_file_name)
        except Exception as e:
            error_message = traceback.format_exc()
            logger.error(f"Failed to process file {file}: {error_message}\n{e}")
            continue


def export_scans(
    save_path: Path,
    days: int = 30,
    export: bool = True,
) -> None:
    """
    Function to export scans from Qualys that were completed in the last x days, defaults to 30

    :param Path save_path: Path to save the scans to as a .json file
    :param int days: # of days of completed scans to export, defaults to 30 days
    :param bool export: Whether to save the scan data as a .json, defaults to True
    :rtype: None
    """
    # see if user has enterprise license
    check_license()
    date = get_current_datetime("%Y%m%d")
    results = get_detailed_scans(days)
    if export:
        check_file_path(save_path)
        save_data_to(
            file=Path(f"{save_path.name}/qualys_scans_{date}.json"),
            data=results,
        )
    else:
        pprint.pprint(results, indent=4)


def save_scan_results_by_id(save_path: Path, scan_id: str) -> None:
    """
    Function to save the queries from Qualys using an ID a .json file

    :param Path save_path: Path to save the scan results to as a .json file
    :param str scan_id: Qualys scan ID to get the results for
    :rtype: None
    """
    # see if user has enterprise license
    check_license()

    check_file_path(save_path)
    with job_progress:
        if scan_id.lower() == "all":
            # get all the scan results from Qualys
            scans = get_scans_summary("all")

            # add task to job progress to let user know # of scans to fetch
            task1 = job_progress.add_task(
                f"[#f8b737]Getting scan results for {len(scans['SCAN'])} scan(s)...",
                total=len(scans["SCAN"]),
            )
            # get the scan results from Qualys
            scan_data = get_scan_results(scans, task1)
        else:
            task1 = job_progress.add_task(f"[#f8b737]Getting scan results for {scan_id}...", total=1)
            # get the scan result for the provided scan id
            scan_data = get_scan_results(scan_id, task1)
    # save the scan_data as the provided file_path
    save_data_to(file=save_path, data=scan_data)


def sync_qualys_to_regscale(
    plan_id: int,
    create_issue: bool = False,
    asset_group_id: int = None,
    asset_group_name: str = None,
    is_component: bool = False,
    include_containers: bool = False,
    include_was: bool = False,
    include_total_cloud: bool = False,
    tc_include_tags: str = None,
    tc_exclude_tags: str = None,
) -> None:
    """
    Sync Qualys assets and vulnerabilities to a security plan or component in RegScale

    :param int plan_id: ID # of the SSP or Component in RegScale
    :param bool create_issue: Flag whether to create an issue in RegScale from Qualys vulnerabilities, defaults to False
    :param int asset_group_id: Optional filter for assets in Qualys with an asset group ID, defaults to None
    :param str asset_group_name: Optional filter for assets in Qualys with an asset group name, defaults to None
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :param bool include_containers: Whether to include Container Security vulnerabilities, defaults to False
    :param bool include_was: Whether to include Web Application Scanning results, defaults to False
    :param bool include_total_cloud: Whether to use Total Cloud API as primary source, defaults to False
    :param str tc_include_tags: Tags to include for Total Cloud (comma-separated), defaults to None
    :param str tc_exclude_tags: Tags to exclude for Total Cloud (comma-separated), defaults to None
    :rtype: None
    """
    # see if user has enterprise license
    check_license()

    # Route to Total Cloud if flag is set
    if include_total_cloud:
        sync_qualys_with_total_cloud(
            plan_id=plan_id,
            create_issue=create_issue,
            is_component=is_component,
            include_was=include_was,
            tc_include_tags=tc_include_tags,
            tc_exclude_tags=tc_exclude_tags,
        )
    # Otherwise use VMDR routing
    elif asset_group_id:
        sync_qualys_assets_and_vulns(
            ssp_id=plan_id,
            create_issue=create_issue,
            asset_group_filter=asset_group_name,
            is_component=is_component,
            include_containers=include_containers,
            include_was=include_was,
        )
    elif asset_group_name:
        sync_qualys_assets_and_vulns(
            ssp_id=plan_id,
            create_issue=create_issue,
            asset_group_filter=asset_group_id,
            is_component=is_component,
            include_containers=include_containers,
            include_was=include_was,
        )
    else:
        sync_qualys_assets_and_vulns(
            ssp_id=plan_id,
            create_issue=create_issue,
            is_component=is_component,
            include_containers=include_containers,
            include_was=include_was,
        )


def _process_and_sync_issues(
    plan_id: int,
    is_component: bool,
    use_total_cloud: bool,
    deduplicated_assets: list,
    was_vulnerabilities: list,
) -> int:
    """
    Process and sync issues from Total Cloud, VMDR, and WAS to RegScale.

    :param int plan_id: RegScale plan ID
    :param bool is_component: Whether this is a component
    :param bool use_total_cloud: Whether Total Cloud was used as data source
    :param list deduplicated_assets: Deduplicated assets from all sources
    :param list was_vulnerabilities: WAS vulnerability data
    :return: Number of issues synced
    :rtype: int
    """
    from regscale.integrations.commercial.qualys.total_cloud_helpers import convert_total_cloud_to_issues
    from regscale.integrations.commercial.qualys.was_helpers import convert_was_vulns_to_issues

    all_issues = []

    if use_total_cloud:
        # Convert Total Cloud detections to issues
        logger.info("Converting Total Cloud detections to issues...")
        tc_issues = convert_total_cloud_to_issues(deduplicated_assets)
        logger.info("Converted %s Total Cloud issues", len(tc_issues))
        all_issues.extend(tc_issues)

    if not use_total_cloud:
        # Fallback: Get VMDR issue data with KB enrichment
        vmdr_assets_for_issues = [asset for asset in deduplicated_assets if isinstance(asset, dict)]

        logger.info("Getting vulnerabilities for %s asset(s) from Qualys...", len(vmdr_assets_for_issues))
        if vmdr_assets_for_issues:
            qualys_assets_and_issues, total_vuln_count = get_issue_data_for_assets(vmdr_assets_for_issues)
            logger.info("Received %s vulnerabilities from Qualys.", total_vuln_count)
            all_issues.extend(qualys_assets_and_issues)

    # Process WAS vulnerabilities to issues
    if was_vulnerabilities:
        logger.info("Converting WAS vulnerabilities to issues...")
        was_issues = convert_was_vulns_to_issues(was_vulnerabilities)
        logger.info("Converted %s WAS issues", len(was_issues))
        all_issues.extend(was_issues)

    # Sync all issues to RegScale
    if all_issues:
        logger.info("Syncing %s issues to RegScale...", len(all_issues))
        sync_issues(
            ssp_id=plan_id,
            qualys_assets_and_issues=all_issues,
            is_component=is_component,
        )
    else:
        logger.warning("No issues to sync.")

    return len(all_issues)


def _fetch_and_validate_total_cloud_data(tc_include_tags: str, tc_exclude_tags: str) -> tuple:
    """
    Fetch and validate Total Cloud data.

    :param str tc_include_tags: Tags to include for Total Cloud
    :param str tc_exclude_tags: Tags to exclude for Total Cloud
    :return: Tuple of (use_total_cloud, tc_xml_data, tc_containers)
    :rtype: tuple
    """
    from regscale.integrations.commercial.qualys.total_cloud_helpers import (
        fetch_total_cloud_containers,
        fetch_total_cloud_data,
        validate_total_cloud_data,
    )

    logger.info("=" * 60)
    logger.info("STEP 1: Fetching Total Cloud data...")
    logger.info("=" * 60)

    tc_xml_data = fetch_total_cloud_data(tc_include_tags, tc_exclude_tags)
    tc_containers = fetch_total_cloud_containers(tc_include_tags, tc_exclude_tags)

    logger.info("=" * 60)
    logger.info("STEP 2: Validating Total Cloud data...")
    logger.info("=" * 60)

    use_total_cloud = False
    if tc_xml_data and validate_total_cloud_data(tc_xml_data, tc_containers):
        logger.info("[OK] Total Cloud data is valid. Using Total Cloud as primary source.")
        use_total_cloud = True
    else:
        logger.warning("[WARN] Total Cloud data is invalid or empty. Falling back to VMDR.")
        logger.info("Fallback strategy: VMDR + standalone containers")

    return use_total_cloud, tc_xml_data, tc_containers


def _get_qualys_assets_with_strategy(use_total_cloud: bool, tc_xml_data, tc_containers) -> list:
    """
    Get Qualys assets using either Total Cloud or VMDR fallback strategy.

    :param bool use_total_cloud: Whether to use Total Cloud data
    :param tc_xml_data: Total Cloud XML data
    :param tc_containers: Total Cloud container data
    :return: List of Qualys assets
    :rtype: list
    """
    logger.info("=" * 60)
    if use_total_cloud:
        logger.info("STEP 3: Processing Total Cloud data...")
    else:
        logger.info("STEP 3: Processing VMDR data (fallback mode)...")
    logger.info("=" * 60)

    qualys_assets = []

    if use_total_cloud:
        qualys_assets = extract_total_cloud_assets(tc_xml_data, tc_containers)
        logger.info("Total Cloud provided %s assets (VMs + containers)", len(qualys_assets))
    else:
        vmdr_assets = get_qualys_assets_and_scan_results()
        if vmdr_assets:
            logger.info("VMDR provided %s VM assets", len(vmdr_assets))
            qualys_assets.extend(vmdr_assets)
        else:
            logger.warning("No VMDR assets found")

        if tc_containers:
            container_assets = _extract_container_assets_from_tc(tc_containers)
            logger.info("Fetched %s standalone container assets", len(container_assets))
            qualys_assets.extend(container_assets)

    if not qualys_assets:
        error_and_exit("No assets found from Total Cloud or VMDR fallback.")

    return qualys_assets


def sync_qualys_with_total_cloud(
    plan_id: int,
    create_issue: bool = False,
    is_component: bool = False,
    include_was: bool = False,
    tc_include_tags: str = None,
    tc_exclude_tags: str = None,
) -> None:
    """
    Sync Qualys assets and vulnerabilities using Total Cloud API as primary source with automatic VMDR fallback.

    This function attempts to use Total Cloud API first, which provides comprehensive VM and container data.
    If Total Cloud fails or returns no data, it automatically falls back to VMDR + standalone containers.

    :param int plan_id: ID # of the SSP or Component in RegScale
    :param bool create_issue: Flag whether to create an issue in RegScale from Qualys vulnerabilities
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :param bool include_was: Whether to include Web Application Scanning results
    :param str tc_include_tags: Tags to include for Total Cloud (comma-separated)
    :param str tc_exclude_tags: Tags to exclude for Total Cloud (comma-separated)
    :rtype: None
    """
    from regscale.integrations.commercial.qualys.total_cloud_helpers import deduplicate_service_data

    config = _get_config()
    parent_module = "components" if is_component else "securityplans"

    logger.info("Getting assets from RegScale for %s #%s...", parent_module, plan_id)
    reg_assets = Asset.get_all_by_search(search=Search(parentID=plan_id, module=parent_module))
    logger.info(
        "Located %s asset(s) associated with %s #%s in RegScale.",
        len(reg_assets),
        parent_module,
        plan_id,
    )
    logger.debug(reg_assets)

    # STEP 1-2: Fetch and validate Total Cloud data
    use_total_cloud, tc_xml_data, tc_containers = _fetch_and_validate_total_cloud_data(tc_include_tags, tc_exclude_tags)

    # STEP 3: Get assets using chosen strategy
    qualys_assets = _get_qualys_assets_with_strategy(use_total_cloud, tc_xml_data, tc_containers)

    # STEP 4: Fetch WAS data if requested
    was_vulnerabilities = []
    if include_was:
        logger.info("=" * 60)
        logger.info("STEP 4: Fetching WAS data...")
        logger.info("=" * 60)
        was_vulnerabilities = fetch_was_vulnerabilities()
        logger.info("Fetched %s WAS vulnerabilities", len(was_vulnerabilities))

        if was_vulnerabilities:
            was_assets = extract_was_assets(was_vulnerabilities)
            logger.info("Extracted %s WAS assets", len(was_assets))
            qualys_assets.extend(was_assets)

    # STEP 5: Deduplicate assets across services
    logger.info("=" * 60)
    logger.info("STEP 5: Deduplicating assets...")
    logger.info("=" * 60)
    logger.info("Total assets before deduplication: %s", len(qualys_assets))

    deduplicated_assets = deduplicate_service_data(qualys_assets, [], [])
    logger.info("Total assets after deduplication: %s", len(deduplicated_assets))

    # STEP 6: Sync assets to RegScale
    logger.info("=" * 60)
    logger.info("STEP 6: Syncing assets to RegScale...")
    logger.info("=" * 60)

    sync_assets(
        qualys_assets=deduplicated_assets,
        reg_assets=reg_assets,
        ssp_id=plan_id,
        config=config,
        is_component=is_component,
    )

    # STEP 7: Process and sync issues if requested
    issues_count = 0
    if create_issue:
        logger.info("=" * 60)
        logger.info("STEP 7: Processing and syncing issues...")
        logger.info("=" * 60)
        issues_count = _process_and_sync_issues(
            plan_id=plan_id,
            is_component=is_component,
            use_total_cloud=use_total_cloud,
            deduplicated_assets=deduplicated_assets,
            was_vulnerabilities=was_vulnerabilities,
        )

    # Final Summary
    logger.info("=" * 60)
    logger.info("Sync Summary:")
    logger.info("  Source: %s", "Total Cloud" if use_total_cloud else "VMDR")
    logger.info("  Assets synced: %s", len(deduplicated_assets))
    if create_issue:
        logger.info("  Issues synced: %s", issues_count)
    logger.info("=" * 60)


def _fetch_container_data(asset_group_filter: Optional[Union[int, str]]) -> list:
    """
    Fetch container vulnerability data from Qualys Container Security.

    :param Optional[Union[int, str]] asset_group_filter: Asset group filter to apply
    :return: List of container vulnerabilities
    :rtype: list
    """
    container_vulnerabilities = []
    logger.info("Fetching Container Security data from Qualys...")
    try:
        from regscale.integrations.commercial.qualys.containers import fetch_all_vulnerabilities

        params = {}
        if asset_group_filter:
            params["filter"] = f"tags:{asset_group_filter}"

        container_vulnerabilities = fetch_all_vulnerabilities(filters=params)
        logger.info("Received %s container vulnerabilities from Qualys.", len(container_vulnerabilities))
    except Exception as e:
        logger.error(f"Error fetching Container Security data: {e}")
        logger.debug(traceback.format_exc())

    return container_vulnerabilities


def _fetch_was_data() -> list:
    """
    Fetch WAS vulnerability data from Qualys Web Application Scanning.

    :return: List of WAS vulnerabilities
    :rtype: list
    """
    was_vulnerabilities = []
    logger.info("Fetching WAS (Web Application Scanning) data from Qualys...")
    try:
        from regscale.integrations.commercial.qualys.was import fetch_all_was_vulnerabilities

        was_vulnerabilities = fetch_all_was_vulnerabilities()
        logger.info("Received %s web applications with vulnerabilities from Qualys WAS", len(was_vulnerabilities))
    except Exception as e:
        logger.error("Failed to fetch WAS data: %s", e)
        logger.debug(traceback.format_exc())

    return was_vulnerabilities


def _merge_additional_assets(qualys_assets: list, container_vulnerabilities: list, was_vulnerabilities: list) -> list:
    """
    Merge container and WAS assets with VMDR assets.

    :param list qualys_assets: VMDR assets
    :param list container_vulnerabilities: Container vulnerabilities
    :param list was_vulnerabilities: WAS vulnerabilities
    :return: Merged list of all assets
    :rtype: list
    """
    if container_vulnerabilities:
        container_assets = _extract_container_assets(container_vulnerabilities)
        logger.info(
            "Extracted %s container assets, merging with %s VMDR assets...", len(container_assets), len(qualys_assets)
        )
        qualys_assets.extend(container_assets)
        logger.info("Total assets to sync: %s", len(qualys_assets))

    if was_vulnerabilities:
        was_assets = extract_was_assets(was_vulnerabilities)
        logger.info("Extracted %s WAS assets, merging with %s existing assets...", len(was_assets), len(qualys_assets))
        qualys_assets.extend(was_assets)
        logger.info("Total assets to sync: %s", len(qualys_assets))

    return qualys_assets


def _process_vmdr_and_additional_issues(
    qualys_assets: list,
    container_vulnerabilities: list,
    was_vulnerabilities: list,
    ssp_id: int,
    is_component: bool,
) -> None:
    """
    Process and sync VMDR, container, and WAS issues through unified KB enrichment.

    Container and WAS assets now have DETECTION_LIST populated, so they can be processed
    through the same get_issue_data_for_assets() path as VMDR for consistent KB enrichment.

    :param list qualys_assets: All Qualys assets
    :param list container_vulnerabilities: Container vulnerabilities
    :param list was_vulnerabilities: WAS vulnerabilities
    :param int ssp_id: RegScale SSP/Component ID
    :param bool is_component: Whether this is a component sync
    :rtype: None
    """
    all_assets_for_issues = []

    # Collect VMDR assets (already have DETECTION_LIST populated from XML parsing)
    vmdr_assets = [asset for asset in qualys_assets if asset.get("TRACKING_METHOD") not in ("Container", "WAS")]
    if vmdr_assets:
        logger.info("Adding %s VMDR asset(s) for KB enrichment...", len(vmdr_assets))
        all_assets_for_issues.extend(vmdr_assets)

    # Collect Container Security assets (now have DETECTION_LIST populated)
    if container_vulnerabilities:
        logger.info(
            "Extracting container assets with DETECTION_LIST from %s containers...", len(container_vulnerabilities)
        )
        container_assets = _extract_container_assets(container_vulnerabilities)
        # Filter for assets that have detections
        container_assets_with_detections = [
            asset for asset in container_assets if asset.get("DETECTION_LIST", {}).get("DETECTION")
        ]
        logger.info(
            "Adding %s container assets with detections for KB enrichment...", len(container_assets_with_detections)
        )
        all_assets_for_issues.extend(container_assets_with_detections)

    # Collect WAS assets (now have DETECTION_LIST populated)
    if was_vulnerabilities:
        logger.info("Extracting WAS assets with DETECTION_LIST from %s webapps...", len(was_vulnerabilities))
        was_assets = extract_was_assets(was_vulnerabilities)
        # Filter for assets that have detections
        was_assets_with_detections = [asset for asset in was_assets if asset.get("DETECTION_LIST", {}).get("DETECTION")]
        logger.info("Adding %s WAS assets with detections for KB enrichment...", len(was_assets_with_detections))
        all_assets_for_issues.extend(was_assets_with_detections)

    # Process ALL assets (VMDR + Container + WAS) through unified KB enrichment
    if all_assets_for_issues:
        logger.info("Processing %s total assets through unified KB enrichment...", len(all_assets_for_issues))
        qualys_assets_and_issues, total_vuln_count = get_issue_data_for_assets(all_assets_for_issues)
        logger.info("Received %s vulnerabilities from Qualys KB enrichment.", total_vuln_count)
        logger.debug(qualys_assets_and_issues)

        sync_issues(
            ssp_id=ssp_id,
            qualys_assets_and_issues=qualys_assets_and_issues,
            is_component=is_component,
        )
    else:
        logger.warning("No assets with detections found for issue processing.")


def sync_qualys_assets_and_vulns(
    ssp_id: int,
    create_issue: bool = False,
    asset_group_filter: Optional[Union[int, str]] = None,
    is_component: bool = False,
    include_containers: bool = False,
    include_was: bool = False,
) -> None:
    """
    Function to query Qualys and sync assets & associated vulnerabilities to RegScale (Security Plan or Component)

    :param int ssp_id: RegScale System Security Plan or Component ID
    :param bool create_issue: Flag to create an issue in RegScale for each vulnerability from Qualys
    :param Optional[Union[int, str]] asset_group_filter: Filter the Qualys assets by an asset group ID or name, if any
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :param bool include_containers: Whether to include Container Security vulnerabilities, defaults to False
    :param bool include_was: Whether to include Web Application Scanning results, defaults to False
    :rtype: None
    """
    config = _get_config()
    parent_module = "components" if is_component else "securityplans"

    logger.info(f"Getting assets from RegScale for {parent_module} #{ssp_id}...")
    reg_assets = Asset.get_all_by_search(search=Search(parentID=ssp_id, module=parent_module))
    logger.info(
        "Located %s asset(s) associated with %s #%s in RegScale.",
        len(reg_assets),
        parent_module,
        ssp_id,
    )
    logger.debug(reg_assets)

    # Get VMDR assets and scan results
    if qualys_assets := get_qualys_assets_and_scan_results(asset_group_filter=asset_group_filter):
        logger.info("Received %s VMDR assets from Qualys.", len(qualys_assets))
        logger.debug(qualys_assets)
    else:
        error_and_exit("No assets found in Qualys.")

    # Fetch additional data sources if requested
    container_vulnerabilities = _fetch_container_data(asset_group_filter) if include_containers else []
    was_vulnerabilities = _fetch_was_data() if include_was else []

    # Merge all assets
    qualys_assets = _merge_additional_assets(qualys_assets, container_vulnerabilities, was_vulnerabilities)

    # Sync all assets to RegScale
    sync_assets(
        qualys_assets=qualys_assets,
        reg_assets=reg_assets,
        ssp_id=ssp_id,
        config=config,
        is_component=is_component,
    )

    # Process and sync issues if requested
    if create_issue:
        _process_vmdr_and_additional_issues(
            qualys_assets, container_vulnerabilities, was_vulnerabilities, ssp_id, is_component
        )


def sync_assets(
    qualys_assets: list[dict], reg_assets: list[Asset], ssp_id: int, config: dict, is_component: bool = False
) -> None:
    """
    Function to sync Qualys assets to RegScale (Security Plan or Component)

    :param list[dict] qualys_assets: List of Qualys assets
    :param list[Asset] reg_assets: List of RegScale assets
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param dict config: Configuration dictionary
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :rtype: None
    """
    parent_module = "components" if is_component else "securityplans"
    update_assets = []

    for qualys_asset in qualys_assets:
        processed_asset = _process_single_qualys_asset(qualys_asset, reg_assets, ssp_id, parent_module)
        if processed_asset:
            update_assets.append(processed_asset)

    update_and_insert_assets(
        qualys_assets=qualys_assets,
        reg_assets=reg_assets,
        ssp_id=ssp_id,
        config=config,
        update_assets=update_assets,
        is_component=is_component,
    )


def _process_single_qualys_asset(
    qualys_asset: dict, reg_assets: list[Asset], ssp_id: int, parent_module: str
) -> Optional[Asset]:
    """
    Process a single Qualys asset and return the updated RegScale asset if found.

    :param dict qualys_asset: Single Qualys asset dictionary
    :param list[Asset] reg_assets: List of RegScale assets
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param str parent_module: Parent module name
    :return: Updated RegScale asset or None if not found
    :rtype: Optional[Asset]
    """
    logger.debug("qualys_asset: %s", qualys_asset)

    if not isinstance(qualys_asset, dict):
        logger.error("Expected dict, got %s: %s", type(qualys_asset), qualys_asset)
        return None

    # Defensive access: support both ASSET_ID (Total Cloud) and ID (legacy VMDR) fields
    asset_id = qualys_asset.get("ASSET_ID") or qualys_asset.get("ID")
    if not asset_id:
        logger.error("Asset missing both ASSET_ID and ID keys. Keys present: %s", list(qualys_asset.keys()))
        logger.debug("Full asset data: %s", qualys_asset)
        return None

    lookup_assets = lookup_asset(reg_assets, asset_id)
    if not lookup_assets:
        return None

    return _update_regscale_asset(lookup_assets[0], qualys_asset, ssp_id, parent_module)


def _update_regscale_asset(asset: Asset, qualys_asset: dict, ssp_id: int, parent_module: str) -> Optional[Asset]:
    """
    Update a RegScale asset with Qualys asset data.

    :param Asset asset: RegScale asset to update
    :param dict qualys_asset: Qualys asset data
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param str parent_module: Parent module name
    :return: Updated asset or None if update failed
    :rtype: Optional[Asset]
    """
    try:
        asset.parentId = ssp_id
        asset.parentModule = parent_module
        # Support both VMDR format (has ID field) and Total Cloud format (ASSET_ID only)
        # ID is the host UUID, ASSET_ID is the Qualys asset ID - use whichever is available
        asset.otherTrackingNumber = qualys_asset.get("ID") or qualys_asset.get("ASSET_ID") or ""
        asset.ipAddress = qualys_asset.get("IP") or ""
        asset.qualysId = qualys_asset.get("ASSET_ID") or ""

        assert asset.id
        return asset
    except AssertionError as aex:
        logger.error("Asset does not have an id, unable to update!\n%s", aex)
        return None


def update_and_insert_assets(
    qualys_assets: list[dict],
    reg_assets: list[Asset],
    ssp_id: int,
    config: dict,
    update_assets: list[Asset],
    is_component: bool = False,
) -> None:
    """
    Function to update and insert Qualys assets into RegScale (Security Plan or Component)

    :param list[dict] qualys_assets: List of Qualys assets as dictionaries
    :param list[Asset] reg_assets: List of RegScale assets
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param dict config: RegScale CLI Configuration dictionary
    :param list[Asset] update_assets: List of assets to update in RegScale
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :rtype: None
    """
    parent_module = "components" if is_component else "securityplans"

    # Handle asset insertion
    insert_assets = _prepare_assets_for_insertion(qualys_assets, reg_assets, ssp_id, parent_module, config)
    if insert_assets:
        _create_assets_in_batch(insert_assets)

    # Handle asset updates
    if update_assets:
        _update_assets_in_batch(update_assets)


def _prepare_assets_for_insertion(
    qualys_assets: list[dict], reg_assets: list[Asset], ssp_id: int, parent_module: str, config: dict
) -> list[Asset]:
    """
    Prepare new assets for insertion into RegScale.

    :param list[dict] qualys_assets: List of Qualys assets
    :param list[Asset] reg_assets: List of RegScale assets
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param str parent_module: Parent module name
    :param dict config: Configuration dictionary
    :return: List of assets to insert
    :rtype: list[Asset]
    """
    # Call inner_join ONCE to get existing assets (avoid repeated calls in list comprehension)
    existing_assets = inner_join(reg_assets, qualys_assets)
    existing_asset_ids = {asset["ASSET_ID"] for asset in existing_assets}

    assets_to_be_inserted = [
        qualys_asset for qualys_asset in qualys_assets if qualys_asset["ASSET_ID"] not in existing_asset_ids
    ]

    insert_assets = []
    for qualys_asset in assets_to_be_inserted:
        r_asset = _create_regscale_asset_from_qualys(qualys_asset, ssp_id, parent_module, config)
        # avoid duplication
        if r_asset.qualysId not in {v["qualysId"] for v in insert_assets}:
            insert_assets.append(r_asset)

    return insert_assets


def _create_regscale_asset_from_qualys(qualys_asset: dict, ssp_id: int, parent_module: str, config: dict) -> Asset:
    """
    Create a RegScale asset from Qualys asset data.

    :param dict qualys_asset: Qualys asset data
    :param int ssp_id: RegScale System Security Plan or Component ID
    :param str parent_module: Parent module name
    :param dict config: Configuration dictionary
    :return: New RegScale asset
    :rtype: Asset
    """
    # Determine asset type from tracking method
    tracking_method = qualys_asset.get("TRACKING_METHOD", "VMDR")

    # Create unique pluginId based on source module to prevent duplication
    # and allow users to identify the source in asset lists
    if tracking_method == "CONTAINER":
        plugin_id = f"Qualys_Container_{qualys_asset['ASSET_ID']}"
        source_label = "Container"
    elif tracking_method == "TOTALCLOUD":
        plugin_id = f"Qualys_TotalCloud_{qualys_asset['ASSET_ID']}"
        source_label = "Total Cloud"
    elif tracking_method == "WAS":
        plugin_id = f"Qualys_WAS_{qualys_asset['ASSET_ID']}"
        source_label = "WAS"
    else:
        plugin_id = f"Qualys_VMDR_{qualys_asset['ASSET_ID']}"
        source_label = "VMDR"

    return Asset(
        name=f'Qualys {source_label} #{qualys_asset.get("ASSET_ID")} IP: {qualys_asset.get("IP")}',
        otherTrackingNumber=qualys_asset.get("ASSET_ID"),  # Use normalized ASSET_ID instead of raw ID
        parentId=ssp_id,
        parentModule=parent_module,
        ipAddress=qualys_asset.get("IP"),
        assetOwnerId=config["userId"],
        assetType="Other",
        assetCategory=regscale_models.AssetCategory.Hardware,
        status="Off-Network",
        qualysId=qualys_asset.get("ASSET_ID"),  # Legacy field for backward compatibility
        pluginId=plugin_id,  # Modern deduplication field - now includes source module
    )


def _create_assets_in_batch(insert_assets: list[Asset]) -> None:
    """
    Create assets in batch and handle any errors.

    :param list[Asset] insert_assets: List of assets to create
    :rtype: None
    """
    try:
        created_assets = Asset.batch_create(insert_assets, job_progress)
        logger.info(
            "RegScale Asset(s) successfully created: %i/%i",
            len(created_assets),
            len(insert_assets),
        )
    except requests.exceptions.RequestException as rex:
        logger.error("Unable to create Qualys Assets in RegScale\n%s", rex)


def _update_assets_in_batch(update_assets: list[Asset]) -> None:
    """
    Update assets in batch and handle any errors.

    :param list[Asset] update_assets: List of assets to update
    :rtype: None
    """
    try:
        updated_assets = Asset.batch_update(update_assets, job_progress)
        logger.info(
            "RegScale Asset(s) successfully updated: %i/%i",
            len(updated_assets),
            len(update_assets),
        )
    except requests.RequestException as rex:
        logger.error("Unable to Update Qualys Assets to RegScale\n%s", rex)


def sync_issues(ssp_id: int, qualys_assets_and_issues: list[dict], is_component: bool = False) -> None:
    """
    Function to sync Qualys issues to RegScale (Security Plan or Component)

    :param int ssp_id: RegScale System Security Plan or Component ID
    :param list[dict] qualys_assets_and_issues: List of Qualys assets and their issues
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :rtype: None
    """
    parent_module = "components" if is_component else "securityplans"
    update_issues = []
    insert_issues = []
    vuln_count = 0
    ssp_assets = Asset.get_all_by_parent(parent_id=ssp_id, parent_module=parent_module)
    for asset in qualys_assets_and_issues:
        # Skip assets without issues
        if "ISSUES" not in asset or not asset["ISSUES"]:
            continue

        # Create issues in RegScale from Qualys vulnerabilities
        regscale_issue_updates, regscale_new_issues = create_regscale_issue_from_vuln(
            regscale_ssp_id=ssp_id,
            qualys_asset=asset,
            regscale_assets=ssp_assets,
            vulns=asset["ISSUES"],
            is_component=is_component,
        )
        update_issues.extend(regscale_issue_updates)
        insert_issues.extend(regscale_new_issues)
        vuln_count += len(asset.get("ISSUES", []))
    if insert_issues:
        deduped_vulns = combine_duplicate_qualys_vulns(insert_issues)
        logger.info(
            "Creating %i new issue(s) in RegScale, condensed from %i Qualys vulnerabilities.",
            len(deduped_vulns),
            vuln_count,
        )
        created_issues = Issue.batch_create(deduped_vulns, job_progress)
        logger.info(
            "RegScale Issue(s) successfully created: %i/%i",
            len(created_issues),
            len(deduped_vulns),
        )
    if update_issues:
        deduped_vulns = combine_duplicate_qualys_vulns(update_issues)
        logger.info(
            "Updating %i existing issue(s) in RegScale, condensed from %i Qualys vulnerabilities.",
            len(deduped_vulns),
            vuln_count,
        )
        updated_issues = Issue.batch_update(deduped_vulns, job_progress)
        logger.info("RegScale Issue(s) successfully updated: %i/%i", len(updated_issues), len(deduped_vulns))


def combine_duplicate_qualys_vulns(qualys_vulns: list[Issue]) -> list:
    """
    Function to combine duplicate Qualys vulnerabilities

    :param list[Issue] qualys_vulns: List of Qualys vulnerabilities as RegScale issues
    :return: List of Qualys vulnerabilities with duplicates combined
    :rtype: list
    """
    with job_progress:
        logger.info("Combining duplicate Qualys vulnerabilities found across multiple assets...")
        deduping_task = job_progress.add_task(
            f"Combining {len(qualys_vulns)} Qualys vulnerabilities...",
            total=len(qualys_vulns),
        )
        combined_vulns: dict[str, Issue] = {}
        for vuln in qualys_vulns:
            if vuln.qualysId in combined_vulns:
                if current_identifier := combined_vulns[vuln.qualysId].assetIdentifier:
                    combined_vulns[vuln.qualysId].assetIdentifier = update_asset_identifier(
                        vuln.assetIdentifier, current_identifier
                    )
                else:
                    combined_vulns[vuln.qualysId].assetIdentifier = vuln.assetIdentifier
            else:
                combined_vulns[vuln.qualysId] = vuln
            job_progress.update(deduping_task, advance=1)
    return list(combined_vulns.values())


def get_qualys_assets_and_scan_results(
    url: Optional[str] = None, asset_group_filter: Optional[Union[int, str]] = None
) -> list:
    """
    function to gather all assets from Qualys API host along with their scan results

    :param Optional[str] url: URL to get the assets from, defaults to None, used for pagination
    :param Optional[Union[int, str]] asset_group_filter: Qualys asset group ID or name to filter by, if provided
    :return: list of dictionaries containing asset data
    :rtype: list
    """
    qualys_url, QUALYS_API = _get_qualys_api()
    # set url
    if not url:
        url = urljoin(qualys_url, "/api/2.0/fo/asset/host/vm/detection/?action=list&show_asset_id=1")

    # check if an asset group filter was provided and append it to the url
    if asset_group_filter:
        if isinstance(asset_group_filter, str):
            # Get the asset group ID from Qualys
            url += f"&ag_titles={asset_group_filter}"
            logger.info("Getting assets from Qualys by group name: %s...", asset_group_filter)
        else:
            url += f"&ag_ids={asset_group_filter}"
            logger.info(
                "Getting assets from from Qualys by group ID: #%s...",
                asset_group_filter,
            )
    else:
        # Get all assets from Qualys
        logger.info("Getting all assets from Qualys...")

    # get the data via Qualys API host
    response = QUALYS_API.get(url=url, headers=HEADERS)
    res_data = xmltodict.parse(response.text)

    try:
        # parse the xml data from response.text and convert it to a dictionary
        # and try to extract the data from the parsed XML dictionary
        asset_data = res_data["HOST_LIST_VM_DETECTION_OUTPUT"]["RESPONSE"]["HOST_LIST"]["HOST"]
        # Always make asset_data a list
        if isinstance(asset_data, dict):
            asset_data = [asset_data]
        elif not isinstance(asset_data, list):
            asset_data = []
        # check if we need to paginate the asset data
        if "WARNING" in res_data["HOST_LIST_VM_DETECTION_OUTPUT"]["RESPONSE"]:
            logger.warning("Not all assets were fetched, fetching more assets from Qualys...")
            asset_data.extend(
                get_qualys_assets_and_scan_results(
                    url=res_data["HOST_LIST_VM_DETECTION_OUTPUT"]["RESPONSE"]["WARNING"]["URL"],
                    asset_group_filter=asset_group_filter,
                )
            )
    except KeyError:
        # if there is a KeyError set the dictionary to nothing
        asset_data = []
    # return the asset_data variable
    return asset_data


def get_scan_results(scans: Any, task: TaskID) -> dict:
    """
    Function to retrieve scan results from Qualys using provided scan list and returns a dictionary

    :param Any scans: list of scans to retrieve from Qualys
    :param TaskID task: task to update in the progress object
    :return: dictionary of detailed Qualys scans
    :rtype: dict
    """
    qualys_url, QUALYS_API = _get_qualys_api()

    scan_data = {}
    # check number of scans requested
    if isinstance(scans, str):
        # only one scan was requested, set up variable for the for loop
        scans = {"SCAN": [{"REF": scans}]}
    for scan in scans["SCAN"]:
        # set up data and parameters for the scans query
        try:
            # try and get the scan id ref #
            scan_id = scan["REF"]
            # set the parameters for the Qualys API call
            params = {
                "action": "fetch",
                "scan_ref": scan_id,
                "mode": "extended",
                "output_format": "json_extended",
            }
            # get the scan data via API
            res = QUALYS_API.get(
                url=urljoin(qualys_url, "/api/2.0/fo/scan/"),
                headers=HEADERS,
                params=params,
            )
            # convert response to json
            if res.status_code == 200:
                try:
                    res_data = res.json()
                    scan_data[scan_id] = res_data
                except JSONDecodeError:
                    error_and_exit("Unable to convert response to JSON.")
            else:
                error_and_exit(f"Received unexpected response from Qualys API: {res.status_code}: {res.text}")
        except KeyError:
            # unable to get the scan id ref #
            continue
        job_progress.update(task, advance=1)
    return scan_data


def get_detailed_scans(days: int) -> list:
    """
    function to get the list of all scans from Qualys using QUALYS_API

    :param int days: # of days before today to filter scans
    :return: list of results from Qualys API
    :rtype: list
    """
    qualys_url, QUALYS_API = _get_qualys_api()

    today = datetime.now()
    scan_date = today - timedelta(days=days)

    # set up data and parameters for the scans query
    params = {
        "action": "list",
        "scan_date_since": scan_date.strftime("%Y-%m-%d"),
        "output_format": "json",
    }
    params2 = {
        "action": "list",
        "scan_datetime_since": scan_date.strftime("%Y-%m-%dT%H:%I:%S%ZZ"),
    }
    res = QUALYS_API.get(
        url=urljoin(qualys_url, "/api/2.0/fo/scan/summary/"),
        headers=HEADERS,
        params=params,
    )
    response = QUALYS_API.get(
        url=urljoin(qualys_url, "/api/2.0/fo/scan/vm/summary/"),
        headers=HEADERS,
        params=params2,
    )
    # convert response to json
    res_data = res.json()
    try:
        response_data = xmltodict.parse(response.text)["SCAN_SUMMARY_OUTPUT"]["RESPONSE"]["SCAN_SUMMARY_LIST"][
            "SCAN_SUMMARY"
        ]
        if len(res_data) < 1:
            res_data = response_data
        else:
            res_data.extend(response_data)
    except JSONDecodeError:
        logger.error("ERROR: Unable to convert to JSON.")
    return res_data


def _get_config():
    """
    Get the Qualys configuration

    :return: Qualys configuration
    :rtype: dict
    """
    app = check_license()
    config = app.config
    return config


def _get_qualys_api():
    """
    Get the Qualys API session

    :return: Qualys API session
    :rtype: Session
    """
    config = _get_config()

    # set the auth for the QUALYS_API session
    QUALYS_API.auth = (config.get("qualysUserName"), config.get("qualysPassword"))
    QUALYS_API.verify = config.get("sslVerify", True)

    # Use qualysMockUrl if available (for testing), otherwise use qualysUrl
    qualys_url = config.get("qualysMockUrl") or config.get("qualysUrl")
    logger.info("Connecting to Qualys server at: %s", qualys_url)

    return qualys_url, QUALYS_API


def get_scans_summary(scan_choice: str) -> dict:
    """
    Get all scans from Qualys Host

    :param str scan_choice: The type of scan to retrieve from Qualys API
    :return: Detailed summary of scans from Qualys API as a dictionary
    :rtype: dict
    """
    qualys_url, QUALYS_API = _get_qualys_api()
    urls = []

    # set up variables for function
    scan_data = {}
    responses = []
    scan_url = urljoin(qualys_url, "/api/2.0/fo/scan/")

    # set up parameters for the scans query
    params = {"action": "list"}
    # check what scan list was requested and set urls list accordingly
    if scan_choice.lower() == "all":
        urls = [scan_url, scan_url + "compliance", scan_url + "scap"]
    elif scan_choice.lower() == "vm":
        urls = [scan_url]
    elif scan_choice.lower() in ["compliance", "scap"]:
        urls = [scan_url + scan_choice.lower()]
    # get the list of vm scans
    for url in urls:
        # get the scan data
        response = QUALYS_API.get(url=url, headers=HEADERS, params=params)
        # store response into a list
        responses.append(response)
    # check the responses received for data
    for response in responses:
        # see if response was successful
        if response.status_code == 200:
            # parse the data
            data = xmltodict.parse(response.text)["SCAN_LIST_OUTPUT"]["RESPONSE"]
            # see if the scan has any data
            try:
                # add the data to the scan_data dictionary
                scan_data.update(data["SCAN_LIST"])
            except KeyError:
                # no data found, continue the for loop
                continue
    return scan_data


def get_scan_details(days: int) -> list:
    """
    Retrieve completed scans from last x days from Qualys Host

    :param int days: # of days before today to filter scans
    :return: Detailed summary of scans from Qualys API as a dictionary
    :rtype: list
    """
    qualys_url, QUALYS_API = _get_qualys_api()
    # get since date for API call
    since_date = datetime.now() - timedelta(days=days)
    # set up data and parameters for the scans query
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "X-Requested-With": "RegScale CLI",
    }
    params = {
        "action": "list",
        "scan_date_since": since_date.strftime("%Y-%m-%d"),
        "output_format": "json",
    }
    params2 = {
        "action": "list",
        "scan_datetime_since": since_date.strftime(QUALYS_DATETIME_FORMAT),
    }
    res = QUALYS_API.get(
        url=urljoin(qualys_url, "/api/2.0/fo/scan/summary/"),
        headers=headers,
        params=params,
    )
    response = QUALYS_API.get(
        url=urljoin(qualys_url, "/api/2.0/fo/scan/vm/summary/"),
        headers=headers,
        params=params2,
    )
    # convert response to json
    res_data = res.json()
    try:
        response_data = xmltodict.parse(response.text)["SCAN_SUMMARY_OUTPUT"]["RESPONSE"]["SCAN_SUMMARY_LIST"][
            "SCAN_SUMMARY"
        ]
        if len(res_data) < 1:
            res_data = response_data
        else:
            res_data.update(response_data)
    except JSONDecodeError as ex:
        error_and_exit(f"Unable to convert to JSON.\n{ex}")
    except KeyError:
        error_and_exit(f"No data found.\n{response.text}")
    return res_data


def _extract_container_assets(container_vulnerabilities: list) -> list[dict]:
    """
    Extract unique containers from vulnerability data and convert to VMDR asset format.

    :param container_vulnerabilities: List of container vulnerability dictionaries from Qualys CS API
    :return: List of assets in VMDR format with DETECTION_LIST populated
    :rtype: list[dict]
    """
    unique_containers = {}

    for container_data in container_vulnerabilities:
        container_id = container_data.get("containerId", "")
        if not container_id:
            continue

        # Only add each container once (unique by containerId)
        if container_id not in unique_containers:
            container_name = container_data.get("name", "Unknown Container")
            operating_system = container_data.get("operatingSystem", "Linux")
            state = container_data.get("state", "unknown")

            # Convert container vulnerabilities to DETECTION format for unified processing
            detections = []
            vulnerabilities = container_data.get("vulnerabilities", [])
            for vuln in vulnerabilities:
                qid = vuln.get("qid") or vuln.get("vulnerabilityId", "")
                if not qid:
                    continue

                # Map container vulnerability to VMDR DETECTION format
                detection = {
                    "QID": str(qid),
                    "TYPE": vuln.get("typeDetected", "Confirmed"),
                    "SEVERITY": str(vuln.get("severity", 3)),
                    "STATUS": "Active",
                    "FIRST_FOUND_DATETIME": _convert_container_timestamp(
                        vuln.get("firstFound") or vuln.get("publishedDate")
                    ),
                    "LAST_FOUND_DATETIME": _convert_container_timestamp(
                        vuln.get("lastFound") or vuln.get("publishedDate")
                    ),
                    "RESULTS": vuln.get("result", vuln.get("description", "")),
                }
                detections.append(detection)

            # Create asset in VMDR format with populated DETECTION_LIST
            unique_containers[container_id] = {
                "ASSET_ID": container_id,
                "IP": "Container",  # Containers may not have IPs in this context
                "DNS": container_name,
                "OS": operating_system,
                "TRACKING_METHOD": "CONTAINER",
                "ID": container_id,
                "DETECTION_LIST": {"DETECTION": detections},  # Now populated with vulnerability data
                # Additional metadata
                "imageId": container_data.get("imageId", ""),
                "sha": container_data.get("sha", ""),
                "state": state,
                "created": container_data.get("created", ""),
                "stateChanged": container_data.get("stateChanged", ""),
            }

    logger.info(
        "Extracted %s unique container assets with %s total detections from vulnerability data.",
        len(unique_containers),
        sum(len(asset["DETECTION_LIST"]["DETECTION"]) for asset in unique_containers.values()),
    )
    return list(unique_containers.values())


def _convert_container_timestamp(timestamp) -> str:
    """
    Convert Qualys Container Security timestamp to ISO 8601 format.

    Qualys CS API returns timestamps in milliseconds since epoch (e.g., 1751925963138)
    or sometimes in ISO 8601 format (e.g., "2024-01-15T10:30:00Z").
    Mock API may return timestamps with microseconds (e.g., "2025-05-04T11:56:09.262423").
    Convert to consistent ISO 8601 format expected by RegScale (without microseconds).

    :param timestamp: Timestamp value (int, str, or None)
    :return: ISO 8601 formatted string or empty string if invalid
    :rtype: str
    """
    from datetime import datetime, timezone

    if not timestamp:
        return ""

    # If already in ISO 8601 format, parse and normalize (remove microseconds)
    if isinstance(timestamp, str) and "T" in timestamp:
        try:
            # Try parsing with microseconds first (Mock API format)
            if "." in timestamp:
                # Remove microseconds by truncating at the decimal point
                timestamp_no_micro = timestamp.split(".")[0] + "Z"
                return timestamp_no_micro
            # Already in correct format without microseconds
            return timestamp
        except (ValueError, AttributeError):
            logger.warning("Could not normalize timestamp: %s", timestamp)
            return timestamp

    # Convert millisecond timestamp to ISO 8601
    try:
        # Qualys CS uses milliseconds, convert to seconds
        if isinstance(timestamp, (int, float)):
            timestamp_seconds = timestamp / 1000 if timestamp > 9999999999 else timestamp
            dt = datetime.fromtimestamp(timestamp_seconds, tz=timezone.utc)
            return dt.strftime(QUALYS_DATETIME_FORMAT)
        elif isinstance(timestamp, str):
            # Try parsing as millisecond timestamp string
            timestamp_int = int(timestamp)
            timestamp_seconds = timestamp_int / 1000 if timestamp_int > 9999999999 else timestamp_int
            dt = datetime.fromtimestamp(timestamp_seconds, tz=timezone.utc)
            return dt.strftime(QUALYS_DATETIME_FORMAT)
    except (ValueError, OSError):
        logger.warning("Could not convert timestamp: %s", timestamp)
        return ""

    return ""


def _construct_container_plugin_id(vuln: dict, container_id: str, cve: str) -> tuple[str, str]:
    """
    Construct plugin name and ID for container vulnerability based on issueCreation mode.

    Follows AWS Security Hub pattern:
    - Consolidated mode: ONE plugin_id per CVE (groups all containers with same CVE)
    - Per-asset mode: Unique plugin_id per container+CVE pair

    :param vuln: Vulnerability dictionary
    :param container_id: Container ID for per-asset mode
    :param cve: CVE identifier
    :return: Tuple of (qualys_id, plugin_id)
    :rtype: tuple[str, str]
    """
    issue_creation_mode = ScannerVariables.issueCreation.lower()

    # Log mode on first call (avoid spam)
    if not hasattr(_construct_container_plugin_id, "_mode_logged"):
        logger.info("[CONTAINER_PLUGIN_ID] issueCreation mode: '%s'", issue_creation_mode)
        _construct_container_plugin_id._mode_logged = True

    # Use CVE as the base identifier (or QID if no CVE)
    qid = vuln.get("qid") or vuln.get("vulnerabilityId", "")
    base_id = cve if cve else qid

    if issue_creation_mode == "consolidated":
        # Consolidated mode: Group by CVE/QID across all containers
        # Examples:
        #   - CVE-2021-89012 on 8 containers → 1 issue
        #   - QID_150001 on 5 containers → 1 issue
        qualys_id = base_id
        plugin_id = f"Qualys_Container_{base_id}".replace("-", "_").replace(".", "_")
    else:
        # Per-asset mode: Unique ID per container+vulnerability pair
        # Examples:
        #   - CVE-2021-89012 on container_abc → CVE-2021-89012_abc
        #   - CVE-2021-89012 on container_xyz → CVE-2021-89012_xyz
        container_suffix = container_id[:12]  # Use first 12 chars of container ID
        qualys_id = f"{base_id}_{container_suffix}"
        plugin_id = f"Qualys_Container_{base_id}_{container_suffix}".replace("-", "_").replace(".", "_")

    return qualys_id, plugin_id


def _map_container_severity(severity_value) -> int:
    """Map container vulnerability severity to numeric value."""
    if isinstance(severity_value, str):
        severity_map = {"CRITICAL": 5, "HIGH": 4, "MEDIUM": 3, "LOW": 2, "INFO": 1}
        return severity_map.get(severity_value.upper(), 3)
    return severity_value


def _extract_container_cve(vuln: dict) -> str:
    """Extract CVE from vulnerability data (handles both production and Mock API formats)."""
    cve_ids = vuln.get("cveids", [])
    if not cve_ids and vuln.get("vulnerabilityId", "").startswith("CVE-"):
        return vuln.get("vulnerabilityId", "")
    return cve_ids[0] if cve_ids else ""


def _build_container_package_info(vuln: dict) -> tuple[str, str, str]:
    """Build package information from vulnerability data."""
    package_info = vuln.get("package", "")
    software_name = vuln.get("softwareName", "")
    software_version = vuln.get("softwareVersion", "")

    if not package_info and vuln.get("installedVersion"):
        package_info = f"{package_info} {vuln.get('installedVersion')}"
    if not software_version:
        software_version = vuln.get("installedVersion", "")

    return package_info, software_name, software_version


def _build_container_solution(vuln: dict) -> str:
    """Build solution text from vulnerability data."""
    solution = vuln.get("solution", "")
    if not solution and vuln.get("fixedVersion"):
        solution = f"Upgrade to version {vuln.get('fixedVersion')} or later"
    return solution


def _create_container_issue_entry(vuln: dict, container_id: str, cve: str) -> tuple[str, dict]:
    """Create a single container issue entry from vulnerability data."""
    qid = vuln.get("qid") or vuln.get("vulnerabilityId", "")
    severity = _map_container_severity(vuln.get("severity", 3))
    qualys_id, plugin_id = _construct_container_plugin_id(vuln, container_id, cve)

    # Build title with CVE
    title = vuln.get("title", vuln.get("description", "Unknown Vulnerability"))
    if cve and cve not in title:
        title = f"{title} ({cve})"

    # Extract CVSS information
    cvss3_score = vuln.get("cvss3Score") or vuln.get("cvssScore", 0.0)
    cvss_info = vuln.get("cvss3Info", {})
    cvss3_vector = cvss_info.get("vector", "") if isinstance(cvss_info, dict) else ""

    # Get package information
    package_info, software_name, software_version = _build_container_package_info(vuln)

    # Handle timestamps
    last_found = _convert_container_timestamp(vuln.get("lastFound") or vuln.get("publishedDate"))
    first_found = _convert_container_timestamp(vuln.get("firstFound") or vuln.get("publishedDate"))

    # Build solution
    solution = _build_container_solution(vuln)

    issue_entry = {
        "QID": str(qid),
        "QUALYS_ID": qualys_id,
        "PLUGIN_ID": plugin_id,
        "SEVERITY": severity,
        "TYPE": vuln.get("typeDetected", "Confirmed"),
        "LAST_FOUND_DATETIME": last_found,
        "FIRST_FOUND_DATETIME": first_found,
        "ISSUE_DATA": {
            "TITLE": title,
            "CONSEQUENCE": vuln.get("consequence", vuln.get("impact", "Container vulnerability")),
            "DIAGNOSIS": vuln.get("result", vuln.get("description", "")),
            "SOLUTION": solution,
            "CVE": cve,
            "CVSS_V3_SCORE": cvss3_score,
            "CVSS_V3_VECTOR": cvss3_vector,
            "SOFTWARE_NAME": software_name,
            "SOFTWARE_VERSION": software_version,
            "PACKAGE": package_info,
        },
    }
    return qualys_id, issue_entry


def _convert_container_vulns_to_issues(container_vulnerabilities: list) -> list[dict]:
    """
    Convert container vulnerability data to RegScale Issue format matching qualys_assets_and_issues structure.

    Supports mode-aware issue consolidation following AWS Security Hub pattern:
    - Consolidated mode: Groups by CVE (one issue per CVE across all containers)
    - Per-asset mode: One issue per container-CVE pair

    :param container_vulnerabilities: List of container vulnerability dictionaries from Qualys CS API
    :param config: Configuration dictionary
    :return: List of dictionaries with asset_data and ISSUES matching qualys_assets_and_issues format
    :rtype: list[dict]
    """
    container_issues = []

    for container_data in container_vulnerabilities:
        container_id = container_data.get("containerId", "")
        if not container_id:
            continue

        container_name = container_data.get("name", "Unknown Container")
        operating_system = container_data.get("operatingSystem", "Linux")

        asset_data = {
            "ASSET_ID": container_id,
            "DNS": container_name,
            "IP": "Container",
            "OS": operating_system,
            "TRACKING_METHOD": "CONTAINER",
        }

        issues = {}
        vulnerabilities = container_data.get("vulnerabilities", [])

        for vuln in vulnerabilities:
            qid = vuln.get("qid") or vuln.get("vulnerabilityId", "")
            if not qid:
                continue

            cve = _extract_container_cve(vuln)
            qualys_id, issue_entry = _create_container_issue_entry(vuln, container_id, cve)
            issues[qualys_id] = issue_entry

        if issues:
            container_issue_entry = {**asset_data, "ISSUES": issues}
            container_issues.append(container_issue_entry)

    logger.info(
        "Converted %s containers with vulnerabilities to issue format (%s total issues).",
        len(container_issues),
        sum(len(c["ISSUES"]) for c in container_issues),
    )
    return container_issues


def get_issue_data_for_assets(asset_list: list) -> Tuple[list[dict], int]:
    """
    Function to get issue data from Qualys via API for assets in Qualys

    :param list asset_list: Assets and their scan results from Qualys
    :return:  Updated asset list of Qualys assets and their associated vulnerabilities, total number of vulnerabilities
    :rtype: Tuple[list[dict], int]
    """
    config = _get_config()
    with job_progress:
        issues = {}
        for asset in asset_list:
            # check if the asset has any vulnerabilities
            if vulns := asset.get("DETECTION_LIST", {}).get("DETECTION", {}):
                # Normalize vulns to always be a list (xmltodict returns dict for single items)
                if isinstance(vulns, dict):
                    vulns = [vulns]

                asset_vulns = {}
                analyzing_vulns = job_progress.add_task(
                    f"Analyzing {len(vulns)} vulnerabilities for asset #{asset.get('ASSET_ID')} from Qualys..."
                )
                # iterate through the vulnerabilities & verify they have a confirmed status
                for vuln in vulns:
                    if vuln["TYPE"] == "Confirmed":
                        issues[vuln["QID"]] = vuln
                        asset_vulns[vuln["QID"]] = vuln
                    job_progress.update(analyzing_vulns, advance=1)
                job_progress.update(analyzing_vulns, completed=len(vulns))
                # add the issues to the asset's dictionary
                asset["ISSUES"] = asset_vulns
                job_progress.remove_task(analyzing_vulns)
    # Only fetch KB data if there are issues to enrich
    if issues:
        asset_list = fetch_vulns_from_qualys(issue_ids=list(issues.keys()), asset_list=asset_list, config=config)
    else:
        logger.info("No confirmed vulnerabilities found, skipping KB enrichment")
    return asset_list, len(issues)


def parse_and_map_vuln_data(xml_data: str) -> dict:
    """
    Function to parse Qualys vulnerability data from XML and map it to a dictionary using the Qualys ID as the key

    :param str xml_data: XML data from Qualys API
    :return: Dictionary of Qualys vulnerability data
    :rtype: dict
    """
    issue_data = (
        xmltodict.parse(xml_data)
        .get("KNOWLEDGE_BASE_VULN_LIST_OUTPUT", {})
        .get("RESPONSE", {})
        .get("VULN_LIST", {})
        .get("VULN", {})
    )
    # change the key for the fetched issues to be the qualys ID
    return {issue["QID"]: issue for issue in issue_data}


def fetch_vulns_from_qualys(issue_ids: list[str], asset_list: list[dict], config: dict, retries: int = 0) -> list[dict]:
    """
    Function to fetch vulnerability data from Qualys for a list of issues and assets

    :param list[str] issue_ids: List of Qualys issue IDs to fetch data for
    :param list[dict] asset_list: List of Qualys assets to update with vulnerability data
    :param dict config: CLI Configuration dictionary
    :param int retries: Number of retries for fetching data, defaults to 0
    :return: Updated asset list with vulnerability data
    :rtype: list[dict]
    """
    logger.info(
        f"Getting vulnerability data for {len(issue_ids)} issue(s) from Qualys for {len(asset_list)} asset(s)..."
    )
    # Use qualysMockUrl if available (for testing), otherwise use qualysUrl
    qualys_url = config.get("qualysMockUrl") or config.get("qualysUrl")
    base_url = urljoin(qualys_url, "/api/2.0/fo/knowledge_base/vuln/?action=list&details=All")
    if len(issue_ids) > 100:
        logger.warning(
            "Too many issues to fetch from Qualys. Downloading the Qualys database to prevent rate limits..."
        )
        # since there are a lot of vulnerabilities, download the database and reference it locally
        chunk_size_calc = 20 * 1024
        with QUALYS_API.post(
            url=base_url,
            headers=HEADERS,
            stream=True,
        ) as response:
            check_file_path("artifacts")
            with open("./artifacts/qualys_vuln_db.xml", "wb") as f:
                for chunk in response.iter_content(chunk_size=chunk_size_calc):
                    f.write(chunk)
        with open("./artifacts/qualys_vuln_db.xml", "r") as f:
            qualys_issue_data = parse_and_map_vuln_data(f.read())
    else:
        response = QUALYS_API.get(
            url=f"{base_url}&ids={','.join(issue_ids)}",
            headers=HEADERS,
        )
        if response.ok:
            qualys_issue_data = parse_and_map_vuln_data(response.text)
            logger.info("Received vulnerability data for %s issues from Qualys.", len(qualys_issue_data))
        elif response.status_code == 409:
            response_data = xmltodict.parse(response.text)["SIMPLE_RETURN"]["RESPONSE"]
            logger.warning(
                "Received timeout error from Qualys API: %s. Waiting %s seconds...",
                response_data["TEXT"],
                response_data["ITEM_LIST"]["ITEM"]["VALUE"],
            )
            sleep(int(response_data["ITEM_LIST"]["ITEM"]["VALUE"]))
            if retries < 3:
                fetch_vulns_from_qualys(issue_ids, asset_list, config, retries + 1)
            else:
                error_and_exit(
                    "Unable to fetch vulnerability data from Qualys after 3 attempts. Please try again later."
                )
        else:
            error_and_exit(
                f"Received unexpected response from Qualys: {response.status_code}: {response.text}: {response.reason}"
            )
    return map_issue_data_to_assets(asset_list, qualys_issue_data)


def map_issue_data_to_assets(assets: list[dict], qualys_issue_data: dict) -> list[dict]:
    """
    Function to map Qualys issue data to Qualys assets

    :param list[dict] assets: List of Qualys assets to map issue data to
    :param dict qualys_issue_data: List of Qualys issues to map to assets
    :return: Updated asset list with Qualys issue data
    :rtype: list[dict]
    """
    for asset in assets:
        if issues := asset.get("ISSUES"):
            mapping_vulns = job_progress.add_task(
                f"Mapping {len(issues)} vulnerabilities to Asset #{asset['ASSET_ID']} from Qualys...",
                total=len(issues),
            )
            for issue in issues:
                if issue in qualys_issue_data:
                    issues[issue]["ISSUE_DATA"] = qualys_issue_data[issue]
                job_progress.update(mapping_vulns, advance=1)
            job_progress.remove_task(mapping_vulns)
    return assets


def lookup_asset(asset_list: list, asset_id: str = None) -> list[Asset]:
    """
    Look up assets in the asset list by Qualys asset ID or return all unique assets.

    Args:
        asset_list (list): List of assets from RegScale.
        asset_id (str, optional): Qualys asset ID to search for. Defaults to None.

    Returns:
        list[Asset]: List of unique Asset objects matching the asset_id, or all unique assets if asset_id is None.
    """
    if asset_id:
        return list({asset for asset in asset_list if getattr(asset, "qualysId", None) == asset_id})
    return list(set(asset_list)) or []


def map_qualys_severity_to_regscale(severity: int) -> tuple[IssueSeverity, str]:
    """
    Map Qualys vulnerability severity to RegScale Issue severity

    :param int severity: Qualys vulnerability severity
    :return: RegScale Issue severity and key for init.yaml
    :rtype: tuple[str, str]
    """
    if severity <= 2:
        return IssueSeverity.Low, "low"
    if severity == 3:
        return IssueSeverity.Moderate, "moderate"
    if severity > 3:
        return IssueSeverity.High, "high"
    return IssueSeverity.NotAssigned, "low"


def create_regscale_issue_from_vuln(
    regscale_ssp_id: int, qualys_asset: dict, regscale_assets: list[Asset], vulns: dict, is_component: bool = False
) -> Tuple[list[Issue], list[Issue]]:
    """
    Sync Qualys vulnerabilities to RegScale issues (Security Plan or Component).

    :param int regscale_ssp_id: RegScale SSP or Component ID
    :param dict qualys_asset: Qualys asset as a dictionary
    :param list[Asset] regscale_assets: list of RegScale assets
    :param dict vulns: dictionary of Qualys vulnerabilities associated with the provided asset
    :param bool is_component: Whether the sync is for a component (True) or security plan (False)
    :return: list of RegScale issues to update, and a list of issues to be created
    :rtype: Tuple[list[Issue], list[Issue]]
    """
    config = _get_config()
    default_status = config["issues"]["qualys"]["status"]
    regscale_issues = []
    parent_module = "components" if is_component else "securityplans"
    regscale_existing_issues = Issue.get_all_by_parent(parent_id=regscale_ssp_id, parent_module=parent_module)
    for vuln in vulns.values():
        asset_identifier = None
        severity, key = map_qualys_severity_to_regscale(int(vuln["SEVERITY"]))

        default_due_delta = config["issues"]["qualys"][key]
        logger.debug("Processing vulnerability# %s", vuln["QID"])
        due_date = datetime.strptime(vuln["LAST_FOUND_DATETIME"], QUALYS_DATETIME_FORMAT) + timedelta(
            days=default_due_delta
        )
        regscale_asset = [asset for asset in regscale_assets if asset.qualysId == qualys_asset["ASSET_ID"]]
        if "DNS" not in qualys_asset.keys() or "IP" not in qualys_asset.keys():
            if regscale_asset:
                asset_identifier = f"RegScale Asset #{regscale_asset[0].id}: {regscale_asset[0].name}"
        else:
            if regscale_asset:
                asset_identifier = (
                    f'RegScale Asset #{regscale_asset[0].id}: {regscale_asset[0].name} Qualys DNS: "'
                    f'{qualys_asset["DNS"]} - IP: {qualys_asset["IP"]}'
                )
            else:
                asset_identifier = f'DNS: {qualys_asset["DNS"]} - IP: {qualys_asset["IP"]}'
        # Build description from available fields (some vulnerabilities may not have all fields)
        issue_data = vuln.get("ISSUE_DATA", {})
        consequence = issue_data.get("CONSEQUENCE", "")
        diagnosis = issue_data.get("DIAGNOSIS", "")
        description_parts = [part for part in [consequence, diagnosis] if part]
        description = "</br>".join(description_parts) if description_parts else "No description available"

        issue = Issue(
            title=issue_data.get("TITLE", f"Vulnerability QID {vuln.get('QID', 'Unknown')}"),
            description=description,
            issueOwnerId=config["userId"],
            status=default_status,
            severityLevel=severity,
            qualysId=vuln.get("QUALYS_ID", vuln.get("QID")),  # Use mode-aware QUALYS_ID if available
            pluginId=vuln.get("PLUGIN_ID"),  # Add mode-aware pluginId
            dueDate=due_date.strftime("%Y-%m-%d"),
            identification="Vulnerability Assessment",
            parentId=regscale_ssp_id,
            parentModule=parent_module,
            recommendedActions=issue_data.get("SOLUTION", ""),
            assetIdentifier=asset_identifier,
        )
        regscale_issues.append(issue)
    regscale_new_issues, regscale_update_issues = determine_issue_update_or_create(
        regscale_issues, regscale_existing_issues
    )
    return regscale_update_issues, regscale_new_issues


def update_asset_identifier(new_identifier: Optional[str], current_identifier: Optional[str]) -> Optional[str]:
    """
    Function to update the asset identifier for a RegScale issue

    :param Optional[str] new_identifier: New asset identifier to add
    :param Optional[str] current_identifier: Current asset identifier
    :return: Updated asset identifier
    :rtype: str
    """
    if not current_identifier and new_identifier:
        return new_identifier
    if current_identifier and new_identifier:
        if new_identifier not in current_identifier:
            return f"{current_identifier}<br>{new_identifier}"
        if new_identifier in current_identifier:
            return current_identifier
        if new_identifier == current_identifier:
            return current_identifier


def determine_issue_update_or_create(
    qualys_issues: list[Issue], regscale_issues: list[Issue]
) -> Tuple[list[Issue], list[Issue]]:
    """
    Function to determine if Qualys issues needs to be updated or created in RegScale

    :param list[Issue] qualys_issues: List of Qualys issues
    :param list[Issue] regscale_issues: List of existing RegScale issues
    :return: List of new issues and list of issues to update
    :rtype: Tuple[list[Issue], list[Issue]]
    """
    new_issues = []
    update_issues = []
    for issue in qualys_issues:
        if issue.qualysId in [iss.qualysId for iss in regscale_issues]:
            update_issue = [iss for iss in regscale_issues if iss.qualysId == issue.qualysId][0]
            # Check if we need to concatenate the asset identifier
            update_issue.assetIdentifier = update_asset_identifier(issue.assetIdentifier, update_issue.assetIdentifier)
            update_issues.append(update_issue)
        else:
            new_issues.append(issue)
    return new_issues, update_issues


def _build_deduplication_sets(reg_list: list) -> tuple[set, set, set]:
    """
    Build sets for deduplication from RegScale assets.

    :param list reg_list: list of assets from RegScale
    :return: tuple of (plugin_id_set, qualys_id_set, asset_name_patterns)
    :rtype: tuple[set, set, set]
    """
    plugin_id_set = set()
    qualys_id_set = set()
    asset_name_patterns = set()

    for asset in reg_list:
        if plugin_id := getattr(asset, "pluginId", None):
            plugin_id_set.add(plugin_id)
        if qualys_id := getattr(asset, "qualysId", None):
            qualys_id_set.add(qualys_id)
        if name := getattr(asset, "name", None):
            if name.startswith("Qualys Asset #"):
                try:
                    asset_id_from_name = name.split("#")[1].split(" IP:")[0]
                    asset_name_patterns.add(asset_id_from_name)
                except (IndexError, AttributeError):
                    pass

    return plugin_id_set, qualys_id_set, asset_name_patterns


def _log_deduplication_sets(plugin_id_set: set, qualys_id_set: set, asset_name_patterns: set) -> None:
    """Log deduplication set statistics."""
    logger.info("Deduplication sets built:")
    logger.info("  - pluginId set size: %s", len(plugin_id_set))
    logger.info("  - qualysId set size: %s", len(qualys_id_set))
    logger.info("  - name pattern set size: %s", len(asset_name_patterns))

    if plugin_id_set:
        logger.info("  - Sample pluginIds: %s", list(plugin_id_set)[:5])
    if qualys_id_set:
        logger.info("  - Sample qualysIds: %s", list(qualys_id_set)[:5])
    if asset_name_patterns:
        logger.info("  - Sample name patterns: %s", list(asset_name_patterns)[:5])


def _get_asset_identifiers(list_qualys) -> tuple[str, str]:
    """
    Extract asset_id and tracking_method from asset (dict or object).

    :param list_qualys: Asset dictionary or object
    :return: tuple of (asset_id, tracking_method)
    :rtype: tuple[str, str]
    """
    if isinstance(list_qualys, dict):
        return list_qualys.get("ASSET_ID"), list_qualys.get("TRACKING_METHOD", "VMDR")
    return getattr(list_qualys, "ASSET_ID", None), getattr(list_qualys, "TRACKING_METHOD", "VMDR")


def _is_asset_duplicate(
    asset_id: str, tracking_method: str, plugin_id_set: set, qualys_id_set: set, asset_name_patterns: set
) -> bool:
    """
    Check if asset exists in RegScale using multiple match strategies.

    :param str asset_id: The asset ID to check
    :param str tracking_method: The tracking method (VMDR or CONTAINER)
    :param set plugin_id_set: Set of existing pluginIds
    :param set qualys_id_set: Set of existing qualysIds
    :param set asset_name_patterns: Set of existing name patterns
    :return: True if asset already exists
    :rtype: bool
    """
    expected_plugin_id = f"Qualys_{tracking_method}_{asset_id}"
    return expected_plugin_id in plugin_id_set or asset_id in qualys_id_set or asset_id in asset_name_patterns


def inner_join(reg_list: list, qualys_list: list) -> list:
    """
    Function to compare assets from Qualys and assets from RegScale.

    Checks pluginId (modern), qualysId (legacy), and asset name patterns for backward compatibility.

    :param list reg_list: list of assets from RegScale
    :param list qualys_list: list of assets from Qualys
    :return: list of assets that are in both RegScale and Qualys
    :rtype: list
    """
    logger.debug("Deduplicating %s Qualys assets against %s RegScale assets", len(qualys_list), len(reg_list))

    # Build deduplication sets
    plugin_id_set, qualys_id_set, asset_name_patterns = _build_deduplication_sets(reg_list)
    logger.debug(
        "Built deduplication sets: %s pluginIds, %s qualysIds, %s name patterns",
        len(plugin_id_set),
        len(qualys_id_set),
        len(asset_name_patterns),
    )

    data = []
    matches_found = 0

    try:
        for list_qualys in qualys_list:
            asset_id, tracking_method = _get_asset_identifiers(list_qualys)

            if not asset_id:
                continue

            if _is_asset_duplicate(asset_id, tracking_method, plugin_id_set, qualys_id_set, asset_name_patterns):
                data.append(list_qualys)
                matches_found += 1
    except KeyError as ex:
        logger.error(ex)

    logger.info("Deduplication: %s existing, %s new", matches_found, len(qualys_list) - matches_found)

    return data


def get_asset_groups_from_qualys() -> list:
    """
    Get all asset groups from Qualys via API

    :return: list of assets from Qualys
    :rtype: list
    """
    asset_groups = []

    qualys_url, QUALYS_API = _get_qualys_api()
    response = QUALYS_API.get(url=urljoin(qualys_url, "/api/2.0/fo/asset/group/?action=list"), headers=HEADERS)
    if response.ok:
        logger.debug(response.text)
        try:
            asset_groups = xmltodict.parse(response.text)["ASSET_GROUP_LIST_OUTPUT"]["RESPONSE"]["ASSET_GROUP_LIST"][
                "ASSET_GROUP"
            ]
        except KeyError:
            logger.debug(response.text)
            error_and_exit(
                f"Unable to retrieve asset groups from Qualys.\nReceived: #{response.status_code}: {response.text}"
            )
    return asset_groups


def _get_required_headers() -> list[str]:
    """
    Get the list of required headers for Qualys CSV validation.

    :return: List of required header names
    :rtype: list[str]
    """
    return [
        "Severity",
        "Title",
        "Exploitability",
        "CVE ID",
        "Solution",
        "DNS",
        "IP",
        "QG Host ID",
        "OS",
        "NetBIOS",
        "FQDN",
    ]


def _read_csv_file(file_path: str, skip_rows: int, console):
    """
    Read and validate CSV file structure.

    :param str file_path: Path to the CSV file
    :param int skip_rows: Number of rows to skip
    :param console: Rich console instance for output
    :return: DataFrame if successful, None if failed
    :rtype: Optional[pd.DataFrame]
    """
    import pandas as pd

    console.print(f"[blue]Reading CSV file: {file_path}[/blue]")
    console.print(f"[blue]Skipping first {skip_rows} rows[/blue]")

    try:
        if skip_rows > 0:
            df = pd.read_csv(file_path, skiprows=skip_rows - 1, on_bad_lines="warn")
        else:
            df = pd.read_csv(file_path, on_bad_lines="warn")

        if df.empty:
            console.print("[red]❌ File is empty after skipping rows[/red]")
            return None

        if len(df.columns) == 0:
            console.print("[red]❌ No columns found in the file[/red]")
            return None

        console.print("[green]✅ Successfully read CSV file[/green]")
        console.print(f"[green]✅ Found {len(df.columns)} columns and {len(df)} rows[/green]")
        return df

    except pd.errors.EmptyDataError:
        console.print("[red]❌ File is empty or contains no parseable data[/red]")
        console.print("[yellow]💡 Try adjusting the skip_rows parameter[/yellow]")
        return None
    except pd.errors.ParserError as e:
        console.print(f"[red]❌ Error parsing CSV file: {e}[/red]")
        console.print("[yellow]💡 Check if the file is properly formatted CSV[/yellow]")
        return None


def _display_columns_table(df, console):
    """
    Display a table showing all columns found in the CSV.

    :param df: DataFrame containing the CSV data
    :param console: Rich console instance for output
    """
    from rich.table import Table

    table = Table(title="Columns Found in CSV")
    table.add_column("Index", style="cyan")
    table.add_column("Column Name", style="magenta")

    for i, col in enumerate(df.columns):
        table.add_row(str(i), str(col))

    console.print(table)


def _check_required_headers(df, console):
    """
    Check for required headers and display results.

    :param df: DataFrame containing the CSV data
    :param console: Rich console instance for output
    :return: Tuple of (missing_headers, found_headers)
    :rtype: tuple[list[str], list[str]]
    """
    required_headers = _get_required_headers()

    console.print("\n[blue]Checking for required headers:[/blue]")
    missing_headers = []
    found_headers = []

    for header in required_headers:
        if header in df.columns:
            found_headers.append(header)
            console.print(f"[green]✅ {header}[/green]")
        else:
            missing_headers.append(header)
            console.print(f"[red]❌ {header}[/red]")

    return missing_headers, found_headers


def _display_header_validation_summary(missing_headers, console):
    """
    Display summary of header validation results.

    :param list[str] missing_headers: List of missing required headers
    :param console: Rich console instance for output
    """
    if missing_headers:
        console.print(f"\n[yellow][WARN]  Missing {len(missing_headers)} required headers[/yellow]")
        console.print("[yellow]You may need to adjust the skip_rows parameter or check your file format[/yellow]")
    else:
        console.print("\n[green]🎉 All required headers found! File should import successfully.[/green]")


def _display_sample_data(df, console):
    """
    Display sample data from the CSV file.

    :param df: DataFrame containing the CSV data
    :param console: Rich console instance for output
    """
    from rich.table import Table

    if len(df) == 0:
        return

    console.print("\n[blue]Sample data (first 3 rows):[/blue]")
    sample_table = Table()

    # Add columns (limit to first 5 for readability)
    display_cols = list(df.columns)[:5]
    for col in display_cols:
        sample_table.add_column(str(col)[:20], style="cyan")

    # Add rows
    for i in range(min(3, len(df))):
        row_data = [str(df.iloc[i][col])[:30] for col in display_cols]
        sample_table.add_row(*row_data)

    console.print(sample_table)

    if len(df.columns) > 5:
        console.print(f"[dim]... and {len(df.columns) - 5} more columns[/dim]")


@qualys.command(name="validate_csv")
@click.option(
    "--file_path",
    "-f",
    type=click.Path(exists=True, file_okay=True, dir_okay=False, readable=True),
    required=True,
    help="Path to the Qualys CSV file to validate.",
    prompt="Enter the path to the Qualys CSV file",
)
@click.option(
    "--skip_rows",
    type=click.INT,
    help="The number of rows in the file to skip to get to the column headers, defaults to 129.",
    default=129,
)
def validate_csv(file_path: str, skip_rows: int):
    """
    Validate a Qualys CSV file format and headers before importing.

    This command helps troubleshoot CSV file issues by checking:
    - File readability
    - Proper column headers after skipping rows
    - Required headers presence
    """
    from rich.console import Console

    console = Console()
    console.print("\n[bold blue]Qualys CSV File Validator[/bold blue]\n")

    # Read and validate CSV structure
    df = _read_csv_file(file_path, skip_rows, console)
    if df is None:
        return

    # Display all columns found
    _display_columns_table(df, console)

    # Check for required headers
    missing_headers, _ = _check_required_headers(df, console)

    # Display validation summary
    _display_header_validation_summary(missing_headers, console)

    # Display sample data
    _display_sample_data(df, console)


qualys.add_command(get_asset_groups)
qualys.add_command(import_container_scans)
qualys.add_command(import_was_scans)


# ============================================================================
# Policy Management Commands
# ============================================================================


@qualys.command(name="list_policies")
@click.option("--output", "-o", type=click.Path(), help="Save output to JSON file")
def list_policies(output):
    """
    List all policies from Qualys.

    Example:
        regscale qualys list_policies
        regscale qualys list_policies --output policies.json
    """
    from rich.console import Console
    from rich.table import Table

    from regscale.integrations.commercial.qualys.policies import list_policies as fetch_policies
    from regscale.integrations.commercial.qualys.policies import save_policy_to_file

    console = Console()

    # Fetch policies from Qualys
    policies = fetch_policies()

    if not policies:
        logger.warning("No policies found in Qualys")
        return

    # Display policies in table
    table = Table(title="Qualys Policy Compliance Policies")
    table.add_column("Policy ID", style="cyan")
    table.add_column("Policy Name", style="green")
    table.add_column("Framework", style="yellow")
    table.add_column("Controls", style="magenta")

    for policy in policies:
        policy_id = str(policy.get("policyId", ""))
        policy_name = policy.get("policyName", "")
        framework = policy.get("framework", "")
        control_count = str(len(policy.get("controls", [])))
        table.add_row(policy_id, policy_name, framework, control_count)

    console.print(table)
    console.print(f"\n[green]Total: {len(policies)} policies[/green]")

    # Optionally save to file
    if output:
        try:
            save_policy_to_file(policies, output)
            console.print(f"\n[green][OK] Policies saved to: {output}[/green]")
        except Exception as e:
            logger.error("Failed to save policies to file: %s", e)
            error_and_exit(f"File write error: {e}")


@qualys.command(name="export_policy")
@click.option("--policy-id", "-id", required=True, help="Qualys policy ID to export")
@click.option("--output", "-o", type=click.Path(), help="Output JSON file path (required)")
def export_policy_cmd(policy_id, output):
    """
    Export a policy from Qualys to JSON format.

    Example:
        regscale qualys export_policy --policy-id 12345 --output policy.json
    """
    from rich.console import Console

    from regscale.integrations.commercial.qualys.policies import export_policy, save_policy_to_file

    console = Console()

    if not output:
        error_and_exit("--output/-o option is required. Please specify an output file path.")

    # Export policy from Qualys
    policy = export_policy(policy_id)

    if not policy:
        error_and_exit(f"Failed to export policy {policy_id}")

    # Save to file
    try:
        save_policy_to_file(policy, output)
        console.print("\n[green][OK] Policy exported successfully[/green]")
        console.print(f"  File: {output}")
        console.print(f"  Policy Name: {policy.get('policyName', 'N/A')}")
        console.print(f"  Framework: {policy.get('framework', 'N/A')}")
        console.print(f"  Controls: {len(policy.get('controls', []))}")
    except Exception as e:
        logger.error("Failed to save policy to file: %s", e)
        error_and_exit(f"File write error: {e}")


@qualys.command(name="import_policy")
@click.option("--policy-file", "-f", type=click.Path(exists=True), required=True, help="JSON file from export_policy")
@click.option("--plan-id", "-p", type=click.INT, help="RegScale Security Plan ID")
@click.option("--component-id", "-c", type=click.INT, help="RegScale Component ID")
@click.option("--policy-type", "-t", help="Override policy type")
@click.option("--status", "-s", default="Active", help="Policy status (Active/Draft/Archived)")
def import_policy_cmd(policy_file, plan_id, component_id, policy_type, status):
    """
    Import a Qualys policy JSON file into RegScale.

    Example:
        regscale qualys import_policy --policy-file policy.json --plan-id 123
        regscale qualys import_policy -f policy.json --component-id 456 --status Draft

    Note:
        Policy owner is automatically set to the current user from your RegScale authentication token.
    """
    from rich.console import Console
    from regscale.integrations.integration.policy_upload import PolicyUploader
    from regscale.integrations.commercial.qualys.policies import (
        convert_qualys_policy_to_regscale,
        load_policy_from_file,
    )

    console = Console()

    # Validate plan_id or component_id (mutually exclusive)
    if not plan_id and not component_id:
        error_and_exit("Either --plan-id/-p or --component-id/-c is required")
    if plan_id and component_id:
        error_and_exit("Cannot specify both --plan-id and --component-id (mutually exclusive)")

    parent_id = plan_id or component_id
    parent_module = "securityplans" if plan_id else "components"

    # Load policy from JSON file
    try:
        qualys_policy = load_policy_from_file(policy_file)
    except Exception as e:
        error_and_exit(f"Failed to load policy file: {e}")

    # Convert to PolicyUploadRequest
    upload_request = convert_qualys_policy_to_regscale(qualys_policy, parent_id, parent_module, policy_type, status)

    # Upload policy using the centralized PolicyUploader
    try:
        uploader = PolicyUploader()
        created_policy = uploader.upload_policy(upload_request)
        console.print("\n[green]Policy imported successfully[/green]")
        console.print(f"  RegScale Policy ID: {created_policy['id']}")
        console.print(f"  Title: {created_policy['title']}")
        console.print(f"  {parent_module.capitalize()}: {parent_id}")
        console.print(f"  Status: {created_policy['status']}")
    except Exception as e:
        logger.error("Failed to create policy in RegScale: %s", e)
        error_and_exit(f"RegScale API error: {e}")


@qualys.command(name="diagnostics")
@click.option(
    "--output", "-o", type=click.Path(), help="Output file path (default: qualys_diagnostics_<timestamp>.json)"
)
@click.option(
    "--fetch-samples",
    is_flag=True,
    default=False,
    help="Fetch sample data from APIs (hosts, containers, webapps) to validate data structure",
)
def diagnostics(output, fetch_samples):
    """
    Run comprehensive diagnostics on Qualys API integration.

    Tests authentication and module availability for:
    - JWT Authentication
    - Total Cloud Service
    - WAS (Web Application Scanning)
    - Container Security
    - VMDR (Vulnerability Management)

    Use --fetch-samples to retrieve actual data samples from each API.

    Example:
        regscale qualys diagnostics
        regscale qualys diagnostics --fetch-samples
        regscale qualys diagnostics --output /path/to/diagnostics.json
    """
    import os
    import tempfile
    from datetime import datetime
    from rich.console import Console
    from regscale.integrations.commercial.qualys.diagnostics import QualysDiagnostics

    console = Console()

    console.print("\n[bold]Qualys Integration Diagnostics[/bold]")
    console.print("=" * 80)

    # Get configuration
    config = _get_config()

    # Create a temporary init.yaml for the diagnostics class
    # (it expects a config file path, not a dict)
    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as temp_config:
        import yaml

        yaml.dump(config, temp_config)
        temp_config_path = temp_config.name

    try:
        # Determine output directory
        if output:
            output_dir = os.path.dirname(output) or "."
        else:
            output_dir = "./diagnostics_output"

        # Run all diagnostic tests
        if fetch_samples:
            console.print("\n[cyan]Running diagnostics with sample data collection...[/cyan]")
        else:
            console.print("\n[cyan]Running diagnostics...[/cyan]")

        # Initialize and run diagnostics
        diagnostics_runner = QualysDiagnostics(
            config_path=temp_config_path, output_dir=output_dir, verbose=False, modules=None
        )

        results = diagnostics_runner.run_diagnostics()

        # The QualysDiagnostics class handles printing, so we're done!
        console.print("\n[green][OK] Diagnostics complete![/green]")

        # Save reports
        if output:
            # User specified custom output path
            output_file = output
            if not output_file.endswith(".json"):
                output_file += ".json"

            with open(output_file, "w") as f:
                import json

                json.dump(
                    {
                        "timestamp": diagnostics_runner.start_time.isoformat(),
                        "duration_seconds": (
                            diagnostics_runner.end_time - diagnostics_runner.start_time
                        ).total_seconds(),
                        "config": {
                            "base_url": diagnostics_runner.base_url,
                            "username": diagnostics_runner.username,
                        },
                        "results": results,
                    },
                    f,
                    indent=2,
                )
            console.print(f"\n[green][OK] Custom report saved to: {output_file}[/green]")
        else:
            # Use default reports generated by diagnostics class
            diagnostics_runner.save_reports(save_json=not fetch_samples, save_text=True)

        console.print("=" * 80 + "\n")

    finally:
        # Cleanup temp config file
        try:
            os.unlink(temp_config_path)
        except Exception:
            pass


def _fetch_scans_parallel(module_list, module_func_map, days, console):
    """Fetch scans from multiple modules in parallel."""
    from concurrent.futures import ThreadPoolExecutor, as_completed

    all_scans = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_module = {
            executor.submit(func, days): mod for mod, func in module_func_map.items() if mod in module_list
        }

        for future in as_completed(future_to_module):
            mod = future_to_module[future]
            try:
                scans = future.result()
                if scans:
                    all_scans.extend(scans)
                    console.print(f"[green]✓[/green] {mod.upper()}: {len(scans)} scans found")
                else:
                    console.print(f"[yellow]○[/yellow] {mod.upper()}: No scans found")
            except Exception as e:
                console.print(f"[red]✗[/red] {mod.upper()}: Failed - {e}")
                logger.error("Failed to fetch scans from %s: %s", mod, e)
    return all_scans


def _create_scans_table(days):
    """Create Rich table for displaying scan results."""
    from rich.table import Table

    table = Table(title=f"Qualys Scans (Last {days} Days)", show_header=True, header_style="bold magenta")
    table.add_column("Module", style="cyan", width=12)
    table.add_column("Scan ID", style="yellow", width=15)
    table.add_column("Title", style="green", width=40)
    table.add_column("Status", style="magenta", width=12)
    table.add_column("Date", style="blue", width=19)
    table.add_column("Targets", justify="right", width=8)
    table.add_column("Findings", justify="right", width=9)
    return table


def _add_scan_to_table(table, scan):
    """Add a single scan row to the table."""
    scan_date_str = scan.scan_date.strftime("%Y-%m-%d %H:%M") if scan.scan_date else "N/A"
    title_truncated = scan.title[:37] + "..." if len(scan.title) > 40 else scan.title
    targets_str = str(scan.target_count) if scan.target_count is not None else "-"
    findings_str = str(scan.vuln_count) if scan.vuln_count is not None else "-"

    table.add_row(
        scan.module.upper(),
        scan.scan_id,
        title_truncated,
        scan.status,
        scan_date_str,
        targets_str,
        findings_str,
    )


def _save_scans_to_json(output, all_scans, module_list, days, console):
    """Save scan results to JSON file."""
    import json
    from datetime import datetime as dt

    output_data = {
        "timestamp": dt.now().isoformat(),
        "total_scans": len(all_scans),
        "filters": {"modules": module_list, "days": days},
        "scans": [scan.to_dict() for scan in all_scans],
    }

    try:
        with open(output, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2)
        console.print(f"\n[green]✓ Results saved to: {output}[/green]")
    except Exception as e:
        console.print(f"\n[red]✗ Failed to save results: {e}[/red]")
        logger.error("Failed to save JSON output: %s", e)


@qualys.command(name="list_scans")
@click.option(
    "--days",
    "-d",
    type=int,
    default=30,
    help="Number of days to look back for scans (default: 30)",
)
@click.option(
    "--module",
    "-m",
    type=click.Choice(["vmdr", "was", "container", "total_cloud", "all"], case_sensitive=False),
    multiple=True,
    default=["vmdr"],
    help="Module to list scans from (default: vmdr). Can specify multiple times for multiple modules.",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(),
    help="Save results to JSON file (optional)",
)
def list_scans(days, module, output):
    """
    List available scans and reports from Qualys modules.

    Fetches scan/report metadata from specified Qualys modules (VMDR, WAS, Container Security,
    Total Cloud) and displays in a formatted table or saves to JSON file.

    By default, lists VMDR scans from the last 30 days. Use --module flag to specify other
    modules or 'all' for all modules. Note that Container Security does not support date
    filtering and will be skipped when using the --days parameter.

    \b
    Examples:
        # List VMDR scans from last 30 days (default)
        regscale qualys list_scans

        # List VMDR scans from last 7 days
        regscale qualys list_scans --days 7

        # List WAS and Total Cloud scans
        regscale qualys list_scans --module was --module total_cloud

        # List all module scans and save to JSON
        regscale qualys list_scans --module all --output scans.json

        # Multiple modules with custom time range
        regscale qualys list_scans --module vmdr --module was --days 14
    """
    from datetime import datetime as dt
    from rich.console import Console

    from .vmdr import list_vmdr_reports
    from .was import list_was_scans
    from .containers import list_container_reports
    from .total_cloud_helpers import list_total_cloud_reports

    console = Console()

    # Normalize module list - expand 'all' to individual modules
    module_list = list(module)
    if "all" in module_list:
        module_list = ["vmdr", "was", "container", "total_cloud"]
    else:
        module_list = list(set(module_list))  # Remove duplicates

    # Map module names to functions
    module_func_map = {
        "vmdr": list_vmdr_reports,
        "was": list_was_scans,
        "container": list_container_reports,
        "total_cloud": list_total_cloud_reports,
    }

    console.print(f"\n[bold cyan]Fetching scans from last {days} days...[/bold cyan]\n")

    # Fetch scans from each module in parallel
    all_scans = _fetch_scans_parallel(module_list, module_func_map, days, console)

    # Sort all scans by date (newest first)
    all_scans.sort(key=lambda s: s.scan_date or dt.min, reverse=True)

    console.print(f"\n[bold]Total scans found: {len(all_scans)}[/bold]\n")

    # Display results
    if not all_scans:
        console.print("[yellow]No scans found in the specified time range.[/yellow]")
        return

    # Create and populate table
    table = _create_scans_table(days)
    for scan in all_scans:
        _add_scan_to_table(table, scan)

    console.print(table)

    # Save to JSON if output file specified
    if output:
        _save_scans_to_json(output, all_scans, module_list, days, console)

    console.print()  # Empty line at end
