"""
Utility functions for working with files and folders.
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Union, List, Iterator, Optional, IO, Any, TextIO


class S3FileDownloadError(Exception):
    """
    Exception raised when an error occurs during S3 file downloadÃŸ
    """

    pass


def is_s3_path(path: Union[str, Path]) -> bool:
    """
    Check if the given path is an S3 URI.

    :param Union[str, Path] path: The path to check
    :return: True if the path is an S3 URI, False otherwise
    :rtype: bool
    """
    return isinstance(path, str) and path.startswith("s3://")


def read_file(file_path: Union[str, Path]) -> str:
    """
    Read a file from local filesystem or S3.

    :param Union[str, Path] file_path: Path to the file or S3 URI
    :return: Content of the file
    :rtype: str
    """
    import smart_open  # type: ignore # Optimize import performance

    encodings = ["utf-8", "latin-1", "cp1252", "iso-8859-1"]

    for encoding in encodings:
        try:
            with smart_open.open(str(file_path), "r", encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue

    # Fallback to utf-8 with error replacement
    with smart_open.open(str(file_path), "r", encoding="utf-8", errors="replace") as f:
        return f.read()


def find_files(path: Union[str, Path], pattern: str) -> List[Union[Path, str]]:
    """
    Find all files matching the pattern in the given path, including S3.

    :param Union[str, Path] path: Path to a file, a folder, or an S3 URI
    :param str pattern: File pattern to match (e.g., "*.nessus")
    :return: List of Path objects for matching files or S3 URIs
    :rtype: List[Union[Path, str]]
    """
    import boto3  # type: ignore # Optimize import performance

    if is_s3_path(path):
        s3_parts = path[5:].split("/", 1)
        bucket = s3_parts[0]
        prefix = s3_parts[1] if len(s3_parts) > 1 else ""

        s3 = boto3.client("s3")
        paginator = s3.get_paginator("list_objects_v2")

        files: List[Union[Path, str]] = []
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                if obj["Key"].endswith(pattern.lstrip("*")):
                    files.append(f"s3://{bucket}/{obj['Key']}")
        return files

    file_path = Path(path)
    if file_path.is_file():
        return [file_path] if file_path.match(pattern) else []
    return list(file_path.glob(pattern))


def move_file(src: Union[str, Path], dst: Union[str, Path]) -> None:
    """
    Move a file from src to dst. Works with local files and S3.

    :param Union[str, Path] src: Source file path or S3 URI
    :param Union[str, Path] dst: Destination file path or S3 URI
    """
    import smart_open  # type: ignore # Optimize import performance

    if is_s3_path(src):
        import boto3  # type: ignore # Optimize import performance

        # S3 to S3 move
        if is_s3_path(dst):
            s3 = boto3.client("s3")
            src_parts = src[5:].split("/", 1)
            dst_parts = dst[5:].split("/", 1)
            s3.copy_object(
                CopySource={"Bucket": src_parts[0], "Key": src_parts[1]}, Bucket=dst_parts[0], Key=dst_parts[1]
            )
            s3.delete_object(Bucket=src_parts[0], Key=src_parts[1])
        else:
            # S3 to local
            with smart_open.open(src, "rb") as s_file, smart_open.open(dst, "wb") as d_file:
                d_file.write(s_file.read())
            s3 = boto3.client("s3")
            src_parts = src[5:].split("/", 1)
            s3.delete_object(Bucket=src_parts[0], Key=src_parts[1])
    else:
        # Local to local or local to S3
        with smart_open.open(src, "rb") as s_file, smart_open.open(dst, "wb") as d_file:
            d_file.write(s_file.read())
        if not isinstance(dst, str) or not dst.startswith("s3://"):
            os.remove(src)


def iterate_files(file_collection: List[Union[Path, str]]) -> Iterator[Union[Path, str]]:
    """
    Iterate over a collection of files, yielding each file path.

    :param List[Union[Path, str]] file_collection: List of file paths or S3 URIs
    :yield: Each file path or S3 URI
    :rtype: Iterator[Union[Path, str]]
    """
    for file in file_collection:
        yield file


def get_processed_file_path(file_path: Union[str, Path], processed_folder: str = "processed") -> Union[str, Path]:
    """
    Generate a path for the processed file, handling both local and S3 paths.

    :param Union[str, Path] file_path: Original file path or S3 URI
    :param str processed_folder: Name of the folder for processed files (default: "processed")
    :return: Path or S3 URI for the processed file
    :rtype: Union[str, Path]
    """
    if is_s3_path(file_path):
        s3_parts = file_path[5:].split("/")  # type: ignore  # is_s3_path ensures string
        bucket = s3_parts[0]
        key = "/".join(s3_parts[1:])
        new_key = f"processed/{os.path.basename(key)}"
        return f"s3://{bucket}/{new_key}"
    else:
        file_path = Path(file_path)
        timestamp = datetime.now().strftime("%Y%m%d-%I%M%S%p")
        new_filename = f"{file_path.stem}_{timestamp}{file_path.suffix}".replace(" ", "_")
        new_path = file_path.parent / processed_folder / new_filename
        os.makedirs(new_path.parent, exist_ok=True)
        return new_path


def get_files_by_folder(
    import_folder: Union[str, Path],
    exclude_non_scan_files: bool,
    file_excludes: Optional[list[str]] = None,
    directory_excludes: Optional[list[str]] = None,
) -> List[str]:
    """
    Retrieves a list of file paths from a specified folder, excluding empty files or files that match the excludes list.

    :param Union[str, Path] import_folder: The path to the folder from which to retrieve file paths.
    :param bool exclude_non_scan_files: exclude files that are not scan files
    :param Optional[List[str]] file_excludes: List of file extensions to exclude from the list of files
    :param Optional[List[str]] directory_excludes: List of directories to exclude from the list of files
    :return: A list of file paths for all non-empty files in the specified folder and subsequent subfolders.
    :rtype: List[str]
    """
    if file_excludes is None:
        file_excludes = []
    if directory_excludes is None:
        directory_excludes = []
    file_path_list = []
    if not os.path.isdir(import_folder):
        from regscale.core.app.logz import create_logger

        logger = create_logger()
        logger.error(f"Folder '{import_folder}' does not exist.")
        return file_path_list
    for root, dir, files in os.walk(import_folder):
        for file in files:
            file_path = os.path.join(root, file)
            if any(exclude_str in file_path for exclude_str in directory_excludes):
                continue
            if any(exclude_str in file for exclude_str in file_excludes) and exclude_non_scan_files:
                continue
            if os.path.getsize(file_path) == 0:
                continue
            file_path_list.append(os.path.join(root, file))
    return file_path_list


def get_file_stream(path: str) -> TextIO:
    """
    Get a file stream for either a local file or an S3 object.

    :param str path: The path to the file (local path or S3 URI in format s3://bucket/prefix)
    :return: A text file stream object
    :rtype: TextIO
    :raises ValueError: If the S3 path format is invalid
    :raises FileNotFoundError: If local file doesn't exist
    :raises Exception: For S3 access errors
    """
    import io
    from regscale.core.app.application import Application

    app = Application()
    config = app.config

    if is_s3_path(path):
        import boto3

        # Parse S3 path
        s3_parts = path[5:].split("/", 1)
        if len(s3_parts) != 2:
            raise ValueError(f"Invalid S3 path format: {path}. Expected s3://bucket/key")

        bucket, key = s3_parts

        # Create a session
        session = boto3.Session(
            aws_access_key_id=config["awsAccessKey"],
            aws_secret_access_key=config["awsSecretKey"],
            region_name=config["awsRegion"],
        )

        # Create S3 client
        s3 = session.client("s3")

        try:
            # Get object from S3
            response = s3.get_object(Bucket=bucket, Key=key)
            # Wrap the binary stream in a text wrapper
            return io.TextIOWrapper(response["Body"], encoding="utf-8")
        except ClientError as e:
            raise OSError(f"Error accessing S3 object {path}: {str(e)}")
    else:
        # Local file
        if not os.path.exists(path):
            raise FileNotFoundError(f"Local file not found: {path}")

        return open(path, "r", encoding="utf-8")


def download_from_s3(bucket: str, prefix: str, local_path: Union[str, os.PathLike], aws_profile: str = "none") -> None:
    """
    Downloads files from an S3 bucket to a local directory.

    :param str bucket: Name of the S3 bucket
    :param str prefix: Prefix (folder path) within the bucket
    :param Union[str, PathLike] local_path: Local directory to download files to
    :param str aws_profile: AWS profile to use for S3 access
    :rtype: None
    """
    import boto3
    import logging
    from botocore.exceptions import ClientError
    from regscale.core.app.application import Application

    logger = logging.getLogger(__name__)
    app = Application()
    config = app.config

    # Create a session
    if aws_profile and aws_profile != "none":
        # Create session using profile
        session = boto3.Session(profile_name=aws_profile)
    else:
        # Create a session using keys
        session = boto3.Session(
            aws_access_key_id=config["awsAccessKey"],
            aws_secret_access_key=config["awsSecretKey"],
            region_name=config["awsRegion"],
        )

    s3_client = session.client("s3")

    try:
        # Create local directory if it doesn't exist
        # os.makedirs(local_path, exist_ok=True)

        # List objects in bucket with given prefix
        paginator = s3_client.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            if "Contents" not in page:
                continue

            for obj in page["Contents"]:
                # Skip if object is a directory (ends with /)
                if obj["Key"].endswith("/"):
                    continue

                # Create the full local path, preserving directory structure
                relative_path = obj["Key"]
                if prefix:
                    # Remove the prefix from the key to get the relative path
                    relative_path = relative_path[len(prefix) :].lstrip("/")

                # Create the directory structure if it doesn't exist
                # local_file_path = os.path.join(local_path, relative_path)
                # os.makedirs(os.path.dirname(local_file_path), exist_ok=True)

                logger.info(f"Downloading {obj['Key']} to {local_path}")
                s3_client.download_file(bucket, obj["Key"], local_path)

    except ClientError as e:
        logger.error(f"Error downloading from S3: {str(e)}")
        raise S3FileDownloadError(f"Failed to download files from S3: {str(e)}")
