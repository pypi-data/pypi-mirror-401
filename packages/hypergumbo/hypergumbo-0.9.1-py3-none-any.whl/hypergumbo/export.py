"""Export capsule functionality for sharing analyzer configurations.

Exports the .hypergumbo capsule directory as a tarball, with optional
privacy-safe sanitization for sharing publicly.

How It Works
------------
1. **Non-shareable mode**: Simply tars up the .hypergumbo directory as-is
2. **Shareable mode**: Applies privacy redactions before archiving:
   - Strips repo_root path from capsule.json
   - Removes features[] from capsule_plan.json (repo-specific queries)
   - Removes repo-specific rules (non-glob patterns, entrypoints)
   - Excludes profile.json (contains repo structure info)
   - Generates SHAREABLE.txt with redaction summary
   - Generates SHA256SUMS for integrity verification

Why This Design
---------------
- Shareable capsules let users share analyzer configurations without
  exposing repository structure, file paths, or internal symbol names
- The redaction approach is conservative: anything that might identify
  the source repo is stripped
- Checksums enable verification that shared capsules weren't tampered with
"""
from __future__ import annotations

import hashlib
import io
import json
import tarfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Tuple

from . import __version__

SHAREABLE_FORMAT_VERSION = "0.1.0"

# Patterns that indicate a generic (non-repo-specific) rule
GENERIC_GLOB_PATTERNS = [
    "**/*",  # Contains wildcard prefix
    "**/",   # Directory wildcard
]


def is_repo_specific_rule(rule: Dict[str, Any]) -> bool:
    """Check if a rule contains repo-specific information.

    Generic rules use glob patterns that start with ** wildcards.
    Repo-specific rules reference specific paths, symbols, or patterns.

    Args:
        rule: Rule dict from capsule_plan.json

    Returns:
        True if the rule appears repo-specific
    """
    rule_type = rule.get("type", "")

    # Entrypoint patterns are always repo-specific
    if rule_type == "entrypoint_pattern":
        return True

    # Check glob patterns for wildcards
    glob = rule.get("glob", "")
    if glob:
        # Generic patterns START with ** (e.g., **/*_test.py, **/node_modules/**)
        # Patterns like "my_project/**" are repo-specific despite having **
        if glob.startswith("**"):
            return False
        # Patterns not starting with ** are likely repo-specific paths
        return True

    return True  # Default to repo-specific if unclear


def sanitize_plan(plan: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, int]]:
    """Sanitize a capsule plan for sharing.

    Removes repo-specific information:
    - features[] entirely (queries reference repo symbols/routes)
    - rules[] entries that don't use generic glob patterns

    Args:
        plan: Original capsule_plan.json contents

    Returns:
        Tuple of (sanitized plan dict, removed counts dict)
    """
    sanitized = plan.copy()
    removed = {
        "features_count": 0,
        "rules_removed": 0,
    }

    # Remove all features (commonly repo-specific)
    original_features = plan.get("features", [])
    removed["features_count"] = len(original_features)
    sanitized["features"] = []

    # Filter rules to keep only generic ones
    original_rules = plan.get("rules", [])
    generic_rules = []
    for rule in original_rules:
        if not is_repo_specific_rule(rule):
            generic_rules.append(rule)

    removed["rules_removed"] = len(original_rules) - len(generic_rules)
    sanitized["rules"] = generic_rules

    return sanitized, removed


def sanitize_capsule(capsule: Dict[str, Any]) -> Dict[str, Any]:
    """Sanitize capsule.json for sharing.

    Strips repo_root and other path information.

    Args:
        capsule: Original capsule.json contents

    Returns:
        Sanitized capsule dict
    """
    sanitized = capsule.copy()

    # Redact repo_root path
    if "repo_root" in sanitized:
        sanitized["repo_root"] = "<redacted>"

    return sanitized


def create_shareable_txt(
    removed: Dict[str, int],
    checksums: Dict[str, str],
) -> str:
    """Generate SHAREABLE.txt content.

    Documents what was redacted and includes metadata for verification.

    Args:
        removed: Counts of removed items from sanitize_plan
        checksums: SHA256 checksums of included files

    Returns:
        SHAREABLE.txt content as string
    """
    timestamp = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

    lines = [
        "# Shareable Capsule Export",
        f"# Generated by hypergumbo {__version__}",
        f"# Timestamp: {timestamp}",
        "",
        f"shareable_format_version: {SHAREABLE_FORMAT_VERSION}",
        "",
        "# Redactions Applied",
        "# -------------------",
        "# The following repo-specific content was removed:",
        "",
        f"features_removed: {removed['features_count']}",
        f"rules_removed: {removed['rules_removed']}",
        "repo_root: redacted",
        "profile.json: excluded",
        "",
        "# Integrity",
        "# ---------",
        "# See SHA256SUMS for file checksums",
        "",
    ]

    return "\n".join(lines)


def compute_checksums(files: List[Path]) -> Dict[str, str]:
    """Compute SHA256 checksums for files.

    Args:
        files: List of file paths to checksum

    Returns:
        Dict mapping filename to hex checksum
    """
    checksums = {}
    for file_path in files:
        if file_path.exists():
            content = file_path.read_bytes()
            checksum = hashlib.sha256(content).hexdigest()
            checksums[file_path.name] = checksum
    return checksums


def export_capsule(
    repo_root: Path,
    out_path: Path,
    shareable: bool = False,
) -> None:
    """Export the capsule directory as a tarball.

    Args:
        repo_root: Path to repository root
        out_path: Output tarball path
        shareable: If True, apply privacy redactions
    """
    capsule_dir = repo_root / ".hypergumbo"

    # Ensure output parent directory exists
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with tarfile.open(out_path, "w:gz") as tar:
        if shareable:
            _export_shareable(tar, capsule_dir)
        else:
            _export_full(tar, capsule_dir)


def _export_full(tar: tarfile.TarFile, capsule_dir: Path) -> None:
    """Export capsule without sanitization.

    Args:
        tar: Open tarfile to write to
        capsule_dir: Path to .hypergumbo directory
    """
    for file_path in capsule_dir.iterdir():
        if file_path.is_file():
            tar.add(file_path, arcname=file_path.name)


def _export_shareable(tar: tarfile.TarFile, capsule_dir: Path) -> None:
    """Export capsule with privacy sanitization.

    Args:
        tar: Open tarfile to write to
        capsule_dir: Path to .hypergumbo directory
    """
    # Files to exclude in shareable mode
    excluded = {"profile.json", "cache"}

    # Track files for checksums
    exported_files: List[Path] = []

    # Process capsule.json
    capsule_path = capsule_dir / "capsule.json"
    if capsule_path.exists():
        capsule = json.loads(capsule_path.read_text())
        sanitized_capsule = sanitize_capsule(capsule)
        _add_json_to_tar(tar, "capsule.json", sanitized_capsule)
        exported_files.append(capsule_path)

    # Process capsule_plan.json
    plan_path = capsule_dir / "capsule_plan.json"
    removed = {"features_count": 0, "rules_removed": 0}
    if plan_path.exists():
        plan = json.loads(plan_path.read_text())
        sanitized_plan, removed = sanitize_plan(plan)
        _add_json_to_tar(tar, "capsule_plan.json", sanitized_plan)
        exported_files.append(plan_path)

    # Add any other files (except excluded)
    for file_path in capsule_dir.iterdir():
        if file_path.is_file() and file_path.name not in excluded:
            if file_path.name not in ("capsule.json", "capsule_plan.json"):
                tar.add(file_path, arcname=file_path.name)
                exported_files.append(file_path)

    # Compute checksums of original files
    checksums = compute_checksums(exported_files)

    # Generate and add SHAREABLE.txt
    shareable_content = create_shareable_txt(removed, checksums)
    _add_text_to_tar(tar, "SHAREABLE.txt", shareable_content)

    # Generate and add SHA256SUMS
    checksum_lines = [f"{v}  {k}" for k, v in sorted(checksums.items())]
    checksums_content = "\n".join(checksum_lines) + "\n"
    _add_text_to_tar(tar, "SHA256SUMS", checksums_content)


def _add_json_to_tar(tar: tarfile.TarFile, name: str, data: Dict[str, Any]) -> None:
    """Add a JSON object to the tarball.

    Args:
        tar: Open tarfile to write to
        name: Filename in archive
        data: JSON-serializable data
    """
    content = json.dumps(data, indent=2).encode("utf-8")
    _add_bytes_to_tar(tar, name, content)


def _add_text_to_tar(tar: tarfile.TarFile, name: str, content: str) -> None:
    """Add a text file to the tarball.

    Args:
        tar: Open tarfile to write to
        name: Filename in archive
        content: Text content
    """
    _add_bytes_to_tar(tar, name, content.encode("utf-8"))


def _add_bytes_to_tar(tar: tarfile.TarFile, name: str, content: bytes) -> None:
    """Add bytes to the tarball.

    Args:
        tar: Open tarfile to write to
        name: Filename in archive
        content: Binary content
    """
    info = tarfile.TarInfo(name=name)
    info.size = len(content)
    tar.addfile(info, io.BytesIO(content))
