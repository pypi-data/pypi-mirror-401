name: Test Quality Gates

on:
  pull_request:
    paths:
      - "tests/**"
      - "pyproject.toml"
      - "scripts/validate_test_markers.py"
      - "scripts/check_test_isolation.py"
  push:
    branches: [main]
    paths:
      - "tests/**"
      - "pyproject.toml"
  workflow_dispatch:
  schedule:
    # Run weekly on Sunday at 4 AM UTC to detect test order dependencies
    - cron: "0 4 * * 0"

# Auto-cancel outdated runs for the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Minimal permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write

env:
  PYTHON_VERSION: "3.12"

jobs:
  # ==========================================================================
  # Marker Validation
  # ==========================================================================
  marker-validation:
    name: Validate Test Markers
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --dev

      - name: Validate markers (strict mode)
        run: |
          uv run python scripts/validate_test_markers.py --strict

      - name: Check for unregistered markers
        if: failure()
        run: |
          echo "::error::Test marker validation failed. Run 'uv run python \
          scripts/validate_test_markers.py --fix' to auto-fix missing markers."

  # ==========================================================================
  # Test Isolation Check
  # ==========================================================================
  test-isolation:
    name: Check Test Isolation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Check isolation (sample 15 files)
        run: |
          uv run python scripts/check_test_isolation.py --sample 15
        continue-on-error: false # Strict isolation enforcement

      - name: Upload isolation check results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: isolation-check-results
          path: |
            isolation-*.log
          retention-days: 30
          if-no-files-found: ignore

  # ==========================================================================
  # Test Coverage Markers
  # ==========================================================================
  coverage-markers:
    name: Verify Coverage Markers
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Check marker distribution
        run: |
          python scripts/validate_test_markers.py | tee marker-report.txt

      - name: Analyze marker coverage
        run: |
          echo "=== Marker Distribution Summary ==="
          grep -A 10 "Level Marker Distribution:" marker-report.txt || true
          echo ""
          echo "=== Files Missing Markers ==="
          grep "Files missing required markers:" marker-report.txt || true

      - name: Upload marker report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: marker-distribution-report
          path: marker-report.txt
          retention-days: 30

  # ==========================================================================
  # Randomized Test Order (weekly)
  # ==========================================================================
  randomized-tests:
    name: Randomized Test Order Detection
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 1

      - name: Set up uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run tests with random order (run 1)
        run: |
          uv run pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadfile \
            --hypothesis-profile=ci \
            --randomly-seed=auto \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run1.xml

      - name: Run tests with random order (run 2)
        run: |
          uv run pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadfile \
            --hypothesis-profile=ci \
            --randomly-seed=auto \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run2.xml

      - name: Run tests with random order (run 3)
        run: |
          uv run pytest tests/unit \
            -v \
            -m "unit and not slow and not memory_intensive" \
            -n 4 \
            --maxprocesses=4 \
            --dist loadfile \
            --hypothesis-profile=ci \
            --randomly-seed=auto \
            --benchmark-disable \
            --tb=short \
            --maxfail=20 \
            --junit-xml=randomized-run3.xml

      - name: Upload randomized test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: randomized-test-results
          path: |
            randomized-*.xml
          retention-days: 30

      - name: Create issue if failures detected
        if: failure()
        uses: actions/github-script@v8
        with:
          script: |
            const body = `## Test Order Dependency Detected

            The weekly randomized test run has detected potential test order dependencies.

            **What this means:**
            - Some tests may be passing/failing based on execution order
            - Tests should be isolated and not depend on state from other tests

            **Next steps:**
            1. Review the failed test results in the workflow artifacts
            2. Identify which tests failed with randomized order
            3. Fix the test isolation issues
            4. Use \`pytest --randomly-seed=last\` to reproduce the failure locally

            **Workflow run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            `;

            // Check if issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'test-order-dependency'
            });

            if (issues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: '[Test Quality] Test Order Dependencies Detected',
                body: body,
                labels: ['test-order-dependency', 'bug', 'testing']
              });
            }

  # ==========================================================================
  # Final status check
  # ==========================================================================
  quality-gates-success:
    name: Quality Gates Success
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [marker-validation, test-isolation, coverage-markers]
    if: always()
    steps:
      - name: Check all jobs
        run: |
          echo "Marker Validation: ${{ needs.marker-validation.result }}"
          echo "Test Isolation: ${{ needs.test-isolation.result }}"
          echo "Coverage Markers: ${{ needs.coverage-markers.result }}"

          # Marker validation must pass
          if [[ "${{ needs.marker-validation.result }}" != "success" ]]; then
            echo "::error::Marker validation failed - this is a required gate"
            exit 1
          fi

          # Other checks can warn but not fail
          if [[ "${{ needs.test-isolation.result }}" == "failure" ]]; then
            echo "::warning::Test isolation check found issues"
          fi

          if [[ "${{ needs.coverage-markers.result }}" != "success" ]]; then
            echo "::warning::Coverage marker check had issues"
          fi

          echo "Quality gates passed!"
