"""Comprehensive validation tests for analyze() API.

Tests the analyze() API against all generated test data to validate:
- Correct input type detection for all file formats
- Appropriate domain selection based on input type
- Graceful handling of edge cases (empty, NaN, Inf, extreme values)
- Error handling for corrupted/malformed data
- Complete report generation for valid inputs

Test data generated by: scripts/generate_comprehensive_test_data.py
"""

import json
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import numpy as np
import pytest

from tracekit.reporting.analyze import (
    UnsupportedFormatError,
    _detect_input_type_from_file,
    analyze,
)
from tracekit.reporting.config import (
    AnalysisConfig,
    AnalysisDomain,
    AnalysisError,
    AnalysisResult,
    InputType,
)

pytestmark = pytest.mark.integration


# Test data base path
TEST_DATA_DIR = (
    Path(__file__).parent.parent.parent.parent / "test_data" / "comprehensive_validation"
)


def _mock_engine_result(domains: list[AnalysisDomain] | None = None):
    """Create mock engine result for testing."""
    if domains is None:
        domains = [AnalysisDomain.WAVEFORM]
    return {
        "results": {d: {"status": "ok"} for d in domains},
        "errors": [],
        "stats": {
            "total_analyses": len(domains),
            "successful_analyses": len(domains),
            "failed_analyses": 0,
            "skipped_analyses": 0,
        },
    }


class TestInputTypeDetection:
    """Tests for input type detection across all file formats."""

    def test_npz_detected_as_waveform(self):
        """Test .npz files are detected as waveform input."""
        path = Path("/data/signal.npz")
        assert _detect_input_type_from_file(path) == InputType.WAVEFORM

    def test_csv_detected_as_waveform(self):
        """Test .csv files are detected as waveform input."""
        path = Path("/data/signal.csv")
        assert _detect_input_type_from_file(path) == InputType.WAVEFORM

    def test_wav_detected_as_waveform(self):
        """Test .wav files are detected as waveform input."""
        path = Path("/data/audio.wav")
        assert _detect_input_type_from_file(path) == InputType.WAVEFORM

    def test_hdf5_detected_as_waveform(self):
        """Test .hdf5 files are detected as waveform input."""
        for ext in [".hdf5", ".h5"]:
            path = Path(f"/data/capture{ext}")
            assert _detect_input_type_from_file(path) == InputType.WAVEFORM

    def test_vcd_detected_as_digital(self):
        """Test .vcd files are detected as digital input."""
        path = Path("/data/logic.vcd")
        assert _detect_input_type_from_file(path) == InputType.DIGITAL

    def test_sr_detected_as_digital(self):
        """Test .sr files are detected as digital input."""
        path = Path("/data/capture.sr")
        assert _detect_input_type_from_file(path) == InputType.DIGITAL

    def test_bin_detected_as_binary(self):
        """Test .bin files are detected as binary input."""
        path = Path("/data/firmware.bin")
        assert _detect_input_type_from_file(path) == InputType.BINARY

    def test_raw_detected_as_binary(self):
        """Test .raw files are detected as binary input."""
        path = Path("/data/dump.raw")
        assert _detect_input_type_from_file(path) == InputType.BINARY

    def test_pcap_detected_as_pcap(self):
        """Test .pcap files are detected as PCAP input."""
        path = Path("/data/capture.pcap")
        assert _detect_input_type_from_file(path) == InputType.PCAP

    def test_pcapng_detected_as_pcap(self):
        """Test .pcapng files are detected as PCAP input."""
        path = Path("/data/capture.pcapng")
        assert _detect_input_type_from_file(path) == InputType.PCAP

    def test_unknown_extension_raises(self):
        """Test unknown extensions raise UnsupportedFormatError."""
        path = Path("/data/unknown.xyz")
        with pytest.raises(UnsupportedFormatError):
            _detect_input_type_from_file(path)


class TestWaveformInputs:
    """Tests for waveform input processing."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result(
                [
                    AnalysisDomain.WAVEFORM,
                    AnalysisDomain.SPECTRAL,
                    AnalysisDomain.STATISTICS,
                ]
            )
            mock_class.return_value = mock_engine
            yield mock_engine

    @pytest.mark.skipif(not (TEST_DATA_DIR / "waveform").exists(), reason="Test data not generated")
    def test_clean_sine_input(self, mock_engine):
        """Test clean sine wave input is processed correctly."""
        test_file = TEST_DATA_DIR / "waveform" / "clean_sine_1000hz.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.output_dir.exists()
            assert result.input_type == InputType.WAVEFORM

    @pytest.mark.skipif(not (TEST_DATA_DIR / "waveform").exists(), reason="Test data not generated")
    def test_noisy_input(self, mock_engine):
        """Test noisy waveform input is processed correctly."""
        test_file = TEST_DATA_DIR / "waveform" / "noisy_20db.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.WAVEFORM

    @pytest.mark.skipif(not (TEST_DATA_DIR / "waveform").exists(), reason="Test data not generated")
    def test_clipped_input(self, mock_engine):
        """Test clipped waveform input is processed correctly."""
        test_file = TEST_DATA_DIR / "waveform" / "clipped.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "waveform").exists(), reason="Test data not generated")
    def test_dc_input(self, mock_engine):
        """Test DC waveform input is processed correctly."""
        test_file = TEST_DATA_DIR / "waveform" / "dc_1.0v.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)


class TestReportingComprehensiveValidationEdgeCases:
    """Tests for edge case handling."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result()
            mock_class.return_value = mock_engine
            yield mock_engine

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_empty_waveform(self, mock_engine):
        """Test empty waveform is handled gracefully."""
        test_file = TEST_DATA_DIR / "edge_cases" / "empty.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            # Empty data should still produce a result (possibly with errors logged)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_single_sample(self, mock_engine):
        """Test single sample waveform is handled."""
        test_file = TEST_DATA_DIR / "edge_cases" / "single_sample.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_nan_values(self, mock_engine):
        """Test waveform with NaN values is handled."""
        test_file = TEST_DATA_DIR / "edge_cases" / "all_nan.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_inf_values(self, mock_engine):
        """Test waveform with Inf values is handled."""
        test_file = TEST_DATA_DIR / "edge_cases" / "all_inf.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_extreme_small_values(self, mock_engine):
        """Test waveform with extremely small values."""
        test_file = TEST_DATA_DIR / "edge_cases" / "extreme_small.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_extreme_large_values(self, mock_engine):
        """Test waveform with extremely large values."""
        test_file = TEST_DATA_DIR / "edge_cases" / "extreme_large.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)
            assert isinstance(result, AnalysisResult)


class TestDigitalInputs:
    """Tests for digital input processing."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result(
                [
                    AnalysisDomain.DIGITAL,
                    AnalysisDomain.TIMING,
                    AnalysisDomain.PROTOCOLS,
                ]
            )
            mock_class.return_value = mock_engine
            yield mock_engine

    @pytest.mark.skipif(not (TEST_DATA_DIR / "digital").exists(), reason="Test data not generated")
    def test_clean_uart(self, mock_engine):
        """Test clean UART VCD is processed correctly."""
        test_file = TEST_DATA_DIR / "digital" / "uart_clean_9600.vcd"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.DIGITAL

    @pytest.mark.skipif(not (TEST_DATA_DIR / "digital").exists(), reason="Test data not generated")
    def test_noisy_uart(self, mock_engine):
        """Test noisy UART VCD is processed."""
        test_file = TEST_DATA_DIR / "digital" / "uart_noisy_5pct.vcd"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "digital").exists(), reason="Test data not generated")
    def test_framing_error_uart(self, mock_engine):
        """Test UART with framing errors is processed."""
        test_file = TEST_DATA_DIR / "digital" / "uart_framing_error.vcd"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "digital").exists(), reason="Test data not generated")
    def test_spi_protocol(self, mock_engine):
        """Test SPI VCD is processed correctly."""
        test_file = TEST_DATA_DIR / "digital" / "spi_1mhz.vcd"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)


class TestBinaryInputs:
    """Tests for binary input processing."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result(
                [
                    AnalysisDomain.ENTROPY,
                    AnalysisDomain.PATTERNS,
                    AnalysisDomain.INFERENCE,
                ]
            )
            mock_class.return_value = mock_engine
            yield mock_engine

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_clean_packets(self, mock_engine):
        """Test clean packet binary is processed correctly."""
        test_file = TEST_DATA_DIR / "binary" / "packets_clean.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.BINARY

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_corrupted_packets(self, mock_engine):
        """Test corrupted packet binary is handled."""
        test_file = TEST_DATA_DIR / "binary" / "packets_corrupted_5pct.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_packets_with_gaps(self, mock_engine):
        """Test packets with sequence gaps is handled."""
        test_file = TEST_DATA_DIR / "binary" / "packets_with_gaps.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_bad_checksums(self, mock_engine):
        """Test packets with bad checksums is handled."""
        test_file = TEST_DATA_DIR / "binary" / "packets_bad_checksums.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_high_entropy_random(self, mock_engine):
        """Test high entropy random data is processed."""
        test_file = TEST_DATA_DIR / "binary" / "random_high_entropy.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_low_entropy_pattern(self, mock_engine):
        """Test low entropy pattern data is processed."""
        test_file = TEST_DATA_DIR / "binary" / "pattern_low_entropy.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_empty_binary(self, mock_engine):
        """Test empty binary file is handled."""
        test_file = TEST_DATA_DIR / "binary" / "empty.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_single_byte(self, mock_engine):
        """Test single byte binary file is handled."""
        test_file = TEST_DATA_DIR / "binary" / "single_byte.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(input_path=test_file, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)


class TestInMemoryData:
    """Tests for in-memory data processing."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result()
            mock_class.return_value = mock_engine
            yield mock_engine

    def test_numpy_array_waveform(self, mock_engine):
        """Test numpy array as waveform input."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 1000)
        mock_trace.voltage = np.sin(2 * np.pi * 100 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.WAVEFORM

    def test_bytes_as_binary(self, mock_engine):
        """Test bytes as binary input."""
        data = b"\x00\x01\x02\x03" * 100

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(data=data, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.BINARY

    def test_empty_bytes(self, mock_engine):
        """Test empty bytes is handled."""
        data = b""

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False, continue_on_error=True)
            result = analyze(data=data, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    def test_packet_list(self, mock_engine):
        """Test list of packets as input."""

        class MockPacket:
            def __init__(self, ts: float, payload: bytes):
                self.timestamp = ts
                self.data = payload

        packets = [MockPacket(i * 0.001, bytes([i % 256])) for i in range(100)]

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(data=packets, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            assert result.input_type == InputType.PACKETS


class TestConfigurationOptions:
    """Tests for configuration options."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result()
            mock_class.return_value = mock_engine
            yield mock_engine

    def test_specific_domains_only(self, mock_engine):
        """Test running only specific domains."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(
                domains=[AnalysisDomain.SPECTRAL],
                generate_plots=False,
            )
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    def test_exclude_domains(self, mock_engine):
        """Test excluding specific domains."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(
                exclude_domains=[AnalysisDomain.JITTER, AnalysisDomain.EYE],
                generate_plots=False,
            )
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)

    def test_output_formats(self, mock_engine):
        """Test different output formats."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(
                output_formats=["json", "yaml"],
                index_formats=["html", "md"],
                generate_plots=False,
            )
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            # Verify summary.json exists
            assert result.summary_json is not None
            assert result.summary_json.exists()

    def test_continue_on_error(self, mock_engine):
        """Test continue_on_error option."""
        # Simulate an error in one domain using AnalysisError objects
        mock_engine.run.return_value = {
            "results": {AnalysisDomain.WAVEFORM: {"status": "ok"}},
            "errors": [
                AnalysisError(
                    domain=AnalysisDomain.SPECTRAL,
                    function="compute_fft",
                    error_type="ValueError",
                    error_message="Invalid input",
                    traceback=None,
                    duration_ms=10,
                )
            ],
            "stats": {
                "total_analyses": 2,
                "successful_analyses": 1,
                "failed_analyses": 1,
                "skipped_analyses": 0,
            },
        }

        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(
                continue_on_error=True,
                generate_plots=False,
            )
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            assert isinstance(result, AnalysisResult)
            # Should complete despite error
            assert result.failed_analyses == 1
            assert result.successful_analyses == 1


class TestProgressCallback:
    """Tests for progress callback functionality."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result()
            mock_class.return_value = mock_engine
            yield mock_engine

    def test_progress_callback_receives_updates(self, mock_engine):
        """Test progress callback receives all expected updates."""
        progress_updates = []

        def on_progress(info):
            progress_updates.append(info)

        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            analyze(
                data=mock_trace,
                output_dir=tmpdir,
                config=config,
                progress_callback=on_progress,
            )

        # Verify we got updates for key phases
        phases = [u.phase for u in progress_updates]
        assert "initializing" in phases
        assert "complete" in phases

    def test_progress_percent_increases(self, mock_engine):
        """Test progress percent increases over time."""
        progress_updates = []

        def on_progress(info):
            progress_updates.append(info)

        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            analyze(
                data=mock_trace,
                output_dir=tmpdir,
                config=config,
                progress_callback=on_progress,
            )

        # First update should be 0, last should be 100
        if progress_updates:
            assert progress_updates[0].percent == 0.0
            assert progress_updates[-1].percent == 100.0


class TestOutputDirectoryCreation:
    """Tests for output directory creation."""

    @pytest.fixture
    def mock_engine(self):
        """Mock analysis engine."""
        with patch("tracekit.reporting.engine.AnalysisEngine") as mock_class:
            mock_engine = MagicMock()
            mock_engine.run.return_value = _mock_engine_result()
            mock_class.return_value = mock_engine
            yield mock_engine

    def test_creates_timestamped_directory(self, mock_engine):
        """Test timestamped directory is created."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            # Directory name should have timestamp format
            dirname = result.output_dir.name
            assert "_analysis" in dirname
            # First part should be YYYYMMDD
            date_part = dirname.split("_")[0]
            assert len(date_part) == 8
            assert date_part.isdigit()

    def test_output_dir_contains_required_files(self, mock_engine):
        """Test output directory contains required files."""
        mock_trace = MagicMock()
        mock_trace.time = np.linspace(0, 1, 100)
        mock_trace.voltage = np.sin(2 * np.pi * 10 * mock_trace.time)

        with tempfile.TemporaryDirectory() as tmpdir:
            config = AnalysisConfig(generate_plots=False)
            result = analyze(data=mock_trace, output_dir=tmpdir, config=config)

            # Check for expected files
            assert result.summary_json is not None
            assert result.metadata_json is not None
            assert result.config_yaml is not None


class TestDataLoadingFromTestFiles:
    """Tests that verify test data files can be loaded correctly."""

    @pytest.mark.skipif(not (TEST_DATA_DIR / "waveform").exists(), reason="Test data not generated")
    def test_npz_loads_correctly(self):
        """Test NPZ files contain expected data structure."""
        test_file = TEST_DATA_DIR / "waveform" / "clean_sine_1000hz.npz"
        if not test_file.exists():
            pytest.skip("Test file not found")

        data = np.load(test_file, allow_pickle=True)
        assert "time" in data or "t" in data or len(data.files) > 0

    @pytest.mark.skipif(
        not (TEST_DATA_DIR / "edge_cases").exists(), reason="Test data not generated"
    )
    def test_edge_case_json_metadata(self):
        """Test edge case files have corresponding JSON metadata."""
        json_file = TEST_DATA_DIR / "edge_cases" / "empty.json"
        if not json_file.exists():
            pytest.skip("Test file not found")

        with open(json_file) as f:
            metadata = json.load(f)

        assert "description" in metadata or "type" in metadata or len(metadata) > 0

    @pytest.mark.skipif(not (TEST_DATA_DIR / "binary").exists(), reason="Test data not generated")
    def test_binary_files_readable(self):
        """Test binary files are readable."""
        test_file = TEST_DATA_DIR / "binary" / "packets_clean.bin"
        if not test_file.exists():
            pytest.skip("Test file not found")

        data = test_file.read_bytes()
        assert len(data) > 0

    @pytest.mark.skipif(not (TEST_DATA_DIR / "digital").exists(), reason="Test data not generated")
    def test_vcd_files_readable(self):
        """Test VCD files contain valid VCD header."""
        test_file = TEST_DATA_DIR / "digital" / "uart_clean_9600.vcd"
        if not test_file.exists():
            pytest.skip("Test file not found")

        content = test_file.read_text()
        # VCD files should start with $date, $version, or $timescale
        assert "$" in content[:100]
