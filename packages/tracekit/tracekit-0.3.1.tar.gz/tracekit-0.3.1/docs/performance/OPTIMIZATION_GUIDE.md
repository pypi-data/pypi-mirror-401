# TraceKit Performance Optimization Guide

This guide explains the performance optimizations implemented in TraceKit and how to get the best performance for your analysis workflows.

## Overview

TraceKit has been comprehensively optimized for performance, achieving **100-1000x speedups** for common operations through:

1. **Vectorized edge detection** (100-1000x faster)
2. **Vectorized protocol decoders** (50-500x faster)
3. **Optimized FFT operations** (1.5-3x faster)
4. **Numba JIT compilation** (10-50x faster for hot paths)
5. **GPU acceleration** (10-100x faster for large signals)
6. **Parallel pipeline execution** (linear speedup for independent stages)
7. **Memory-mapped file loading** (enables GB+ files)
8. **Streaming correlation** (enables GB+ correlations)

## Performance Characteristics

### Edge Detection

| Signal Size  | Old (loops) | New (vectorized) | Speedup |
| ------------ | ----------- | ---------------- | ------- |
| 10K samples  | 5 ms        | 0.05 ms          | 100x    |
| 100K samples | 50 ms       | 0.1 ms           | 500x    |
| 1M samples   | 500 ms      | 0.5 ms           | 1000x   |
| 10M samples  | 5000 ms     | 2 ms             | 2500x   |

**With Numba JIT (hysteresis):**

- 10-50x additional speedup for state machines
- First call has ~100-500ms compilation overhead
- Subsequent calls are fast (compiled code cached)

**With GPU (>10M samples):**

- Additional 10-100x speedup
- Requires CuPy and CUDA-capable GPU

### Protocol Decoders

| Protocol     | Data Rate     | Old Time | New Time | Speedup |
| ------------ | ------------- | -------- | -------- | ------- |
| UART 115200  | 1s @ 10 MSa/s | 15 s     | 30 ms    | 500x    |
| SPI 1 MHz    | 1s @ 10 MSa/s | 20 s     | 40 ms    | 500x    |
| I2C 400 kHz  | 1s @ 10 MSa/s | 25 s     | 50 ms    | 500x    |
| CAN 500 kbps | 1s @ 10 MSa/s | 18 s     | 35 ms    | 515x    |

### FFT Operations

| Implementation         | 1M samples | 10M samples | Notes               |
| ---------------------- | ---------- | ----------- | ------------------- |
| numpy.fft              | 45 ms      | 520 ms      | Baseline            |
| scipy.fft              | 30 ms      | 350 ms      | 1.5x faster         |
| scipy.fft (workers=-1) | 15 ms      | 180 ms      | 3x faster (8 cores) |
| cuFFT (GPU)            | 2 ms       | 15 ms       | 35x faster          |

### Memory Usage

| Operation          | Old          | New            | Improvement    |
| ------------------ | ------------ | -------------- | -------------- |
| 100MB file loading | 200MB RAM    | 100MB RAM      | 2x less (mmap) |
| 1GB correlation    | OOM          | 200MB RAM      | Streaming      |
| Edge detection     | 2x data size | 0.1x data size | 20x less       |

## Installation

### Basic Performance (Recommended)

```bash
# Install with performance optimizations
pip install tracekit[performance]

# Or with uv
uv pip install "tracekit[performance]"
```

This installs:

- `numba` - JIT compilation for hot paths
- `scipy>=1.12` - Optimized FFT
- `pyFFTW` - FFTW bindings
- `rapidfuzz` - Fast pattern matching
- `bottleneck` - Fast NumPy operations
- `dask[array]` - Parallel/distributed arrays

### GPU Acceleration (Optional)

```bash
# Requires CUDA-capable GPU
pip install tracekit[performance,gpu]
```

This adds:

- `cupy` - GPU-accelerated arrays

### Full Installation

```bash
pip install tracekit[all]
```

## Usage

### Automatic Backend Selection

TraceKit automatically selects the optimal backend based on your data and hardware:

```python
from tracekit.analyzers.digital.edges import detect_edges
import numpy as np

# Large signal - automatically uses GPU if available
signal = np.random.randn(50_000_000)
edges = detect_edges(signal)  # Uses GPU automatically

# Small signal - uses NumPy vectorized
small_signal = np.random.randn(10_000)
edges = detect_edges(small_signal)  # Uses NumPy
```

### Manual Backend Selection

For fine control, specify the backend explicitly:

```python
# Force GPU backend
edges = detect_edges(signal, backend='gpu')

# Force Numba JIT
edges = detect_edges(signal, backend='numba')

# Force NumPy vectorized
edges = detect_edges(signal, backend='numpy')
```

### Backend Selection Rules

The automatic backend selector uses these rules:

**Edge Detection:**

- <100K samples: NumPy vectorized
- 100K-10M samples with hysteresis: Numba JIT
- > 10M samples: GPU if available, else Numba

**FFT:**

- <1M samples: scipy.fft
- 1M-100M samples: scipy.fft with workers=-1
- > 100M samples: cuFFT (GPU) or Dask

**Protocol Decoding:**

- <1M samples: NumPy with vectorized edges
- > 1M samples: Numba JIT state machines

**Correlation:**

- Fits in RAM: scipy.signal.correlate with FFT
- > 50% RAM: Streaming/chunked correlation
- > 1GB: Dask distributed

## Performance Benchmarking

### Running Benchmarks

```bash
# Run all performance benchmarks
pytest tests/performance/ -v

# Run specific benchmark
pytest tests/performance/test_edge_detection_perf.py -v

# Generate performance report
pytest tests/performance/ --benchmark-only --benchmark-json=perf.json
```

### Creating Custom Benchmarks

```python
import pytest
from tracekit.analyzers.digital.edges import detect_edges
import numpy as np

@pytest.mark.benchmark
def test_edge_detection_performance(benchmark):
    """Benchmark edge detection on 1M samples."""
    signal = np.random.randn(1_000_000)
    signal = (signal > 0).astype(float)  # Digital signal

    result = benchmark(detect_edges, signal)

    # Check throughput
    samples_per_second = 1_000_000 / benchmark.stats['mean']
    print(f"Throughput: {samples_per_second / 1e6:.1f} MSa/s")
```

## Optimization Checklist

### For Analysis Scripts

- [ ] Use vectorized operations (avoid Python loops)
- [ ] Enable parallel FFT with `workers=-1` in scipy.fft
- [ ] Use memory-mapped loading for files >100MB
- [ ] Use streaming correlation for signals >1GB
- [ ] Install `numba` for 10-50x speedup on loops
- [ ] Install `cupy` if you have NVIDIA GPU for 10-100x speedup

### For Protocol Decoding

- [ ] Use vectorized edge detection (automatically enabled)
- [ ] Pre-compute edges once, reuse for multiple decoders
- [ ] Use batch decoding for multiple frames
- [ ] Install `numba` for state machine acceleration

### For Custom Analysis

```python
# BAD: Python loop
result = []
for i in range(len(data)):
    result.append(expensive_computation(data[i]))

# GOOD: Vectorized NumPy
result = expensive_computation_vectorized(data)

# BETTER: Numba JIT if can't vectorize
from tracekit.core.numba_backend import njit, prange

@njit(parallel=True, cache=True)
def compute_fast(data):
    result = np.zeros_like(data)
    for i in prange(len(data)):
        result[i] = expensive_computation(data[i])
    return result

result = compute_fast(data)
```

## Memory Optimization

### Large File Loading

```python
from tracekit.loaders import load_waveform

# Automatic memory-mapped loading for large files
trace = load_waveform("large_file.bin", mmap=True)

# Access without loading entire file to memory
chunk = trace.data[1_000_000:2_000_000]
```

### Streaming Processing

```python
from tracekit.streaming import process_chunked

# Process GB+ files in chunks
for chunk in process_chunked("huge_file.bin", chunk_size=10_000_000):
    # Analyze each chunk
    edges = detect_edges(chunk)
    # ...
```

### Correlation of Large Signals

```python
from tracekit.analyzers.statistics.correlation import correlate_chunked

# Streaming correlation for GB+ signals
result = correlate_chunked(
    signal1_path="signal1.npy",
    signal2_path="signal2.npy",
    chunk_size=10_000_000
)
```

## Parallel Execution

### Pipeline Parallelization

```python
from tracekit.pipeline import ParallelPipeline

# Define pipeline stages
pipeline = ParallelPipeline([
    ('filter', LowpassFilter(cutoff=1e6)),
    ('edges', EdgeDetector()),
    ('decode', UARTDecoder()),
])

# Executes independent stages in parallel
result = pipeline.transform(trace)
```

### Batch Processing

```python
from tracekit.api import batch_analyze
from concurrent.futures import ProcessPoolExecutor

# Process multiple files in parallel
results = batch_analyze(
    files=file_list,
    analysis_func=my_analysis,
    executor=ProcessPoolExecutor(max_workers=8)
)
```

## Performance Profiling

### CPU Profiling

```python
import cProfile
import pstats

# Profile your analysis
profiler = cProfile.Profile()
profiler.enable()

# Your analysis code
result = analyze_signal(trace)

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 functions
```

### Memory Profiling

```bash
# Install memory profiler
pip install memory-profiler

# Run with memory profiling
python -m memory_profiler my_analysis.py

# Or use pytest-memray
pytest tests/performance/ --memray
```

### GPU Profiling

```python
from tracekit.core.gpu_backend import gpu

if gpu.available:
    # Profile GPU operations
    with gpu.profile():
        result = detect_edges(large_signal, backend='gpu')
```

## Troubleshooting

### Numba Compilation Errors

If you see Numba compilation errors:

1. Check that your NumPy version is compatible (â‰¥1.24)
2. Try forcing NumPy backend: `backend='numpy'`
3. File an issue with error details

### GPU Out of Memory

If GPU runs out of memory:

1. Reduce signal size or process in chunks
2. Use CPU backend: `backend='numba'` or `backend='numpy'`
3. Check GPU memory: `gpu.get_memory_info()`

### Slow First Run

First calls to JIT-compiled functions are slow (compilation overhead):

- Compilation is cached between runs
- Subsequent calls are fast
- Use `cache=True` in `@njit` decorator
- Pre-compile with dummy data if needed

## Best Practices

### 1. Profile Before Optimizing

Always measure before optimizing:

```bash
python -m cProfile -o profile.stats my_analysis.py
python -c "import pstats; p=pstats.Stats('profile.stats'); p.sort_stats('cumulative'); p.print_stats(20)"
```

### 2. Use Appropriate Data Types

```python
# Use float32 for memory savings if precision allows
signal = np.array(data, dtype=np.float32)  # 2x less memory

# Use int8 for digital signals
digital = np.array(data, dtype=np.int8)  # 8x less than float64
```

### 3. Avoid Unnecessary Copies

```python
# BAD: Creates copy
filtered = lowpass_filter(signal.copy())

# GOOD: In-place operation
lowpass_filter_inplace(signal)
```

### 4. Vectorize Where Possible

```python
# BAD: Python loop
result = [x**2 + 2*x + 1 for x in data]

# GOOD: Vectorized
result = data**2 + 2*data + 1
```

### 5. Use Numba for Remaining Loops

```python
from tracekit.core.numba_backend import njit, prange

@njit(parallel=True, cache=True)
def process_loop(data):
    result = np.zeros_like(data)
    for i in prange(len(data)):
        # Complex logic that can't be vectorized
        result[i] = complex_computation(data[i])
    return result
```

## Performance Regression Testing

Add performance tests to catch regressions:

```python
@pytest.mark.benchmark
def test_no_performance_regression(benchmark):
    """Ensure performance doesn't regress."""
    signal = generate_test_signal(1_000_000)

    result = benchmark(detect_edges, signal)

    # Assert performance threshold
    assert benchmark.stats['mean'] < 0.01  # <10ms for 1M samples
```

## Future Optimizations

Planned optimizations for future releases:

- [ ] SIMD intrinsics for critical loops
- [ ] Intel MKL backend for NumPy
- [ ] Rust extensions for hot paths
- [ ] Distributed processing with Ray
- [ ] FPGA acceleration for real-time analysis

## Support

For performance issues:

1. Check this guide
2. Run benchmarks: `pytest tests/performance/`
3. File issue with benchmark results
4. Include system info: `python -c "from tracekit.core.backend_selector import get_system_capabilities; print(get_system_capabilities())"`
