import asyncio
from dataclasses import dataclass, field
from typing import Any

from asyncmy import Pool, create_pool

from .base_driver import BaseDriver
from .retry_utils import RetryStrategy, calculate_retry_delay


@dataclass
class MySQLDriver(BaseDriver):
    """MySQL-based queue driver with transactional dequeue and dead-letter support.

    Architecture:
        - Tasks stored in configurable table (default: task_queue)
        - Dead-letter table for failed tasks (default: dead_letter_queue)
        - Transactional dequeue using SELECT ... FOR UPDATE SKIP LOCKED
        - BLOB payload storage for binary data
        - DATETIME for delay calculations
        - Visibility timeout to handle worker crashes

    Features:
        - Concurrent workers with SKIP LOCKED
        - Dead-letter queue for failed tasks
        - Configurable retry logic with exponential backoff
        - Visibility timeout for crash recovery
        - Connection pooling with asyncmy
        - Auto-recovery of stuck tasks via poll loop

    Requirements:
        - MySQL 8.0+ (SKIP LOCKED support)
        - InnoDB storage engine (for row-level locking)
        - asyncmy library
    """

    dsn: str = "mysql://user:pass@localhost:3306/dbname"
    queue_table: str = "task_queue"
    dead_letter_table: str = "dead_letter_queue"
    retry_strategy: RetryStrategy = "exponential"
    retry_delay_seconds: int = 60
    min_pool_size: int = 10
    max_pool_size: int = 10
    keep_completed_tasks: bool = False
    warmup_connections: int = 0
    pool: Pool | None = field(default=None, init=False, repr=False)
    _receipt_handles: dict[bytes, int] = field(default_factory=dict, init=False, repr=False)

    async def connect(self) -> None:
        """Initialize asyncmy connection pool with configurable size."""
        if self.pool is None:
            self.pool = await create_pool(
                host=self._parse_host(),
                port=self._parse_port(),
                user=self._parse_user(),
                password=self._parse_password(),
                db=self._parse_database(),
                minsize=self.min_pool_size,
                maxsize=self.max_pool_size,
            )

            # Pre-establish connections to avoid cold-start latency
            if self.warmup_connections > 0:
                await self._warmup_connection_pool(self.warmup_connections)

    async def _warmup_connection_pool(self, n: int) -> None:
        """Pre-establish N connections to avoid cold-start latency (~4-5ms per connection).

        The asyncmy client creates connections on-demand, which adds latency on first use.
        This method forces the pool to create connections upfront for faster subsequent operations.
        """
        assert self.pool is not None

        async def ping() -> bool:
            assert self.pool is not None
            async with self.pool.acquire() as conn:
                async with conn.cursor() as cur:
                    await cur.execute("SELECT 1")
            return True

        await asyncio.gather(*[ping() for _ in range(n)])

    async def disconnect(self) -> None:
        """Close connection pool and cleanup."""
        if self.pool:
            self.pool.close()
            await self.pool.wait_closed()
            self.pool = None
        self._receipt_handles.clear()

    def _parse_host(self) -> str:
        """Parse host from DSN."""
        # Parse mysql://user:pass@host:port/dbname
        if "://" in self.dsn:
            part = self.dsn.split("://")[1]
            if "@" in part:
                part = part.split("@")[1]
            if ":" in part:
                return part.split(":")[0]
            if "/" in part:
                return part.split("/")[0]
            return part
        return "localhost"

    def _parse_port(self) -> int:
        """Parse port from DSN."""
        if "://" in self.dsn:
            part = self.dsn.split("://")[1]
            if "@" in part:
                part = part.split("@")[1]
            if ":" in part and "/" in part:
                port_str = part.split(":")[1].split("/")[0]
                return int(port_str)
        return 3306

    def _parse_user(self) -> str:
        """Parse user from DSN."""
        if "://" in self.dsn:
            part = self.dsn.split("://")[1]
            if "@" in part:
                user_part = part.split("@")[0]
                if ":" in user_part:
                    return user_part.split(":")[0]
                return user_part
        return "root"

    def _parse_password(self) -> str:
        """Parse password from DSN."""
        if "://" in self.dsn:
            part = self.dsn.split("://")[1]
            if "@" in part:
                user_part = part.split("@")[0]
                if ":" in user_part:
                    return user_part.split(":")[1]
        return ""

    def _parse_database(self) -> str:
        """Parse database name from DSN."""
        if "://" in self.dsn:
            part = self.dsn.split("://")[1]
            if "/" in part:
                return part.split("/")[1].split("?")[0]
        return "test_db"

    async def init_schema(self) -> None:
        """Initialize database schema for queue and dead-letter tables.

        Creates tables if they don't exist. Safe to call multiple times (idempotent).
        Should be called once during application setup.

        Raises:
            asyncmy.Error: If table creation fails
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                # Create queue table
                await cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS {self.queue_table} (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        queue_name VARCHAR(255) NOT NULL,
                        payload BLOB NOT NULL,
                        available_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
                        locked_until DATETIME(6) NULL,
                        status VARCHAR(50) NOT NULL DEFAULT 'pending',
                        current_attempt INT NOT NULL DEFAULT 0,
                        max_attempts INT NOT NULL DEFAULT 3,
                        visibility_timeout_seconds INT NOT NULL DEFAULT 3600,
                        created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
                        updated_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
                        INDEX idx_{self.queue_table}_lookup (queue_name, status, available_at, locked_until)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                """)

                # Add visibility_timeout_seconds column if it doesn't exist (migration)
                await cursor.execute(f"""
                    SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA = DATABASE()
                    AND TABLE_NAME = '{self.queue_table}'
                    AND COLUMN_NAME = 'visibility_timeout_seconds'
                """)
                row = await cursor.fetchone()
                if row and row[0] == 0:
                    await cursor.execute(f"""
                        ALTER TABLE {self.queue_table}
                        ADD COLUMN visibility_timeout_seconds INT NOT NULL DEFAULT 3600
                    """)

                # Create dead-letter table
                await cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS {self.dead_letter_table} (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        queue_name VARCHAR(255) NOT NULL,
                        payload BLOB NOT NULL,
                        current_attempt INT NOT NULL,
                        error_message TEXT,
                        failed_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                """)

    async def enqueue(
        self,
        queue_name: str,
        task_data: bytes,
        delay_seconds: int = 0,
        current_attempt: int = 0,
        visibility_timeout: int = 3600,
        max_attempts: int = 3,
    ) -> None:
        """Add task to queue with optional delay.

        Args:
            queue_name: Name of the queue
            task_data: Serialized task data
            delay_seconds: Seconds to delay task visibility (0 = immediate)
            current_attempt: Current attempt number (0 for first attempt)
            visibility_timeout: Crash recovery timeout in seconds (default: 3600)
            max_attempts: Maximum retry attempts for this task (default: 3)
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        query = f"""
            INSERT INTO {self.queue_table}
                (queue_name, payload, available_at, status, current_attempt, max_attempts, visibility_timeout_seconds, created_at)
            VALUES (%s, %s, DATE_ADD(NOW(6), INTERVAL %s SECOND), 'pending', %s, %s, %s, NOW(6))
        """
        async with self.pool.acquire() as conn:
            await conn.begin()
            try:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        query,
                        (
                            queue_name,
                            task_data,
                            delay_seconds,
                            current_attempt,
                            max_attempts,
                            visibility_timeout,
                        ),
                    )
                await conn.commit()
            except Exception:
                await conn.rollback()
                raise

    async def dequeue(self, queue_name: str, poll_seconds: int = 0) -> bytes | None:
        """Retrieve next available task with transactional locking and polling support.

        Args:
            queue_name: Name of the queue
            poll_seconds: Seconds to poll for task (0 = non-blocking)

        Returns:
            Serialized task data (bytes) or None if no tasks available

        Implementation:
            - Uses SELECT FOR UPDATE SKIP LOCKED for concurrent workers
            - Sets visibility timeout (locked_until) to prevent duplicate processing
            - Implements polling with 200ms interval if poll_seconds > 0
            - Auto-recovers stuck tasks (locked_until expired)
            - Stores task_data -> task_id mapping for ack/nack operations
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        deadline = None
        if poll_seconds > 0:
            loop = asyncio.get_running_loop()
            deadline = loop.time() + poll_seconds

        while True:
            # Select and lock a task that's ready to process
            # Includes: 1) pending tasks that are ready and not locked
            #           2) processing tasks with expired locks (stuck/crashed workers)
            query = f"""
                SELECT id, payload, visibility_timeout_seconds FROM {self.queue_table}
                WHERE queue_name = %s
                  AND (
                    (status = 'pending' AND available_at <= NOW(6) AND (locked_until IS NULL OR locked_until < NOW(6)))
                    OR (status = 'processing' AND locked_until < NOW(6))
                  )
                ORDER BY created_at
                LIMIT 1
                FOR UPDATE SKIP LOCKED
            """

            async with self.pool.acquire() as conn:
                await conn.begin()
                try:
                    async with conn.cursor() as cursor:
                        await cursor.execute(query, (queue_name,))
                        row = await cursor.fetchone()

                        if row:
                            task_id = row[0]
                            task_data = bytes(row[1])
                            visibility_timeout_seconds = row[2]

                            # Update status and set visibility timeout using the per-task value
                            # Increment current_attempt ONLY if status was 'pending' (new attempt)
                            # If status was 'processing', it's a retry of a stuck task (don't increment again)
                            # NOTE: Use IF() and set current_attempt BEFORE status to evaluate against old status value
                            await cursor.execute(
                                f"""
                                UPDATE {self.queue_table}
                                SET current_attempt = IF(status = 'pending', current_attempt + 1, current_attempt),
                                    status = 'processing',
                                    locked_until = DATE_ADD(NOW(6), INTERVAL %s SECOND),
                                    updated_at = NOW(6)
                                WHERE id = %s
                                """,
                                (visibility_timeout_seconds, task_id),
                            )

                            # Store mapping from task_data to task_id for ack/nack
                            # Note: Uses task_data as key (matching SQS/RabbitMQ pattern)
                            self._receipt_handles[task_data] = task_id

                            await conn.commit()
                            return task_data
                        else:
                            await conn.rollback()
                except Exception:
                    await conn.rollback()
                    raise

            # No task found - check if we should poll
            if poll_seconds == 0:
                return None

            if deadline is not None:
                loop = asyncio.get_running_loop()
                if loop.time() >= deadline:
                    return None

                # Sleep for 200ms before next poll
                await asyncio.sleep(0.2)
            else:
                return None

    async def ack(self, queue_name: str, receipt_handle: bytes) -> None:
        """Acknowledge successful processing and mark task as completed.

        Args:
            queue_name: Name of the queue (unused but required by protocol)
            receipt_handle: Receipt handle from dequeue (UUID bytes)

        Implementation:
            If keep_completed_tasks is True: Updates status to 'completed' to maintain task history.
            If keep_completed_tasks is False: Deletes the task to save storage space.
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        task_id = self._receipt_handles.get(receipt_handle)
        if task_id:
            async with self.pool.acquire() as conn:
                await conn.begin()
                try:
                    async with conn.cursor() as cursor:
                        if self.keep_completed_tasks:
                            await cursor.execute(
                                f"UPDATE {self.queue_table} SET status = 'completed', updated_at = NOW(6) WHERE id = %s",
                                (task_id,),
                            )
                        else:
                            await cursor.execute(
                                f"DELETE FROM {self.queue_table} WHERE id = %s",
                                (task_id,),
                            )
                    await conn.commit()
                except Exception:
                    await conn.rollback()
                    raise
            self._receipt_handles.pop(receipt_handle, None)

    async def nack(self, queue_name: str, receipt_handle: bytes) -> None:
        """Reject task for retry or move to dead letter queue.

        Args:
            queue_name: Name of the queue (unused but required by protocol)
            receipt_handle: Receipt handle from dequeue (UUID bytes)

        Implementation:
            - Checks current_attempt (already incremented by dequeue)
            - If current_attempt < max_attempts: requeue with backoff, keep attempt count
            - If current_attempt >= max_attempts: move to dead letter queue
            - Does NOT increment attempt (dequeue already did)
            - Does NOT update payload (task object's _current_attempt may desync)

        Note:
            nack() reuses the original payload without updating serialized metadata.
            This means the task's _current_attempt in the payload may be stale.
            This is acceptable for permanent failures (deserialization errors) since
            the task never successfully runs. The DB current_attempt remains authoritative
            for enforcing max_attempts limits.
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        task_id = self._receipt_handles.get(receipt_handle)
        if not task_id:
            return

        async with self.pool.acquire() as conn:
            await conn.begin()
            try:
                async with conn.cursor() as cursor:
                    # Get current attempt
                    await cursor.execute(
                        f"SELECT current_attempt, max_attempts, queue_name, payload FROM {self.queue_table} WHERE id = %s",
                        (task_id,),
                    )
                    row = await cursor.fetchone()

                    if row:
                        existing_attempt = row[0]
                        max_attempts = row[1]
                        task_queue_name = row[2]
                        payload = row[3]

                        # Check if task can be retried (dequeue already incremented the attempt)
                        if existing_attempt < max_attempts:
                            # Calculate retry delay based on strategy (fixed or exponential)
                            retry_delay = calculate_retry_delay(
                                self.retry_strategy, self.retry_delay_seconds, existing_attempt
                            )
                            await cursor.execute(
                                f"""
                                UPDATE {self.queue_table}
                                SET available_at = DATE_ADD(NOW(6), INTERVAL %s SECOND),
                                    status = 'pending',
                                    locked_until = NULL,
                                    updated_at = NOW(6)
                                WHERE id = %s
                                """,
                                (retry_delay, task_id),
                            )
                        else:
                            await cursor.execute(
                                f"""
                                INSERT INTO {self.dead_letter_table}
                                    (queue_name, payload, current_attempt, error_message, failed_at)
                                VALUES (%s, %s, %s, 'Max attempts exceeded', NOW(6))
                                """,
                                (task_queue_name, payload, existing_attempt),
                            )
                            await cursor.execute(
                                f"DELETE FROM {self.queue_table} WHERE id = %s", (task_id,)
                            )
                await conn.commit()
            except Exception:
                await conn.rollback()
                raise

        self._receipt_handles.pop(receipt_handle, None)

    async def mark_failed(self, queue_name: str, receipt_handle: bytes) -> None:
        """Mark task as permanently failed (move to dead letter queue).

        Args:
            queue_name: Name of the queue
            receipt_handle: Receipt handle from dequeue (UUID bytes)

        Implementation:
            Moves task to dead_letter_queue and removes from main queue.
            Should be called when a task fails permanently (no more retries).
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        task_id = self._receipt_handles.get(receipt_handle)
        if not task_id:
            return

        async with self.pool.acquire() as conn:
            await conn.begin()
            try:
                async with conn.cursor() as cursor:
                    # Get task data
                    await cursor.execute(
                        f"SELECT queue_name, payload, current_attempt FROM {self.queue_table} WHERE id = %s",
                        (task_id,),
                    )
                    row = await cursor.fetchone()

                    if row:
                        task_queue_name = row[0]
                        payload = row[1]
                        current_attempt = row[2]

                        # Move to dead letter queue
                        await cursor.execute(
                            f"""
                            INSERT INTO {self.dead_letter_table}
                                (queue_name, payload, current_attempt, error_message, failed_at)
                            VALUES (%s, %s, %s, 'Permanently failed', NOW(6))
                            """,
                            (task_queue_name, payload, current_attempt),
                        )
                        # Delete from main queue
                        await cursor.execute(
                            f"DELETE FROM {self.queue_table} WHERE id = %s", (task_id,)
                        )
                await conn.commit()
            except Exception:
                await conn.rollback()
                raise

        self._receipt_handles.pop(receipt_handle, None)

    async def get_queue_size(
        self, queue_name: str, include_delayed: bool, include_in_flight: bool
    ) -> int:
        """Get number of tasks in queue based on parameters.

        Args:
            queue_name: Name of the queue
            include_delayed: Include delayed tasks (available_at > NOW())
            include_in_flight: Include in-flight/processing tasks

        Returns:
            Count of tasks based on the inclusion parameters:
            - Both False: Only ready tasks (pending, available_at <= NOW(), not locked)
            - include_delayed=True: Ready + delayed tasks
            - include_in_flight=True: Ready + in-flight tasks
            - Both True: Ready + delayed + in-flight tasks
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        # Build WHERE clause based on parameters
        conditions = ["queue_name = %s"]

        if include_delayed and include_in_flight:
            # All tasks: pending (ready + delayed) + processing
            conditions.append("(status = 'pending' OR status = 'processing')")
        elif include_delayed:
            # All pending tasks (ready + delayed), not locked
            conditions.append("status = 'pending'")
        elif include_in_flight:
            # Ready tasks + processing
            conditions.append(
                "((status = 'pending' AND available_at <= NOW(6) AND (locked_until IS NULL OR locked_until < NOW(6))) OR status = 'processing')"
            )
        else:
            # Only ready tasks (pending, available, not locked)
            conditions.append("status = 'pending'")
            conditions.append("available_at <= NOW(6)")
            conditions.append("(locked_until IS NULL OR locked_until < NOW(6))")

        where_clause = " AND ".join(conditions)
        query = f"SELECT COUNT(*) FROM {self.queue_table} WHERE {where_clause}"

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(query, (queue_name,))
                row = await cursor.fetchone()
                return row[0] if row else 0

    async def get_queue_stats(self, queue: str) -> dict[str, Any]:
        """Return basic stats for a single queue."""
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        # depth = ready pending tasks
        depth_q = f"SELECT COUNT(*) FROM {self.queue_table} WHERE queue_name=%s AND status='pending' AND available_at <= NOW(6) AND (locked_until IS NULL OR locked_until < NOW(6))"
        processing_q = (
            f"SELECT COUNT(*) FROM {self.queue_table} WHERE queue_name=%s AND status='processing'"
        )
        failed_q = f"SELECT COUNT(*) FROM {self.dead_letter_table} WHERE queue_name=%s"

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(depth_q, (queue,))
                depth_row = await cursor.fetchone()
                depth = int(depth_row[0]) if depth_row else 0

                await cursor.execute(processing_q, (queue,))
                proc_row = await cursor.fetchone()
                processing = int(proc_row[0]) if proc_row else 0

                await cursor.execute(failed_q, (queue,))
                failed_row = await cursor.fetchone()
                failed_total = int(failed_row[0]) if failed_row else 0

        return {
            "name": queue,
            "depth": depth,
            "processing": processing,
            "completed_total": 0,
            "failed_total": failed_total,
            "avg_duration_ms": None,
            "throughput_per_minute": None,
        }

    async def get_all_queue_names(self) -> list[str]:
        """Return list of distinct queue names."""
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        q = f"SELECT DISTINCT queue_name FROM {self.queue_table}"
        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(q)
                rows = await cursor.fetchall()
                return [r[0] for r in rows] if rows else []

    async def get_global_stats(self) -> dict[str, int]:
        """Return simple global stats across all queues."""
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        pending_q = f"SELECT COUNT(*) FROM {self.queue_table} WHERE status='pending'"
        processing_q = f"SELECT COUNT(*) FROM {self.queue_table} WHERE status='processing'"
        failed_q = f"SELECT COUNT(*) FROM {self.dead_letter_table}"
        total_q = f"SELECT COUNT(*) FROM {self.queue_table}"

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(pending_q)
                pending = (await cursor.fetchone())[0]
                await cursor.execute(processing_q)
                processing = (await cursor.fetchone())[0]
                await cursor.execute(failed_q)
                failed = (await cursor.fetchone())[0]
                await cursor.execute(total_q)
                total = (await cursor.fetchone())[0]

        return {
            "pending": int(pending),
            "running": int(processing),
            "failed": int(failed),
            "total": int(total),
        }

    async def get_running_tasks(self, limit: int = 50, offset: int = 0) -> list[tuple[bytes, str]]:
        """Return list of tasks currently processing as raw payloads."""
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        q = f"SELECT payload, queue_name FROM {self.queue_table} WHERE status='processing' ORDER BY updated_at DESC LIMIT %s OFFSET %s"
        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(q, (limit, offset))
                rows = await cursor.fetchall()

        return [(bytes(row[0]), row[1]) for row in rows or []]

    async def get_tasks(
        self,
        status: str | None = None,
        queue: str | None = None,
        limit: int = 50,
        offset: int = 0,
    ) -> tuple[list[tuple[bytes, str, str]], int]:
        """Return list of tasks with pagination and total count.

        Returns:
            Tuple of (list of (payload_bytes, queue_name, status), total_count)
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        conditions = []
        params: list = []
        if status:
            conditions.append("status = %s")
            params.append(status)
        if queue:
            conditions.append("queue_name = %s")
            params.append(queue)

        where = " AND ".join(conditions) if conditions else "1=1"

        q = f"SELECT payload, queue_name, status FROM {self.queue_table} WHERE {where} ORDER BY created_at DESC LIMIT %s OFFSET %s"
        count_q = f"SELECT COUNT(*) FROM {self.queue_table} WHERE {where}"

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(q, (*params, limit, offset))
                rows = await cursor.fetchall()
                await cursor.execute(count_q, tuple(params))
                total = (await cursor.fetchone())[0]

        tasks: list[tuple[bytes, str, str]] = []
        for row in rows or []:
            payload, queue_name, status_ = row
            tasks.append((bytes(payload), queue_name, status_))

        return tasks, int(total)

    async def get_task_by_id(self, task_id: str) -> bytes | None:
        """Return raw task payload by id (searches queue table).

        Returns:
            Raw payload bytes or None if not found
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        q = f"SELECT payload FROM {self.queue_table} WHERE id = %s"
        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(q, (task_id,))
                row = await cursor.fetchone()
                if not row:
                    return None

        return bytes(row[0])

    async def retry_task(self, task_id: str) -> bool:
        """Retry a failed task from dead-letter table by moving it back to the queue.

        Returns True if retried, False if not found.
        """
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        # Dead-letter table columns: id, queue_name, payload, current_attempt, error_message, failed_at
        sel = f"SELECT queue_name, payload, current_attempt FROM {self.dead_letter_table} WHERE id = %s"
        ins = f"INSERT INTO {self.queue_table} (queue_name, payload, available_at, status, current_attempt, max_attempts, created_at) VALUES (%s, %s, NOW(6), 'pending', %s, %s, NOW(6))"
        del_q = f"DELETE FROM {self.dead_letter_table} WHERE id = %s"

        async with self.pool.acquire() as conn:
            await conn.begin()
            try:
                async with conn.cursor() as cursor:
                    await cursor.execute(sel, (task_id,))
                    row = await cursor.fetchone()
                    if not row:
                        await conn.rollback()
                        return False
                    # row may contain exactly (queue_name, payload, current_attempt)
                    # or additional columns depending on driver/schema; be defensive
                    if len(row) >= 3:
                        queue_name, payload, current_attempt = row[0], row[1], row[2]
                    else:
                        # Unexpected shape - treat as not found
                        await conn.rollback()
                        return False
                    # current_attempt should be present; default to 1 defensively
                    # Default max_attempts from TaskDefaultsConfig when retrying from DLQ
                    from asynctasq.config import Config

                    config = Config.get()
                    await cursor.execute(
                        ins,
                        (
                            queue_name,
                            payload,
                            1 if current_attempt is None else current_attempt,
                            config.task_defaults.max_attempts,
                        ),
                    )
                    await cursor.execute(del_q, (task_id,))
                await conn.commit()
                return True
            except Exception:
                await conn.rollback()
                raise

    async def delete_task(self, task_id: str) -> bool:
        """Delete a task from queue or dead-letter tables. Returns True if deleted."""
        if self.pool is None:
            await self.connect()
            assert self.pool is not None

        del_q = f"DELETE FROM {self.queue_table} WHERE id = %s"
        del_dlq = f"DELETE FROM {self.dead_letter_table} WHERE id = %s"

        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(del_q, (task_id,))
                # Try dead-letter if not found in queue
                if getattr(cursor, "rowcount", 0) > 0:
                    return True
                await cursor.execute(del_dlq, (task_id,))
                return getattr(cursor, "rowcount", 0) > 0

    async def get_worker_stats(self) -> list[dict[str, Any]]:
        """Return worker stats. Not implemented in MySQL driver; return empty list."""
        # No worker registry table in this driver implementation
        return []
