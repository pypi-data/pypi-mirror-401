import asyncio
from dataclasses import dataclass, field
from datetime import UTC, datetime
from inspect import isawaitable
import logging
from time import time
from typing import Any

from redis.asyncio import Redis

from .base_driver import BaseDriver

logger = logging.getLogger(__name__)


async def maybe_await(result: Any) -> Any:
    """Helper to handle redis-py's inconsistent type stubs.

    Redis-py has inline types but they're incomplete/incorrect for asyncio.
    This helper checks if the result is awaitable and awaits it if so.

    Note on Type Stubs:
        redis-py defines EncodableT = Union[bytes, bytearray, memoryview, str, int, float]
        but some command stubs incorrectly narrow parameters to just `str`:
        - lrem(value: str) should be lrem(value: EncodableT)

        Runtime behavior is correct - these commands accept bytes when decode_responses=False.
        We use `# type: ignore[arg-type]` for these known redis-py type stub bugs.

    Args:
        result: Result from redis-py operation

    Returns:
        The awaited result if awaitable, otherwise the result itself
    """
    if isawaitable(result):
        return await result
    return result


@dataclass
class RedisDriver(BaseDriver):
    """Redis-based queue driver using Lists for immediate tasks and Sorted Sets for delayed tasks.

    Architecture:
        - Immediate tasks: Redis List at "queue:{name}" (LPUSH/LMOVE for FIFO)
        - Processing tasks: Redis List at "queue:{name}:processing" (in-flight tracking)
        - Delayed tasks: Sorted Set at "queue:{name}:delayed" (score = Unix timestamp)
        - Ready delayed tasks are moved to main queue atomically via pipeline transaction

    Design Decisions:
        - Reliable Queue Pattern: Uses LMOVE to atomically move tasks to processing list
        - Processing list enables crash recovery and prevents nack-after-ack bugs
        - Sorted sets over TTL/Lua: Simpler, atomic, no external dependencies
        - Pipeline with MULTI/EXEC: Prevents duplicate processing during delayedâ†’main transfer
        - RESP3 protocol: Better performance than RESP2 (requires Redis 6.0+)

    Requirements:
        - Python 3.11+, redis-py 7.0+, Redis server 6.2.0+ (for LMOVE command)
    """

    url: str = "redis://localhost:6379"
    password: str | None = None
    db: int = 0
    max_connections: int = 100
    keep_completed_tasks: bool = False
    warmup_connections: int = 0  # Number of connections to pre-establish (0 = disabled)
    delayed_task_interval: float = 1.0  # Interval in seconds for background delayed task processing
    client: Redis | None = field(default=None, init=False, repr=False)
    _delayed_task_bg: asyncio.Task[None] | None = field(default=None, init=False, repr=False)
    _delayed_queues: set[str] = field(default_factory=set, init=False, repr=False)

    async def connect(self) -> None:
        """Initialize Redis connection with pooling (connection is lazy)."""
        if self.client is not None:
            return

        self.client = Redis.from_url(
            self.url,
            password=self.password,
            db=self.db,
            decode_responses=False,  # Return bytes, not strings
            max_connections=self.max_connections,
            protocol=3,  # Use RESP3 protocol for better performance
        )

        # Warm up connection pool if configured
        # This pre-establishes connections to avoid cold-start latency
        # See: https://github.com/redis/redis-py/issues/3412
        if self.warmup_connections > 0:
            await self._warmup_connection_pool(self.warmup_connections)

    def start_delayed_processor(self, queue_name: str) -> None:
        """Register a queue for background delayed task processing.

        Call this when a worker starts processing a queue. The background task
        will periodically move ready delayed tasks to the main queue.

        Args:
            queue_name: Name of the queue to monitor for delayed tasks
        """
        self._delayed_queues.add(queue_name)

        # Start background task if not already running
        if self._delayed_task_bg is None or self._delayed_task_bg.done():
            self._delayed_task_bg = asyncio.create_task(
                self._delayed_task_loop(), name="delayed-task-processor"
            )

    async def _delayed_task_loop(self) -> None:
        """Background loop that periodically processes delayed tasks for all registered queues."""
        while True:
            try:
                await asyncio.sleep(self.delayed_task_interval)

                # Process all registered queues
                for queue_name in list(self._delayed_queues):
                    try:
                        await self._process_delayed_tasks(queue_name)
                    except Exception as e:
                        logger.warning(f"Error processing delayed tasks for {queue_name}: {e}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Delayed task loop error: {e}")

    async def _warmup_connection_pool(self, n: int) -> None:
        """Pre-establish N connections in the pool to avoid cold-start latency.

        The async redis-py client creates connections on-demand, which adds
        ~4-5ms latency on first use. This method forces the pool to create
        connections upfront for faster subsequent operations.

        Args:
            n: Number of connections to pre-establish
        """
        assert self.client is not None

        # Simple PING commands establish connections
        async def ping() -> bool:
            assert self.client is not None
            return await maybe_await(self.client.ping())

        await asyncio.gather(*[ping() for _ in range(n)])

    async def disconnect(self) -> None:
        """Close all connections and cleanup resources."""
        # Cancel delayed task background processor
        if self._delayed_task_bg is not None:
            self._delayed_task_bg.cancel()
            try:
                await self._delayed_task_bg
            except asyncio.CancelledError:
                pass
            self._delayed_task_bg = None

        if self.client is not None:
            await self.client.aclose()
            self.client = None

    async def enqueue(
        self,
        queue_name: str,
        task_data: bytes,
        delay_seconds: int = 0,
        current_attempt: int = 0,
        visibility_timeout: int = 3600,
        max_attempts: int = 3,
    ) -> None:
        """Add task to queue.

        Args:
            queue_name: Name of the queue
            task_data: Serialized task data
            delay_seconds: Seconds to delay task visibility (0 = immediate)
            current_attempt: Current attempt number (ignored by Redis driver)
            visibility_timeout: Crash recovery timeout (ignored by Redis driver)
            max_attempts: Maximum retry attempts (ignored by Redis driver, stored in task)

        Implementation:
            - Immediate: LPUSH to list for O(1) insertion
            - Delayed: ZADD to sorted set with score = current_time + delay_seconds
            - current_attempt and max_attempts are tracked in serialized task, not in Redis
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        if delay_seconds > 0:
            # Calculate absolute execution time (Unix timestamp)
            # This is the "score" that Redis will use for sorting
            process_at: float = time() + delay_seconds

            # ZADD adds to sorted set - Redis automatically maintains sort order
            # Mapping format: {member: score} where member=task_data, score=process_at
            await maybe_await(
                self.client.zadd(f"queue:{queue_name}:delayed", {task_data: process_at})
            )
        else:
            # LPUSH adds to left of list - workers RPOP from right (FIFO)
            await maybe_await(self.client.lpush(f"queue:{queue_name}", task_data))

    async def dequeue(self, queue_name: str, poll_seconds: int = 0) -> bytes | None:
        """Retrieve next task from queue using Reliable Queue pattern.

        Args:
            queue_name: Name of the queue
            poll_seconds: Seconds to poll for task (0 = non-blocking)

        Returns:
            Serialized task data or None if queue empty

        Implementation:
            Uses LMOVE to atomically move task from main queue to processing list.
            This implements Redis's "Reliable Queue" pattern for crash recovery.
        """

        if self.client is None:
            await self.connect()
            assert self.client is not None

        # Process delayed tasks: use background task if running, else inline fallback
        # This ensures delayed tasks work even if start_delayed_processor() wasn't called
        if queue_name not in self._delayed_queues:
            # Background processor not active for this queue - process inline
            await self._process_delayed_tasks(queue_name)

        main_key = f"queue:{queue_name}"
        processing_key = f"queue:{queue_name}:processing"

        # Atomically move from main queue to processing list (Reliable Queue pattern)
        if poll_seconds > 0:
            # BLMOVE: blocking version with timeout
            result: bytes | None = await maybe_await(
                self.client.blmove(main_key, processing_key, poll_seconds, "RIGHT", "LEFT")
            )
            return result
        else:
            # LMOVE: non-blocking version
            return await maybe_await(self.client.lmove(main_key, processing_key, "RIGHT", "LEFT"))

    async def ack(self, queue_name: str, receipt_handle: bytes) -> None:
        """Acknowledge successful task processing (remove from processing list).

        Args:
            queue_name: Name of the queue
            receipt_handle: Task data from dequeue

        Implementation:
            Uses Redis pipeline to batch LREM + INCR (+ optional LPUSH) into a single
            network round-trip for optimal performance. Pipeline uses transaction=False
            to avoid MULTI/EXEC overhead (5-15% faster per AWS benchmarks).
            Idempotent operation.
        """
        await self._ack_impl(queue_name, receipt_handle)

    async def _ack_impl(self, queue_name: str, receipt_handle: bytes) -> None:
        """Internal ack implementation."""
        if self.client is None:
            await self.connect()
            assert self.client is not None

        processing_key = f"queue:{queue_name}:processing"
        completed_key = f"queue:{queue_name}:stats:completed"

        # Use pipeline to batch all operations in a single round-trip
        # transaction=False disables MULTI/EXEC wrapping for better performance
        # See: https://aws.amazon.com/blogs/database/optimize-redis-client-performance-for-amazon-elasticache/
        async with self.client.pipeline(transaction=False) as pipe:
            # LREM: Remove task from processing list (count=1 removes first occurrence)
            # redis-py's lrem type stub expects str, but runtime accepts bytes
            pipe.lrem(processing_key, 1, receipt_handle)  # type: ignore[arg-type]
            # INCR: Increment completed counter
            pipe.incr(completed_key)
            # Optional LPUSH: Keep completed tasks for history
            if self.keep_completed_tasks:
                completed_list_key = f"queue:{queue_name}:completed"
                pipe.lpush(completed_list_key, receipt_handle)

            results = await pipe.execute()

        # Results: [lrem_count, incr_result, (optional lpush_result)]
        removed_count = results[0]

        # If task wasn't in processing list (already acked), decrement counter to undo
        # This maintains correct counting for idempotent ack calls
        if removed_count == 0:
            await maybe_await(self.client.decr(completed_key))

    async def nack(self, queue_name: str, receipt_handle: bytes) -> None:
        """Reject task and re-queue for immediate retry.

        Args:
            queue_name: Name of the queue
            receipt_handle: Task data from dequeue

        Implementation:
            Only requeues if task exists in processing list (prevents nack-after-ack).
            First checks if task is in processing, then moves it back if found.
        """

        if self.client is None:
            await self.connect()
            assert self.client is not None

        processing_key = f"queue:{queue_name}:processing"
        main_key = f"queue:{queue_name}"

        # Only requeue if task exists in processing list (prevents nack-after-ack)
        # LREM returns count of removed items: 0 if not found, 1 if found and removed
        # redis-py's lrem type stub expects str, but runtime accepts bytes (see maybe_await docs)
        removed_count: int = await maybe_await(
            self.client.lrem(processing_key, 1, receipt_handle)  # type: ignore[arg-type]
        )

        # Only add back to queue if task was actually in processing list
        # This prevents nack-after-ack from re-adding already completed tasks
        if removed_count > 0:
            await maybe_await(self.client.lpush(main_key, receipt_handle))

    async def mark_failed(self, queue_name: str, receipt_handle: bytes) -> None:
        """Mark task as permanently failed (remove from processing list and increment failed counter).

        Args:
            queue_name: Name of the queue
            receipt_handle: Task data from dequeue

        Implementation:
            Uses Redis pipeline to batch LREM + INCR into a single network round-trip.
            Pipeline uses transaction=False to avoid MULTI/EXEC overhead.
            Should be called when a task fails permanently (no more retries).
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        processing_key = f"queue:{queue_name}:processing"
        failed_key = f"queue:{queue_name}:stats:failed"

        # Use pipeline to batch LREM + INCR in a single round-trip
        async with self.client.pipeline(transaction=False) as pipe:
            # LREM: Remove task from processing list
            pipe.lrem(processing_key, 1, receipt_handle)  # type: ignore[arg-type]
            # INCR: Increment failed counter
            pipe.incr(failed_key)
            results = await pipe.execute()

        # If task wasn't in processing list, decrement counter to undo
        removed_count = results[0]
        if removed_count == 0:
            await maybe_await(self.client.decr(failed_key))

    async def get_queue_size(
        self,
        queue_name: str,
        include_delayed: bool,
        include_in_flight: bool,
    ) -> int:
        """Get number of tasks in queue.

        Args:
            queue_name: Name of the queue
            include_delayed: Include delayed tasks in count
            include_in_flight: Include in-flight tasks in count

        Returns:
            Task count based on parameters
        """

        if self.client is None:
            await self.connect()
            assert self.client is not None

        size: int = await maybe_await(self.client.llen(f"queue:{queue_name}"))

        if include_delayed:
            delayed_size: int = await maybe_await(self.client.zcard(f"queue:{queue_name}:delayed"))
            size += delayed_size

        if include_in_flight:
            processing_size: int = await maybe_await(
                self.client.llen(f"queue:{queue_name}:processing")
            )
            size += processing_size

        return size

    async def _process_delayed_tasks(self, queue_name: str) -> None:
        """Move ready delayed tasks to main queue atomically.

        Process:
            1. Query sorted set for tasks with score <= current_time (ZRANGEBYSCORE)
            2. If ready tasks found, use pipeline transaction to:
               - LPUSH tasks to main queue
               - ZREMRANGEBYSCORE to remove from delayed queue
            3. MULTI/EXEC ensures atomicity (prevents duplicate processing)

        Args:
            queue_name: Name of the queue
        """
        now: float = time()
        delayed_key: str = f"queue:{queue_name}:delayed"
        main_key: str = f"queue:{queue_name}"

        assert self.client is not None

        # Get all tasks ready to process (score <= current time)
        ready_tasks: list[bytes] = await self.client.zrangebyscore(delayed_key, min="-inf", max=now)

        if ready_tasks:
            # Move tasks atomically: add to main queue and remove from delayed queue
            async with self.client.pipeline(transaction=True) as pipe:
                pipe.lpush(main_key, *ready_tasks)
                pipe.zremrangebyscore(delayed_key, 0, now)
                await pipe.execute()

    # --- Metadata / Inspection methods -------------------------------------------------

    async def get_queue_stats(self, queue: str) -> dict[str, Any]:
        """
        Basic stats for a specific queue.

        Note: This implementation uses simple counters derived from list/zset sizes.
        More advanced stats (avg_duration, throughput) are not collected here and
        will return defaults.
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        depth = int(await maybe_await(self.client.llen(f"queue:{queue}")))
        processing = int(await maybe_await(self.client.llen(f"queue:{queue}:processing")))
        completed_raw = await maybe_await(self.client.get(f"queue:{queue}:stats:completed"))
        completed_total = int(completed_raw or 0)
        failed_raw = await maybe_await(self.client.get(f"queue:{queue}:stats:failed"))
        failed_total = int(failed_raw or 0)

        return {
            "name": queue,
            "depth": depth,
            "processing": processing,
            "completed_total": completed_total,
            "failed_total": failed_total,
            "avg_duration_ms": None,
            "throughput_per_minute": None,
        }

    async def get_all_queue_names(self) -> list[str]:
        """Return all queue names discovered by key patterns.

        Uses SCAN to avoid blocking Redis in production.
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        queues: set[str] = set()

        # Use scan_iter helper when available; redis-py provides scan_iter as sync and async
        # but to keep compatibility we'll use scan until cursor==0
        cur = 0
        while True:
            cur, keys = await maybe_await(self.client.scan(cur))
            for k in keys:
                # keys are bytes because decode_responses=False
                if isinstance(k, bytes):
                    k = k.decode()
                if k.startswith("queue:"):
                    # strip prefix and any suffix like :processing or :delayed
                    name = k.split(":")[1]
                    queues.add(name)
            if cur == 0:
                break

        return sorted(queues)

    async def get_global_stats(self) -> dict[str, int]:
        """Aggregate simple global stats across all queues.

        Implementation is intentionally conservative: sums per-queue counters.
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        queues = await self.get_all_queue_names()
        pending = 0
        running = 0
        completed = 0
        failed = 0

        for q in queues:
            pending += int(await maybe_await(self.client.llen(f"queue:{q}")))
            running += int(await maybe_await(self.client.llen(f"queue:{q}:processing")))
            completed_raw = await maybe_await(self.client.get(f"queue:{q}:stats:completed"))
            completed += int(completed_raw or 0)
            failed_raw = await maybe_await(self.client.get(f"queue:{q}:stats:failed"))
            failed += int(failed_raw or 0)

        total = pending + running + completed + failed
        return {
            "pending": pending,
            "running": running,
            "completed": completed,
            "failed": failed,
            "total": total,
        }

    async def get_running_tasks(self, limit: int = 50, offset: int = 0) -> list[tuple[bytes, str]]:
        """Return raw task bytes for tasks currently in processing lists.

        Returns:
            List of (raw_bytes, queue_name) tuples for running tasks
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        queues = await self.get_all_queue_names()
        running: list[tuple[bytes, str]] = []

        for q in queues:
            processing_key = f"queue:{q}:processing"
            items = await maybe_await(self.client.lrange(processing_key, 0, -1))
            for raw in items:
                if isinstance(raw, (bytes, bytearray)):
                    running.append((bytes(raw), q))

        # Apply pagination
        return running[offset : offset + limit]

    async def get_tasks(
        self,
        status: str | None = None,
        queue: str | None = None,
        limit: int = 50,
        offset: int = 0,
    ) -> tuple[list[tuple[bytes, str, str]], int]:
        """Return raw serialized task bytes with queue and status metadata.

        Returns raw serialized bytes as stored in Redis, allowing the caller
        to deserialize using the appropriate serializer.
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        queues = [queue] if queue else await self.get_all_queue_names()
        results: list[tuple[bytes, str, str]] = []

        for q in queues:
            # pending tasks
            if status is None or status == "pending":
                pending_items = await maybe_await(self.client.lrange(f"queue:{q}", 0, -1))
                for raw in pending_items:
                    if isinstance(raw, (bytes, bytearray)):
                        results.append((bytes(raw), q, "pending"))

            # running tasks
            if status is None or status == "running":
                processing_items = await maybe_await(
                    self.client.lrange(f"queue:{q}:processing", 0, -1)
                )
                for raw in processing_items:
                    if isinstance(raw, (bytes, bytearray)):
                        results.append((bytes(raw), q, "running"))

        total = len(results)
        paged = results[offset : offset + limit]
        return paged, total

    async def get_task_by_id(self, task_id: str) -> bytes | None:
        """Redis cannot efficiently look up tasks by ID without deserialization.

        Returns None. Use MonitoringService.get_task_info_by_id() which uses
        the serializer to scan and find tasks by ID.

        Note:
            Redis stores tasks as raw bytes in lists. Finding a task by its
            internal task_id field requires deserializing every task, which
            should be done at the service layer with access to a serializer.
        """
        # Redis cannot do primary key lookup - task_id is inside the serialized payload
        # Return None; callers should use MonitoringService which has access to serializer
        return None

    async def retry_task(self, task_id: str) -> bool:
        """Redis cannot efficiently retry tasks by ID.

        Redis stores tasks as raw bytes in lists. Finding a task by its
        internal task_id field requires deserializing every task, which
        should be done at the service layer with access to a serializer.

        Returns:
            False - use TaskService.retry_task() which can scan and deserialize.
        """
        # Redis cannot do primary key lookup - task_id is inside the serialized payload
        # Return False; callers should use TaskService which has access to serializer
        return False

    async def delete_task(self, task_id: str) -> bool:
        """Redis cannot efficiently delete tasks by ID.

        Redis stores tasks as raw bytes in lists. Finding a task by its
        internal task_id field requires deserializing every task, which
        should be done at the service layer with access to a serializer.

        Returns:
            False - use TaskService.delete_task() which can scan and deserialize.
        """
        # Redis cannot do primary key lookup - task_id is inside the serialized payload
        # Return False; callers should use TaskService which has access to serializer
        return False

    async def delete_raw_task(self, queue_name: str, raw_bytes: bytes) -> bool:
        """Delete a task by its raw serialized bytes.

        Uses LREM to remove the exact bytes from pending, processing, and dead lists.

        Args:
            queue_name: Name of the queue containing the task
            raw_bytes: The exact raw bytes of the task to delete

        Returns:
            True if deleted from any list, False if not found
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        removed = 0
        for key_suffix in ("", ":processing", ":dead"):
            key = f"queue:{queue_name}{key_suffix}"
            # LREM returns count of removed elements
            count: int = await maybe_await(
                self.client.lrem(key, 1, raw_bytes)  # type: ignore[arg-type]
            )
            removed += count

        return removed > 0

    async def retry_raw_task(self, queue_name: str, raw_bytes: bytes) -> bool:
        """Retry a task by its raw serialized bytes.

        Removes the task from dead list and re-enqueues to main queue.

        Args:
            queue_name: Name of the queue containing the task
            raw_bytes: The exact raw bytes of the task to retry

        Returns:
            True if retried, False if not found in dead list
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        dead_key = f"queue:{queue_name}:dead"
        main_key = f"queue:{queue_name}"

        # Remove from dead list
        removed: int = await maybe_await(
            self.client.lrem(dead_key, 1, raw_bytes)  # type: ignore[arg-type]
        )

        if removed > 0:
            # Re-enqueue to main queue
            await maybe_await(self.client.rpush(main_key, raw_bytes))
            return True

        return False

    async def get_worker_stats(self) -> list[dict[str, Any]]:
        """Return worker heartbeats stored in `workers:{id}` hashes.

        Best-effort: scans keys `worker:*` and builds worker dicts.
        """
        if self.client is None:
            await self.connect()
            assert self.client is not None

        workers: list[dict[str, Any]] = []
        cur = 0
        while True:
            cur, keys = await maybe_await(self.client.scan(cur, match="worker:*"))
            for k in keys:
                if isinstance(k, bytes):
                    k = k.decode()
                worker_id = k.split(":", 1)[1]
                data = await maybe_await(self.client.hgetall(k))
                status = (
                    (data.get(b"status") or data.get("status") or b"idle").decode()
                    if data
                    else "idle"
                )
                last_hb = None
                if data:
                    ts = data.get(b"last_heartbeat") or data.get("last_heartbeat")
                    try:
                        last_hb = datetime.fromtimestamp(float(ts), UTC) if ts else None
                    except Exception:
                        last_hb = None

                workers.append(
                    {
                        "worker_id": worker_id,
                        "status": status,
                        "current_task_id": None,
                        "tasks_processed": int(
                            data.get(b"tasks_processed") or data.get("tasks_processed") or 0
                        ),
                        "uptime_seconds": int(
                            data.get(b"uptime_seconds") or data.get("uptime_seconds") or 0
                        ),
                        "last_heartbeat": last_hb,
                        "load_percentage": 0.0,
                    }
                )
            if cur == 0:
                break

        return workers
