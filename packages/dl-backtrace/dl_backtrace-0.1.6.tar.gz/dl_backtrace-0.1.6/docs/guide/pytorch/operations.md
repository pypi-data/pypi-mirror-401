# Supported Operations

DLBacktrace supports 100+ PyTorch ATen operations. This page lists all supported operations.

---

## Core Operations

### Linear Layers

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `linear` | Fully connected layer | âœ… |
| `addmm` | Matrix multiply and add | âœ… |
| `mm` | Matrix multiply | âœ… |

---

### Convolutional Layers

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `conv2d` | 2D convolution | âœ… |
| `conv1d` | 1D convolution | âœ… |
| `conv_transpose2d` | 2D transposed convolution | ğŸ”„ |
| `conv_transpose1d` | 1D transposed convolution | ğŸ”„ |

---

### Pooling Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `max_pool2d` | 2D max pooling | âœ… |
| `avg_pool2d` | 2D average pooling | âœ… |
| `adaptive_avg_pool2d` | Adaptive average pooling 2D | âœ… |
| `adaptive_max_pool2d` | Adaptive max pooling 2D | âœ… |
| `max_pool1d` | 1D max pooling | âœ… |
| `avg_pool1d` | 1D average pooling | âœ… |
| `adaptive_avg_pool1d` | Adaptive average pooling 1D | âœ… |
| `adaptive_max_pool1d` | Adaptive max pooling 1D | âœ… |

---

### Activation Functions

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `relu` | ReLU activation | âœ… |
| `gelu` | GELU activation | âœ… |
| `silu` / `swish` | SiLU/Swish activation | âœ… |
| `sigmoid` | Sigmoid activation | âœ… |
| `tanh` | Tanh activation | âœ… |
| `leaky_relu` | Leaky ReLU | âœ… |
| `elu` | ELU activation | âœ… |
| `softmax` | Softmax activation | âœ… |
| `log_softmax` | Log softmax | âœ… |

---

## Tensor Operations

### Shape Manipulation

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `view` | Reshape tensor | âœ… |
| `reshape` | Reshape tensor | âœ… |
| `flatten` | Flatten tensor | âœ… |
| `squeeze` | Remove dimensions of size 1 | âœ… |
| `unsqueeze` | Add dimension of size 1 | âœ… |
| `transpose` | Transpose dimensions | âœ… |
| `permute` | Permute dimensions | âœ… |
| `contiguous` | Make tensor contiguous | âœ… |

---

### Slicing & Indexing

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `slice` | Slice tensor along dimension | âœ… |
| `index_select` | Select indices | âœ… |
| `gather` | Gather values | âœ… |
| `select` | Select single index | âœ… |
| `narrow` | Narrow tensor | âœ… |

---

### Concatenation & Stacking

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `cat` | Concatenate tensors | âœ… |
| `stack` | Stack tensors | âœ… |
| `split` | Split tensor | âœ… |
| `chunk` | Chunk tensor | âœ… |

---

## Arithmetic Operations

### Basic Arithmetic

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `add` | Addition | âœ… |
| `sub` | Subtraction | âœ… |
| `mul` | Multiplication | âœ… |
| `div` | Division | âœ… |
| `pow` | Power | âœ… |
| `sqrt` | Square root | âœ… |
| `exp` | Exponential | âœ… |
| `log` | Logarithm | âœ… |

---

### Matrix Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `matmul` | Matrix multiplication | âœ… |
| `bmm` | Batch matrix multiplication | âœ… |
| `addmm` | Matrix multiply and add | âœ… |
| `baddbmm` | Batch matrix multiply and add | âœ… |

---

### Comparison Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `eq` | Equal | âœ… |
| `ne` | Not equal | âœ… |
| `lt` | Less than | âœ… |
| `le` | Less than or equal | âœ… |
| `gt` | Greater than | âœ… |
| `ge` | Greater than or equal | âœ… |

---

## Normalization

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `layer_norm` | Layer normalization | âœ… |
| `batch_norm` | Batch normalization | âœ… |
| `group_norm` | Group normalization | âœ… |
| `instance_norm` | Instance normalization | âœ… |
---

## Attention Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `scaled_dot_product_attention` | Scaled dot-product attention | âœ… |
| `softmax` | Softmax for attention weights | âœ… |
| `dropout` | Dropout (pass-through in eval) | âœ… |

---

## Embedding Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `embedding` | Embedding lookup | âœ… |
| `embedding_bag` | Embedding bag | ğŸ”„ |

---

## Utility Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `clone` | Clone tensor | âœ… |
| `detach` | Detach from graph | âœ… |
| `to` | Convert dtype/device | âœ… |
| `type` | Type conversion | âœ… |
| `arange` | Create range | âœ… |
| `zeros` | Create zeros | âœ… |
| `ones` | Create ones | âœ… |
| `full` | Create filled tensor | âœ… |

---

## Special Operations

### Symbolic Operations

| Operation | Description | Supported |
|-----------|-------------|-----------|
| `sym_size` | Symbolic size | âœ… |
| `sym_numel` | Symbolic number of elements | âœ… |

These are used internally during graph tracing.

---

## Negative Indexing Support

All dimension-based operations support PyTorch's negative indexing:

```python
# All of these work!
x.transpose(-1, -2)         # Last two dimensions
x.permute([0, -1, -2, 1])   # Mix of positive and negative
x.unsqueeze(-1)             # Add dimension at end
x.slice(dim=-1, ...)        # Slice last dimension
torch.cat([x, y], dim=-1)   # Concatenate on last dimension
```

---

## Operation Categories

### âœ… Fully Supported
Operations that are fully implemented and tested.

### ğŸ”„ In Development
Operations that are planned but not yet implemented.

### âŒ Not Supported
Operations that are not currently supported.

---

## Custom Operations

### Adding Custom Operations

If you need support for a custom operation, you can:

1. **Request support**: Open an issue on GitHub
2. **Contribute**: Submit a pull request
3. **Workaround**: Decompose into supported operations

---

## Testing Operations

To test if an operation is supported:

```python
from dl_backtrace.pytorch_backtrace import DLBacktrace

# Create simple model using the operation
class TestModel(torch.nn.Module):
    def forward(self, x):
        return your_operation(x)

# Try to trace it
try:
    model = TestModel()
    dlb = DLBacktrace(
        model=model,
        input_for_graph=(dummy_input,)
    )
    print("âœ… Operation supported!")
except Exception as e:
    print(f"âŒ Operation not supported: {e}")
```

---

## Next Steps

- [Execution Engines](execution-engines.md) - Learn how operations are executed
- [Model Tracing](tracing.md) - Understand graph tracing
- [Examples](../../examples/colab-notebooks.md) - See operations in action
- [Best Practices](../best-practices.md) - Optimization tips



