"""Unified MLOps Manager for Stock Recommendation System."""

import json
import pickle
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import joblib
import mlflow  # type: ignore[import-untyped]
import mlflow.pytorch  # type: ignore[import-untyped]
import mlflow.sklearn  # type: ignore[import-untyped]

from .dvc_config import DVCConfig, get_dvc_config
from .mlflow_config import MLflowConfig, get_mlflow_config


class MLOpsManager:
    """Unified manager for MLflow and DVC operations."""

    def __init__(
        self, mlflow_config: Optional[MLflowConfig] = None, dvc_config: Optional[DVCConfig] = None
    ):
        self.mlflow_config = mlflow_config or get_mlflow_config()
        self.dvc_config = dvc_config or get_dvc_config()
        self._setup_completed = False

    def setup(self) -> None:
        """Initialize MLOps infrastructure."""
        if self._setup_completed:
            return

        print("Setting up MLOps infrastructure...")

        # Setup MLflow
        self.mlflow_config.setup_tracking()

        # Setup DVC
        self.dvc_config.setup_data_directories()

        self._setup_completed = True
        print("âœ… MLOps infrastructure setup completed")

    def start_experiment_run(
        self,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
        description: Optional[str] = None,
    ) -> str:
        """Start a new MLflow experiment run."""
        if not self._setup_completed:
            self.setup()

        run_tags = tags or {}
        if description:
            run_tags["description"] = description

        run = mlflow.start_run(run_name=run_name, tags=run_tags)
        print(f"Started MLflow run: {run.info.run_id}")
        return run.info.run_id

    def log_parameters(self, params: Dict[str, Any]) -> None:
        """Log parameters to MLflow."""
        for key, value in params.items():
            mlflow.log_param(key, value)

    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:
        """Log metrics to MLflow."""
        for key, value in metrics.items():
            mlflow.log_metric(key, value, step=step)

    def log_artifacts(self, artifact_path: Path, artifact_name: Optional[str] = None) -> None:
        """Log artifacts to MLflow."""
        if artifact_path.is_file():
            mlflow.log_artifact(str(artifact_path), artifact_name)
        else:
            mlflow.log_artifacts(str(artifact_path), artifact_name)

    def save_and_log_model(
        self,
        model: Any,
        model_name: str,
        model_type: str = "pytorch",
        signature: Optional[Any] = None,
        input_example: Optional[Any] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Save model locally and log to MLflow."""
        # Create model directory
        model_dir = self.dvc_config.models_dir / model_type / model_name
        model_dir.mkdir(parents=True, exist_ok=True)

        # Save model based on type
        if model_type == "pytorch":
            import torch

            model_path = model_dir / "model.pt"
            torch.save(model.state_dict(), model_path)

            # Log to MLflow
            mlflow.pytorch.log_model(
                pytorch_model=model,
                artifact_path=f"models/{model_name}",
                signature=signature,
                input_example=input_example,
                extra_files=[str(model_path)] if model_path.exists() else None,
            )

        elif model_type == "sklearn":
            model_path = model_dir / "model.pkl"
            joblib.dump(model, model_path)

            # Log to MLflow
            mlflow.sklearn.log_model(
                sk_model=model,
                artifact_path=f"models/{model_name}",
                signature=signature,
                input_example=input_example,
            )

        else:
            # Generic pickle save
            model_path = model_dir / "model.pkl"
            with open(model_path, "wb") as f:
                pickle.dump(model, f)

            # Log as generic artifact
            mlflow.log_artifact(str(model_path), f"models/{model_name}")

        # Save metadata
        if metadata:
            metadata_path = model_dir / "metadata.json"
            with open(metadata_path, "w") as f:
                json.dump(metadata, f, indent=2, default=str)
            mlflow.log_artifact(str(metadata_path), f"models/{model_name}")

        # Add to DVC tracking
        self.dvc_config.add_data_to_dvc(model_dir)

        print(f"Model saved and logged: {model_name} ({model_type})")
        return str(model_path)

    def register_model(
        self,
        model_name: str,
        stage: str = "Staging",
        description: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
    ) -> str:
        """Register model in MLflow Model Registry."""
        # Get the current run
        run = mlflow.active_run()
        if not run:
            raise Exception("No active MLflow run. Start a run first.")

        model_uri = f"runs:/{run.info.run_id}/models/{model_name}"

        # Register model
        model_version = self.mlflow_config.register_model(
            model_uri=model_uri, model_name=model_name, tags=tags, description=description
        )

        # Transition to requested stage
        if stage != "None":
            self.mlflow_config.transition_model_stage(
                model_name=model_name, version=model_version.version, stage=stage
            )

        return model_version.version

    def load_model(
        self, model_name: str, version: Optional[str] = None, stage: Optional[str] = None
    ) -> Any:
        """Load model from MLflow Model Registry."""
        if version:
            model_uri = f"models:/{model_name}/{version}"
        elif stage:
            model_uri = f"models:/{model_name}/{stage}"
        else:
            model_uri = f"models:/{model_name}/latest"

        return mlflow.pytorch.load_model(model_uri)

    def end_run(self, status: str = "FINISHED") -> None:
        """End the current MLflow run."""
        mlflow.end_run(status=status)

    def version_data(self, data_path: Path, message: Optional[str] = None) -> str:
        """Version data using DVC."""
        self.dvc_config.add_data_to_dvc(data_path, message)
        return self.dvc_config.get_data_version(data_path) or "unknown"

    def create_ml_pipeline_stage(
        self,
        stage_name: str,
        script_path: str,
        dependencies: List[str],
        outputs: List[str],
        parameters: Optional[List[str]] = None,
    ) -> None:
        """Create a DVC pipeline stage for ML workflow."""
        command = f"python {script_path}"

        self.dvc_config.create_pipeline_stage(
            stage_name=stage_name,
            command=command,
            dependencies=dependencies,
            outputs=outputs,
            parameters=parameters,
        )

    def run_ml_pipeline(self, stage_name: Optional[str] = None) -> None:
        """Run DVC ML pipeline."""
        self.dvc_config.run_pipeline(stage_name)

    def get_experiment_metrics(self, experiment_name: str) -> List[Dict[str, Any]]:
        """Get metrics from all runs in an experiment."""
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if not experiment:
            return []

        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        return runs.to_dict("records")

    def compare_models(
        self, model_names: List[str], metric_name: str = "accuracy"
    ) -> Dict[str, Any]:
        """Compare models by a specific metric."""
        comparison = {}

        for model_name in model_names:
            try:
                # Get latest version metrics
                latest_versions = self.mlflow_config.client.get_latest_versions(
                    model_name, stages=["Production", "Staging"]
                )

                if latest_versions:
                    version = latest_versions[0]
                    run = self.mlflow_config.client.get_run(version.run_id)
                    comparison[model_name] = {
                        "version": version.version,
                        "stage": version.current_stage,
                        "metric_value": run.data.metrics.get(metric_name, 0.0),
                        "run_id": version.run_id,
                    }
            except Exception as e:
                comparison[model_name] = {"error": str(e)}

        return comparison

    def cleanup_old_runs(self, days_old: int = 30) -> None:
        """Clean up old experiment runs."""
        # This would implement cleanup logic for old runs
        # For now, just a placeholder
        print(f"Cleanup of runs older than {days_old} days would be implemented here")

    def get_system_info(self) -> Dict[str, Any]:
        """Get MLOps system information."""
        return {
            "mlflow_tracking_uri": self.mlflow_config.tracking_uri,
            "mlflow_experiment": self.mlflow_config.experiment_name,
            "dvc_project_root": str(self.dvc_config.project_root),
            "data_directory": str(self.dvc_config.data_dir),
            "models_directory": str(self.dvc_config.models_dir),
            "setup_completed": self._setup_completed,
            "timestamp": datetime.now().isoformat(),
        }


# Global MLOps manager instance
mlops_manager = MLOpsManager()


def get_mlops_manager() -> MLOpsManager:
    """Get the global MLOps manager instance."""
    return mlops_manager


if __name__ == "__main__":
    # Test the MLOps setup
    manager = get_mlops_manager()
    manager.setup()

    print("\n" + "=" * 50)
    print("MLOPs System Information:")
    print("=" * 50)

    system_info = manager.get_system_info()
    for key, value in system_info.items():
        print(f"{key}: {value}")
