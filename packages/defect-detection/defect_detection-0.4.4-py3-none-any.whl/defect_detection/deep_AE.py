##########################################################################
## Implementation of the super deep AE class.                           ##
## Class members :                                                      ##
##     encoder : the encoder model                                      ##
##     decoder : the decoder model                                      ##
##     loss_fn : the loss function used for training the model          ##
##     opt : the torch optimizer used for gradient and parameter update ##
##     config : dict of model hyperparameters                           ##
##     apply_only : bool telling if model is for application only       ##
## Class methods :                                                      ##
##     to_dev : assign the model to a specific device                   ##
##     batch_train : perform one training step on a batch of inputs     ##
##     batch_apply : perform one evaluation step on a batch of inputs   ##
##     save_save : save model hyperparameter and trained wight on disk  ##
##########################################################################


import torch
import os


class AE_cls(torch.nn.Module):
    """
    Class inplementing the deep AE architecture using pyTorch.
    Provides also methods for training and applying model using input batches.

    Class methods :
        __init__
        to_dev
        forward
        batch_train
        batch_apply
        save_model

    Class members :
        encoder
        decoder
        apply_only
        loss_fn
        opt
        config
    """

    # Initialization method
    def __init__(self, p, apply_only=False, loss_fn=None, opt=None, opt_param=None):
        """
        Arguments :
            p : (dict)
                Python dict containing all the information about the AE architecture.

            apply_only : (bool)
                Specify if the model is used only for application (default: False).
                Note that the training method is not usable if set to True.

            loss_fn : (callable or None)
                The loss function used to train the model.
                Must be compatible with torch API.
                If you plan to use event weights, use a loss that returns one value per individual inputs.
                If None, it will default to torch.nn.MSELoss().
                Default to None

            opt : (torch.optim object)
                Instance of the torch optimizer used to compute gradient and update model parameters.
                Not required if apply_only is set to True.

            opt_param : (dict)
                Python dict containing the additionnal arguments to be passed to the optimizer.
                If not specified, no arguments are given to the optimizer.
        """
        super().__init__()

        dt = torch.float32

        # Function to define one encoding block
        def get_block_en(bf, ksize, drop):
            b = torch.nn.Sequential(
                torch.nn.Conv2d(
                    bf[0], bf[1], kernel_size=ksize, padding=ksize // 2, dtype=dt
                ),
                torch.nn.LeakyReLU(negative_slope=0.1),
                torch.nn.Conv2d(
                    bf[1],
                    bf[2],
                    kernel_size=ksize,
                    padding=ksize // 2,
                    stride=2,
                    dtype=dt,
                ),
                torch.nn.Dropout(drop),
                torch.nn.LeakyReLU(negative_slope=0.1),
            )
            return b

        # Function to define one decoding block
        def get_block_de(bf, ksize, drop, opad, last=False):
            if last:
                b = torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(
                        bf[0],
                        bf[1],
                        kernel_size=ksize,
                        padding=ksize // 2,
                        output_padding=opad,
                        stride=2,
                        dtype=dt,
                    ),
                    torch.nn.Dropout(drop),
                    torch.nn.LeakyReLU(negative_slope=0.1),
                    torch.nn.ConvTranspose2d(
                        bf[1], bf[2], kernel_size=ksize, padding=ksize // 2, dtype=dt
                    ),
                )
            else:
                b = torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(
                        bf[0],
                        bf[1],
                        kernel_size=ksize,
                        padding=ksize // 2,
                        output_padding=opad,
                        stride=2,
                        dtype=dt,
                    ),
                    torch.nn.Dropout(drop),
                    torch.nn.LeakyReLU(negative_slope=0.1),
                    torch.nn.ConvTranspose2d(
                        bf[1], bf[2], kernel_size=ksize, padding=ksize // 2, dtype=dt
                    ),
                    torch.nn.LeakyReLU(negative_slope=0.1),
                )
            return b

        # Define the encoding sequense
        en_seq = (
            get_block_en(bf, ksize, drop)
            for bf, ksize, drop in zip(p["block_size_in"], p["block_ker"], p["drop"])
        )

        # Define the decoding sequence
        de_seq = (
            get_block_de(bf, ksize, drop, opad, i + 1 == len(p["drop"]))
            for i, (bf, ksize, drop, opad) in enumerate(
                zip(
                    p["block_size_out"],
                    p["block_ker"][::-1],
                    p["drop"][::-1],
                    p["out_pad"],
                )
            )
        )

        # Set the encoder sequence
        self.encoder = torch.nn.Sequential(
            *en_seq,
            torch.nn.Conv2d(
                p["block_size_in"][-1][-1],
                p["latent_size"],
                kernel_size=p["latent_ker"],
                padding=p["latent_ker"] // 2,
                stride=2,
                dtype=dt,
            )
        )

        # Set the decoder sequence
        self.decoder = torch.nn.Sequential(
            torch.nn.ConvTranspose2d(
                p["latent_size"],
                p["block_size_out"][0][0],
                kernel_size=p["block_ker"][-1],
                padding=p["block_ker"][-1] // 2,
                stride=2,
                output_padding=p["latent_opad"],
                dtype=dt,
            ),
            *de_seq
        )

        # Store the configuration
        self.config = p

        # Set the loss function and optimiser (if needed)
        self.apply_only = apply_only
        if loss_fn is None:
            self.loss_fn = torch.nn.MSEloss()
        else:
            self.loss_fn = loss_fn
        if apply_only:
            self.opt = None
        else:
            if opt_param is None:
                opt_param = dict()
            self.opt = opt(self.parameters(), **opt_param)

        return

    # Device selection method (includes automatic selection)
    def to_dev(self, dev):
        """
        Select the device torch should use for this model.
        Supports automatic selection.

        Arguments :
            dev : (str)
                Specify the device to use for this model.
                Supported values are 'cuda', 'cpu' and 'auto'.
                If set to 'auto', the device is selected autimatically based on Cuda availability.
        """
        # Check if auto
        if dev == "auto":
            # Check if cuda is available
            if torch.cuda.is_available():
                dev = "cuda"
            else:
                dev = "cpu"

        # Set device
        self.to(dev)

        return

    # Forward pass method (internal use only)
    def forward(self, x):
        """
        Return the output of the model given input x.
        This is for internal use only, please use ae_instance(x) instead.
        """
        return self.decoder(self.encoder(x))

    # Model training method
    def batch_train(self, x, x_n, w=None):
        """
        Train model on a batch of inputs with the possibility to use input weights.
        It will not do anything if the model is on apply only mode.

        Arguments :
            x : (torch.tensor)
                Input batch tensor, stored on the same device than the model.
                Used as target for the loss calculation.

            x_n : (torch.tensor)
                Noisy version of the input tensor, stored on the same device than the model.
                Used as input of the model.

            w : (torch.tensor or None)
                The input weight tensor, stored on the same device than the model.
                If None, no weights will be used to compute the loss function.
                Default to None

        Returns :
            loss : (numpy.ndarray or None)
                The value of the loss function computed during the training.
                If the model is in apply only mode, None will be returned instead.
        """
        # Check if apply_only
        if self.apply_only:
            print("This model is in apply ony mode, cannot call the train method.")
            return None

        # Apply model and get output
        y = self(x_n)

        # Compute loss
        loss = self.loss_fn(x, y)

        # Check if weights must be applied
        if w is not None:
            # Ponderate mean
            loss = (w * loss).sum() / w.sum()
        else:
            # Simple mean
            loss = loss.mean()

        # Gradient and optimization step
        self.opt.zero_grad()
        loss.backward()
        self.opt.step()

        return loss.detach().to("cpu").numpy()

    # Model aplication method
    def batch_apply(self, x, alt_loss_fn=None):
        """
        Apply model on a batch of inputs and return both output and loss values.
        Make sure to set the model in evaluation mode before calling this method.

        Arguments :
            x : (torch.tensor)
                Input tensor on which inference is performed (must be on same device as model).
                It should have the shape (nbatch, nchannel, sizex, sizey)

            alt_loss_fn : (callabe or None)
                Alternative loss function to be used for model evaluation.
                If None, the default training loss function is used.
                Default to None

        Returns :
            y : (numpy.ndarray)
                The output of the model in numpy format.

            loss : (numpy.ndarray)
                The corresponding loss values in numpy format.
        """
        # Skip gradient computation (not needed for inference)
        with torch.inference_mode():
            # Get output
            y = self(x)

            # Get loss (check for alternative)
            if alt_loss_fn is None:
                loss = self.loss_fn(x, y)
            else:
                loss = alt_loss_fn(x, y)

        return y.detach().to("cpu").numpy(), loss.detach().to("cpu").numpy()

    # Model save method
    def save_model(self, path):
        """
        Save both the model configuration and state in two separate files.

        Argument :
            path : (str)
                Path to the directory in which save files are created given as a string.
                If save files with the same names are already present, they will be overwriten.
        """
        # Check if path exits and try to create it if not
        if not os.path.exists(path):
            os.mkdir(path, 0o755)

        # Save config in text file
        with open(path + "AE_config.txt", "w") as f:
            print(self.config, file=f)

        # Save state_dict
        torch.save(self.state_dict(), path + "AE_state.save")

        return

    # END OF AE_cls
