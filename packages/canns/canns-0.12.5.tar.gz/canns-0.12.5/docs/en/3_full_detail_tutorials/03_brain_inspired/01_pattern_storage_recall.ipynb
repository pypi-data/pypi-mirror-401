{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": "Tutorial: Pattern Storage and Recall with Hebbian Learning\n==========================================================\n\n.. note::\n   \n   **Reading Time**: 30-35 minutes\n   \n   **Difficulty**: Intermediate\n   \n   **Prerequisites**: Basic understanding of neural networks and Python\n\nThis tutorial introduces brain-inspired learning through Hebbian plasticity and associative memory using Hopfield networks :cite:p:`amari1977neural,hopfield1982neural`.\n\n----\n"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": ".. _introduction-hebbian-learning-01:\n\n1. Introduction to Hebbian Learning\n-----------------------------------\n\n1.1 The Hebbian Principle\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**\"Neurons that fire together, wire together\"**\n\nHebbian learning :cite:p:`hebb2005organization` is a biologically-inspired learning rule where synaptic strength increases when pre- and post-synaptic neurons are simultaneously active:\n\n$$\n\\Delta W_{ij} = \\eta \\cdot x_i \\cdot x_j\n$$\n\nWhere:\n- $W_{ij}$—Synaptic weight from neuron i to neuron j\n- $\\eta$—Learning rate\n- $x_i, x_j$—Activities of neurons i and j\n\n1.2 Why Hebbian Learning?\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**Biological realism**:\n- Local learning rule (no global error signal needed)\n- Activity-dependent plasticity\n- Matches experimental observations in cortex\n\n**Computational advantages**:\n- Unsupervised learning\n- Pattern completion and noise resistance\n- Attractor dynamics for memory\n\n1.3 Hebbian Learning in CANNs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canns.models.brain_inspired import AmariHopfieldNetwork  # :cite:p:`amari1977neural,hopfield1982neural`\n",
    "from canns.trainer import HebbianTrainer\n",
    "\n",
    "# Create network\n",
    "model = AmariHopfieldNetwork(num_neurons=784)  # 28x28 images\n",
    "\n",
    "# Create trainer with Hebbian learning\n",
    "trainer = HebbianTrainer(model, compiled_prediction=True)\n",
    "\n",
    "# Train on patterns\n",
    "trainer.train(pattern_list)\n",
    "\n",
    "# Retrieve from corrupted input\n",
    "output = trainer.predict(noisy_pattern)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _hopfield-networks-memory-01:\n",
    "\n",
    "2. Hopfield Networks for Associative Memory\n",
    "-------------------------------------------\n",
    "\n",
    "2.1 What is a Hopfield Network?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "A **Hopfield network** is a recurrent neural network that stores patterns as stable attractors in its energy landscape:\n",
    "\n",
    "- **Storage**: Hebbian learning creates attractors at training patterns\n",
    "- **Retrieval**: Network dynamics converge to nearest stored pattern\n",
    "- **Capacity**: Can store approximately $0.138 \\times N$ patterns ($N$ = number of neurons)\n",
    "\n",
    "2.2 Network Dynamics\n",
    "~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The Hopfield network updates its state to minimize energy:\n",
    "\n",
    "$$\n",
    "E = -\\frac{1}{2} \\sum_{i,j} W_{ij} \\cdot x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "**Update rules**:\n",
    "- **Asynchronous**: Update one neuron at a time\n",
    "- **Synchronous**: Update all neurons simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canns.models.brain_inspired import AmariHopfieldNetwork\n",
    "\n",
    "# Discrete activation (sign function): x ∈ {-1, +1}\n",
    "model = AmariHopfieldNetwork(\n",
    "    num_neurons=784,\n",
    "    threshold=80.0,      # Convergence threshold\n",
    "    asyn=False,          # Synchronous updates\n",
    "    activation=\"sign\"    # Binary states {-1, +1}\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "2.3 Key Parameters\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    ".. list-table::\n",
    "   :widths: 20 15 65\n",
    "   :header-rows: 1\n",
    "\n",
    "   * - Parameter\n",
    "     - Type\n",
    "     - Description\n",
    "   * - ``num_neurons``\n",
    "     - int\n",
    "     - Network size (e.g., 784 for 28×28 images)\n",
    "   * - ``threshold``\n",
    "     - float\n",
    "     - Convergence criterion (max iterations)\n",
    "   * - ``asyn``\n",
    "     - bool\n",
    "     - Asynchronous (True) or synchronous (False) updates\n",
    "   * - ``activation``\n",
    "     - str\n",
    "     - \"sign\" (discrete) or \"tanh\"/\"sigmoid\" (continuous)\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _mnist-digit-memory-01:\n",
    "\n",
    "3. Complete Example: MNIST Digit Memory\n",
    "---------------------------------------\n",
    "\n",
    "3.1 Setup and Data Loading\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc22478bf461420c9e0e33fccc60f1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a50fddb09f24ce7befc71c32eb621f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mnist/train-00000-of-00001.parquet:   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085728030c164fa6a9b719d24c6255e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mnist/test-00000-of-00001.parquet:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aef3fd18ef40108f904a796c6bd6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7386e9d182e4df7b0c8c035bc23897d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60000 training images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from canns.models.brain_inspired import AmariHopfieldNetwork\n",
    "from canns.trainer import HebbianTrainer\n",
    "\n",
    "# Load MNIST data (automatically tries multiple sources)\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load MNIST using available library (datasets/torchvision/keras/sklearn)\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        ds_train = load_dataset(\"mnist\", split=\"train\")\n",
    "        x_train = np.stack([np.array(img, dtype=np.float32) for img in ds_train[\"image\"]])\n",
    "        y_train = np.array(ds_train[\"label\"], dtype=np.int64)\n",
    "        return x_train, y_train\n",
    "    except:\n",
    "        # Fallback to other sources...\n",
    "        from torchvision.datasets import MNIST\n",
    "        ds_train = MNIST(root=\"~/.cache/torchvision\", train=True, download=True)\n",
    "        x_train = ds_train.data.numpy().astype(np.float32)\n",
    "        y_train = ds_train.targets.numpy().astype(np.int64)\n",
    "        return x_train, y_train\n",
    "\n",
    "x_train, y_train = load_mnist_data()\n",
    "print(f\"Loaded {len(x_train)} training images\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "3.2 Data Preprocessing\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Hopfield networks work best with binary patterns {-1, +1}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 3 training patterns\n",
      "Pattern dimensions: (784,)\n"
     ]
    }
   ],
   "source": [
    "def threshold_to_binary(image, use_mean=True):\n",
    "    \"\"\"Convert grayscale image to {-1, +1}\"\"\"\n",
    "    if use_mean:\n",
    "        threshold = image.mean()\n",
    "    else:\n",
    "        from skimage.filters import threshold_mean\n",
    "        threshold = threshold_mean(image)\n",
    "\n",
    "    binary = image > threshold\n",
    "    return np.where(binary, 1.0, -1.0).astype(np.float32)\n",
    "\n",
    "def flatten_image(image_2d):\n",
    "    \"\"\"Flatten 2D image to 1D vector\"\"\"\n",
    "    return image_2d.reshape(-1)\n",
    "\n",
    "# Select patterns to store (e.g., digits 0, 1, 2)\n",
    "classes = [0, 1, 2]\n",
    "\n",
    "# Get one training example per class\n",
    "train_patterns = []\n",
    "for digit in classes:\n",
    "    # Find first occurrence of this digit\n",
    "    idx = np.where(y_train == digit)[0][0]\n",
    "    img_2d = x_train[idx]\n",
    "\n",
    "    # Convert to binary and flatten\n",
    "    binary = threshold_to_binary(img_2d)\n",
    "    flat = flatten_image(binary)\n",
    "    train_patterns.append(flat)\n",
    "\n",
    "print(f\"Prepared {len(train_patterns)} training patterns\")\n",
    "print(f\"Pattern dimensions: {train_patterns[0].shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "3.3 Create and Train Hopfield Network\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create Hopfield network\n",
    "n_neurons = train_patterns[0].size  # 784 for 28×28 images\n",
    "model = AmariHopfieldNetwork(\n",
    "    num_neurons=n_neurons,\n",
    "    threshold=80.0,        # Max iterations for convergence\n",
    "    asyn=False,            # Synchronous updates\n",
    "    activation=\"sign\"      # Binary activation\n",
    ")\n",
    "\n",
    "# Create Hebbian trainer\n",
    "trainer = HebbianTrainer(\n",
    "    model,\n",
    "    compiled_prediction=True  # Use JIT compilation for speed\n",
    ")\n",
    "\n",
    "# Train on patterns (one-shot learning!)\n",
    "print(\"Training network...\")\n",
    "trainer.train(train_patterns)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Training details**:\n",
    "- **One-shot learning**: Patterns stored in single pass\n",
    "- **Weight update**: $W = \\frac{1}{P} \\sum_{\\mu} \\mathbf{p}_\\mu \\mathbf{p}_\\mu^T$ (normalized Hebbian)\n",
    "- **Diagonal zeroing**: $W_{ii} = 0$ (no self-connections)\n",
    "- **Mean subtraction**: Optional for better storage\n",
    "\n",
    "3.4 Pattern Retrieval\n",
    "~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Test the network's ability to retrieve stored patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|█████████████| 3/3 [00:00<00:00,  6.89it/s, sample=3/3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test patterns (different examples of same digits)\n",
    "test_patterns = []\n",
    "for digit in classes:\n",
    "    # Find second occurrence (different from training)\n",
    "    idx = np.where(y_train == digit)[0][1]\n",
    "    img_2d = x_train[idx]\n",
    "    binary = threshold_to_binary(img_2d)\n",
    "    flat = flatten_image(binary)\n",
    "    test_patterns.append(flat)\n",
    "\n",
    "# Retrieve patterns\n",
    "print(\"Retrieving patterns...\")\n",
    "retrieved = trainer.predict_batch(\n",
    "    test_patterns,\n",
    "    show_sample_progress=True  # Show convergence progress\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(retrieved)} patterns\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "3.5 Visualization\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Compare training, input, and retrieved patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape_to_image(flat_vector):\n",
    "    \"\"\"Reshape 1D vector back to 2D image\"\"\"\n",
    "    dim = int(np.sqrt(flat_vector.size))\n",
    "    return flat_vector.reshape(dim, dim)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(len(classes), 3, figsize=(6, 2*len(classes)))\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    # Training pattern\n",
    "    axes[i, 0].imshow(reshape_to_image(train_patterns[i]), cmap='gray')\n",
    "    axes[i, 0].axis('off')\n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title('Stored Pattern')\n",
    "\n",
    "    # Test input\n",
    "    axes[i, 1].imshow(reshape_to_image(test_patterns[i]), cmap='gray')\n",
    "    axes[i, 1].axis('off')\n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title('Input Pattern')\n",
    "\n",
    "    # Retrieved pattern\n",
    "    axes[i, 2].imshow(reshape_to_image(retrieved[i]), cmap='gray')\n",
    "    axes[i, 2].axis('off')\n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title('Retrieved Pattern')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hopfield_mnist_retrieval.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Expected results**:\n",
    "- Retrieved patterns closely match stored patterns\n",
    "- Network completes patterns despite differences in input\n",
    "- Some distortions possible (depends on pattern similarity)\n",
    "\n",
    "3.6 Testing with Noise\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Add noise to test robustness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 0% noise:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|█████████████| 3/3 [00:00<00:00, 15.25it/s, sample=3/3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean accuracy: 97.07%\n",
      "\n",
      "Testing with 10% noise:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|█████████████| 3/3 [00:00<00:00, 15.36it/s, sample=3/3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean accuracy: 97.07%\n",
      "\n",
      "Testing with 20% noise:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|█████████████| 3/3 [00:00<00:00, 16.99it/s, sample=3/3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean accuracy: 97.07%\n",
      "\n",
      "Testing with 30% noise:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|█████████████| 3/3 [00:00<00:00, 15.49it/s, sample=3/3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean accuracy: 88.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_noise(pattern, noise_level=0.1):\n",
    "    \"\"\"Flip random bits with given probability\"\"\"\n",
    "    noise_mask = np.random.random(pattern.shape) < noise_level\n",
    "    noisy = pattern.copy()\n",
    "    noisy[noise_mask] *= -1  # Flip sign\n",
    "    return noisy\n",
    "\n",
    "# Test with noisy inputs\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "for noise in noise_levels:\n",
    "    print(f\"\\nTesting with {int(noise*100)}% noise:\")\n",
    "    noisy_inputs = [add_noise(p, noise) for p in test_patterns]\n",
    "    recovered = trainer.predict_batch(noisy_inputs)\n",
    "\n",
    "    # Calculate accuracy (proportion of correct bits)\n",
    "    accuracies = []\n",
    "    for orig, recov in zip(train_patterns, recovered):\n",
    "        accuracy = np.mean(orig == recov)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"  Mean accuracy: {np.mean(accuracies):.2%}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Observations**:\n",
    "- Low noise (< 20%): Near-perfect retrieval\n",
    "- Medium noise (20-30%): Degraded but recognizable\n",
    "- High noise (> 40%): May converge to wrong pattern\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": ".. _next-steps-01-pattern:\n\n4. Next Steps\n-------------\n\nCongratulations! You've learned the basics of brain-inspired learning with Hebbian plasticity and associative memory.\n\nKey Takeaways\n~~~~~~~~~~~~~\n\n1. **Hebbian learning** is local, unsupervised, and biologically realistic\n2. **Hopfield networks** :cite:p:`hopfield1982neural` store patterns as energy minima\n3. **One-shot learning** possible with Hebbian rule\n4. **Pattern completion** works with partial/noisy inputs\n\nWhen to Use Hebbian Learning\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- **Associative memory**—Store and retrieve patterns\n- **Unsupervised learning**—No labels required\n- **One-shot learning**—Learn from few examples\n- **Pattern completion**—Robust to noise and missing data\n- **Biological modeling**—Match neural plasticity\n\nLimitations\n~~~~~~~~~~~\n\n- **Capacity limits**—$\\sim 0.138 \\times N$ patterns for $N$ neurons\n- **Spurious attractors**—Network may converge to unwanted states\n- **No error minimization**—Unlike backpropagation\n- **Binary patterns**—Works best with $\\{-1, +1\\}$ states\n\nContinue Learning\n~~~~~~~~~~~~~~~~~\n\nAdvanced Topics\n~~~~~~~~~~~~~~~\n\n- **Storage capacity**—Theoretical limits and practical considerations\n- **Energy landscape**—Understanding attractor basins\n- **Continuous Hopfield**—Using continuous activations (tanh/sigmoid)\n- **Modern Hopfield**—Recent advances with exponential capacity\n\nRelated Models\n~~~~~~~~~~~~~~\n\nThese brain-inspired models are available in the CANNs library:\n\n- ``AmariHopfieldNetwork``—Pattern storage and associative memory\n- ``LinearLayer``—Generic feedforward layer with Hebbian learning\n- ``SpikingLayer``—Leaky integrate-and-fire neurons for STDP\n\nFor more details, explore ``canns.models.brain_inspired`` and ``canns.trainer`` modules."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}