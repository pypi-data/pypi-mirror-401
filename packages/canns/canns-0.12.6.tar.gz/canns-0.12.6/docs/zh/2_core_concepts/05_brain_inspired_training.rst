========================
脑启发训练
========================

本文档解释了CANNs库中的脑启发学习机制和Trainer框架。

概览
========

训练器模块（``canns.trainer``）提供了统一的接口，用于使用生物学合理的学习规则训练脑启发模型。与使用反向传播的传统深度学习不同，这些方法依赖于局部、活动依赖的可塑性。

核心概念：活动依赖可塑性
============================================

脑启发学习背后的统一原则是 **突触修改取决于神经活动模式** 而不是显式误差信号。

关键特征
-------------------

**仅局部信息**
   权重变化取决于：

   * 突触前神经元活动
   * 突触后神经元活动
   * 可能的局部神经调节信号

   没有全局误差信号通过网络传播。

**基于相关的学习**
   当突触前和突触后神经元共同活跃时，突触增强。这捕获了输入模式中的统计规律性。

**自组织**
   网络结构从经验中涌现。吸引子模式自然形成于对相似输入的重复暴露。

学习规则
==============

库支持几种经典学习规则，每种都捕获了生物突触可塑性的不同方面。

Hebbian学习
----------------

**基本原则："一起放电的神经元，连接在一起。"** :cite:p:`hebb2005organization`

机制
~~~~~~~~~

当两个连接的神经元同时活跃时，它们之间的突触增强。数学上：

.. math::

   \Delta W_{ij} \propto r_i \times r_j

其中 :math:`r_i` 是突触前活动， :math:`r_j` 是突触后活动。

用例
~~~~~~~~~

* 联想记忆中的模式存储
* 无监督特征学习
* 自组织映射

STDP（尖峰时序依赖可塑性）
-----------------------------------------

权重变化取决于突触前和突触后尖峰的精确时序 :cite:p:`bi1998synaptic`。

机制
~~~~~~~~~

* **前先于后**：增强（加强突触）
* **后先于前**：抑制（削弱突触）
* 幅度取决于时间差

用例
~~~~~~~~~

* 时间序列学习
* 因果关系检测
* 输入时序敏感任务

BCM（Bienenstock-Cooper-Munro）规则
------------------------------------

权重变化取决于突触后活动相对于滑动阈值 :cite:p:`bienenstock1982theory`。

机制
~~~~~~~~~

* **高于阈值**：增强
* **低于阈值**：抑制
* 阈值根据平均活动适应

用例
~~~~~~~~~

* 选择性发展
* 防止失控兴奋
* 具有稳态的稳定学习

Trainer框架
=================

设计理念
----------------

Trainer类与模型类分离有几个原因：

**关注点分离**
   模型定义动力学。训练器定义学习。这种分离允许：

   * 具有不同学习规则的相同模型架构
   * 应用于不同模型的相同学习规则
   * 更清晰的代码组织

**可交换的学习规则**
   轻松实验不同的可塑性机制：

   * Hebbian用于某些实验
   * STDP用于时间任务
   * 自定义规则用于特定假设

**统一API**
   所有训练器遵循相同的接口：

   * ``train(train_data)``: 主训练循环
   * ``predict(pattern)``: 单样本推理
   * 进度显示的配置方法

实现自定义训练器
-----------------------------

要创建新训练器，从 ``canns.trainer.Trainer`` 继承并实现：

**构造函数**
   存储目标模型引用和配置：

   * 要训练的模型实例
   * 学习率参数
   * 进度显示设置

**train(self, train_data)**
   定义参数更新策略：

   * 迭代训练模式
   * 应用学习规则修改权重
   * 跟踪进度和收敛

**predict(self, pattern, \*args, \*\*kwargs)**
   定义单样本推理：

   * 向网络呈现模式
   * 允许动力学演化
   * 返回最终状态或读出

**predict_batch(self, patterns)**
   可选的批量推理包装器：

   * 为每个模式调用 ``predict()``
   * 聚合结果
   * 高效评估测试集

**configure_progress(self, show_progress, use_compile)**
   标准进度配置：

   * 启用/禁用进度条
   * 切换JIT编译以提速
   * 用户友好的训练监控

模型-训练器交互
--------------------------

训练器通过约定的属性与模型交互：

权重访问
~~~~~~~~~~~~~

训练器期望模型将权重提供为 ``ParamState``:

* 默认属性: ``model.W``
* 通过 ``model.weight_attr`` 属性自定义属性
* 学习期间直接修改

状态访问
~~~~~~~~~~~~

训练器读取网络状态：

* ``model.s`` 用于状态向量
* ``model.energy`` 用于收敛监控
* 模型特定的诊断量

初始化
~~~~~~~~~~~~~~

训练器可能调用：

* ``model.init_state()`` 在每个模式之前重置
* ``model.update()`` 在动力学演化期间
* 模型方法用于专门操作

训练工作流
=================

典型用法
-------------

1. **创建模型**

   * 实例化脑启发模型
   * 初始化状态和权重

2. **创建训练器**

   * 实例化适当的训练器
   * 配置学习参数

3. **准备训练数据**

   * 将模式格式化为数组
   * 确保与模型大小兼容

4. **训练**

   * 调用 ``trainer.train(patterns)``
   * 监控进度和能量

5. **评估**

   * 测试模式完成
   * 测量吸引子质量
   * 评估存储容量

进度监控
-------------------

训练器提供反馈：

* 当前训练迭代
* 网络能量演化
* 收敛指标

可选编译：

* JIT编译以加快训练
* 禁用以进行调试
* 用户控制的权衡

高级主题
===============

自定义学习规则
---------------------

除了标准规则外，用户还可以实现：

* 稳态机制
* 基于竞争的学习
* 调节门控

只需在自定义Trainer子类中覆盖权重更新逻辑。

容量和泛化
----------------------------

脑启发训练引发的问题：

* 可以存储多少个模式？
* 什么决定检索准确性？
* 噪声如何影响性能？

训练器框架支持对这些属性的系统研究。

与分析集成
--------------------------

训练后：

* 可视化学习的权重矩阵
* 分析吸引子盆地
* 与理论预测进行比较

分析工具与训练后的脑启发模型无缝配合。

总结
=======

脑启发训练模块提供：

1. **活动依赖可塑性** - 基于神经相关的局部学习
2. **多种学习规则** - Hebbian、STDP、BCM实现
3. **统一训练器接口** - 所有学习方法的一致API
4. **模型-训练器分离** - 实现灵活性的清晰架构

该框架支持对生物学合理的学习机制、联想记忆形成和自组织神经系统的研究——所有这些都在 CANN 范式内。
