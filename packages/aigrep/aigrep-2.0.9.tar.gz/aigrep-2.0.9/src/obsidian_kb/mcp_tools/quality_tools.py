"""MCP tools –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""

import asyncio
import logging
import time
from collections import Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING

from obsidian_kb.service_container import get_service_container
from obsidian_kb.types import RetrievalGranularity, SearchRequest, VaultNotFoundError

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ—Ä–≤–∏—Å–æ–≤
services = get_service_container()


async def index_coverage(vault_name: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –∏–Ω–¥–µ–∫—Å–∞.
    
    Args:
        vault_name: –ò–º—è vault'–∞
    
    Returns:
        –û—Ç—á—ë—Ç –æ –ø–æ–∫—Ä—ã—Ç–∏–∏:
        - –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ / –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ / –æ–∂–∏–¥–∞–µ—Ç
        - –§–∞–π–ª—ã –±–µ–∑ enrichment
        - –§–∞–π–ª—ã —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º –∏–Ω–¥–µ–∫—Å–æ–º
        - –ë–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∏–Ω–¥–µ–∫—Å–µ
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–µ—Ä–µ–∑ DiagnosticsService
        coverage_data = await services.diagnostics_service.index_coverage(vault_name)
        
        if "error" in coverage_data:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–∫—Ä—ã—Ç–∏—è: {coverage_data['error']}"
        
        lines = [f"## –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –∏–Ω–¥–µ–∫—Å–∞: {vault_name}\n"]
        
        total_files = coverage_data.get("total_files", 0)
        indexed_files = coverage_data.get("indexed_files", 0)
        coverage_percent = coverage_data.get("coverage_percent", 0.0)
        total_chunks = coverage_data.get("total_chunks", 0)
        
        lines.append("### –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
        lines.append(f"- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:** {total_files}")
        lines.append(f"- **–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ:** {indexed_files}")
        lines.append(f"- **–û–∂–∏–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:** {total_files - indexed_files}")
        lines.append(f"- **–ü–æ–∫—Ä—ã—Ç–∏–µ:** {coverage_percent:.1f}%")
        lines.append(f"- **–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤:** {total_chunks}")
        lines.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        type_stats = coverage_data.get("type_stats", {})
        if type_stats:
            lines.append("### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")
            lines.append("| –¢–∏–ø | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
            lines.append("|-----|-----------|")
            for doc_type, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
                lines.append(f"| {doc_type} | {count} |")
            lines.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–≥–∞–º
        tag_stats = coverage_data.get("tag_stats", {})
        if tag_stats:
            # tag_stats –º–æ–∂–µ—Ç –±—ã—Ç—å {"frontmatter": {...}, "inline": {...}} –∏–ª–∏ {tag: count}
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–≥–∏ –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å
            combined_tags: dict[str, int] = {}
            if isinstance(tag_stats, dict):
                for key, value in tag_stats.items():
                    if isinstance(value, dict):
                        # –í–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (frontmatter/inline)
                        for tag, count in value.items():
                            if isinstance(count, (int, float)):
                                combined_tags[tag] = combined_tags.get(tag, 0) + int(count)
                    elif isinstance(value, (int, float)):
                        # –ü—Ä–æ—Å—Ç–æ–π {tag: count}
                        combined_tags[key] = int(value)

            if combined_tags:
                lines.append("### –¢–æ–ø-10 —Ç–µ–≥–æ–≤\n")
                lines.append("| –¢–µ–≥ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
                lines.append("|-----|-----------|")
                for tag, count in sorted(combined_tags.items(), key=lambda x: x[1], reverse=True)[:10]:
                    lines.append(f"| {tag} | {count} |")
                lines.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Å—ã–ª–∫–∞–º
        link_stats = coverage_data.get("link_stats", {})
        if link_stats:
            lines.append("### –¢–æ–ø-10 —Å—Å—ã–ª–æ–∫\n")
            lines.append("| –°—Å—ã–ª–∫–∞ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
            lines.append("|--------|-----------|")
            for link, count in sorted(link_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
                lines.append(f"| {link} | {count} |")
            lines.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        lines.append("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n")
        if coverage_percent < 50:
            lines.append("- ‚ö†Ô∏è –ü–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∏–∑–∫–æ–µ (<50%). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è.")
        elif coverage_percent < 80:
            lines.append("- ‚ö†Ô∏è –ü–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å—Ä–µ–¥–Ω–µ–µ (<80%). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤.")
        else:
            lines.append("- ‚úÖ –ü–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ö–æ—Ä–æ—à–µ–µ (‚â•80%).")
        
        if total_files - indexed_files > 0:
            lines.append(
                f"- üìù –ù–∞–π–¥–µ–Ω–æ {total_files - indexed_files} —Ñ–∞–π–ª–æ–≤, –æ–∂–∏–¥–∞—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. "
                f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `index_documents(\"{vault_name}\")` –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."
            )
        
        return "\n".join(lines)
    
    except VaultNotFoundError:
        return f"‚ùå Vault '{vault_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω."
    except Exception as e:
        logger.error(f"Error in index_coverage: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è: {e}"


async def test_retrieval(
    vault_name: str,
    queries: list[str],
    expected_docs: list[str] | None = None,
) -> str:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ retrieval.
    
    Args:
        vault_name: –ò–º—è vault'–∞
        queries: –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        expected_docs: –û–∂–∏–¥–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ recall)
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
        - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: top-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        - Recall@5 –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã expected_docs
        - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
    """
    try:
        lines = [f"## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ retrieval: {vault_name}\n"]
        lines.append(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤:** {len(queries)}\n")
        
        results = []
        total_time = 0.0
        
        for i, query in enumerate(queries, 1):
            lines.append(f"### –ó–∞–ø—Ä–æ—Å {i}: `{query}`\n")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            start_time = time.time()
            request = SearchRequest(
                vault_name=vault_name,
                query=query,
                limit=5,
                search_type="hybrid",
                granularity=RetrievalGranularity.CHUNK,
            )
            response = await services.search_service.search(request)
            elapsed_time = (time.time() - start_time) * 1000
            
            total_time += elapsed_time
            
            if not response.results:
                lines.append("*–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã*\n")
                results.append({
                    "query": query,
                    "found": 0,
                    "found_paths": [],
                    "time_ms": elapsed_time,
                })
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º recall –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç
                if expected_docs and i <= len(expected_docs):
                    expected = expected_docs[i - 1]
                    lines.append(f"**Recall@5:** ‚ùå 0%")
                    lines.append(f"**–û–∂–∏–¥–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:** `{expected}`")
                    lines.append(f"‚ö†Ô∏è –û–∂–∏–¥–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω (—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç)")
                    lines.append("")
                continue
            
            lines.append(f"**–ù–∞–π–¥–µ–Ω–æ:** {len(response.results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            lines.append(f"**–í—Ä–µ–º—è:** {elapsed_time:.1f} –º—Å\n")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for j, result in enumerate(response.results[:5], 1):
                score = result.score.value if result.score else 0.0
                lines.append(f"{j}. **{result.title or result.file_path}** (score: {score:.3f})")
                lines.append(f"   - –ü—É—Ç—å: `{result.file_path}`")
                if result.snippet:
                    snippet_preview = result.snippet[:100].replace("\n", " ")
                    if len(result.snippet) > 100:
                        snippet_preview += "..."
                    lines.append(f"   - Snippet: {snippet_preview}")
                lines.append("")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º recall –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã expected_docs
            if expected_docs and i <= len(expected_docs):
                expected = expected_docs[i - 1]
                found_paths = [r.file_path for r in response.results]
                recall = 1.0 if expected in found_paths else 0.0
                lines.append(f"**Recall@5:** {'‚úÖ' if recall > 0 else '‚ùå'} {recall * 100:.0f}%")
                lines.append(f"**–û–∂–∏–¥–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:** `{expected}`")
                if expected not in found_paths:
                    lines.append(f"‚ö†Ô∏è –û–∂–∏–¥–∞–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ top-5")
                lines.append("")
            
            results.append({
                "query": query,
                "found": len(response.results),
                "found_paths": [r.file_path for r in response.results],
                "time_ms": elapsed_time,
            })
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lines.append("### –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
        avg_time = total_time / len(queries) if queries else 0.0
        avg_results = sum(r["found"] for r in results) / len(results) if results else 0.0
        
        lines.append(f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞:** {avg_time:.1f} –º—Å")
        lines.append(f"- **–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:** {avg_results:.1f}")
        
        if expected_docs:
            total_recall = sum(
                1.0 if expected_docs[i] in results[i].get("found_paths", [])
                else 0.0
                for i in range(min(len(results), len(expected_docs)))
            )
            avg_recall = total_recall / len(expected_docs) if expected_docs else 0.0
            lines.append(f"- **–°—Ä–µ–¥–Ω–∏–π Recall@5:** {avg_recall * 100:.1f}%")
        
        return "\n".join(lines)
    
    except VaultNotFoundError:
        return f"‚ùå Vault '{vault_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω."
    except Exception as e:
        logger.error(f"Error in test_retrieval: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è retrieval: {e}"


async def audit_index(vault_name: str) -> str:
    """–ê—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω–¥–µ–∫—Å–∞.
    
    Args:
        vault_name: –ò–º—è vault'–∞
    
    Returns:
        –û—Ç—á—ë—Ç –æ–± –∞—É–¥–∏—Ç–µ:
        - –ö–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ/–º–∞–ª–µ–Ω—å–∫–∏–µ)
        - –ö–∞—á–µ—Å—Ç–≤–æ enrichment (–ø—É—Å—Ç—ã–µ context_prefix)
        - –î—É–±–ª–∏–∫–∞—Ç—ã
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    """
    try:
        lines = [f"## –ê—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω–¥–µ–∫—Å–∞: {vault_name}\n"]
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤
        from obsidian_kb.config.manager import get_config_manager

        config_manager = get_config_manager()
        config = config_manager.get_config(vault_name)
        
        chunk_size = config.indexing.chunk_size
        min_chunk_size = config.indexing.min_chunk_size
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        chunk_repository = services.chunk_repository
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É vault'–∞
        stats = await services.db_manager.get_vault_stats(vault_name)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        chunks_table = await services.db_manager._ensure_table(vault_name, "chunks")
        db = services.db_manager._get_db()
        
        def _analyze_chunks():
            try:
                arrow_table = chunks_table.to_arrow()
                
                if arrow_table.num_rows == 0:
                    return {
                        "total": 0,
                        "large_chunks": 0,
                        "small_chunks": 0,
                        "empty_context": 0,
                        "duplicates": 0,
                    }
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞)
                texts = arrow_table["content"].to_pylist()
                
                # –û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
                large_chunks = sum(1 for text in texts if len(text) > chunk_size * 4)
                small_chunks = sum(1 for text in texts if len(text) < min_chunk_size * 4)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º context_prefix –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ
                empty_context = 0
                if "context_prefix" in arrow_table.column_names:
                    context_prefixes = arrow_table["context_prefix"].to_pylist()
                    empty_context = sum(1 for cp in context_prefixes if not cp or cp.strip() == "")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
                text_counter = Counter(texts)
                duplicates = sum(count - 1 for count in text_counter.values() if count > 1)
                
                return {
                    "total": arrow_table.num_rows,
                    "large_chunks": large_chunks,
                    "small_chunks": small_chunks,
                    "empty_context": empty_context,
                    "duplicates": duplicates,
                }
            except Exception as e:
                logger.error(f"Error analyzing chunks: {e}")
                return {
                    "total": 0,
                    "large_chunks": 0,
                    "small_chunks": 0,
                    "empty_context": 0,
                    "duplicates": 0,
                }
        
        analysis = await asyncio.to_thread(_analyze_chunks)
        
        total_chunks = analysis["total"]
        large_chunks = analysis["large_chunks"]
        small_chunks = analysis["small_chunks"]
        empty_context = analysis["empty_context"]
        duplicates = analysis["duplicates"]
        
        lines.append("### –ö–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤\n")
        lines.append(f"- **–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤:** {total_chunks}")
        lines.append(f"- **–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ (> {chunk_size} —Ç–æ–∫–µ–Ω–æ–≤):** {large_chunks}")
        lines.append(f"- **–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ (< {min_chunk_size} —Ç–æ–∫–µ–Ω–æ–≤):** {small_chunks}")
        lines.append("")
        
        if large_chunks > 0:
            large_percent = (large_chunks / total_chunks * 100) if total_chunks > 0 else 0
            lines.append(f"‚ö†Ô∏è {large_percent:.1f}% —á–∞–Ω–∫–æ–≤ –ø—Ä–µ–≤—ã—à–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä.")
        
        if small_chunks > 0:
            small_percent = (small_chunks / total_chunks * 100) if total_chunks > 0 else 0
            lines.append(f"‚ö†Ô∏è {small_percent:.1f}% —á–∞–Ω–∫–æ–≤ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.")
        
        if large_chunks == 0 and small_chunks == 0:
            lines.append("‚úÖ –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã.")
        
        lines.append("")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ enrichment
        lines.append("### –ö–∞—á–µ—Å—Ç–≤–æ enrichment\n")
        if empty_context > 0:
            empty_percent = (empty_context / total_chunks * 100) if total_chunks > 0 else 0
            lines.append(f"‚ö†Ô∏è **–ü—É—Å—Ç—ã–µ context_prefix:** {empty_context} ({empty_percent:.1f}%)")
            lines.append("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å –≤–∫–ª—é—á—ë–Ω–Ω—ã–º enrichment.")
        else:
            lines.append("‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ –∏–º–µ—é—Ç context_prefix.")
        
        lines.append("")
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã
        lines.append("### –î—É–±–ª–∏–∫–∞—Ç—ã\n")
        if duplicates > 0:
            lines.append(f"‚ö†Ô∏è **–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:** {duplicates}")
            lines.append("   –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç.")
        else:
            lines.append("‚úÖ –î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
        lines.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        lines.append("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n")
        
        recommendations = []
        
        if large_chunks > total_chunks * 0.1:  # >10% –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤
            recommendations.append(
                f"- –£–º–µ–Ω—å—à–∏—Ç–µ `chunk_size` –¥–æ {int(chunk_size * 0.8)} –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é `semantic` chunking"
            )
        
        if small_chunks > total_chunks * 0.1:  # >10% –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
            recommendations.append(
                f"- –£–≤–µ–ª–∏—á—å—Ç–µ `min_chunk_size` –¥–æ {int(min_chunk_size * 1.2)} –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏"
            )
        
        if empty_context > total_chunks * 0.2:  # >20% –±–µ–∑ enrichment
            recommendations.append(
                f"- –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å `enrichment=\"contextual\"` –∏–ª–∏ `enrichment=\"full\"`"
            )
        
        if duplicates > 0:
            recommendations.append(
                "- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
            )
        
        if not recommendations:
            recommendations.append("‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –∏–Ω–¥–µ–∫—Å–∞ —Ö–æ—Ä–æ—à–µ–µ, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–µ—Ç.")
        
        for rec in recommendations:
            lines.append(rec)
        
        return "\n".join(lines)
    
    except VaultNotFoundError:
        return f"‚ùå Vault '{vault_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω."
    except Exception as e:
        logger.error(f"Error in audit_index: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –∞—É–¥–∏—Ç–∞ –∏–Ω–¥–µ–∫—Å–∞: {e}"


async def cost_report(
    vault_name: str | None = None,
    period: str = "month",  # day | week | month | all
) -> str:
    """–û—Ç—á—ë—Ç –æ –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ LLM.
    
    Args:
        vault_name: –§–∏–ª—å—Ç—Ä –ø–æ vault'—É
        period: –ü–µ—Ä–∏–æ–¥ –æ—Ç—á—ë—Ç–∞ (day | week | month | all)
    
    Returns:
        –†–∞–∑–±–∏–≤–∫–∞ –∑–∞—Ç—Ä–∞—Ç:
        - –ü–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º
        - –ü–æ –æ–ø–µ—Ä–∞—Ü–∏—è–º (embedding / enrichment)
        - –ü–æ vault'–∞–º
        - –¢—Ä–µ–Ω–¥ vs –ø—Ä–æ—à–ª—ã–π –ø–µ—Ä–∏–æ–¥
    """
    try:
        from obsidian_kb.quality.cost_tracker import CostTracker
        
        cost_tracker = CostTracker()
        costs_data = await cost_tracker.get_costs(
            vault_name=vault_name,
            period=period,
        )
        
        lines = ["## –û—Ç—á—ë—Ç –æ –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ LLM\n"]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞
        period_names = {
            "day": "–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞",
            "week": "–ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è",
            "month": "–ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü",
            "all": "–≤—Å—ë –≤—Ä–µ–º—è",
        }
        period_name = period_names.get(period, period)
        
        lines.append(f"**–ü–µ—Ä–∏–æ–¥:** {period_name}")
        if vault_name:
            lines.append(f"**Vault:** {vault_name}")
        lines.append("")
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_cost = costs_data.get("total_cost_usd", 0.0)
        total_input_tokens = costs_data.get("total_input_tokens", 0)
        total_output_tokens = costs_data.get("total_output_tokens", 0)
        record_count = costs_data.get("record_count", 0)
        
        lines.append("### –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∑–∞—Ç—Ä–∞—Ç:** ${total_cost:.4f}")
        lines.append(f"- **–í—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:** {total_input_tokens:,}")
        lines.append(f"- **–í—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:** {total_output_tokens:,}")
        lines.append(f"- **–ó–∞–ø–∏—Å–µ–π:** {record_count}")
        lines.append("")
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º
        by_provider = costs_data.get("by_provider", {})
        if by_provider:
            lines.append("### –ü–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º\n")
            lines.append("| –ü—Ä–æ–≤–∞–π–¥–µ—Ä | –ó–∞—Ç—Ä–∞—Ç—ã (USD) | % |")
            lines.append("|-----------|---------------|---|")
            for provider, cost in sorted(by_provider.items(), key=lambda x: x[1], reverse=True):
                percentage = (cost / total_cost * 100) if total_cost > 0 else 0
                lines.append(f"| {provider} | ${cost:.4f} | {percentage:.1f}% |")
            lines.append("")
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ vault'–∞–º
        by_vault = costs_data.get("by_vault", {})
        if by_vault:
            lines.append("### –ü–æ vault'–∞–º\n")
            lines.append("| Vault | –ó–∞—Ç—Ä–∞—Ç—ã (USD) | % |")
            lines.append("|-------|---------------|---|")
            for vault, cost in sorted(by_vault.items(), key=lambda x: x[1], reverse=True):
                percentage = (cost / total_cost * 100) if total_cost > 0 else 0
                lines.append(f"| {vault} | ${cost:.4f} | {percentage:.1f}% |")
            lines.append("")
        
        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –æ–ø–µ—Ä–∞—Ü–∏–π
        by_operation = costs_data.get("by_operation", {})
        if by_operation:
            lines.append("### –ü–æ —Ç–∏–ø–∞–º –æ–ø–µ—Ä–∞—Ü–∏–π\n")
            lines.append("| –û–ø–µ—Ä–∞—Ü–∏—è | –ó–∞—Ç—Ä–∞—Ç—ã (USD) | % |")
            lines.append("|----------|---------------|---|")
            for op_type, cost in sorted(by_operation.items(), key=lambda x: x[1], reverse=True):
                percentage = (cost / total_cost * 100) if total_cost > 0 else 0
                lines.append(f"| {op_type} | ${cost:.4f} | {percentage:.1f}% |")
            lines.append("")
        
        if total_cost == 0.0:
            lines.append("### –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ\n")
            lines.append(
                "–ó–∞—Ç—Ä–∞—Ç—ã –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç—Å—è. –î–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç:\n"
                "- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ CostTracker –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏\n"
                "- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–∏ LLM –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç –∑–∞—Ç—Ä–∞—Ç—ã —á–µ—Ä–µ–∑ CostTracker"
            )
        
        return "\n".join(lines)
    
    except Exception as e:
        logger.error(f"Error in cost_report: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ –æ –∑–∞—Ç—Ä–∞—Ç–∞—Ö: {e}"


async def performance_report(vault_name: str | None = None) -> str:
    """–û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    
    Args:
        vault_name: –§–∏–ª—å—Ç—Ä –ø–æ vault'—É
    
    Returns:
        –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
        - Latency –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (p50, p95, p99)
        - Latency –ø–æ–∏—Å–∫–∞
        - Throughput
        - –û—à–∏–±–∫–∏ –∏ retries
    """
    try:
        lines = ["## –û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n"]
        
        if vault_name:
            lines.append(f"**Vault:** {vault_name}")
        lines.append("")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ MetricsCollector
        summary = await services.metrics_collector.get_summary(
            days=7,
            limit=100,
            vault_name=vault_name,
        )
        
        lines.append("### –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–∏—Å–∫–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤:** {summary.total_searches}")
        lines.append(f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {summary.avg_execution_time_ms:.2f} –º—Å")
        lines.append(f"- **–°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** {summary.avg_relevance_score:.3f}")
        lines.append(f"- **–ü—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:** {summary.empty_results_count} ({summary.empty_results_percentage:.1f}%)")
        lines.append("")
        
        # –ü–æ —Ç–∏–ø–∞–º –ø–æ–∏—Å–∫–∞
        if summary.searches_by_type:
            lines.append("### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –ø–æ–∏—Å–∫–∞\n")
            lines.append("| –¢–∏–ø | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º—Å) |")
            lines.append("|-----|-----------|-------------------|")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            # (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º—è –ø–æ —Ç–∏–ø–∞–º –æ—Ç–¥–µ–ª—å–Ω–æ)
            for search_type, count in sorted(
                summary.searches_by_type.items(), key=lambda x: x[1], reverse=True
            ):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ
                lines.append(
                    f"| {search_type} | {count} | {summary.avg_execution_time_ms:.2f} |"
                )
            lines.append("")
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if summary.popular_queries:
            lines.append("### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n")
            for idx, (query, count) in enumerate(summary.popular_queries[:10], 1):
                query_preview = query[:50] + "..." if len(query) > 50 else query
                lines.append(f"{idx}. `{query_preview}` ‚Äî {count} —Ä–∞–∑")
            lines.append("")
        
        # –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if summary.queries_with_no_results:
            lines.append("### –ó–∞–ø—Ä–æ—Å—ã –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")
            lines.append("| –ó–∞–ø—Ä–æ—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
            lines.append("|--------|-----------|")
            for query, count in summary.queries_with_no_results[:10]:
                query_preview = query[:50] + "..." if len(query) > 50 else query
                lines.append(f"| `{query_preview}` | {count} |")
            lines.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        lines.append("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n")
        
        recommendations = []
        
        if summary.avg_execution_time_ms > 1000:
            recommendations.append(
                "‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤—ã—Å–æ–∫–æ–µ (>1s). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."
            )
        
        if summary.empty_results_percentage > 20:
            recommendations.append(
                f"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—É—Å—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({summary.empty_results_percentage:.1f}%). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."
            )
        
        if summary.avg_relevance_score < 0.5:
            recommendations.append(
                f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å ({summary.avg_relevance_score:.3f}). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ embeddings –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ reranking."
            )
        
        if not recommendations:
            recommendations.append("‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –Ω–æ—Ä–º–µ.")
        
        for rec in recommendations:
            lines.append(f"- {rec}")
        
        return "\n".join(lines)
    
    except Exception as e:
        logger.error(f"Error in performance_report: {e}", exc_info=True)
        return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}"

