# Copyright (c) 2025 Tyl Consulting di Pancotti Marco
# This file is part of Thoth and is released under the Apache License 2.0.
# See the LICENSE.md file in the project root for full license information.

# Thoth AI Configuration Template
# Copy this file to config.yml.local and fill in your configuration
version: "1.0"

# AI Provider Configuration
# Configure only the providers you want to use
# At least one provider must be enabled with a valid API key
ai_providers:
  # OpenAI
  openai:
    enabled: false
    api_key: ""
  
  # Anthropic Claude
  anthropic:
    enabled: false
    api_key: ""
  
  # Google Gemini
  gemini:
    enabled: false
    api_key: ""
  
  # Mistral AI
  mistral:
    enabled: false
    api_key: ""
  
  # DeepSeek (OpenAI-compatible)
  deepseek:
    enabled: false
    api_key: ""
    api_base: "https://api.deepseek.com/v1"
  
  # OpenRouter (OpenAI-compatible)
  openrouter:
    enabled: true
    api_key: ""
    api_base: "https://openrouter.ai/api/v1"
  
  # Ollama (Local - no API key required)
  ollama:
    enabled: false
    api_base: "http://127.0.0.1:11434"
  
  # LM Studio (Local - no API key required)
  lm_studio:
    enabled: false
    api_base: "http://localhost:1234"
  
  # Groq (Fast inference)
  groq:
    enabled: false
    api_key: ""

# Embedding Service Configuration (Required)
# Configure the embedding provider for semantic search
embedding:
  # Active provider - uncomment ONE of the following blocks:
  
  # Option 1: OpenAI (Recommended)
  provider: "openai"
  api_key: ""  # Leave empty to use OpenAI API key from above
  model: "text-embedding-3-small"
  
  # Option 2: Mistral
  # provider: "mistral"
  # api_key: ""  # Leave empty to use Mistral API key from above
  # model: "mistral-embed"
  
  # Option 3: Cohere
  # provider: "cohere"
  # api_key: ""  # Cohere API key required
  # model: "embed-multilingual-v3.0"

# Default Backend AI Model (Required)
# Select which provider/model the Django backend will use for LLM tasks
backend_ai_model:
  ai_provider: "openrouter"   # One of: openai, anthropic, gemini, mistral, deepseek, openrouter, ollama, lm_studio, groq
  ai_model: "google/gemini-2.5-flash"  # Model identifier for the chosen provider

# Database Support Configuration
# Enable the database drivers you need
databases:
  sqlite: true       # Always required, cannot be disabled
  postgresql: true   # Enable for PostgreSQL support
  mysql: false       # Enable for MySQL support
  mariadb: true      # Enable for MariaDB support
  sqlserver: true    # Enable for SQL Server support
  informix: false    # Enable for IBM Informix support (requires SSH tunnel)

# Admin User Configuration
admin:
  email: ""  # Optional - admin email address
  username: "admin"  # Default admin username
  password: "admin123"  # Admin password (min 8 chars) - if empty, will be requested interactively

# Demo User Configuration
demo:
  email: ""  # Optional - admin email address
  username: "demo"  # Default admin username
  password: "demo1234"  # Admin password (min 8 chars) - if empty, will be requested interactively

# Monitoring Configuration
monitoring:
  enabled: true  # Enable application monitoring
  logfire_token: ""  # Logfire.io token (required if monitoring enabled)

# Service Ports Configuration
ports:
  frontend: 3040      # Next.js frontend port
  backend: 8000       # Django backend internal port
  sql_generator: 8020 # SQL generation service port
  nginx: 8040         # Main nginx proxy port (external access)
  qdrant: 6333        # Qdrant vector database port

# Docker Configuration
docker:
  deployment_mode: "compose"      # "compose" or "swarm"
  network_name: "thoth-network"
  compose_file: "docker-compose.yml"
  stack_file: "docker-stack.yml"
  image_registry: "tylconsulting"   # Docker Hub username/registry
  image_version: "latest"          # Image tag to deploy
  build_cache: true  # Use Docker build cache for faster rebuilds

# Runtime Settings
runtime:
  debug: false        # Enable debug mode (not recommended for production)
  backend_log_level: "WARNING"   # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  frontend_log_level: "INFO"     # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL


# RelevanceGuard configuration (used by SQL Generator; env overrides prevail)
relevance_guard:
  strict_min_score: 0.75
  weak_min_score: 0.45
  drop_below: 0.30
  w_bm25: 0.60
  w_struct: 0.40
  use_rrf: false
  # Number of STRICT failures required to trigger ModelRetry in gating
  strict_fails_required: 2
