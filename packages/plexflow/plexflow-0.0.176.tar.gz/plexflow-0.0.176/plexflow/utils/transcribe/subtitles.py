import os
import re
import sys
import shutil
import json
from typing import List, Optional, Any
from contextlib import contextmanager

# ==========================================
# HELPER: SILENCE OUTPUT
# ==========================================

@contextmanager
def suppress_stdout():
    """
    Context manager to temporarily silence stdout.
    Used to stop libraries from printing unwanted logs during save operations.
    """
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout

# ==========================================
# HELPER: LANGUAGE MAPPING (Whisper -> NLLB)
# ==========================================

def get_nllb_code(whisper_code: str) -> str:
    """
    Maps Whisper (ISO 639-1) codes to NLLB (Flores-200) codes.
    Defaults to 'eng_Latn' if not found.
    """
    MAPPING = {
        "en": "eng_Latn", "es": "spa_Latn", "fr": "fra_Latn", "de": "deu_Latn",
        "it": "ita_Latn", "pt": "por_Latn", "nl": "nld_Latn", "ru": "rus_Cyrl",
        "zh": "zho_Hans", "ja": "jpn_Jpan", "ko": "kor_Hang", "ar": "arb_Arab",
        "hi": "hin_Deva", "tr": "tur_Latn", "pl": "pol_Latn", "uk": "ukr_Cyrl",
        "sv": "swe_Latn", "da": "dan_Latn", "fi": "fin_Latn", "no": "nob_Latn",
        "cs": "ces_Latn", "el": "ell_Grek", "he": "heb_Hebr", "ro": "ron_Latn",
        "hu": "hun_Latn", "id": "ind_Latn", "vi": "vie_Latn", "th": "tha_Thai",
    }
    return MAPPING.get(whisper_code, "eng_Latn")

# ==========================================
# MODULE 1: TRANSCRIBER
# ==========================================

class AudioTranscriber:
    def __init__(self, model_size="medium", device="cpu", compute_type="int8", threads=8):
        self.model_size = model_size
        self.device = device
        self.compute_type = compute_type
        self.threads = threads
        self.model = None

    def _load_model(self):
        # LAZY IMPORT
        import stable_whisper
        if self.model is None:
            print(f"ðŸ”¹ Loading Whisper model ({self.model_size}) on {self.device}...", flush=True)
            self.model = stable_whisper.load_faster_whisper(
                self.model_size,
                device=self.device,
                compute_type=self.compute_type,
                cpu_threads=self.threads
            )

    def transcribe(self, audio_path: str, cache_path: Optional[str] = None) -> Any:
        # LAZY IMPORT
        import stable_whisper

        if cache_path and os.path.exists(cache_path):
            print(f"âœ… Found cached transcription: {cache_path}", flush=True)
            return stable_whisper.WhisperResult(cache_path)

        self._load_model()
        print(f"ðŸŽ™ï¸  Transcribing {os.path.basename(audio_path)}...", flush=True)
        
        # verbose=True ensures logs are printed to Airflow during long processes
        result = self.model.transcribe_stable(
            audio_path,
            beam_size=5,
            vad=True,
            regroup=True,
            verbose=True 
        )

        if cache_path:
            with suppress_stdout():
                result.save_as_json(cache_path)
            print(f"ðŸ’¾ Saved transcription cache to {cache_path}", flush=True)
        
        return result

    @staticmethod
    def refine_segments(result: Any):
        print("ðŸ”§ Refining segments (Merge & Split)...", flush=True)
        result.merge_by_gap(0.5)
        result.split_by_punctuation(['.', '?', '!', '...'])
        result.split_by_length(max_chars=42)
        return result

# ==========================================
# MODULE 2: TRANSLATOR
# ==========================================

class NeuralTranslator:
    def __init__(self, model_name="facebook/nllb-200-distilled-600M", src_lang="eng_Latn", tgt_lang="nld_Latn", device=-1):
        self.model_name = model_name
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.device = device
        self.pipe = None

    def _load_pipeline(self):
        # LAZY IMPORT
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

        if self.pipe is None or self.pipe.tokenizer.src_lang != self.src_lang:
            print(f"ðŸ”¹ Loading Translation model ({self.model_name}) | Src: {self.src_lang} -> Tgt: {self.tgt_lang}...", flush=True)
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            self.pipe = pipeline(
                "translation",
                model=self.model,
                tokenizer=self.tokenizer,
                src_lang=self.src_lang,
                tgt_lang=self.tgt_lang,
                device=self.device
            )

    @staticmethod
    def clean_text(text: str) -> str:
        text = re.sub(r'([.!?])([a-zA-Z])', r'\1 \2', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    @staticmethod
    def distribute_text_by_duration(full_text: str, durations: List[float]) -> List[str]:
        words = full_text.split()
        total_duration = sum(durations)
        chunks = []
        current_word_idx = 0
        
        for i, duration in enumerate(durations):
            if i == len(durations) - 1:
                chunks.append(" ".join(words[current_word_idx:]))
                break
            
            percent = duration / total_duration if total_duration > 0 else 0
            target_word_count = int(len(words) * percent)
            
            if target_word_count == 0 and duration > 0.2 and current_word_idx < len(words):
                target_word_count = 1
                
            end_idx = current_word_idx + target_word_count
            chunks.append(" ".join(words[current_word_idx:end_idx]))
            current_word_idx = end_idx
            
        return chunks

    def _overwrite_segment(self, segment, new_text):
        if not segment.words:
            return
        template_word = segment.words[0]
        if isinstance(template_word, dict):
            template_word['word'] = new_text
            template_word['start'] = segment.start
            template_word['end'] = segment.end
            segment.words = [template_word]
        else:
            template_word.word = new_text
            template_word.start = segment.start
            template_word.end = segment.end
            if hasattr(template_word, 'tokens'):
                template_word.tokens = [] 
            segment.words = [template_word]

    def translate_whisper_result(self, result: Any, checkpoint_path: str, progress_path: str, start_index: int = -1):
        self._load_pipeline()
        
        # --- PROGRESS BAR LOGIC ---
        # Detect if we are in a terminal (TTY) AND not in Airflow
        is_tty = sys.stdout.isatty()
        in_airflow = "AIRFLOW_CTX_DAG_ID" in os.environ
        use_tqdm = is_tty and not in_airflow

        total_segments = len(result.segments)
        start_msg = f"ðŸ”„ Starting translation of {total_segments} segments (Resuming from {start_index + 1})"
        
        pbar = None
        if use_tqdm:
            from tqdm import tqdm
            print(start_msg) 
            pbar = tqdm(
                total=total_segments, 
                initial=start_index + 1, 
                desc=f"Translating ({self.src_lang})", 
                unit="seg",
                ncols=100,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {postfix}"
            )
        else:
            print(start_msg, flush=True)

        buffer_indices = []
        buffer_text = []
        buffer_durations = []
        
        save_counter = 0
        SAVE_FREQ = 5 
        
        for i, segment in enumerate(result.segments):
            if i <= start_index:
                continue

            # Update Progress
            if use_tqdm:
                pbar.update(1)
            elif (i + 1) % 10 == 0:
                print(f"   ... Processing segment {i + 1}/{total_segments}", flush=True)

            text = segment.text.strip()
            buffer_indices.append(i)
            buffer_text.append(text)
            buffer_durations.append(segment.end - segment.start)

            is_end_of_sentence = text.endswith(('.', '?', '!', '...'))
            is_last_segment = (i == total_segments - 1)

            if is_end_of_sentence or is_last_segment:
                full_eng_sentence = " ".join(buffer_text)
                
                if full_eng_sentence.strip():
                    try:
                        output = self.pipe(full_eng_sentence, max_length=512, src_lang=self.src_lang, tgt_lang=self.tgt_lang)
                        raw_translated = output[0]['translation_text']
                        cleaned_translated = self.clean_text(raw_translated)
                        split_translated = self.distribute_text_by_duration(cleaned_translated, buffer_durations)
                        
                        last_processed_idx = -1
                        for j, idx in enumerate(buffer_indices):
                            if j < len(split_translated):
                                self._overwrite_segment(result.segments[idx], split_translated[j])
                            else:
                                self._overwrite_segment(result.segments[idx], "")
                            last_processed_idx = idx
                        
                        save_counter += 1
                        if save_counter >= SAVE_FREQ or is_last_segment:
                            if use_tqdm:
                                pbar.set_postfix_str("Saving...", refresh=True)
                                with suppress_stdout():
                                    result.save_as_json(checkpoint_path)
                                pbar.set_postfix_str("Saved", refresh=True)
                            else:
                                print(f"ðŸ’¾ Saving checkpoint at segment {i + 1}/{total_segments}", flush=True)
                                with suppress_stdout():
                                    result.save_as_json(checkpoint_path)
                            
                            with open(progress_path, 'w') as f:
                                f.write(str(last_processed_idx))
                            save_counter = 0

                    except Exception as e:
                        msg = f"\nâš ï¸ Error segment {i}: {e}"
                        if use_tqdm:
                            pbar.write(msg)
                        else:
                            print(msg, flush=True)
                        with suppress_stdout():
                            result.save_as_json(checkpoint_path)
                        raise e

                buffer_indices = []
                buffer_text = []
                buffer_durations = []

        if use_tqdm:
            pbar.close()
        
        if os.path.exists(progress_path):
            os.remove(progress_path)
        print("âœ… Translation completed.", flush=True)


# ==========================================
# MODULE 3: PIPELINE ORCHESTRATOR
# ==========================================

class SubtitlePipeline:
    def __init__(self, tgt_lang="nld_Latn", model_size="medium", translation_model="facebook/nllb-200-distilled-600M"):
        self.transcriber = AudioTranscriber(model_size=model_size)
        self.translator = NeuralTranslator(
            model_name=translation_model,
            src_lang="eng_Latn", # Placeholder, updated via autodetection
            tgt_lang=tgt_lang
        )

    def run(self, audio_path: str, output_srt_path: str):
        # LAZY IMPORT
        import stable_whisper

        base_path = os.path.splitext(audio_path)[0]
        paths = {
            "source_audio": audio_path,
            "cache_en": base_path + ".stable_cache.json",
            "checkpoint_tl": base_path + ".translated_checkpoint.json",
            "progress_tl": base_path + ".translation_progress.txt",
            "output_srt": output_srt_path
        }

        result = None
        start_index = -1

        # 1. Load or Transcribe
        if os.path.exists(paths["checkpoint_tl"]) and os.path.exists(paths["progress_tl"]):
            print(f"ðŸš€ Resuming from checkpoint: {paths['checkpoint_tl']}", flush=True)
            try:
                with open(paths["progress_tl"], 'r') as f:
                    content = f.read().strip()
                    start_index = int(content) if content else -1
                result = stable_whisper.WhisperResult(paths["checkpoint_tl"])
            except Exception as e:
                print(f"âš ï¸ Checkpoint corrupted ({e}). Restarting translation.", flush=True)
                start_index = -1

        if start_index == -1:
            # Force cache check or transcribe
            result = self.transcriber.transcribe(paths["source_audio"], paths["cache_en"])
            result = self.transcriber.refine_segments(result)

            print(f"ðŸ“ Initializing translation checkpoint...", flush=True)
            with suppress_stdout():
                result.save_as_json(paths["checkpoint_tl"])
            with open(paths["progress_tl"], 'w') as f:
                f.write("-1")

        # 2. Detect Language & Update Translator
        detected_whisper_lang = getattr(result, 'language', 'en')
        nllb_lang = get_nllb_code(detected_whisper_lang)
        
        print(f"ðŸŒ Detected Whisper Language: '{detected_whisper_lang}' -> NLLB Code: '{nllb_lang}'", flush=True)
        self.translator.src_lang = nllb_lang

        # 3. Translate
        self.translator.translate_whisper_result(
            result, 
            checkpoint_path=paths["checkpoint_tl"], 
            progress_path=paths["progress_tl"],
            start_index=start_index
        )

        print(f"âœ… Saving final subtitles to {paths['output_srt']}...", flush=True)
        result.to_srt_vtt(paths['output_srt'], word_level=False, min_dur=0.5)

if __name__ == "__main__":
    # Example usage for manual testing
    pipeline_runner = SubtitlePipeline(
        tgt_lang="nld_Latn",
        model_size="medium"
    )

    pipeline_runner.run(
        audio_path="/Users/david/code/plexflow/data/audio/output.wav",
        output_srt_path="/Users/david/code/plexflow/data/audio/output.srt"
    )