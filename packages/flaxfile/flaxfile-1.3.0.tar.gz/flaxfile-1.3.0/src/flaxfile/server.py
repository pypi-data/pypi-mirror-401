#!/usr/bin/env python3
"""
FlaxFile å¼‚æ­¥å•ç«¯å£æœåŠ¡å™¨ - ä½¿ç”¨ DEALER/ROUTER æ¨¡å¼
æ”¯æŒå¤šè¿æ¥ + aiofileså¼‚æ­¥æ–‡ä»¶I/O
"""

import zmq
import zmq.asyncio
import json
import hashlib
import time
import argparse
import asyncio
import aiofiles
import shutil
from pathlib import Path
from typing import Optional, Dict
from loguru import logger
from .crypto import get_password, configure_server_encryption, get_key_fingerprint, derive_server_keypair

# å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤å€¼ï¼Œä¼šåœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åˆ›å»ºï¼‰
STORAGE_DIR = Path("zmq_streaming_storage")


def _sanitize_path(base_path: Path, relative_path: str) -> Optional[Path]:
    """
    å®‰å…¨åœ°æ„é€ è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»

    Args:
        base_path: åŸºç¡€ç›®å½•
        relative_path: ç›¸å¯¹è·¯å¾„

    Returns:
        å®‰å…¨çš„ç»å¯¹è·¯å¾„ï¼Œå¦‚æœæ£€æµ‹åˆ°è·¯å¾„éå†åˆ™è¿”å› None
    """
    try:
        # ç§»é™¤å¼€å¤´çš„ /
        relative_path = relative_path.lstrip('/')

        # æ„é€ å®Œæ•´è·¯å¾„
        full_path = (base_path / relative_path).resolve()

        # æ£€æŸ¥æ˜¯å¦åœ¨åŸºç¡€ç›®å½•å†…
        base_resolved = base_path.resolve()
        if base_resolved in full_path.parents or full_path == base_resolved:
            return full_path
        else:
            # è·¯å¾„éå†æ”»å‡»
            logger.warning(f"æ£€æµ‹åˆ°è·¯å¾„éå†æ”»å‡»: {relative_path}")
            return None
    except (ValueError, OSError):
        return None

# ç»Ÿè®¡ä¿¡æ¯
stats = {
    'uploads': 0,
    'downloads': 0,
    'bytes_uploaded': 0,
    'bytes_downloaded': 0
}


class FlaxFileServer:
    """FlaxFile å¼‚æ­¥å•ç«¯å£æ–‡ä»¶ä¼ è¾“æœåŠ¡å™¨"""

    def __init__(
        self,
        host: str = "0.0.0.0",
        port: int = 25555,
        password: Optional[str] = None,
    ):
        self.host = host
        self.port = port
        self.password = password

        self.context = zmq.asyncio.Context()
        self.socket = None

        # æ–°çš„ä¼šè¯ç®¡ç†ï¼šæŒ‰ session_id ç´¢å¼•ï¼ˆæ”¯æŒå¤šæ–‡ä»¶å¹¶å‘ä¸Šä¼ ï¼‰
        self.upload_sessions: Dict[str, dict] = {}  # session_id -> session_info

        # æ˜ å°„ identity åˆ° session_idï¼ˆç”¨äºå¤„ç†chunkå’Œæ¸…ç†ï¼‰
        self.identity_to_session: Dict[bytes, str] = {}

        # å…¨å±€é”ï¼šä¿æŠ¤ä¼šè¯åˆ›å»º
        self.session_creation_lock = asyncio.Lock()

        # ä¼šè¯è¶…æ—¶æ¸…ç†ä»»åŠ¡
        self._cleanup_task: Optional[asyncio.Task] = None
        self._session_timeout = 300  # 5åˆ†é’Ÿæ— æ´»åŠ¨è¶…æ—¶

    async def _cleanup_stale_sessions(self):
        """å®šæœŸæ¸…ç†è¶…æ—¶çš„ä¸Šä¼ ä¼šè¯ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰"""
        while True:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                now = time.time()
                stale_sessions = []

                for session_id, session in self.upload_sessions.items():
                    if now - session.get('last_activity', session['start_time']) > self._session_timeout:
                        stale_sessions.append(session_id)

                for session_id in stale_sessions:
                    session = self.upload_sessions.pop(session_id, None)
                    if session:
                        # å…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        try:
                            await session['file'].close()
                            if session['file_path'].exists():
                                session['file_path'].unlink()
                            logger.warning(f"âš ï¸ æ¸…ç†è¶…æ—¶ä¼šè¯: {session_id[:8]}... (æ–‡ä»¶: {session['file_key']})")
                        except Exception as e:
                            logger.error(f"æ¸…ç†ä¼šè¯å¤±è´¥: {e}")

                        # æ¸…ç† identity æ˜ å°„
                        for identity in list(session.get('identities', [])):
                            self.identity_to_session.pop(identity, None)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")

    def _get_optimal_chunk_size(self, file_size: int, requested_chunk_size: Optional[int] = None) -> int:
        """
        è·å–æœ€ä¼˜chunkå¤§å°ï¼ˆKISSåŸåˆ™ï¼šç®€å•çš„è‡ªé€‚åº”é€»è¾‘ï¼‰

        Args:
            file_size: æ–‡ä»¶å¤§å°
            requested_chunk_size: å®¢æˆ·ç«¯è¯·æ±‚çš„chunkå¤§å°ï¼ˆå¯é€‰ï¼‰

        Returns:
            æœ€ä¼˜çš„chunkå¤§å°
        """
        if requested_chunk_size:
            return requested_chunk_size

        # æ ¹æ®æ–‡ä»¶å¤§å°è‡ªé€‚åº”
        if file_size < 500 * 1024 * 1024:  # < 500MB
            return 16 * 1024 * 1024  # 16MB
        else:  # >= 500MB
            return 32 * 1024 * 1024  # 32MB

    async def start(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        # è·å–å¯†ç ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if self.password is None:
            self.password = get_password(
                prompt="è¯·è¾“å…¥æœåŠ¡å™¨å¯†ç ï¼ˆç”¨äºåŠ å¯†ä¼ è¾“ï¼‰: ",
                allow_empty=True,
                env_var="FLAXFILE_PASSWORD",
                is_server=True
            )

        # åˆ›å»ºå­˜å‚¨ç›®å½•ï¼ˆåªåœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åˆ›å»ºï¼‰
        STORAGE_DIR.mkdir(exist_ok=True)

        logger.info("="*70)
        logger.info("FlaxFile å¼‚æ­¥å•ç«¯å£æ–‡ä»¶ä¼ è¾“æœåŠ¡å™¨ (DEALER/ROUTER)")
        logger.info("="*70)
        logger.info(f"å­˜å‚¨ç›®å½•: {STORAGE_DIR.absolute()}")
        logger.info(f"æœåŠ¡åœ°å€: tcp://{self.host}:{self.port}")

        # åˆ›å»º ROUTER socket (å•ç«¯å£å¤„ç†æ‰€æœ‰é€šä¿¡)
        self.socket = self.context.socket(zmq.ROUTER)
        self.socket.setsockopt(zmq.RCVBUF, 128 * 1024 * 1024)
        self.socket.setsockopt(zmq.SNDBUF, 128 * 1024 * 1024)
        self.socket.setsockopt(zmq.LINGER, 0)

        # é…ç½®åŠ å¯†
        encryption_enabled = configure_server_encryption(self.socket, self.password)

        self.socket.bind(f"tcp://{self.host}:{self.port}")

        logger.info("="*70)
        logger.info(f"âœ“ æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ {self.host}:{self.port}")
        if self.host == "0.0.0.0":
            logger.warning("  ç›‘å¬æ‰€æœ‰ç½‘å¡ï¼Œå…è®¸è¿œç¨‹è¿æ¥")

        # æ˜¾ç¤ºåŠ å¯†çŠ¶æ€
        if encryption_enabled:
            _, server_public_key = derive_server_keypair(self.password)
            fingerprint = get_key_fingerprint(server_public_key)
            logger.info(f"ğŸ”’ å·²å¯ç”¨ CurveZMQ åŠ å¯†")
            logger.info(f"   æœåŠ¡å™¨å…¬é’¥æŒ‡çº¹: {fingerprint}")
        else:
            logger.warning("âš ï¸  æœªå¯ç”¨åŠ å¯† - æ•°æ®å°†æ˜æ–‡ä¼ è¾“")
            logger.warning("   å»ºè®®è®¾ç½® FLAXFILE_PASSWORD ç¯å¢ƒå˜é‡æˆ–äº¤äº’è¾“å…¥å¯†ç ")

        logger.info("="*70)
        logger.info("")

        # å¯åŠ¨ä¼šè¯è¶…æ—¶æ¸…ç†ä»»åŠ¡
        self._cleanup_task = asyncio.create_task(self._cleanup_stale_sessions())

        try:
            while True:
                # æ¥æ”¶æ¶ˆæ¯: [identity, b'', command_type, ...args]
                frames = await self.socket.recv_multipart()

                if len(frames) < 3:
                    logger.warning(f"æ”¶åˆ°æ— æ•ˆæ¶ˆæ¯: {len(frames)} frames")
                    continue

                identity = frames[0]
                # frames[1] æ˜¯ç©ºåˆ†éš”ç¬¦
                command = frames[2]

                # å¼‚æ­¥å¤„ç†å‘½ä»¤
                asyncio.create_task(self.handle_command(identity, command, frames[3:]))

        except KeyboardInterrupt:
            logger.info("\næœåŠ¡å™¨åœæ­¢")
        finally:
            await self.stop()

    async def handle_command(self, identity: bytes, command: bytes, args: list):
        """å¤„ç†å®¢æˆ·ç«¯å‘½ä»¤"""
        try:
            if command == b'PING':
                await self.socket.send_multipart([identity, b'', b'PONG'])

            elif command == b'UPLOAD_START_CONCURRENT':
                await self.handle_upload_start_concurrent(identity, args)

            elif command == b'UPLOAD_CHUNK_CONCURRENT':
                await self.handle_upload_chunk_concurrent(identity, args)

            elif command == b'UPLOAD_END':
                await self.handle_upload_end(identity)

            elif command == b'DOWNLOAD_START_CONCURRENT':
                await self.handle_download_start_concurrent(identity, args)

            elif command == b'DOWNLOAD_CHUNK_CONCURRENT':
                await self.handle_download_chunk_concurrent(identity, args)

            elif command == b'DELETE':
                await self.handle_delete(identity, args)

            elif command == b'LIST':
                await self.handle_list(identity, args)

            else:
                logger.warning(f"æœªçŸ¥å‘½ä»¤: {command}")
                await self.socket.send_multipart([identity, b'', b'ERROR', b'Unknown command'])

        except Exception as e:
            logger.error(f"å¤„ç†å‘½ä»¤å¤±è´¥: {e}")
            try:
                await self.socket.send_multipart([identity, b'', b'ERROR', str(e).encode('utf-8')])
            except Exception:
                pass

    async def handle_upload_end(self, identity: bytes):
        """å®Œæˆä¸Šä¼ ï¼ˆæ”¯æŒå¤šsocketååŒï¼‰"""
        if identity not in self.identity_to_session:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'No active upload'])
            return

        result = {'status': 'ok', 'message': 'Upload ended'}  # é»˜è®¤result

        session_id = self.identity_to_session.get(identity)
        if session_id and session_id in self.upload_sessions:
            session = self.upload_sessions[session_id]

            # ä½¿ç”¨ä¼šè¯é”ä¿æŠ¤UPLOAD_ENDçš„å¹¶å‘è®¿é—®
            async with session['lock']:
                session['identities'].discard(identity)

                # åªæœ‰å½“æ‰€æœ‰identityéƒ½ç»“æŸæ—¶ï¼Œæ‰çœŸæ­£å…³é—­æ–‡ä»¶
                if len(session['identities']) == 0:
                    # æ‰€æœ‰socketéƒ½å®Œæˆäº†ï¼Œå…³é—­æ–‡ä»¶
                    await session['file'].close()  # aiofileså¼‚æ­¥å…³é—­

                    upload_time = time.time() - session['start_time']
                    throughput = (session['bytes_received'] / (1024 * 1024)) / upload_time if upload_time > 0 else 0

                    # æ›´æ–°ç»Ÿè®¡
                    stats['uploads'] += 1
                    stats['bytes_uploaded'] += session['bytes_received']

                    result = {
                        'status': 'ok',
                        'file_key': session['file_key'],
                        'size': session['bytes_received'],
                        'time': upload_time,
                        'throughput': throughput,
                        'sha256': session['hash'].hexdigest()
                    }

                    logger.info(f"âœ“ ä¸Šä¼ å®Œæˆ [{session_id[:8]}...]: {session['file_key']} "
                               f"({session['bytes_received']/(1024*1024):.1f} MB, "
                               f"{throughput:.2f} MB/s, "
                               f"{session['chunks_received']} chunks)")

                    # æ¸…ç†ä¼šè¯
                    self.upload_sessions.pop(session_id)
                else:
                    # è¿˜æœ‰å…¶ä»–socketåœ¨ä¸Šä¼ ï¼Œåªè¿”å›ä¸´æ—¶ç¡®è®¤
                    result = {
                        'status': 'ok',
                        'message': 'Socket finished, waiting for others'
                    }
                    logger.debug(f"âœ“ Socketå®Œæˆ [{session_id[:8]}...]: identity={identity.hex()[:8]}..., å‰©ä½™{len(session['identities'])}ä¸ª")

        # æ¸…ç†è¯¥identityçš„æ˜ å°„
        self.identity_to_session.pop(identity, None)

        await self.socket.send_multipart([identity, b'', b'OK', json.dumps(result).encode('utf-8')])

    async def handle_upload_start_concurrent(self, identity: bytes, args: list):
        """å¼€å§‹å¹¶å‘ä¸Šä¼ ï¼ˆæ”¯æŒå¤šæ–‡ä»¶å¹¶å‘ + å•æ–‡ä»¶å¤šsocketååŒï¼‰"""
        if len(args) < 4:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Missing arguments'])
            return

        session_id = args[0].decode('utf-8')  # æ–°å¢ï¼šä¼šè¯ID
        file_key = args[1].decode('utf-8')
        file_size = int(args[2].decode('utf-8'))
        max_concurrency = int(args[3].decode('utf-8'))

        # è¾“å…¥éªŒè¯
        if len(session_id) > 256:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'session_id too long'])
            return
        if len(file_key) > 4096:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'file_key too long'])
            return
        if file_size < 0 or file_size > 10 * 1024 * 1024 * 1024 * 1024:  # 10TBä¸Šé™
            await self.socket.send_multipart([identity, b'', b'ERROR', b'invalid file_size'])
            return
        if max_concurrency < 1 or max_concurrency > 1024:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'invalid max_concurrency'])
            return

        # è·¯å¾„å®‰å…¨æ£€æŸ¥
        safe_path = _sanitize_path(STORAGE_DIR, file_key)
        if safe_path is None:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Invalid file path'])
            return

        # ä½¿ç”¨å…¨å±€é”ä¿æŠ¤ä¼šè¯åˆ›å»º
        async with self.session_creation_lock:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥session_idçš„ä¸Šä¼ ä¼šè¯
            if session_id not in self.upload_sessions:
                # é¦–æ¬¡ä¸Šä¼ ï¼Œåˆ›å»ºæ–°ä¼šè¯
                file_path = safe_path
                file_path.parent.mkdir(parents=True, exist_ok=True)

                # ä½¿ç”¨aiofileså¼‚æ­¥æ‰“å¼€æ–‡ä»¶
                f = await aiofiles.open(file_path, 'wb')
                hash_obj = hashlib.sha256()

                logger.info(f"ğŸ“¤ å¹¶å‘ä¸Šä¼  (session={session_id[:8]}..., x{max_concurrency}): {file_key} ({file_size/(1024*1024):.1f} MB)")

                # æŒ‰session_idç´¢å¼•çš„ä¼šè¯ï¼ˆæ”¯æŒå¤šä¸ªidentityå…±äº«åŒä¸€sessionï¼‰
                self.upload_sessions[session_id] = {
                    'session_id': session_id,
                    'file_key': file_key,
                    'file_path': file_path,
                    'file': f,
                    'bytes_received': 0,
                    'expected_size': file_size,
                    'hash': hash_obj,
                    'start_time': time.time(),
                    'last_activity': time.time(),  # æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨äºè¶…æ—¶æ¸…ç†ï¼‰
                    'chunks_received': 0,
                    'concurrent': True,
                    'chunks': {},  # {chunk_id: data}
                    'next_chunk_id': 0,  # ä¸‹ä¸€ä¸ªè¦å†™å…¥çš„chunk_id
                    'max_concurrency': max_concurrency,
                    'lock': asyncio.Lock(),  # ä¿æŠ¤å¹¶å‘å†™å…¥çš„é”
                    'identities': set()  # å‚ä¸ä¸Šä¼ çš„æ‰€æœ‰identity
                }
            else:
                logger.debug(f"ğŸ“¤ åŠ å…¥ä¼šè¯: {session_id[:8]}... (identity: {identity.hex()[:8]}...)")

            # æ³¨å†Œè¯¥identityåˆ°ä¼šè¯
            self.upload_sessions[session_id]['identities'].add(identity)
            self.identity_to_session[identity] = session_id

        await self.socket.send_multipart([identity, b'', b'OK'])

    async def handle_upload_chunk_concurrent(self, identity: bytes, args: list):
        """å¤„ç†å¹¶å‘ä¸Šä¼ çš„chunkï¼ˆå¯èƒ½ä¹±åºåˆ°è¾¾ï¼‰"""
        if identity not in self.identity_to_session:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'No active upload'])
            return

        if len(args) < 2:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'No data'])
            return

        session_id = self.identity_to_session[identity]
        if session_id not in self.upload_sessions:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Session not found'])
            return

        session = self.upload_sessions[session_id]
        session['last_activity'] = time.time()  # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
        chunk_id = int(args[0].decode('utf-8'))
        data = args[1]

        # ä½¿ç”¨é”ä¿æŠ¤å¹¶å‘å†™å…¥çš„ä¸´ç•ŒåŒº
        async with session['lock']:
            # ç¼“å­˜chunkï¼ˆå¯èƒ½ä¹±åºåˆ°è¾¾ï¼‰
            session['chunks'][chunk_id] = data

            # æŒ‰åºå†™å…¥chunk (ä½¿ç”¨aiofileså¼‚æ­¥å†™å…¥)
            while session['next_chunk_id'] in session['chunks']:
                chunk_data = session['chunks'].pop(session['next_chunk_id'])
                await session['file'].write(chunk_data)  # aiofileså¼‚æ­¥å†™å…¥
                session['hash'].update(chunk_data)
                session['bytes_received'] += len(chunk_data)
                session['chunks_received'] += 1
                session['next_chunk_id'] += 1

        # å‘é€ACKï¼ˆå¸¦chunk_idï¼‰- åœ¨é”å¤–å‘é€ï¼Œé¿å…é˜»å¡å…¶ä»–chunk
        await self.socket.send_multipart([identity, b'', b'ACK', args[0]])

        # æ‰“å°è¿›åº¦ï¼ˆæ¯10%ï¼‰
        if session['expected_size'] > 0:
            progress = session['bytes_received'] / session['expected_size'] * 100
            if int(progress) % 10 == 0 and session['chunks_received'] % 100 == 1:
                logger.info(f"  è¿›åº¦ [{session['session_id'][:8]}...]: {progress:.0f}% ({session['bytes_received']/(1024*1024):.1f} MB)")

    async def handle_download_start_concurrent(self, identity: bytes, args: list):
        """å¤„ç†å¹¶å‘ä¸‹è½½å¼€å§‹è¯·æ±‚ï¼ˆKISSä¼˜åŒ–ï¼šä¾èµ–OSé¡µç¼“å­˜ï¼‰"""
        if len(args) < 1:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Missing file_key'])
            return

        file_key = args[0].decode('utf-8')
        # æ”¯æŒå®¢æˆ·ç«¯æŒ‡å®šchunk_sizeï¼ˆå¯é€‰å‚æ•°ï¼‰
        requested_chunk_size = None
        if len(args) >= 2:
            try:
                requested_chunk_size = int(args[1].decode('utf-8'))
            except (ValueError, UnicodeDecodeError):
                pass

        # è·¯å¾„å®‰å…¨æ£€æŸ¥
        file_path = _sanitize_path(STORAGE_DIR, file_key)
        if file_path is None:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Invalid file path'])
            return

        try:
            file_size = file_path.stat().st_size
        except FileNotFoundError:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'File not found'])
            return

        # ä½¿ç”¨æœ€ä¼˜chunk_size
        chunk_size = self._get_optimal_chunk_size(file_size, requested_chunk_size)
        total_chunks = (file_size + chunk_size - 1) // chunk_size

        logger.info(f"ğŸ“¥ å¹¶å‘ä¸‹è½½: {file_key} ({file_size/(1024*1024):.1f} MB, {total_chunks} chunks, chunk={chunk_size/(1024*1024):.0f}MB)")

        # è¿”å›æ–‡ä»¶ä¿¡æ¯
        await self.socket.send_multipart([
            identity, b'', b'OK',
            str(file_size).encode('utf-8'),
            str(total_chunks).encode('utf-8'),
            str(chunk_size).encode('utf-8')
        ])

    async def handle_download_chunk_concurrent(self, identity: bytes, args: list):
        """å¤„ç†å¹¶å‘ä¸‹è½½chunkè¯·æ±‚ï¼ˆKISSä¼˜åŒ–ï¼šä¾èµ–OSé¡µç¼“å­˜ä¼˜åŒ–æ€§èƒ½ï¼‰"""
        if len(args) < 2:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Missing arguments'])
            return

        file_key = args[0].decode('utf-8')
        chunk_id = int(args[1].decode('utf-8'))

        # è·¯å¾„å®‰å…¨æ£€æŸ¥
        file_path = _sanitize_path(STORAGE_DIR, file_key)
        if file_path is None:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Invalid file path'])
            return

        if not file_path.exists():
            await self.socket.send_multipart([identity, b'', b'ERROR', b'File not found'])
            return

        try:
            # ğŸ”¥ KISSåŸåˆ™ï¼šç®€åŒ–è®¾è®¡ï¼Œæ¯æ¬¡æ‰“å¼€æ–‡ä»¶è¯»å–
            # OSé¡µç¼“å­˜ä¼šè‡ªåŠ¨ä¼˜åŒ–é‡å¤è¯»å–çš„æ€§èƒ½
            # é¿å…äº†å¤æ‚çš„ä¼šè¯ç®¡ç†å’Œå¤šSocketå…±äº«æ–‡ä»¶å¯¹è±¡çš„é—®é¢˜
            file_size = file_path.stat().st_size
            chunk_size = self._get_optimal_chunk_size(file_size)

            async with aiofiles.open(file_path, 'rb') as f:
                offset = chunk_id * chunk_size

                await f.seek(offset)
                chunk_data = await f.read(chunk_size)

            # è¿”å›chunkæ•°æ®
            await self.socket.send_multipart([
                identity, b'', b'CHUNK',
                str(chunk_id).encode('utf-8'),
                chunk_data
            ])

        except Exception as e:
            logger.error(f"è¯»å–chunkå¤±è´¥: {e}")
            await self.socket.send_multipart([identity, b'', b'ERROR', str(e).encode('utf-8')])

    async def handle_delete(self, identity: bytes, args: list):
        """åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•"""
        if len(args) < 1:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Missing file_key'])
            return

        file_key = args[0].decode('utf-8')

        # è·¯å¾„å®‰å…¨æ£€æŸ¥
        file_path = _sanitize_path(STORAGE_DIR, file_key)
        if file_path is None:
            await self.socket.send_multipart([identity, b'', b'ERROR', b'Invalid file path'])
            return

        try:
            if file_path.is_dir():
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡æ“ä½œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
                def _delete_directory(path: Path) -> int:
                    """åœ¨çº¿ç¨‹ä¸­åˆ é™¤ç›®å½•å¹¶è¿”å›å¤§å°"""
                    total_size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
                    shutil.rmtree(path)
                    return total_size

                total_size = await asyncio.to_thread(_delete_directory, file_path)
                logger.info(f"âœ“ åˆ é™¤ç›®å½•: {file_key} ({total_size/(1024*1024):.1f} MB)")
            elif file_path.exists():
                # åˆ é™¤æ–‡ä»¶ï¼ˆå•æ–‡ä»¶åˆ é™¤å¾ˆå¿«ï¼Œæ— éœ€çº¿ç¨‹æ± ï¼‰
                file_size = file_path.stat().st_size
                file_path.unlink()
                logger.info(f"âœ“ åˆ é™¤æ–‡ä»¶: {file_key} ({file_size/(1024*1024):.1f} MB)")
            else:
                await self.socket.send_multipart([identity, b'', b'ERROR', b'File not found'])
                return

            await self.socket.send_multipart([identity, b'', b'OK'])
        except Exception as e:
            logger.error(f"åˆ é™¤å¤±è´¥: {e}")
            await self.socket.send_multipart([identity, b'', b'ERROR', str(e).encode('utf-8')])

    async def handle_list(self, identity: bytes, args: list):
        """åˆ—å‡ºæŒ‡å®šå‰ç¼€ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        # è·å–å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        prefix = args[0].decode('utf-8') if args else ""
        # è·å–æ˜¯å¦è®¡ç®—å“ˆå¸Œï¼ˆå¯é€‰ï¼Œé»˜è®¤Falseï¼‰
        compute_hash = args[1].decode('utf-8') == 'true' if len(args) > 1 else False

        try:
            files_info = []

            # 1. å…ˆæ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
            matched_files = []
            for file_path in STORAGE_DIR.rglob('*'):
                if file_path.is_file():
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„
                    relative_path = file_path.relative_to(STORAGE_DIR)
                    key = str(relative_path)

                    # å¦‚æœæŒ‡å®šäº†å‰ç¼€ï¼Œåªè¿”å›åŒ¹é…çš„æ–‡ä»¶
                    if prefix:
                        # ç¡®ä¿å‰ç¼€ä»¥ / ç»“å°¾ï¼Œé¿å…åŒ¹é…åˆ°å‰ç¼€ç›¸ä¼¼çš„å…¶ä»–ç›®å½•
                        search_prefix = prefix if prefix.endswith('/') else prefix + '/'
                        if not key.startswith(search_prefix):
                            continue

                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    stat = file_path.stat()
                    file_info = {
                        'key': key,
                        'size': stat.st_size,
                        'mtime': stat.st_mtime
                    }
                    matched_files.append((file_path, file_info))

            # 2. å¦‚æœéœ€è¦è®¡ç®—å“ˆå¸Œï¼Œå¹¶å‘æ‰§è¡Œï¼ˆé¿å…ä¸²è¡Œé˜»å¡ï¼‰
            if compute_hash and matched_files:
                import aiofiles
                import xxhash

                # å¹¶å‘å“ˆå¸Œè®¡ç®—å‡½æ•°
                async def compute_file_hash(file_path: Path, semaphore: asyncio.Semaphore) -> str:
                    async with semaphore:
                        hash_obj = xxhash.xxh3_64()
                        async with aiofiles.open(file_path, 'rb') as f:
                            while True:
                                chunk = await f.read(1024 * 1024)  # 1MB chunks
                                if not chunk:
                                    break
                                hash_obj.update(chunk)
                        return hash_obj.hexdigest()

                # é™åˆ¶å¹¶å‘æ•°ä¸º 8ï¼Œé¿å…æ‰“å¼€è¿‡å¤šæ–‡ä»¶
                semaphore = asyncio.Semaphore(8)
                hash_tasks = [compute_file_hash(fp, semaphore) for fp, _ in matched_files]
                hashes = await asyncio.gather(*hash_tasks)

                # åˆå¹¶å“ˆå¸Œç»“æœåˆ°æ–‡ä»¶ä¿¡æ¯
                for (_, file_info), file_hash in zip(matched_files, hashes):
                    file_info['hash'] = file_hash
                    files_info.append(file_info)
            else:
                # ä¸è®¡ç®—å“ˆå¸Œï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶ä¿¡æ¯
                files_info = [file_info for _, file_info in matched_files]

            # åºåˆ—åŒ–æ–‡ä»¶åˆ—è¡¨
            import json
            files_json = json.dumps(files_info).encode('utf-8')

            hash_msg = " (å«xxHash)" if compute_hash else ""
            logger.info(f"ğŸ“‹ åˆ—å‡ºæ–‡ä»¶: å‰ç¼€='{prefix}', æ•°é‡={len(files_info)}{hash_msg}")
            await self.socket.send_multipart([identity, b'', b'OK', files_json])

        except Exception as e:
            logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            await self.socket.send_multipart([identity, b'', b'ERROR', str(e).encode('utf-8')])

    async def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        # å–æ¶ˆæ¸…ç†ä»»åŠ¡
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        # å…³é—­æ‰€æœ‰æ´»è·ƒçš„ä¸Šä¼ ä¼šè¯
        for session in self.upload_sessions.values():
            try:
                await session['file'].close()
            except Exception:
                pass

        if self.socket:
            self.socket.close()
        self.context.term()

        logger.info("")
        logger.info("ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  ä¸Šä¼ : {stats['uploads']} ä¸ªæ–‡ä»¶, {stats['bytes_uploaded']/(1024*1024):.1f} MB")
        logger.info(f"  ä¸‹è½½: {stats['downloads']} ä¸ªæ–‡ä»¶, {stats['bytes_downloaded']/(1024*1024):.1f} MB")


def main():
    parser = argparse.ArgumentParser(description="FlaxFile Server")
    parser.add_argument('--host', default='0.0.0.0', help='Host to bind')
    parser.add_argument('--port', type=int, default=25555, help='Port to bind')
    parser.add_argument('--password', default=None, help='Password for encryption (or set FLAXFILE_PASSWORD env var)')

    args = parser.parse_args()

    server = FlaxFileServer(host=args.host, port=args.port, password=args.password)
    asyncio.run(server.start())


if __name__ == "__main__":
    main()
