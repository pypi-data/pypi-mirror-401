#####################################################################
# Agent Tools Dataset Configuration (Multi-Turn)
#####################################################################
# This configuration file sets up a dataset generation process using
# topic generation and data generation engines.
# The dataset focuses on Python programming fundamentals.
# Advanced features like Tool Use with agent reasoning style.
# A multi-turn agent mode of tool usage occurs.
#####################################################################

# Topic generation - creates a tree of related topics
topics:
  prompt: "JavaScript programming fundamentals in react framework"
  mode: graph
  prompt_style: anchored
  system_prompt: ""
  depth: 3 
  degree: 3
  save_as: "agent-eact-topics.jsonl"

  # LLM configuration for topic generation
  llm:
    provider: "gemini"
    model: "gemini-2.5-flash-lite"
    temperature: 0.8

# Data generation - creates the actual training examples
generation:
  system_prompt: |
    Generate examples of tool usage with clear reasoning over multiple turns,
    for a react application development context.
    Show why tools are chosen and how results are interpreted.
    The tools available are defined in the tool registry.

  instructions: "Create realistic scenarios to resolve react issues, with scenarios requiring tool usage to complete tasks."

  # Chain of thought with agent mode
  conversation:
    type: cot
    reasoning_style: agent
    # Multi-turn: extended conversations with multiple tool calls
    agent_mode: single_turn
    min_turns: 2
    max_turns: 4
    min_tool_calls: 2

  # Tool configuration - requires Spin service running
  # Start with: cd tools-sdk && spin build && spin up
  tools:
    spin_endpoint: "http://localhost:3000"  # Spin service URL
    components:
      builtin:  # Built-in VFS tools (routes to /vfs/execute)
        - read_file
        - write_file
        - list_files
    max_per_query: 3       # Maximum tools per example
    max_agent_steps: 5     # Max ReAct iterations

  max_retries: 3  # Retry on API failures

  # Optional: Seed initial files into the VFS before generation
  scenario_seed:
      files:
        "package.json": |
          {
            "name": "react-app",
            "version": "1.0.0",
            "dependencies": {
              "fastapi": "^0.70.0",
              "uvicorn": "^0.15.0"
            }
          }
        "tailwind.config.js": |
          module.exports = {
            content: ["./src/**/*.{html,js}"],
            theme: {
              extend: {},
            },
            plugins: [],
          }
        ".gitignore": |
          __pycache__/
          *.pyc
          .env
        
        "Dockerfile": |
          FROM reactapp/base-image:latest
          WORKDIR /usr/src/app
          COPY package.json ./
          RUN npm install
          COPY . .
          EXPOSE 3000
          CMD ["npm", "start"]

        "App.js": |
          import React from 'react';

          function App() {
            return (
              <div className="App">
                <h1>Hello, World!</h1>
              </div>
            );
          }

          export default App();
      "postcss.config.js": |
          module.exports = {
            plugins: {
              tailwindcss: {},
              autoprefixer: {},
            },
          }

      "tsconfig.json": |
          {
            "compilerOptions": {
              "target": "es5",
              "lib": ["dom", "dom.iterable", "esnext"],
              "allowJs": true,
              "skipLibCheck": true,
              "esModuleInterop": true,
              "allowSyntheticDefaultImports": true,
              "strict": true,
              "forceConsistentCasingInFileNames": true,
              "noFallthroughCasesInSwitch": true,
              "module": "esnext",
              "moduleResolution": "node",
              "resolveJsonModule": true,
              "isolatedModules": true,
              "noEmit": true,
              "jsx": "react-jsx"
            },
            "include": ["src"]
          }
      


  # LLM configuration for generation
  llm:
    provider: "gemini"
    model: "gemini-2.5-flash-lite"
    temperature: 0.7

# Output configuration
output:
  # System prompt that goes INTO the training data
  system_prompt: |
    You are an AI assistant with access to various tools and functions.
    When given a task:
    1. Analyze what tools are needed
    2. Execute tools with proper parameters
    3. Interpret results and provide a clear answer

  include_system_message: true
  num_samples: 4
  batch_size: 2
  save_as: "agent-react-multi-dataset.jsonl"
