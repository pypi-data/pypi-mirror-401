# RAG 평가 분석 보고서
**비교 대상**: `e2e-auto-insurance-qa-korean` 버전 1.0.0
**모델**: `ollama/gpt-oss-safeguard:20b`
**연산 ID**: A(`8f825b22-87f1-4d9b-b3a0-8ff65dbec2c5`) vs B(`4516d358-2797-4c46-9f14-c1d975588025`)

| 항목 | 내용 |
|---|---|
| 총 테스트 케이스 수 | 18 |
| 사용한 메트릭 | faithfulness, answer_relevancy |
| 테스트 중 발생한 이슈 | 표본 수가 적음, 프롬프트 스냅샷 누락 |

---

## ⚡️ 요약
Run A가 Answer Relevancy에서 31 % 높은 성능을 보였으며, 두 모델 모두 Faithfulness는 동일했지만 B는 대다수 질문에서 낮은 관련성을 기록했다. 핵심 근거는 `answer_relevancy` 35 % (p < 0.00001) 차이이며, 이는 18개 샘플의 작은 표본 크기에 의해 크게 영향을 받지 않는 강한 효과를 보여준다.

**핵심 3개의 빌드**
- **지표**: Faithfulness ≈ 0.833, Relevancy A: 0.566  vs B: 0.388
- **변경 사항**: `evaluation_task`만 변경 (프롬프트 스냅샷 없음)
- **사용자 영향**: 사용자 질문에 대한 답변과 정보가 명확히 떨어져 정확도 저하 → 사용자 만족도 감소 가능

---

## 🔄 변경 사항 요약
| 항목 | A | B | 비고 |
|---|---|---|---|
| **데이터셋** | 동일 (`e2e-auto-insurance-qa-korean v1.0.0`) | 동일 | - |
| **모델** | 동일 | 동일 | - |
| **Configuration** | evaluation_task `null` | evaluation_task `run` | 한 번의 설정 변경 [Change Summary] |
| **Prompt** | Snapshot 없음 | Snapshot 없음 | 프롬프트 변동 없음 → 추후 기록 필수 |
| **Metric Pass Rate** | 0.5 | 0.5 | 두 실험 모두 동일 |

> **Note**: 프롬프트 스냅샷이 없으므로 내부 프롬프트가 의도치 않게 변할 가능성 존재합니다.

---

## 📊 지표 비교 스코어카드

| metric | mean_a | mean_b | diff | diff% | p‑value | effect_size | effect_level | is_significant | winner | direction |
|--------|--------|--------|------|-------|---------|-------------|--------------|---------------|--------|-----------|
| faithfulness | 0.8333 | 0.8333 | 0.0000 | 0.0% | 1.0000 | 0.0 | negligible | FALSE | — | flat |
| answer_relevancy | 0.5660 | 0.3881 | -0.1780 | -31.44% | 6.35×10⁻⁶ | -1.829 | large | TRUE | A | down |

*표본 수가 18개에 불과하여 일반화 가능성에 한계가 있습니다.*

---

## 🔬 통계적 신뢰도
- **Faithfulness**: p = 1.0 → 변동 없음.
- **Answer Relevancy**: p = 6.35 × 10⁻⁶, 효과량(η²) = 1.829 → 대규모 감쇠.
- **표본 크기 경고**: `quality_summary.flags`에 “표본 수가 적음”이 기록돼 있어, 결과가 통계적으로 강하지만 실제 성능 차이는 변동성에 민감할 수 있음.
- **재검증 필요**: 추가 샘플을 확보해 동일한 차이가 지속되는지 확인해야 함.

> **제한**: 두 실행 모두 같은 데이터셋과 모델을 사용했지만, `evaluation_task` 설정 차이보다 프롬프트 스냅샷 누락이 보다 큰 영향을 줄 수 있으므로 현재 분석에서는 차이를 완전히 설명하기 어렵습니다.

---

## 🧩 원인 분석
1. **설정 차이**: `evaluation_task` 가 `run` 으로 바뀜.
   - 이는 내부 scoring 모듈이 답변을 평가하는 방식을 바꿔 answer_relevancy를 저해할 수 있다. [Change Summary]
2. **프롬프트 불일치**: 프롬프트 스냅샷이 없으므로 B 실행에서 실제 프롬프트가 A와 다를 가능성 존재.
3. **데이터셋 차이 없음**: dataset은 동일하며 해당 차이는 외부 요인에 의해 발생.

> **Evidence**:
> - A‑1 (auto‑007) *Answer Relevancy* 0.495 → B‑1 *Answer Relevancy* 0.416
> - A‑2 (*블랙박스*) 0.632 vs B‑2 0.430
> - A‑3 (*대인배상 II*) 0.715 vs B‑3 0.545
> (각 케이스마다 A가 높음)

---

## 📈 개선 제안
| 우선순위 | 제안 | 기대효과 | 실현 방법 |
|--------|------|----------|----------|
| 높은 | **프롬프트 스냅샷 확보 및 버전 관리** | 무작위 프롬프트 변화 방지 | `--prompt` 옵션 사용, 결과 로그에 매핑 기록 |
| 중간 | **표본 확대** (≥ 100개) | 통계적 신뢰성 향상 | 새로운 퀘스쳔 세트 채용 또는 기존 세트 재활용 |
| 낮은 | **Evaluation task 재검토** | 모듈 설정 일관성 확보 | `evaluation_task` 변경이 실제 평가 방식에 미치는 영향 명시 |
| 낮은 | **모델 재학습** | 답변 정확성, 관련성 향상 | 데이터 정제, 파인튜닝 수행 |

---

## 🚀 다음 단계
1. **프롬프트 캡처**: 모든 실행에 대해 프롬프트 스냅샷 기록.
2. **확장 테스트**: 100+ 질문으로 테스트 재실행.
3. **A/B 재검증**: 이번에 변경 사항을 최소화하고 동일한 프롬프트, 설정으로 재비교.
4. **대시보드 업데이트**: `artifact_manifest`에 포함된 `run_metric_comparison.json`을 기반으로 시각화.

---

## 📎 부록 (산출물)
- `load_runs.json`
- `run_change_detection.json`
- `run_metric_comparison.json`
- 원본 평가 결과: 각 `evidence_id`별 메트릭 목록

---

**결론**: 현재 버전의 `ollama/gpt-oss-safeguard:20b`은 Faithfulness는 변동이 없지만, 구성 변경과 프롬프트 불일치 가능성이 높은 상황에서 Answer Relevancy가 크게 감소하였다. 충분한 표본과 프롬프트 관리가 필요하다.

## 부록(산출물 링크)

- [load_runs.json](artifacts/comparison_8f825b22_4516d358/load_runs.json)
- [run_metric_comparison.json](artifacts/comparison_8f825b22_4516d358/run_metric_comparison.json)
- [run_change_detection.json](artifacts/comparison_8f825b22_4516d358/run_change_detection.json)
- [report.json](artifacts/comparison_8f825b22_4516d358/report.json)
- [final_output.json](artifacts/comparison_8f825b22_4516d358/final_output.json)
- [index.json](artifacts/comparison_8f825b22_4516d358/index.json)
