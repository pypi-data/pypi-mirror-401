# RAG 평가 분석 보고서
**Run ID**: f1287e90-43b6-42c8-b3ac-e6cb3e06a71e | **Dataset**: test_dataset v1.0.0 | **모델**: ollama/gpt‑oss‑safeguard:20b

---

## 1. 요약
**주요 결론**: 전체 합계 0.692의 과반자(50 %)가 기준치를 초과하지 않으며, 신뢰성(faithfulness) 부족이 가장 큰 문제입니다.

- **지표**: 대다수 테스트에서 answer_relevancy는 0.884 이상인 반면 faithfulness는 평균 0.5(위험 수준)입니다.
- **원인**: 답변이 컨텍스트와 충분히 정합되지 않아 근거 인용이 미흡한 점이 크게 영향을 끼칩니다.
- **사용자 영향**: 신뢰가 낮아 사용자는 정보의 정확성에 의문을 가질 수 있고, 이해와 인지부하가 증가합니다.

---

## 2. 지표 스코어카드

| metric | 평균 | threshold | pass_rate | 상태 |
|--------|------|-----------|-----------|------|
| answer_relevancy | 0.884 | 0.7 | 1.00 | Pass |
| faithfulness | 0.500 | 0.7 | 0.50 | Risk |

> **전체 pass_rate**: 0.50 (4개 테스트 중 2개만 성공)

---

## 3. 데이터 품질/신뢰도

| 항목 | 상태 |
|------|------|
| 샘플 수 | 4(표본 수가 소수) |
| coverage | 1.0 (모든 테스트에 필수 메트릭 적용) |
| 추세 분석 | 부족 (데이터가 한 번에 한 실행) |

> *표시된 경고*: `표본 수가 적음`, `추세 분석을 위한 실행 이력 부족`. 작은 샘플로 인해 통계 해석에 주의 필요.

---

## 4. 증거 기반 인사이트

| 테스트 ID | 질문 | answer_relevancy | faithfulness | 주요 문제 포인트 | evidence_id |
|-----------|------|------------------|--------------|------------------|-------------|
| test_001 | What is Python? | 0.837 | 0.0 | 컨텍스트와 불일치 | [E1] |
| test_003 | What is RAG? | 0.909 | 0.0 | 근거 없이 정리된 정의 | [E2] |
| test_002 | What is machine learning? | 0.872 | 1.0 | 컨텍스트와 일치 | [E3] |
| test_004 | What is TDD? | 0.918 | 1.0 | 정확히 정정 | [E4] |

- **패턴**: 핵심 키워드 `is`, `tdd`, `what`, `python`, `rag` 순위 하위.
- **대부분의 사례**(test_001, test_003)에서 faithfulness가 0으로 떨어져 **위험 요인**이 두 번 발생.

**사용자 인식**
- *신뢰성* 낮은 답변은 사용자에게 부정확한 정보를 제공함으로써 **신뢰 저하**를 초래.
- 사실 질문에서 신뢰성이 보장되지 않을 경우 **인지부하**가 증가해 추론이 필요.

---

## 5. 원인 가설

1. **컨텍스트 정합성 부재**
   - 테스트 E1, E2에서 답변이 제공된 컨텍스트와 크게 차이나며 핵심 사실이 누락.
2. **근거 인용 미흡**
   - 모델이 답변을 생성할 때 외부 정보(컨텍스트) 참조를 불충분히 활용.
3. **지문 길이 및 복잡성**
   - 질문이 짧고 단순(`factual`)이어서 모델이 외부 정보에 대한 의존도를 낮춤.

> **root_cause**: faithfulness 징후가 0.5 미달 → **비판적 근거 부재**.

---

## 6. 개선 제안

| 영역 | 제안 | 근거 |
|------|------|------|
| **컨텍스트 활용** | 문맥 매칭 알고리즘(예: BM25 또는 DPR)으로 가장 관련 있는 문장을 선택 후 강조 표시 | E1, E2 에서 낮은 faithfulness |
| **근거 기반 인용** | 답변에 인용 부호 또는 링크 삽입 → 사용자가 직접 컨텍스트 확인 가능 | *제안* 메트릭 |
| **데이터 보강** | 테스트 샘플 증가 및 다양한 질문 유형 추가 | “표본 수가 적음” 경고 |
| **모델 튜닝** | `faithfulness` 목표치를 0.7 이상으로 강제하도록 파인튜닝 또는 후처리 기준 적용 | `thresholds.fai­l­ty­ness = 0.7` |
| **평가 프레임워크 보강** | 실시간 추세 및 시계열 분석 추가 | “추세 분석 부족” |

---

## 7. 다음 단계

1. **샘플 확장** – 10~20개 추가 테스트 수행, 다양한 질문 유형(문법, 개념, 인용 등).
2. **컨텍스트 매핑 재설계** – 더 정밀한 맥락 연관성 스코어링 도입.
3. **사후 검증** – 자동화된 근거 인용 검증 스크립트 개발.
4. **A/B 테스트** – 두 버전(현재 vs. 개선) 비교하여 `faithfulness` 향상 여부 확인.

---

## 8. 부록(산출물)

| 파일 | 내용 |
|------|------|
| causal_analysis.json | 원인 분석 결과 |
| load_data.json | 데이터 로딩 스크립트 |
| nlp_analysis.json | 키워드, 문장 통계 |
| pattern_detection.json | Top keyword, question type 패턴 |
| priority_summary.json | 위험 및 우선순위 평가 |
| ragas_eval.json | 평가 메트릭 결과 |
| root_cause.json | root_cause 및 추천 |
| statistics.json | 전체 통계 |
| trend_detection.json | 추세 및 변화 |

---

> **추가 데이터 필요**: 현 시점에서 샘플 수가 적어 추세 파악이 어려우며, 다른 모델 간 비교 분석이 필요하면 추가 run data 수집을 권장합니다.
