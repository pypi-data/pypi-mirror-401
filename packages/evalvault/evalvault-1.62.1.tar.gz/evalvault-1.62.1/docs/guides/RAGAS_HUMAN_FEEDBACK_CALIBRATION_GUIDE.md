# RAGAS 평가를 인간 피드백으로 보정하는 방법론

> **대상**: RAG 평가/분석을 운영하는 ML/데이터/플랫폼 엔지니어
>
> **목적**: RAGAS 점수의 신뢰도와 사용자 만족도 정합성을 높이기 위해 대표 샘플링 + 인간 평가 + 학습 기반 보정 모델을 설계한다.
>
> **상태**: Draft v0.1

## 1. 서론

RAGAS(Retrieval-Augmented Generation Assessment Suite)는 RAG 기반 질의응답 시스템을 평가하기 위한 오픈소스 프레임워크다. Faithfulness, Answer Relevance, Context Precision/Recall 같은 지표로 답변이 컨텍스트에 근거해 생성됐는지와 질문에 제대로 답했는지를 자동 평가한다. 그러나 실무에서는 RAGAS 점수가 실제 사용자 만족도와 잘 맞지 않는 사례가 반복적으로 보고된다. 따라서 자동 지표만으로는 서비스 품질을 판단하기 어렵고, 일부 인간 평가를 포함한 보정이 필요하다.

## 2. 사용자 상황과 목표

- 이미 RAGAS 평가/분석을 위한 Web UI 또는 배치 평가가 운영 중이다.
- 이해관계자는 RAGAS 점수를 단독 KPI로 신뢰하지 않는다.
- 전체 데이터셋에 대한 인간 평가는 불가능하므로 대표 샘플만 평가해야 한다.
- 인간 평가를 학습하여 전체 데이터셋에 대해 사용자 관점의 보정 점수를 추정해야 한다.

## 3. RAGAS의 한계와 인간 피드백 필요성

### 3.1 점수 불안정성과 재현성 문제
LLM judge 기반 평가는 동일 입력에서도 점수 변동이 발생할 수 있다. 일부 케이스는 0점 또는 NaN 같은 비정상 점수가 나오며 신뢰도를 떨어뜨린다.

### 3.2 사용자 판단과의 상관 부족
RAGAS가 사용자 만족도와 높은 상관을 가진다는 실증이 충분하지 않다. 자동 지표만으로는 “사용자가 느끼는 품질”을 대변하기 어렵다.

### 3.3 지표 범위의 한계
RAGAS는 근거 충실성/관련성 중심이다. 사용자 만족은 다음 요소에도 크게 좌우된다.
- 답변의 완결성, 실행 가능성
- 명료성, 가독성
- 톤/형식의 적절성
- 유용성(정확하지만 쓸모없는 답변 문제)

### 3.4 임계치 해석의 어려움
점수 0.8이 만족을 의미하는지 같은 해석 기준이 불명확하다. 결과적으로 이해관계자는 RAGAS를 단독 KPI로 인정하지 않기도 한다.

## 4. 제안 솔루션 개요

핵심 아이디어는 대표 문항을 선택해 인간 평가를 수집하고, 이를 기반으로 보정 모델을 학습하여 전체 데이터셋에 적용하는 것이다.

1. 대표 문항 선정(클러스터링/이산화)
2. 대표 문항에 대한 인간 평가 수집
3. RAGAS + 언어/행동 특성 기반 피처 구성
4. 만족도 예측 모델 학습
5. 전체 데이터셋에 보정 점수 적용 및 UI 통합
6. 반복 개선(액티브 러닝)

## 5. 단계별 방법론

### 5.1 1단계: 대표 문항 선정

**목표**: 적은 라벨로 데이터 분포를 넓게 커버한다.

- **데이터 표현**
  - 질문/답변 임베딩(질문+답변 결합 또는 각각 결합)
  - RAGAS 점수(예: faithfulness, answer relevance, context precision/recall) 결합
  - 임베딩과 점수는 스케일링을 맞춰 결합

- **알고리즘 선택**
  - K-Means(기본)
  - 대안: 계층적 클러스터링, K-Medoids, K-Center Greedy

- **대표 샘플 선택 규칙**
  - 중심(Centroid) 근접 샘플
  - 필요 시 극단값(최저/최고 점수) 소수 추가

### 5.2 2단계: 인간 평가 수집

**목표**: 사용자 만족도를 직접 라벨로 수집한다.

- **UI/엑셀 입력 설계**
  - 질문, 답변, (선택)컨텍스트
  - 만족도 점수(1~5 등)
  - (선택)코멘트

- **평가 기준(가이드)**
  - 질문에 제대로 답했는가(관련성)
  - 사실/규정/근거가 정확한가(정확성)
  - 답변이 충분히 구체적이고 실행 가능한가(유용성)
  - 표현이 명확하고 읽기 쉬운가(명료성)
  - 톤/형식이 서비스 목적에 맞는가(커뮤니케이션 품질)

- **다중 평가자 권장**
  - 가능하면 2명 이상 평가 후 평균 또는 합의

### 5.3 3단계: 피처 구성

**목표**: RAGAS만으로 부족한 정보를 보완한다.

- **RAGAS 기반 피처**
  - faithfulness
  - answer relevance
  - context precision/recall
  - (가능하면) 통합 지표/추가 메트릭

- **언어적 피처**
  - 답변 길이(토큰/문장 수)
  - 질문-답변 임베딩 유사도
  - 답변 구조(목록/단계/근거 인용 여부)
  - 가독성 지표, 톤/감성 플래그

- **행동/멀티턴 피처(있다면 강력)**
  - 재질문 여부
  - 후속 질문/목표 달성까지 턴 수
  - thumbs up/down 등 암묵 피드백

### 5.4 4단계: 만족도 보정 모델 학습

- **문제 정의**
  - 만족도 1~5: 회귀
  - Good/Bad/Okay: 분류

- **추천 모델**
  - XGBoost/LightGBM/RandomForest: 소량 라벨에서도 안정적
  - 선형 회귀/로지스틱: 해석 가능한 베이스라인

- **검증**
  - 라벨 적으면 K-fold cross validation
  - 회귀: RMSE/MAE, Spearman/Pearson
  - 분류: Accuracy, F1

- **베이스라인 비교**
  - RAGAS 단일 지표 vs 보정 모델 성능 비교로 개선 폭 제시

### 5.5 5단계: 전체 데이터셋 적용 및 UI 통합

- 전체 문항에 동일 피처 생성 후 모델 점수 산출
- RAGAS 점수와 보정 점수를 나란히 표시
- RAGAS와 보정 점수 불일치 케이스는 리뷰 대상으로 표시
- 낮은 보정 점수만 모아 QA 개선 우선순위로 활용

### 5.6 6단계: 반복 개선(액티브 러닝)

- 불확실성 높은 케이스, 불일치 큰 케이스 우선 라벨링
- 서비스 변화에 따라 대표 문항/라벨 세트 주기적 갱신
- 주기적 재학습으로 사용자 만족 정합성 개선

## 6. 실행 가능성과 효과

### 6.1 구현 가능성
- 클러스터링/샘플 선택은 표준 라이브러리로 구현 가능
- 라벨 수가 적어도 트리/회귀 모델로 충분히 학습 가능
- UI 통합은 기존 평가 UI에 점수 컬럼/필터 추가 수준

### 6.2 효과 기대 근거
- 자동 지표가 사용자 만족을 완전히 대변하지 못한다는 문제는 LLM 평가 전반의 공통 이슈
- 소량 라벨을 활용한 보정은 액티브 러닝/휴먼-인-더-루프의 대표적 접근
- 인간 피드백 기반 학습은 사용자 선호 정합성을 높이는 데 효과적이라는 사례가 많음

## 7. 리스크와 대응

- **라벨 품질/일관성**: 평가 가이드 제공, 다중 평가자, 불일치 검토
- **과적합**: 단순 모델 우선, 피처 수 제한, CV 적용
- **희귀 케이스 미포착**: 불확실/불일치 샘플 추가 라벨링
- **운영 변화**: 대표 세트/모델 주기적 갱신

## 8. 결론

대표 문항 선정 → 사용자 평가 → 학습 기반 보정 점수 확장은 실무적으로 구현 가능하며, RAGAS 단독 지표의 한계를 보완하는 유효한 방법이다. 특히 이해관계자에게 “우리 사용자 평가로 학습한 점수”라는 근거를 제공함으로써 신뢰도를 높일 수 있다. 반복 개선 루프를 적용하면 시간이 지날수록 사용자 만족 정합성이 강화된다.

## 9. 참고 문헌

- Srinivas Bommena, “A Practitioner’s Guide to Evaluating GenAI Applications with RAGAS,” Medium, 2025
- Reddit r/LangChain, “Why is everyone using RAGAS for RAG evaluation? It looks very unreliable,” 2024
- Ethan M. Rudd et al., “A Practical Guide for Evaluating LLMs and LLM-Reliant Systems,” arXiv 2506.13023, 2025
- Nguyen & Smeulders, “Active Learning Using Pre-clustering,” ICML 2004
- Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback,” NeurIPS 2022

## 변경 이력
- 2026-01-13 v0.1: 두 문서 통합 초안 작성
