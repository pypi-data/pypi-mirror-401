# LENA + RAGAS 인간 피드백 보정 통합 개발 계획서 (개발 실행안)

- 문서 목적: **평가 신뢰도(reliability) 분석(LENA)**과 **사용자 만족도 정합성 보정(인간 피드백 기반 보정 모델)**을 하나의 구현 로드맵으로 통합한다.
- 기준 문서(유일 근거):
  - `docs/guides/RAGAS_HUMAN_FEEDBACK_CALIBRATION_GUIDE.md`
  - `docs/guides/PRD_LENA.md`
- 적용 전제: 이미 RAGAS 평가/분석(Web UI 또는 배치)이 운영 중이며, 전체 데이터에 대한 전수 인간 평가는 불가능하다.

---

## 0. 이해관계자 관점(필수)과 설계 원칙

### 0.1 관점별 우선순위
- **인지과학자**
  - 인간 평가가 어떤 인지적 편향(확증편향, 최신효과, 앵커링)과 과제 설계(문항 난이도/맥락 제공 방식)에 의해 흔들리는지 관리해야 한다.
  - 요구: 평가 가이드(루브릭) 명확화, 다중 평가자, 불일치 검토 루프, 대표 샘플링의 “분포 커버리지” 검증.
- **편집자**
  - 평가 항목(명료성/가독성/형식)의 정의가 모호하면 라벨이 붕괴한다.
  - 요구: 문장/구조 기준의 체크포인트(목록/단계/근거 인용 등)와 코멘트 수집 체계.
- **국문학자**
  - 한국어 답변의 어조/문체/독자 친화성은 만족도에 직접 영향을 준다.
  - 요구: “톤/형식 적절성” 평가 기준의 한국어 특성 반영(문체 일관, 존대/반말, 정보 구조화).
- **소프트웨어 개발자**
  - 파이프라인이 실패하지 않고(NaN/0점 등) 재현 가능해야 하며, 결과가 저장/재사용 가능해야 한다.
  - 요구: 입력/출력 스키마, 오류 처리, 성능(벡터화), 테스트(단위/시뮬/통합) 기준.
- **아키텍트**
  - 신뢰도(LENA)와 보정(인간 피드백)의 책임 경계를 명확히 하고, 모듈 결합을 최소화해야 한다.
  - 원칙: LENA는 **메트릭 신뢰도**(CI/p-value/노이즈 분해/NK 추천)를 제공, 보정은 **만족도 정합성 추정**을 제공. 두 모듈은 “공통 결과 스키마/리포트”로만 결합.
- **UI/UX 전문가**
  - 사용자가 “결론을 믿어도 되는가”를 즉시 이해해야 한다(오차막대, 모드 토글, 불일치 리뷰).
  - 요구: 핵심 3위젯(LENA) + 보정 점수 병렬 표시 + 불일치/불확실성 기반 큐레이션(라벨링 우선순위) 흐름.

### 0.2 통합 설계 원칙(문서 근거 요약)
- 대표 샘플링 → 인간 평가 → 피처 구성 → 보정 모델 학습/적용 → UI 통합 → 액티브 러닝 반복
- 노이즈 분해(Total=Data+Prediction) + paired 비교 + SE 모드(single/mean_k/expected) + CI/p-value + MDE 및 (N,K) 추천
- “NK를 독립 표본으로 보지 않는다”: 질문 단위 평균을 기본 단위로 유지

---

## 1. 목표/범위

### 1.1 목표
1) **신뢰도 강화(LENA)**
- A/B 비교 결과에 diff, CI, p-value, 유의성 판단을 제공한다.
- 노이즈를 Data vs Prediction으로 분해하고, paired 분석으로 검정력을 높인다.
- 목표 MDE/power/alpha에 대해 최소 비용의 (N,K) 추천과 비용 추정을 제공한다.

2) **만족도 정합성 보정(인간 피드백 기반 보정 모델)**
- 대표 문항만 인간 평가(만족도)를 수집하고, RAGAS 및 언어/행동 특성 피처로 만족도를 예측(회귀/분류)한다.
- 전체 데이터셋에 보정 점수를 산출하여 RAGAS 단독 점수의 해석 불명확(“0.8이 만족인가?”) 문제를 완화한다.
- RAGAS 점수와 보정 점수가 불일치하는 케이스를 리뷰 대상으로 노출한다.

### 1.2 범위(포함)
- 데이터: 질문/답변/(선택)컨텍스트, RAGAS 서브메트릭(예: faithfulness, answer relevance, context precision/recall) 및 추가 피처(길이/유사도/구조/가독성 등 문서 범위 내)
- 분석: LENA의 unpaired/paired 노이즈 분해 + small-K 보정 + SE 모드 + z-test 기반 p-value/CI + 샘플 사이즈 추천(격자 탐색)
- UI: LENA 위젯 3종(비교 CI, 노이즈 breakdown, N/K 추천) + 보정 점수 병렬 표시 + 불일치/불확실성 기반 라벨링 큐
- 인터페이스: CLI(MVP), REST(선택) 범위를 문서 스펙에 맞게 정의

### 1.3 비목표(명시)
- 온라인 트래픽 기반 A/B 실험을 대체하지 않는다.
- 메트릭이 만족도를 “자동으로 보장”하지 않는다(보정은 만족도 추정 모델이며, 라벨 품질/편향에 의존).
- 순차검정(중간중단 정책 포함)은 MVP에 포함하지 않는다(기본 fixed-horizon).
- All-pairs, bootstrap, sign test 등은 확장(Should)로 단계 분리한다.

---

## 2. 문제정의(통합 관점)

### 2.1 현재 문제(의사결정 리스크)
- 같은 설정에서도 seed/temperature/샘플링으로 점수가 흔들려 결론이 뒤집힌다.
- 작은 개선(약 0.5~2%p)을 구분하기 어렵고, 거짓 양성/거짓 음성이 발생한다.
- 무엇을 늘려야(N? K?) 신뢰도가 오르는지 근거가 없다.
- RAGAS 점수가 사용자 만족도와 잘 맞지 않거나, 임계치 해석 기준이 불명확하다.
- 전수 인간 평가는 불가능하므로 “대표성 있는 소량 라벨”로 전체를 추정해야 한다.

### 2.2 통합 해법(한 문장)
- **LENA로 “결론의 신뢰도”를 계량(오차막대/유의성/노이즈/NK 추천)**하고, **인간 피드백 보정으로 “사용자 만족도 정합성”을 학습해 점수를 해석 가능하게 만든다.**

---

## 3. 사용자 시나리오(유즈케이스)

### 시나리오 A: 프롬프트 A/B가 진짜 개선인지 판단
1) 동일 질문 세트(N)에 대해 A/B를 같은 K로 실행한다.
2) LENA 비교에서 diff+CI+p-value를 확인한다(모드: mean_k 기본).
3) 결과가 유의하지 않으면 추천된 (N,K)로 다음 평가를 설계한다.
4) 동시에 보정 점수(만족도 추정)가 A/B 방향성과 일치하는지 확인한다.

### 시나리오 B: RAG 설정 변경(청킹/랭커 등)의 효과 확인
1) paired 비교로 질문 정렬을 강제한다.
2) 노이즈 breakdown에서 데이터 노이즈가 큰지/예측 노이즈가 큰지 확인한다.
3) 예측 노이즈가 크면 K를 늘리는 전략이 비용 효율적인지 추천을 확인한다.

### 시나리오 C: “RAGAS는 높지만 사용자는 불만” 케이스 찾기
1) 전체 데이터셋에 RAGAS 점수와 보정 점수를 산출한다.
2) UI에서 불일치(예: RAGAS 높음/보정 낮음) 케이스를 필터링한다.
3) 해당 케이스를 우선 리뷰/개선 대상으로 지정한다.

### 시나리오 D: 인간 평가 비용을 최소화하면서 보정 모델 성능을 올리기
1) 대표 샘플링으로 라벨링 세트를 구성한다(클러스터 중심 + 필요 시 극단값).
2) 다중 평가자 점수 평균/합의로 라벨 품질을 관리한다.
3) 불확실성/불일치가 큰 샘플을 우선 추가 라벨링(액티브 러닝)한다.

---

## 4. 기능 요구사항(통합)

### 4.1 보정(인간 피드백) 기능 요구사항
| ID | 기능 | 설명 | 완료 기준(요약) |
|---|---|---|---|
| CAL-01 | 대표 문항 선정 | 임베딩(+RAGAS 점수) 결합 후 K-Means 등으로 대표 샘플 선택 | 소량 라벨로 분포 커버리지 설명 가능 |
| CAL-02 | 인간 평가 수집 | 질문/답변/(선택)컨텍스트, 만족도(1~5), (선택)코멘트 입력 | 평가 가이드 제공, 입력 누락 방지 |
| CAL-03 | 평가 가이드(루브릭) | 관련성/정확성/유용성/명료성/톤·형식 기준 제공 | 평가자 간 불일치 감소 절차 포함 |
| CAL-04 | 피처 구성 | RAGAS + 언어적 피처(길이/유사도/구조/가독성/톤 플래그 등) + (있다면)행동 피처 | 피처 생성이 재현 가능 |
| CAL-05 | 보정 모델 학습 | 만족도 회귀/분류, 소량 라벨에 강한 모델(XGBoost/LightGBM/RandomForest, 선형 베이스라인) | CV 기반 성능 리포트 제공 |
| CAL-06 | 전체 적용 및 UI 통합 | 전체 문항 보정 점수 산출, RAGAS와 병렬 표시, 불일치 케이스 표시 | 불일치 큐가 생성되고 탐색 가능 |
| CAL-07 | 반복 개선(액티브 러닝) | 불확실/불일치 우선 라벨링, 주기적 재학습 | 라벨 확장 전략이 문서화됨 |

### 4.2 LENA 기능 요구사항(PRD Must Have 중심)
| ID | 기능 | 설명 | 완료 기준(요약) |
|---|---|---|---|
| M01 | 다중 예측 수집 | 동일 질문에 대해 K회 예측 수집(시드/샘플링 정책 포함) | 질문×K 결과가 누락 없이 집계 |
| M02 | 노이즈 분해 | Total/Data/Prediction 노이즈 분해(총분산 기반) + small-K 보정 | |total-(data+pred)| 허용오차 충족 |
| M03 | Paired 분석 | 동일 질문 기반 A/B 비교(질문 정렬/매칭 강제) | 매칭 실패 시 명시 오류 |
| M04 | 유의성/신뢰구간 | 평균/차이/CI/p-value/유의성/효과크기 | 모드별 산출 일관 |
| M05 | SE 모드 지원 | single / mean_k / expected 모드 토글 | 모드별 SE 관계 성립 |
| M06 | N/K 추천 | 목표 MDE, power, alpha에 따른 (N,K) 추천 + 비용 추정 | 파일럿 기반 추천 재현 |
| M07 | 결과 시각화(핵심 3종) | (1) CI 바, (2) Noise breakdown, (3) SE vs K/N 곡선 | UI에서 동일 결과 확인 |

### 4.3 통합 기능 요구사항(결합 지점)
| ID | 기능 | 설명 | 완료 기준(요약) |
|---|---|---|---|
| INT-01 | 공통 평가 단위 정렬 | LENA는 질문 단위 평균을 기본으로, 보정도 동일 질문 ID를 기준으로 결합 | 질문 ID 불일치 시 결합 불가 처리 |
| INT-02 | 보정 점수의 신뢰도 표현 | 보정 점수(만족도 추정)도 LENA 분석 대상으로 포함 가능하도록 “메트릭”으로 취급 | 보정 점수에 대해 CI/SE 산출 가능 |
| INT-03 | 라벨링 우선순위 큐 | (불일치 큼) ∪ (불확실성 큼) 샘플을 라벨링 대상으로 표시 | 큐 기준이 일관되게 설명됨 |
| INT-04 | 리포트/아티팩트 표준화 | 비교 리포트(JSON+UI) 및 보정 요약이 함께 조회 가능 | 동일 실행에서 재현 가능 |

---

## 5. 데이터/모델/평가 파이프라인(단계별)

### 5.1 입력 데이터(필수 단위)
- 질문 ID, 질문, 답변, (선택)컨텍스트
- 평가 실행 반복(K): 동일 질문에 대해 seed/샘플링을 달리한 K회 예측(또는 평가) 결과
- RAGAS 서브메트릭(문서에서 언급된 항목 중심)
- 인간 라벨(대표 샘플에 한정): 만족도 점수(예: 1~5) + (선택)코멘트

### 5.2 파이프라인 단계(운영 흐름)
1) **평가 실행(질문 N × 반복 K)**
   - 동일 질문 단위로 K회 결과를 확보한다( LENA의 N/K 정의 준수 ).
2) **자동 메트릭 산출(RAGAS 등)**
   - RAGAS 서브메트릭을 케이스별로 생성한다.
3) **대표 샘플링(라벨링 셋 생성)**
   - 질문/답변 임베딩(+RAGAS 점수 결합)를 사용해 클러스터링(K-Means 기본)
   - 클러스터 중심 근접 샘플을 대표로 선택, 필요 시 극단값을 소수 추가
4) **인간 평가 수집**
   - UI/엑셀 입력: 질문/답변/(선택)컨텍스트 + 만족도(1~5 등) + (선택)코멘트
   - 가능하면 2명 이상 평가 후 평균 또는 합의
5) **피처 엔지니어링**
   - RAGAS 기반 피처 + 언어적 피처(길이/유사도/구조/가독성/톤 플래그)
   - (있다면) 행동/멀티턴 피처(재질문 여부 등)
6) **보정 모델 학습 및 검증**
   - 회귀/분류 문제로 정의
   - 소량 라벨에 강한 모델(XGBoost/LightGBM/RandomForest) + 해석 가능한 베이스라인(선형/로지스틱)
   - K-fold CV로 RMSE/MAE, Spearman/Pearson(회귀) 또는 Accuracy/F1(분류) 평가
   - “RAGAS 단일 지표 vs 보정 모델” 비교로 개선 폭 제시
7) **전체 데이터셋 보정 점수 적용**
   - 전체 케이스에 동일 피처 생성 후 보정 점수 산출
   - UI에서 RAGAS 점수와 보정 점수 병렬 표시, 불일치 케이스 강조
8) **LENA 분석(신뢰도/유의성/NK 추천)**
   - 단일 Evaluator 노이즈 분해(unpaired)
   - A/B 비교(paired) diff/CI/p-value 및 모드 토글
   - 파일럿(N0,K0) 기반 data_var/pred_var 추정 → 목표 MDE/power/alpha의 (N,K) 추천
9) **반복 개선(액티브 러닝)**
   - 불확실성 높은 케이스, 불일치 큰 케이스 우선 라벨링
   - 서비스 변화에 맞춰 대표 세트/라벨 세트 갱신 및 재학습

---

## 6. 통계/노이즈 분석 모듈(LENA) 상세 설계 요건

### 6.1 핵심 정의(문서 기준)
- Data variance: 질문 샘플이 바뀌면 평균이 달라지는 변동성
- Prediction variance: 동일 질문에서 샘플링/랜덤성으로 점수가 달라지는 변동성
- Total variance: 전체 불확실성 = Data + Prediction(총분산 법칙)

### 6.2 small-K 보정(필수)
- 질문별 분산 기반 보정 항(b)을 포함하여 Total=Data+Pred 일관성을 유지한다.
- K가 작을 때 분해가 깨지는 문제를 완화한다.

### 6.3 SE 모드(필수)와 의미
- `single`: 질문당 1회 예측 기준 불확실성(리더보드형)
- `mean_k`: 질문당 K회 평균의 불확실성(튜닝/실험 기본)
- `expected`: K→∞ 가정(예측 노이즈가 완전히 평균화된 이론적 하한)

UI에서 모드를 명확히 토글할 수 있어야 하며, 문서 기준의 관계를 테스트로 보장한다:
- expected SE ≤ mean_k SE ≤ single SE
- K 증가 → mean_k SE 감소(단조성)

### 6.4 유의성 검정(MVP)
- 기본: z-test(양측), p-value, 95% CI
- paired 비교를 기본으로 하여 데이터 노이즈를 상쇄하고 검정력을 높인다.

### 6.5 샘플 사이즈 추천(MVP)
- 파일럿(N0,K0) 1회로 data_var/pred_var 추정
- 후보 (N,K) 격자 탐색:
  - 예상 SE 및 MDE 계산
  - 비용 = N×K×(평가자 수) 기반 최소 비용 조합 추천
- UI에서 목표 MDE 슬라이더와 함께 비용/시간 추정치를 제시한다.

---

## 7. UI/CLI/API 범위(통합)

### 7.1 UI 범위(필수 위젯/화면)
1) **A/B 비교 화면(필수)**
- 상단: meanA, meanB, diff
- 모드 토글: single / mean_k / expected
- CI bar + p-value + 유의성 배지
- Noise breakdown(Data vs Prediction)
- (N,K) 추천 박스: 목표 MDE 설정 + 비용/시간 추정

2) **노이즈 탐색 화면(필수)**
- 단일 Evaluator: total/data/pred 추정치, SE 곡선(K 변화)

3) **보정 점수 통합 표시(필수)**
- RAGAS 점수와 보정 점수(만족도 추정)를 나란히 표시
- “불일치 큼” 케이스를 리뷰 대상으로 표시
- 라벨링 우선순위(불확실/불일치 기반)로 다음 라벨링 작업을 안내

### 7.2 CLI 범위(MVP)
- LENA: compare / noise / recommend 명령 범위는 PRD 스펙을 따른다.
- 보정: 대표 샘플 추출, 라벨 입력 반영, 모델 학습/적용, 요약 출력이 가능해야 한다(실행 단위/아티팩트 단위는 기존 평가 워크플로에 맞춘다).

### 7.3 REST 범위(선택)
- LENA: `/compare`, `/noise`, `/recommend`, `/all_pairs(확장)`
- 결과는 비교/노이즈/추천 결과를 JSON으로 직렬화해 UI가 소비한다.

---

## 8. 테스트/품질/리스크

### 8.1 테스트 요구사항(필수)
- LENA 단위 테스트
  - 분해식 일관성: Total ≈ Data + Prediction(K>1에서, 허용오차 설정)
  - small-K 보정이 분해 오차를 개선하는지 비교
  - SE 모드 단조성/관계: expected ≤ mean_k ≤ single, K 증가 시 mean_k SE 감소
- 시뮬레이션 테스트(강력 추천)
  - Bernoulli(정답/오답) 생성 모델에서 이론적 SE와 근접
  - (확장 시) bootstrap CI와 분석식 CI 비교
- 통합 테스트
  - 입력(CSV/JSONL/로그) → 분석 → UI 표시까지 스모크 테스트
  - Inter/Intra 비교 시나리오 각각 1개 이상

### 8.2 리스크 및 완화(통합)
- 라벨 품질/일관성 저하
  - 완화: 평가 가이드 제공, 다중 평가자, 불일치 검토, 코멘트 기반 재교육
- 보정 모델 과적합
  - 완화: 단순 모델 우선, 피처 수 제한, K-fold CV, 베이스라인 비교
- 희귀 케이스 미포착(대표성 부족)
  - 완화: 불확실/불일치 샘플 우선 추가 라벨링(액티브 러닝)
- optional stopping으로 유의성 왜곡
  - 완화: MVP는 fixed-horizon 원칙, 순차 모니터링은 정책 포함 후 확장
- 다중비교(all-pairs)로 false positive 증가(확장)
  - 완화: FDR(BH) 또는 Bonferroni 보정, UI에서 “보정 후 유의” 강조
- seed 미지원/비결정성
  - 완화: seed_supported 플래그/경고 및 재현성 레벨 표시(문서 스펙 반영)

---

## 9. 단계별 일정(권장 로드맵) 및 산출물

> LENA MVP는 PRD 기준 “2~3주” 범위를 따른다. 보정 기능은 대표 샘플링/라벨링/학습/적용/UI 통합 단계를 동일 기간에 일부 병렬화하되, 검증(라벨 품질·CV) 단계를 반드시 포함한다.

### 9.1 0단계: 준비(착수 조건 정리)
- 산출물
  - D0-1: 데이터/로그 입력 형식 확정(질문 ID, N/K 정의, A/B 매칭 규칙)
  - D0-2: 인간 평가 가이드(루브릭) 초안 및 라벨링 템플릿(UI/엑셀) 확정
- 인수 기준
  - 질문 단위 정렬 기준과 “NK 비독립” 원칙이 팀 합의로 고정
  - 평가 루브릭이 5개 축(관련성/정확성/유용성/명료성/톤·형식)을 포함

### 9.2 1단계: MVP 구현(신뢰도 + 보정 기본 흐름)
- LENA 산출물
  - D1-1: EvalMatrix(N×K) 구성 및 validate
  - D1-2: unpaired/paired 노이즈 분해 + small-K 보정
  - D1-3: compare(diff/CI/p-value, SE 모드), recommend(N/K 추천)
  - D1-4: UI 위젯 3종(비교 CI, noise breakdown, SE 곡선/추천)
- 보정 산출물
  - D1-5: 대표 샘플링(클러스터링 기반) + 라벨 수집 경로
  - D1-6: 피처 생성(RAGAS + 언어적 피처) + 보정 모델 학습(CV 포함)
  - D1-7: 전체 데이터셋 보정 점수 적용 + UI 병렬 표시 + 불일치 필터
- 인수 기준(예시 체크)
  - LENA: |total-(data+pred)|가 허용오차 내, SE 모드 관계 테스트 통과
  - 보정: CV 리포트가 생성되며, “RAGAS 단일 vs 보정 모델” 비교 결과가 재현 가능
  - UI: 모드 토글 및 핵심 3위젯이 동일 결과를 일관되게 표시
  - 불일치 케이스가 식별/필터링 가능

### 9.3 2단계: 확장(강건 통계 + 운영 강화 + 반복 개선)
- 산출물(Should)
  - D2-1: paired bootstrap CI, sign test 옵션
  - D2-2: all-pairs 비교 + 다중비교 보정(FDR(BH))
  - D2-3: 결과 캐시/재현성 메타데이터(버전/해시/실험 설정) 정리
  - D2-4: 액티브 러닝 루프 운영 정책(불확실/불일치 기반 라벨링) 고도화
- 인수 기준
  - bootstrap CI와 분석식 CI가 특정 조건에서 근접(테스트로 보장)
  - all-pairs 결과에서 보정 후 유의성 표시가 동작하며, false positive 리스크가 문서화

---

## 10. 의존성(Dependencies)

### 10.1 데이터/운영 의존성
- 동일 질문 ID로 A/B를 paired 비교할 수 있는 평가 로그/결과
- K 반복 실행을 가능하게 하는 평가 실행 정책(시드/샘플링 정책 포함)
- 대표 샘플링을 위한 임베딩 및 RAGAS 점수 산출 가능 환경

### 10.2 인력/프로세스 의존성
- 인간 평가자(최소 2명 권장) 및 평가 가이드 교육/합의 프로세스
- UI/백엔드/분석 엔진 간 배포 순서 조율(기존 평가 UI에 추가 위젯/컬럼 반영)

### 10.3 비용/ROI 프레임(문서 기반)
- 평가 비용 근사: 비용 ≈ (#Evaluator) × N × K × (API 호출 단가)
- 파일럿 1회(N0,K0)로 data_var/pred_var를 추정해 목표 MDE/power/alpha 달성에 필요한 최소 비용 (N,K)을 추천
- 효과 측정: “무작정 N만 증가” 대비 비용 절감, 결론 뒤집힘 비율 감소

---

## 11. 롤아웃(배포) 계획

### 11.1 단계적 릴리즈
1) **파일럿(내부)**
- 목적: (N0,K0)로 노이즈 추정 및 (N,K) 추천의 합리성 확인, 대표 샘플링/라벨링 흐름 점검
2) **MVP 공개(제한 사용자)**
- 목적: UI 위젯 3종 + 보정 점수 병렬 표시 + 불일치 리뷰 워크플로 정착
3) **정식 운영(확장 포함 여부 결정)**
- 기준: 결론 뒤집힘 비율 감소, 비용 최적화 효과, 사용자(엔지니어/리서처) 신뢰 확보

### 11.2 롤백/안전장치
- UI에서는 기본적으로 기존 점수(자동 메트릭)를 유지하고, LENA/보정은 “추가 정보”로 노출한다.
- seed 미지원/데이터 불일치 등 핵심 전제 위반 시 명확한 경고와 결과 제한을 표시한다.

---

## 12. 운영/모니터링(필수)

### 12.1 운영 지표(예시)
- LENA 관점
  - CI 폭 추이(동일 평가셋에서 시간/버전별), 결론 뒤집힘 비율(반복 실험 시)
  - data_var vs pred_var 비중(무엇을 늘려야 하는지 근거)
  - 추천 (N,K) 대비 실제 수행 (N,K) 및 비용 편차
- 보정 관점
  - 라벨 수/클러스터 커버리지, 평가자 간 불일치율
  - 보정 모델 CV 성능(RMSE/MAE, 상관) 및 시간 경과에 따른 드리프트
  - RAGAS-보정 불일치 케이스 비율 및 개선 후 변화

### 12.2 운영 주기
- 대표 샘플/라벨 세트 주기적 갱신(서비스 변화 반영)
- 보정 모델 주기적 재학습 및 성능 점검
- LENA 분석은 비교/리포트 생성 시 자동 포함(의사결정 단위에 묶기)

---

## 13. 성공 지표(Success Metrics)

### 13.1 LENA MVP 성공 기준(문서 기반)
- 동일 A/B 실험을 반복했을 때 결론 뒤집힘 비율 감소
- 동일 목표 MDE에서 필요한 평가 비용(총 호출 수) 감소(“무작정 N만 증가” 대비)
- 사용자(엔지니어/리서처)가 “이 결과는 믿고 의사결정 가능”하다고 느끼는 리포트/UX 제공

### 13.2 보정 성공 기준(문서 기반)
- RAGAS 단일 지표 대비 보정 모델이 만족도 라벨을 더 잘 설명(회귀/분류 성능 및 상관 개선)
- RAGAS와 보정 점수 불일치 케이스를 명확히 식별하고, 리뷰/개선 우선순위로 활용 가능

---

## 14. 자체 점검 및 개선 반복(필수)

### 14.1 자체 점검 체크리스트(릴리즈 전/후 반복)
- [ ] 대표 샘플링이 데이터 분포를 넓게 커버하는가(클러스터별 대표성 확인)
- [ ] 평가 가이드가 모호하지 않은가(평가자 피드백 반영)
- [ ] 다중 평가자 불일치율이 관리 가능한 수준인가(불일치 검토/합의 프로세스 작동)
- [ ] 보정 모델이 과적합 신호를 보이지 않는가(CV 성능 안정성, 피처 과다 여부)
- [ ] RAGAS 단일 대비 보정의 개선 근거가 리포트로 제시되는가
- [ ] LENA 분해식 일관성(total≈data+pred)이 테스트로 보장되는가
- [ ] SE 모드 토글이 의미적으로 올바르게 설명/표시되는가(single/mean_k/expected)
- [ ] paired 비교에서 질문 정렬/매칭이 강제되고, 실패 시 조용히 진행되지 않는가
- [ ] 추천 (N,K)가 비용/시간 관점에서 해석 가능하게 제시되는가
- [ ] 불일치/불확실성 기반 라벨링 우선순위가 실제 개선으로 이어지는가(액티브 러닝 루프)

### 14.2 2차 개선 지침(“한 번 더 좋아지게” 만드는 규칙)
- 라벨 품질이 흔들리면 모델 복잡도를 올리기 전에 **가이드/평가자 합의/불일치 처리**를 먼저 고친다.
- 불확실/불일치 샘플을 무작정 늘리기 전에 **대표성 결손(희귀 클러스터)**을 먼저 보완한다.
- 유의성 논쟁이 반복되면, MVP의 fixed-horizon 원칙을 유지하면서 **강건 옵션(bootstrap/sign test)**을 확장 단계로 추가한다.
- “N만 늘리는” 관행이 남아 있으면, noise breakdown과 추천(N/K)을 리포트/회의 템플릿의 기본 항목으로 고정한다.
- 보정 점수 해석이 혼란스럽다면, RAGAS-보정 불일치 사례를 중심으로 **리뷰 큐**를 운영하고, 개선 전/후 변화를 정기적으로 비교한다.

---

## 15. 명시적 산출물(Deliverables) 요약 및 인수 기준(Acceptance Criteria)

### 15.1 산출물 목록
- LENA 분석 엔진: 노이즈 분해, 비교, 추천 결과(구조화 데이터)
- UI 위젯 3종: diff+CI, noise breakdown, SE/NK 추천 시각화(모드 토글 포함)
- 대표 샘플링 모듈: 임베딩+RAGAS 기반 클러스터링 및 대표 샘플 추출
- 라벨링 입력 흐름: UI/엑셀 기반 만족도 수집(코멘트 포함 가능)
- 보정 모델: 피처 생성 + 학습 + 검증(CV) + 전체 적용(보정 점수 산출)
- 통합 리포트: RAGAS 점수/보정 점수/LENA 결과를 함께 조회 가능한 형태
- 테스트 패키지: 단위/시뮬레이션/통합 테스트(문서 요구사항 충족)

### 15.2 인수 기준(필수 충족 조건)
- LENA
  - 분해식 일관성 및 SE 모드 관계 테스트 통과
  - paired 비교에서 질문 매칭이 강제되며, 실패가 명확히 드러남
  - 추천(N,K)이 파일럿 기반으로 재현 가능
- 보정
  - 라벨링 가이드/입력 포맷이 제공되고, 다중 평가자 불일치 처리 루프가 존재
  - CV 결과가 산출되며, RAGAS 단일 대비 비교가 가능
  - RAGAS와 보정 점수 병렬 표시 및 불일치 케이스 탐색이 가능
- 통합/운영
  - 결과가 “의사결정에 쓰이는 화면/리포트”로 제공되어 실제 사용 흐름을 갖춤
  - 운영 지표(노이즈/라벨/성능)가 추적 가능

---

## 16. 참고 링크(간단)
- `docs/guides/RAGAS_HUMAN_FEEDBACK_CALIBRATION_GUIDE.md`
- `docs/guides/PRD_LENA.md`
