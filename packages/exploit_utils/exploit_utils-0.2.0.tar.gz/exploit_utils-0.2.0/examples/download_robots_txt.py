"""示例: 访问目标网站的 robots.txt 并保存到本地"""

import logging

import exploit_utils as exp
from exploit_utils.http import URL

# 获取 session
# 该对象已提前配置过: 随机 UA 头, 5s 超时, 关闭 SSL 证书验证
http = exp.http.get_session()
# 获取 logger, 记录日志
logger = exp.log.get_logger()


def task(target: URL) -> None:
    logger.debug(f"downloading robots.txt from {target}")
    # 访问目标的 robots.txt
    try:
        # 你可以像使用 pathlib.Path 那样使用 URL 对象
        resp = http.get(url=(target / "robots.txt").url, stream=True)
        resp.raise_for_status()
    except Exception as e:
        logger.error(f"error when request: {e}")
        return
    # URL.as_path 会自动将 url 转换为文件系统的路径格式
    # uget 会自动将文件路径定位到用户运行该脚本时所在目录
    save_path = exp.fs.uget(target.as_path())
    # 将响应下载到本地, save 函数会自动处理流式传输并输出成功与失败日志
    exp.http.save(resp, save_path)


if __name__ == "__main__":
    # 获取命令行参数
    # BASIC_PRESET 预设了 --url --file --output 等参数
    parser = exp.args.Parser()
    parser.load_args(exp.args.BASIC_PRESET + [exp.args.DEBUG])
    args = parser.parse_args()
    # 关闭 http 警告
    exp.http.no_warn()
    # 开启调试输出
    if args.debug:
        logger.set_level(logging.DEBUG)
    # 设置日志输出文件
    if args.output:
        exp.log.add_outputfile(args.output)
    if args.file:
        # 使用 parse_lines 可以快速读取批量的 url 输入
        urls = exp.fs.parse_lines(args.file, parser=URL)
        # 开启线程池进行多线程下载
        # 使用 handle_fs 可以获取一个与日志系统兼容的进度条, 实时查看任务完成情况
        with exp.mt.ThreadPool(max_workers=args.threads) as pool:
            exp.mt.handle_fs([pool.submit(task, url) for url in urls])
    # 处理单个目标
    elif args.url:
        task(URL(args.url))
    else:
        print("use `--url` or `--file` to enter target")
