# Loreley configuration example
#
# Copy this file to `.env` and adjust values for your environment:
#
#   cp env.example .env
#
# Notes:
# - This file is loaded by `loreley.config.Settings` via pydantic-settings.
# - Complex types (lists/dicts) should be provided as JSON literals.

# -----------------------------------------------------------------------------
# Core application
# -----------------------------------------------------------------------------
APP_ENV=development
LOG_LEVEL=INFO

# Optional: override where long-running process logs are written.
# LOGS_BASE_DIR=

# -----------------------------------------------------------------------------
# Database (PostgreSQL)
# -----------------------------------------------------------------------------
# If you use `docker-compose.yml` in this repo, keep these defaults.
DATABASE_URL=postgresql+psycopg://loreley:loreley@localhost:5432/loreley

# -----------------------------------------------------------------------------
# Task queue / Dramatiq broker (Redis)
# -----------------------------------------------------------------------------
TASKS_REDIS_URL=redis://127.0.0.1:6379/0
TASKS_REDIS_NAMESPACE=loreley
# Queue name prefix. Loreley derives a per-experiment queue as:
#   "{TASKS_QUEUE_NAME}.{WORKER_EXPERIMENT_ID hex}"
TASKS_QUEUE_NAME=loreley.evolution

# -----------------------------------------------------------------------------
# Scheduler
# -----------------------------------------------------------------------------
# Path to a git checkout of the TARGET repository that the scheduler monitors.
# If unset, the scheduler falls back to WORKER_REPO_WORKTREE or the current directory.
SCHEDULER_REPO_ROOT=

# Startup approval: before entering the main loop, the scheduler scans the experiment
# root commit (MAPELITES_EXPERIMENT_ROOT_COMMIT), prints the eligible repo-state file
# count + filter knobs, and asks for an interactive y/n confirmation.
# This requires stdin to be a TTY; in non-interactive environments the scheduler exits.
#
# You can bypass the prompt with either:
# - `--yes` (CLI flag), or
# - `SCHEDULER_STARTUP_APPROVE=true` (environment variable).
# SCHEDULER_STARTUP_APPROVE=false

# -----------------------------------------------------------------------------
# Worker repository
# -----------------------------------------------------------------------------
# Attach this worker process to a single experiment UUID so experiment-scoped
# settings can be loaded once at startup and remain frozen for the process.
WORKER_EXPERIMENT_ID=
# Upstream git remote URL used by workers to clone/fetch/push job branches.
# For local demos you can point this to a local bare repo path (e.g. /abs/path/remote.git).
WORKER_REPO_REMOTE_URL=
WORKER_REPO_BRANCH=main

# Base clone path used by the worker. Each evolution job runs in its own
# isolated git worktree under "<WORKER_REPO_WORKTREE>-worktrees/" and the worker
# removes that per-job worktree after the job completes.
WORKER_REPO_WORKTREE=~/.cache/loreley/worker-repo

# Optional: isolate the base clone per worker process by randomising the final
# path segment of WORKER_REPO_WORKTREE (useful when sharing a filesystem is
# undesirable). When enabled, a suffix like "-a1b2c3d4" is appended.
# WORKER_REPO_WORKTREE_RANDOMIZE=false
# WORKER_REPO_WORKTREE_RANDOM_SUFFIX_LEN=8

# -----------------------------------------------------------------------------
# Planning / coding backends
# -----------------------------------------------------------------------------
# Defaults assume the Codex CLI is installed and available as `codex`.
# You can override the backend using dotted references:
#   WORKER_PLANNING_BACKEND=some.module:backend_factory
#   WORKER_CODING_BACKEND=some.module:backend_factory
#
# WORKER_PLANNING_CODEX_BIN=codex
# WORKER_CODING_CODEX_BIN=codex
# WORKER_PLANNING_CODEX_PROFILE=
# WORKER_CODING_CODEX_PROFILE=

# -----------------------------------------------------------------------------
# Evaluator (required for the worker)
# -----------------------------------------------------------------------------
# Dotted reference to a callable plugin returning `EvaluationResult` or a dict.
# Optional python paths are applied before importing the plugin.
#
# Circle-packing example:
#   WORKER_EVALUATOR_PYTHON_PATHS=["examples/circle_packing_env"]
#   WORKER_EVALUATOR_PLUGIN=evaluate:plugin
WORKER_EVALUATOR_PYTHON_PATHS=[]
WORKER_EVALUATOR_PLUGIN=

# -----------------------------------------------------------------------------
# OpenAI-compatible API (required for embeddings and some summaries)
# -----------------------------------------------------------------------------
OPENAI_API_KEY=
# OPENAI_BASE_URL=
# OPENAI_API_SPEC=responses

# -----------------------------------------------------------------------------
# MAP-Elites (experiment behaviour)
# -----------------------------------------------------------------------------
# Required for the scheduler: pins the experiment root commit used for repo-state
# bootstrap and incremental-only ingestion.
MAPELITES_EXPERIMENT_ROOT_COMMIT=

# Fixed output dimensionality for code embeddings (experiment-scoped).
# The scheduler must set this when deriving an experiment; UI/API/workers load it
# from the persisted experiment config snapshot and do not require it at startup.
MAPELITES_CODE_EMBEDDING_DIMENSIONS=1536

# File-level embedding cache backend used by repo-state embeddings.
# The scheduler requires "db" for incremental-only repo-state ingestion.
MAPELITES_FILE_EMBEDDING_CACHE_BACKEND=db


