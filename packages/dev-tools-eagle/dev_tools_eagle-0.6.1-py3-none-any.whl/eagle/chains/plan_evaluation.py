from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence, RunnableLambda
from langchain_core.language_models.base import BaseLanguageModel
from pydantic import BaseModel, Field
from typing import ClassVar
from eagle.utils.prompt_utils import EagleJsonOutputParser

# Schemas for output
class PlanEvaluationOutputSchemaEN(BaseModel):
    description: str = Field(description="Evaluation of the plan according to the criteria.")
    score: float = Field(description="Score from 0 to 1.")

class PlanEvaluationOutputSchemaPT_BR(BaseModel):
    descricao: str = Field(description="Avaliação do plano de acordo com os critérios.")
    nota: float = Field(description="Nota de 0 a 1.")

# Output Parser
class PlanEvaluationOutputParser(EagleJsonOutputParser):
    """Custom output parser for the plan evaluation chain."""

    CONVERTION_SCHEMA: ClassVar[dict] = {
        "pt-br": {
            "class_for_parsing": PlanEvaluationOutputSchemaPT_BR,
            "convertion_schema": {
                "descricao": {
                    "target_key": "description",
                    "value_mapping": {}
                },
                "nota": {
                    "target_key": "score",
                    "value_mapping": {}
                }
            }
        },
        "en": {
            "class_for_parsing": PlanEvaluationOutputSchemaEN,
            "convertion_schema": {
                "description": {
                    "target_key": "description",
                    "value_mapping": {}
                },
                "score": {
                    "target_key": "score",
                    "value_mapping": {}
                }
            }
        },
    }

    TARGET_SCHEMA: BaseModel = PlanEvaluationOutputSchemaEN

PLAN_EVALUATION_PROMPT_STR_EN = """
You are an expert evaluator of plans generated by AI agents.
Below is the plan to be evaluated:
---------------- Plan -----------------
{{plan}}
---------------------------------------

Evaluation criteria:
{{evaluation_criteria}}

Please provide a concise evaluation of the plan according to the criteria above, and assign a score from 0 to 1 (where 1 is perfect and 0 is completely inadequate).

Output format (JSON):
{
    "description": "<your evaluation>",
    "score": <score between 0 and 1>
}
"""

PLAN_EVALUATION_PROMPT_STR_PT_BR = """
Você é um avaliador especialista de planos gerados por agentes de IA.
Abaixo está o plano a ser avaliado:
---------------- Plano -----------------
{{plan}}
----------------------------------------

Critérios de avaliação:
{{evaluation_criteria}}

Forneça uma avaliação concisa do plano de acordo com os critérios acima e atribua uma nota de 0 a 1 (onde 1 é perfeito e 0 é completamente inadequado).

Formato de saída (JSON):
{
    "descricao": "<sua avaliação>",
    "nota": <nota entre 0 e 1>
}
"""

PLAN_EVALUATION_PROMPTS = {
    "en": PromptTemplate.from_template(PLAN_EVALUATION_PROMPT_STR_EN, template_format="jinja2"),
    "pt-br": PromptTemplate.from_template(PLAN_EVALUATION_PROMPT_STR_PT_BR, template_format="jinja2"),
}

def create_plan_evaluation_chain(
    prompt_language: str,
    llm: BaseLanguageModel,
    use_structured_output: bool = False
) -> RunnableSequence:
    if prompt_language not in PLAN_EVALUATION_PROMPTS:
        raise ValueError(f"Unsupported prompt language: {prompt_language}")

    prompt = PLAN_EVALUATION_PROMPTS[prompt_language]
    output_parser = PlanEvaluationOutputParser(
        source_lang=prompt_language,
        llm=llm,
        use_structured_output=use_structured_output
    )
    _parse = RunnableLambda(lambda x: output_parser.parse(x))

    chain = (
        {
            "plan": lambda d: d["plan"],
            "evaluation_criteria": lambda d: d["evaluation_criteria"]
        }
        | prompt
        | llm
        | _parse
    )
    return chain
