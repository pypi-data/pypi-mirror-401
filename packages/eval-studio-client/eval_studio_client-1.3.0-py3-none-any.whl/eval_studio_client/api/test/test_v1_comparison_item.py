# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_comparison_item import V1ComparisonItem

class TestV1ComparisonItem(unittest.TestCase):
    """V1ComparisonItem unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1ComparisonItem:
        """Test V1ComparisonItem
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1ComparisonItem`
        """
        model = V1ComparisonItem()
        if include_optional:
            return V1ComparisonItem(
                question = '',
                diff_flipped_metrics = [
                    eval_studio_client.api.models.flipped_metric_information.Flipped metric information(
                        metric_name = '', 
                        baseline_value = 1.337, 
                        current_value = 1.337, )
                    ],
                baseline_test_case_result = eval_studio_client.api.models.test_case_result.Test case result(
                    key = '', 
                    input = '', 
                    corpus = [
                        ''
                        ], 
                    context = [
                        ''
                        ], 
                    categories = [
                        ''
                        ], 
                    relationships = [
                        eval_studio_client.api.models.test_case_relationship_information.Test case relationship information(
                            type = '', 
                            target = '', 
                            target_type = '', )
                        ], 
                    expected_output = '', 
                    output_constraints = [
                        ''
                        ], 
                    output_condition = '', 
                    actual_output = '', 
                    actual_duration = 1.337, 
                    cost = 1.337, 
                    model_key = '', 
                    test_key = '', 
                    test_case_key = '', 
                    metrics = [
                        eval_studio_client.api.models.metric_information.Metric information(
                            key = '', 
                            value = 1.337, )
                        ], 
                    metrics_meta = {
                        'key' : ''
                        }, 
                    actual_output_meta = [
                        eval_studio_client.api.models.actual_output_metadata.Actual output metadata(
                            tokenization = '', 
                            data = [
                                eval_studio_client.api.models.data_fragment.Data fragment(
                                    text = '', 
                                    meta = {
                                        'key' : ''
                                        }, )
                                ], )
                        ], 
                    metric_scores = [
                        eval_studio_client.api.models.comparison_metric_score_information_(specific_to_comparison_reports).Comparison metric score information (specific to comparison reports)(
                            metric_name = '', 
                            metric_score = 1.337, )
                        ], 
                    result_error_message = '', ),
                baseline_diff_actual_output_meta = eval_studio_client.api.models.actual_output_metadata_diff.Actual output metadata diff(
                    sentences = [
                        ''
                        ], 
                    sentences_count = 56, 
                    common_sentences = [
                        ''
                        ], 
                    common_count = 56, 
                    unique_sentences = [
                        ''
                        ], 
                    unique_count = 56, 
                    identical = True, 
                    sentence_similarity = {
                        'key' : 1.337
                        }, ),
                baseline_diff_retrieved_context = eval_studio_client.api.models.retrieved_context_diff.Retrieved context diff(
                    chunks = [
                        ''
                        ], 
                    chunks_count = 56, 
                    common_chunks = [
                        ''
                        ], 
                    common_count = 56, 
                    unique_chunks = [
                        ''
                        ], 
                    unique_count = 56, 
                    identical = True, 
                    chunk_similarity = {
                        'key' : 1.337
                        }, ),
                current_test_case_result = eval_studio_client.api.models.test_case_result.Test case result(
                    key = '', 
                    input = '', 
                    corpus = [
                        ''
                        ], 
                    context = [
                        ''
                        ], 
                    categories = [
                        ''
                        ], 
                    relationships = [
                        eval_studio_client.api.models.test_case_relationship_information.Test case relationship information(
                            type = '', 
                            target = '', 
                            target_type = '', )
                        ], 
                    expected_output = '', 
                    output_constraints = [
                        ''
                        ], 
                    output_condition = '', 
                    actual_output = '', 
                    actual_duration = 1.337, 
                    cost = 1.337, 
                    model_key = '', 
                    test_key = '', 
                    test_case_key = '', 
                    metrics = [
                        eval_studio_client.api.models.metric_information.Metric information(
                            key = '', 
                            value = 1.337, )
                        ], 
                    metrics_meta = {
                        'key' : ''
                        }, 
                    actual_output_meta = [
                        eval_studio_client.api.models.actual_output_metadata.Actual output metadata(
                            tokenization = '', 
                            data = [
                                eval_studio_client.api.models.data_fragment.Data fragment(
                                    text = '', 
                                    meta = {
                                        'key' : ''
                                        }, )
                                ], )
                        ], 
                    metric_scores = [
                        eval_studio_client.api.models.comparison_metric_score_information_(specific_to_comparison_reports).Comparison metric score information (specific to comparison reports)(
                            metric_name = '', 
                            metric_score = 1.337, )
                        ], 
                    result_error_message = '', ),
                current_diff_actual_output_meta = eval_studio_client.api.models.actual_output_metadata_diff.Actual output metadata diff(
                    sentences = [
                        ''
                        ], 
                    sentences_count = 56, 
                    common_sentences = [
                        ''
                        ], 
                    common_count = 56, 
                    unique_sentences = [
                        ''
                        ], 
                    unique_count = 56, 
                    identical = True, 
                    sentence_similarity = {
                        'key' : 1.337
                        }, ),
                current_diff_retrieved_context = eval_studio_client.api.models.retrieved_context_diff.Retrieved context diff(
                    chunks = [
                        ''
                        ], 
                    chunks_count = 56, 
                    common_chunks = [
                        ''
                        ], 
                    common_count = 56, 
                    unique_chunks = [
                        ''
                        ], 
                    unique_count = 56, 
                    identical = True, 
                    chunk_similarity = {
                        'key' : 1.337
                        }, )
            )
        else:
            return V1ComparisonItem(
        )
        """

    def testV1ComparisonItem(self):
        """Test V1ComparisonItem"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
