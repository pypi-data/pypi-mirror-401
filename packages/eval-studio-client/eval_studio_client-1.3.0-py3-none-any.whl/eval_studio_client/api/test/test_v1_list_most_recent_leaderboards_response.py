# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_list_most_recent_leaderboards_response import V1ListMostRecentLeaderboardsResponse

class TestV1ListMostRecentLeaderboardsResponse(unittest.TestCase):
    """V1ListMostRecentLeaderboardsResponse unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1ListMostRecentLeaderboardsResponse:
        """Test V1ListMostRecentLeaderboardsResponse
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1ListMostRecentLeaderboardsResponse`
        """
        model = V1ListMostRecentLeaderboardsResponse()
        if include_optional:
            return V1ListMostRecentLeaderboardsResponse(
                leaderboards = [
                    eval_studio_client.api.models.v1_leaderboard.v1Leaderboard(
                        name = '', 
                        create_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        creator = '', 
                        update_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        updater = '', 
                        delete_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        deleter = '', 
                        display_name = '', 
                        description = '', 
                        status = 'LEADERBOARD_STATUS_UNSPECIFIED', 
                        evaluator = '', 
                        tests = [
                            ''
                            ], 
                        model = '', 
                        create_operation = '', 
                        leaderboard_report = '', 
                        leaderboard_table = '', 
                        leaderboard_summary = '', 
                        llm_models = [
                            ''
                            ], 
                        leaderboard_problems = [
                            eval_studio_client.api.models.v1_problem_and_action.v1ProblemAndAction(
                                description = '', 
                                severity = '', 
                                problem_type = '', 
                                problem_attrs = {
                                    'key' : ''
                                    }, 
                                actions_description = '', 
                                explainer_id = '', 
                                explainer_name = '', 
                                explanation_type = '', 
                                explanation_name = '', 
                                explanation_mime = '', 
                                resources = [
                                    ''
                                    ], )
                            ], 
                        evaluator_parameters = '', 
                        insights = [
                            eval_studio_client.api.models.v1_insight.v1Insight(
                                description = '', 
                                actions_description = '', 
                                actions_codes = [
                                    ''
                                    ], 
                                evaluator_id = '', 
                                evaluator_display_name = '', 
                                explanation_type = '', 
                                explanation_name = '', 
                                explanation_mime = '', 
                                insight_type = '', 
                                insight_attrs = {
                                    'key' : ''
                                    }, )
                            ], 
                        model_parameters = '', 
                        h2ogpte_collection = '', 
                        type = 'LEADERBOARD_TYPE_UNSPECIFIED', 
                        demo = True, 
                        test_lab = '', 
                        evaluation_type = 'EVALUATION_TYPE_UNSPECIFIED', )
                    ]
            )
        else:
            return V1ListMostRecentLeaderboardsResponse(
        )
        """

    def testV1ListMostRecentLeaderboardsResponse(self):
        """Test V1ListMostRecentLeaderboardsResponse"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
