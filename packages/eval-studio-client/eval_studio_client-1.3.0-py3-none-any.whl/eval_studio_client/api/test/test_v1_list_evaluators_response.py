# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_list_evaluators_response import V1ListEvaluatorsResponse

class TestV1ListEvaluatorsResponse(unittest.TestCase):
    """V1ListEvaluatorsResponse unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1ListEvaluatorsResponse:
        """Test V1ListEvaluatorsResponse
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1ListEvaluatorsResponse`
        """
        model = V1ListEvaluatorsResponse()
        if include_optional:
            return V1ListEvaluatorsResponse(
                evaluators = [
                    eval_studio_client.api.models.v1_evaluator.v1Evaluator(
                        name = '', 
                        create_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        creator = '', 
                        update_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        updater = '', 
                        delete_time = datetime.datetime.strptime('2013-10-20 19:20:30.00', '%Y-%m-%d %H:%M:%S.%f'), 
                        deleter = '', 
                        display_name = '', 
                        description = '', 
                        content = 'YQ==', 
                        mime_type = '', 
                        filename = '', 
                        identifier = '', 
                        tags = [
                            ''
                            ], 
                        parameters = [
                            eval_studio_client.api.models.based_on_the_h2_o/ai_eval_studio_evaluator_resource/
https://github/com/h2oai/h2o_sonar/blob/0492b2f2651fb2fde08d981b72c9fda7dc6b9697/h2o_sonar/lib/api/evaluators/py#l53_l81.Based on the H2O.ai Eval Studio Evaluator resource.
https://github.com/h2oai/h2o-sonar/blob/0492b2f2651fb2fde08d981b72c9fda7dc6b9697/h2o_sonar/lib/api/evaluators.py#L53-L81(
                                name = '', 
                                type = 'EVALUATOR_PARAM_TYPE_UNSPECIFIED', 
                                description = '', 
                                comment = '', 
                                string_val = '', 
                                float_val = 1.337, 
                                bool_val = True, 
                                min = 1.337, 
                                max = 1.337, 
                                predefined = [
                                    ''
                                    ], 
                                category = [
                                    ''
                                    ], )
                            ], 
                        brief_description = '', 
                        enabled = True, 
                        tagline = '', 
                        primary_metric = '', 
                        primary_metric_default_threshold = 1.337, )
                    ]
            )
        else:
            return V1ListEvaluatorsResponse(
        )
        """

    def testV1ListEvaluatorsResponse(self):
        """Test V1ListEvaluatorsResponse"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
