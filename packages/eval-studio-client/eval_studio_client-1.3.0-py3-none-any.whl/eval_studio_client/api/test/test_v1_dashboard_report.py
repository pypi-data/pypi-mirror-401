# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_dashboard_report import V1DashboardReport

class TestV1DashboardReport(unittest.TestCase):
    """V1DashboardReport unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1DashboardReport:
        """Test V1DashboardReport
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1DashboardReport`
        """
        model = V1DashboardReport()
        if include_optional:
            return V1DashboardReport(
                results = [
                    eval_studio_client.api.models.v1_dashboard_report_result.v1DashboardReportResult(
                        key = '', 
                        input = '', 
                        expected_output = '', 
                        actual_output = '', 
                        model_key = '', 
                        test_case_key = '', 
                        metrics = {
                            'key' : eval_studio_client.api.models.v1_metric_scores.v1MetricScores(
                                scores = [
                                    eval_studio_client.api.models.v1_metric_score.v1MetricScore(
                                        key = '', 
                                        value = 1.337, )
                                    ], )
                            }, 
                        result_error_map = {
                            'key' : ''
                            }, 
                        human_decision = 'HUMAN_DECISION_UNSPECIFIED', 
                        comment = '', 
                        annotations = {
                            'key' : None
                            }, )
                    ],
                evaluator = [
                    eval_studio_client.api.models.v1_leaderboard_report_evaluator.v1LeaderboardReportEvaluator(
                        id = '', 
                        name = '', 
                        display_name = '', 
                        tagline = '', 
                        description = '', 
                        brief_description = '', 
                        model_types = [
                            ''
                            ], 
                        can_explain = [
                            ''
                            ], 
                        explanation_scopes = [
                            ''
                            ], 
                        explanations = [
                            eval_studio_client.api.models.v1_leaderboard_report_explanation.v1LeaderboardReportExplanation(
                                explanation_type = '', 
                                name = '', 
                                category = '', 
                                scope = '', 
                                has_local = '', 
                                formats = [
                                    ''
                                    ], )
                            ], 
                        parameters = [
                            eval_studio_client.api.models.v1_leaderboard_report_evaluator_parameter.v1LeaderboardReportEvaluatorParameter(
                                name = '', 
                                description = '', 
                                comment = '', 
                                type = '', 
                                predefined = [
                                    None
                                    ], 
                                tags = [
                                    ''
                                    ], 
                                min = 1.337, 
                                max = 1.337, 
                                category = '', )
                            ], 
                        keywords = [
                            ''
                            ], 
                        metrics_meta = [
                            eval_studio_client.api.models.v1_leaderboard_report_metrics_meta_entry.v1LeaderboardReportMetricsMetaEntry(
                                key = '', 
                                display_name = '', 
                                data_type = '', 
                                display_value = '', 
                                description = '', 
                                value_range = [
                                    1.337
                                    ], 
                                value_enum = [
                                    ''
                                    ], 
                                higher_is_better = True, 
                                threshold = 1.337, 
                                is_primary_metric = True, 
                                parent_metric = '', 
                                exclude = True, )
                            ], )
                    ]
            )
        else:
            return V1DashboardReport(
        )
        """

    def testV1DashboardReport(self):
        """Test V1DashboardReport"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
