# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from eval_studio_client.api.models.v1_leaderboard_report_result import V1LeaderboardReportResult

class TestV1LeaderboardReportResult(unittest.TestCase):
    """V1LeaderboardReportResult unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> V1LeaderboardReportResult:
        """Test V1LeaderboardReportResult
            include_option is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `V1LeaderboardReportResult`
        """
        model = V1LeaderboardReportResult()
        if include_optional:
            return V1LeaderboardReportResult(
                key = '',
                input = '',
                corpus = [
                    ''
                    ],
                context = [
                    ''
                    ],
                categories = [
                    ''
                    ],
                relationships = [
                    eval_studio_client.api.models.v1_leaderboard_report_result_relationship.v1LeaderboardReportResultRelationship(
                        type = '', 
                        target = '', 
                        target_type = '', )
                    ],
                expected_output = '',
                output_constraints = [
                    ''
                    ],
                output_condition = '',
                actual_output = '',
                actual_duration = 1.337,
                cost = 1.337,
                model_key = '',
                test_case_key = '',
                metrics = [
                    eval_studio_client.api.models.v1_metric_score.v1MetricScore(
                        key = '', 
                        value = 1.337, )
                    ],
                result_error_message = '',
                actual_output_meta = [
                    eval_studio_client.api.models.v1_leaderboard_report_actual_output_meta.v1LeaderboardReportActualOutputMeta(
                        tokenization = '', 
                        data = [
                            eval_studio_client.api.models.v1_leaderboard_report_actual_output_data.v1LeaderboardReportActualOutputData(
                                text = '', 
                                metrics = eval_studio_client.api.models.metrics.metrics(), )
                            ], 
                        agent_chat_activity_diagram = eval_studio_client.api.models.v1_agent_chat_activity_diagram.v1AgentChatActivityDiagram(
                            rows = [
                                eval_studio_client.api.models.v1_agent_chat_activity_diagram_row.v1AgentChatActivityDiagramRow(
                                    nodes = [
                                        eval_studio_client.api.models.v1_agent_chat_activity_diagram_node.v1AgentChatActivityDiagramNode(
                                            id = '', 
                                            role = '', 
                                            label = '', )
                                        ], )
                                ], 
                            edges = [
                                eval_studio_client.api.models.v1_agent_chat_activity_diagram_edge.v1AgentChatActivityDiagramEdge(
                                    from = '', 
                                    to = '', 
                                    label = '', )
                                ], ), 
                        agent_chat_tools_bar_chart = eval_studio_client.api.models.v1_agent_chat_tools_bar_chart.v1AgentChatToolsBarChart(
                            tools = {
                                'key' : eval_studio_client.api.models.v1_agent_chat_tool_usage.v1AgentChatToolUsage(
                                    name = '', 
                                    success_count = 56, 
                                    failure_count = 56, 
                                    total_count = 56, )
                                }, ), 
                        agent_chat_scripts_bar_chart = eval_studio_client.api.models.v1_agent_chat_scripts_bar_chart.v1AgentChatScriptsBarChart(
                            scripts = {
                                'key' : eval_studio_client.api.models.v1_agent_chat_script_usage.v1AgentChatScriptUsage(
                                    name = '', 
                                    success_count = 56, 
                                    failure_count = 56, 
                                    total_count = 56, )
                                }, ), )
                    ],
                human_decision = 'HUMAN_DECISION_UNSPECIFIED',
                comment = '',
                annotations = {
                    'key' : None
                    }
            )
        else:
            return V1LeaderboardReportResult(
        )
        """

    def testV1LeaderboardReportResult(self):
        """Test V1LeaderboardReportResult"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
