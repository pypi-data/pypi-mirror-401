# coding: utf-8

"""
    ai/h2o/eval_studio/v1/insight.proto

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: version not set
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from eval_studio_client.api.models.v1_all_metric_scores import V1AllMetricScores
from eval_studio_client.api.models.v1_metric_scores import V1MetricScores
from eval_studio_client.api.models.v1_model import V1Model
from eval_studio_client.api.models.v1_test_cases_generator import V1TestCasesGenerator
from typing import Optional, Set
from typing_extensions import Self

class AdversarialInputsServiceTestAdversarialInputsRobustnessRequest(BaseModel):
    """
    AdversarialInputsServiceTestAdversarialInputsRobustnessRequest
    """ # noqa: E501
    operation: Optional[StrictStr] = Field(default=None, description="Required. The Operation processing adversarial inputs robustness testing.")
    generator_input_types: Optional[List[V1TestCasesGenerator]] = Field(default=None, description="Optional. The list of adversarial input types to generate.", alias="generatorInputTypes")
    generator_document_urls: Optional[List[StrictStr]] = Field(default=None, description="Required. The document URLs which were used to generate the baseline TestCases.", alias="generatorDocumentUrls")
    generator_model: Optional[V1Model] = Field(default=None, alias="generatorModel")
    generator_base_llm_model: Optional[StrictStr] = Field(default=None, description="Required. Base LLM model to use for generation of adversarial the prompts.", alias="generatorBaseLlmModel")
    generator_count: Optional[StrictInt] = Field(default=None, description="Required. The number of adversarial TestCases to generate.", alias="generatorCount")
    generator_topics: Optional[List[StrictStr]] = Field(default=None, description="Optional. Topics to generate questions for. If not specified, use document summarization as topic generation.", alias="generatorTopics")
    generator_chunks: Optional[List[StrictStr]] = Field(default=None, description="Optional. The list of chunks to use for generation. If set, the Documents assigned to the Test and h2ogpte_collection_id are ignored.", alias="generatorChunks")
    generator_h2ogpte_collection_id: Optional[StrictStr] = Field(default=None, description="Optional. ID of the h2oGPTe collection to use. If provided, documents referenced by Test and any specified chunks are ignored. This field is required if Test does not reference any documents and no chunks are provided. If this field is left empty, a temporary collection will be created.", alias="generatorH2ogpteCollectionId")
    evaluator_identifiers: Optional[List[StrictStr]] = Field(default=None, description="Required. Evaluator identifiers to use for the model evaluation using the adversarial inputs.", alias="evaluatorIdentifiers")
    evaluators_parameters: Optional[Dict[str, StrictStr]] = Field(default=None, description="Optional. Additional evaluators configuration, for all the evaluators used in the evaluation. Key is the evaluator identifier, and the value is a JSON string containing the configuration dictionary.", alias="evaluatorsParameters")
    model: Optional[V1Model] = None
    base_llm_model: Optional[StrictStr] = Field(default=None, description="Required. Base LLM model to be evaluated using the adversarial inputs.", alias="baseLlmModel")
    model_parameters: Optional[StrictStr] = Field(default=None, description="Optional. Parameters overrides for the Model host in JSON format.", alias="modelParameters")
    default_h2ogpte_model: Optional[V1Model] = Field(default=None, alias="defaultH2ogpteModel")
    baseline_eval: Optional[StrictStr] = Field(default=None, description="Required. Baseline evaluation name.", alias="baselineEval")
    baseline_metrics: Optional[Dict[str, V1MetricScores]] = Field(default=None, description="Required. Map of baseline metrics from the evaluator to the average metric scores for the evaluator.", alias="baselineMetrics")
    all_baseline_metrics_scores: Optional[Dict[str, V1AllMetricScores]] = Field(default=None, description="Required. Map of baseline metric to all and every test case metric score.", alias="allBaselineMetricsScores")
    __properties: ClassVar[List[str]] = ["operation", "generatorInputTypes", "generatorDocumentUrls", "generatorModel", "generatorBaseLlmModel", "generatorCount", "generatorTopics", "generatorChunks", "generatorH2ogpteCollectionId", "evaluatorIdentifiers", "evaluatorsParameters", "model", "baseLlmModel", "modelParameters", "defaultH2ogpteModel", "baselineEval", "baselineMetrics", "allBaselineMetricsScores"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of AdversarialInputsServiceTestAdversarialInputsRobustnessRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of generator_model
        if self.generator_model:
            _dict['generatorModel'] = self.generator_model.to_dict()
        # override the default output from pydantic by calling `to_dict()` of model
        if self.model:
            _dict['model'] = self.model.to_dict()
        # override the default output from pydantic by calling `to_dict()` of default_h2ogpte_model
        if self.default_h2ogpte_model:
            _dict['defaultH2ogpteModel'] = self.default_h2ogpte_model.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each value in baseline_metrics (dict)
        _field_dict = {}
        if self.baseline_metrics:
            for _key_baseline_metrics in self.baseline_metrics:
                if self.baseline_metrics[_key_baseline_metrics]:
                    _field_dict[_key_baseline_metrics] = self.baseline_metrics[_key_baseline_metrics].to_dict()
            _dict['baselineMetrics'] = _field_dict
        # override the default output from pydantic by calling `to_dict()` of each value in all_baseline_metrics_scores (dict)
        _field_dict = {}
        if self.all_baseline_metrics_scores:
            for _key_all_baseline_metrics_scores in self.all_baseline_metrics_scores:
                if self.all_baseline_metrics_scores[_key_all_baseline_metrics_scores]:
                    _field_dict[_key_all_baseline_metrics_scores] = self.all_baseline_metrics_scores[_key_all_baseline_metrics_scores].to_dict()
            _dict['allBaselineMetricsScores'] = _field_dict
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of AdversarialInputsServiceTestAdversarialInputsRobustnessRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj, strict=False)

        _obj = cls.model_validate({
            "operation": obj.get("operation"),
            "generatorInputTypes": obj.get("generatorInputTypes"),
            "generatorDocumentUrls": obj.get("generatorDocumentUrls"),
            "generatorModel": V1Model.from_dict(obj["generatorModel"]) if obj.get("generatorModel") is not None else None,
            "generatorBaseLlmModel": obj.get("generatorBaseLlmModel"),
            "generatorCount": obj.get("generatorCount"),
            "generatorTopics": obj.get("generatorTopics"),
            "generatorChunks": obj.get("generatorChunks"),
            "generatorH2ogpteCollectionId": obj.get("generatorH2ogpteCollectionId"),
            "evaluatorIdentifiers": obj.get("evaluatorIdentifiers"),
            "evaluatorsParameters": obj.get("evaluatorsParameters"),
            "model": V1Model.from_dict(obj["model"]) if obj.get("model") is not None else None,
            "baseLlmModel": obj.get("baseLlmModel"),
            "modelParameters": obj.get("modelParameters"),
            "defaultH2ogpteModel": V1Model.from_dict(obj["defaultH2ogpteModel"]) if obj.get("defaultH2ogpteModel") is not None else None,
            "baselineEval": obj.get("baselineEval"),
            "baselineMetrics": dict(
                (_k, V1MetricScores.from_dict(_v))
                for _k, _v in obj["baselineMetrics"].items()
            )
            if obj.get("baselineMetrics") is not None
            else None,
            "allBaselineMetricsScores": dict(
                (_k, V1AllMetricScores.from_dict(_v))
                for _k, _v in obj["allBaselineMetricsScores"].items()
            )
            if obj.get("allBaselineMetricsScores") is not None
            else None
        }, strict=False)
        return _obj


