{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d96e87d",
   "metadata": {},
   "source": [
    "# devqubit: Getting Started\n",
    "\n",
    "devqubit is a lightweight experiment tracker for quantum workflows. It helps you:\n",
    "\n",
    "- **Track** parameters, metrics, tags, and artifacts for every run\n",
    "- **Reproduce** results via fingerprints and stored program artifacts\n",
    "- **Compare** runs (including statistical interpretation for shot noise)\n",
    "- **Verify** candidates against a project baseline (great for CI)\n",
    "- **Share** portable bundles (`.zip`) with everything needed to re-run or review\n",
    "\n",
    "This notebook is intentionally small and \"batteries-included\": you can copy/paste the patterns into\n",
    "your own experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll build\n",
    "\n",
    "1. Configure a workspace (store + registry)\n",
    "2. Track a simple optimization run (params, metrics, artifacts)\n",
    "3. Run a parameter sweep (grouped runs)\n",
    "4. Compare runs and verify a candidate against a baseline\n",
    "5. Package a run as a portable bundle and unpack it elsewhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c66884",
   "metadata": {},
   "source": [
    "## 1. Storage configuration\n",
    "\n",
    "devqubit stores everything in a **workspace**: run records plus a content-addressable object store.\n",
    "\n",
    "You can configure the workspace in two ways (highest precedence first):\n",
    "\n",
    "1. **In code** (explicit `store` / `registry` arguments to `track()`)\n",
    "2. **Environment variables** (`DEVQUBIT_*`)\n",
    "\n",
    "A typical local workspace looks like:\n",
    "\n",
    "```\n",
    "~/.devqubit/\n",
    "├── objects/          # content-addressable object store\n",
    "│   └── sha256/       # artifacts stored by digest\n",
    "├── registry.db       # SQLite database for run metadata\n",
    "└── baselines.json    # project baselines\n",
    "```\n",
    "\n",
    "For this demo we create a *temporary local folder* so you can run safely anywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbacb29",
   "metadata": {},
   "source": [
    "### Environment variables\n",
    "\n",
    "The most common environment variables are:\n",
    "\n",
    "- `DEVQUBIT_HOME` — workspace root directory (default: `~/.devqubit`)\n",
    "- `DEVQUBIT_STORAGE_URL` — object store URL\n",
    "- `DEVQUBIT_REGISTRY_URL` — registry URL\n",
    "- `DEVQUBIT_CAPTURE_GIT` — capture git provenance (default: `true`)\n",
    "- `DEVQUBIT_CAPTURE_PIP` — capture pip freeze (default: `false`)\n",
    "\n",
    "In this notebook we use an **explicit local workspace** so everything stays self-contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup: create an isolated local workspace for this demo.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Public devqubit API\n",
    "from devqubit import (\n",
    "    track,\n",
    "    diff,\n",
    "    create_registry,\n",
    "    create_store,\n",
    "    verify_against_baseline,\n",
    "    pack_run,\n",
    "    unpack_bundle,\n",
    "    Bundle,\n",
    ")\n",
    "from devqubit.compare import VerifyPolicy\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DemoConfig:\n",
    "    \"\"\"Configuration for this notebook demo.\"\"\"\n",
    "\n",
    "    # Workspace\n",
    "    workspace_dir: Path = Path(\".devqubit_demo\")\n",
    "\n",
    "    # Reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "    # Mock optimization\n",
    "    project: str = \"optimization_study\"\n",
    "    iterations: int = 50\n",
    "\n",
    "    # Sweep settings\n",
    "    lr_sweep: tuple[float, ...] = (0.05, 0.10, 0.15)\n",
    "    sweep_group_id: str = \"lr_sweep_demo\"\n",
    "    sweep_group_name: str = \"Learning Rate Sweep (Demo)\"\n",
    "\n",
    "    # Verification policy\n",
    "    tvd_max: float = 0.10\n",
    "    noise_factor: float = 3.0\n",
    "\n",
    "\n",
    "CFG = DemoConfig()\n",
    "\n",
    "# Reset demo workspace\n",
    "if CFG.workspace_dir.exists():\n",
    "    shutil.rmtree(CFG.workspace_dir)\n",
    "CFG.workspace_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create store and registry for this workspace\n",
    "store = create_store(f\"file://{CFG.workspace_dir}/objects\")\n",
    "registry = create_registry(f\"file://{CFG.workspace_dir}\")\n",
    "\n",
    "np.random.seed(CFG.seed)\n",
    "\n",
    "print(f\"Workspace: {CFG.workspace_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afe2d6",
   "metadata": {},
   "source": [
    "## 2. Basic tracking\n",
    "\n",
    "The `track()` context manager is the main entry point. Inside the block you can log:\n",
    "\n",
    "- **Parameters**: configuration / hyperparameters\n",
    "- **Metrics**: scalars and time-series (via `step=...`)\n",
    "- **Tags**: lightweight labels for filtering and grouping\n",
    "- **Artifacts**: JSON, text, and binary files (e.g., circuits, counts, configs)\n",
    "\n",
    "On exit, devqubit finalizes the run, computes fingerprints, and writes the record to the registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_optimization(learning_rate: float, iterations: int, seed: int) -> list[float]:\n",
    "    \"\"\"Simulate a simple optimization loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate\n",
    "        Step size used to update the synthetic loss.\n",
    "    iterations\n",
    "        Number of iterations.\n",
    "    seed\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        Loss values over iterations.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    loss = 1.0\n",
    "    history: list[float] = []\n",
    "    for _ in range(iterations):\n",
    "        loss *= 1 - learning_rate * rng.uniform(0.8, 1.0)\n",
    "        history.append(float(loss))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run a single tracked experiment.\"\"\"\n",
    "\n",
    "with track(\n",
    "    project=CFG.project,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    "    run_name=\"baseline\",\n",
    ") as run:\n",
    "    # Parameters: things you'd want to reproduce exactly\n",
    "    run.log_params(\n",
    "        {\n",
    "            \"learning_rate\": 0.10,\n",
    "            \"max_iterations\": CFG.iterations,\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"seed\": CFG.seed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Tags: convenient labels for filtering later\n",
    "    run.set_tags({\"demo\": \"getting_started\", \"kind\": \"baseline\"})\n",
    "\n",
    "    # Execute the experiment (here: synthetic optimization)\n",
    "    t0 = time.time()\n",
    "    history = mock_optimization(\n",
    "        learning_rate=0.10,\n",
    "        iterations=CFG.iterations,\n",
    "        seed=CFG.seed,\n",
    "    )\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    # Time-series metric\n",
    "    for step, loss in enumerate(history):\n",
    "        run.log_metric(\"loss\", loss, step=step)\n",
    "\n",
    "    # Summary metrics\n",
    "    run.log_metrics(\n",
    "        {\n",
    "            \"final_loss\": history[-1],\n",
    "            \"best_loss\": float(min(history)),\n",
    "            \"duration_s\": duration_s,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Artifacts (examples)\n",
    "    run.log_json(\n",
    "        name=\"run_config\",\n",
    "        obj={\n",
    "            \"learning_rate\": 0.10,\n",
    "            \"iterations\": CFG.iterations,\n",
    "            \"seed\": CFG.seed,\n",
    "        },\n",
    "        role=\"config\",\n",
    "    )\n",
    "\n",
    "    run.log_text(\n",
    "        name=\"notes\",\n",
    "        text=\"Baseline run for getting-started demo.\",\n",
    "        role=\"documentation\",\n",
    "    )\n",
    "\n",
    "    run.log_bytes(\n",
    "        kind=\"loss_history.json\",\n",
    "        data=json.dumps(history).encode(\"utf-8\"),\n",
    "        media_type=\"application/json\",\n",
    "        role=\"results\",\n",
    "    )\n",
    "\n",
    "    baseline_id = run.run_id\n",
    "\n",
    "print(f\"Baseline run_id: {baseline_id}\")\n",
    "print(f\"Final loss:      {history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efd991",
   "metadata": {},
   "source": [
    "### Inspect the stored run\n",
    "\n",
    "A run record can be loaded back from the registry at any time. Typical things to look at:\n",
    "\n",
    "- `status` (FINISHED/FAILED)\n",
    "- `params`, `metrics`, `tags`\n",
    "- `artifacts` (with role/kind/media_type metadata)\n",
    "- fingerprints (used for reproducibility checks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ae055",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = registry.load(baseline_id)\n",
    "\n",
    "print(f\"Run ID:   {record.run_id}\")\n",
    "print(f\"Status:   {record.status}\")\n",
    "\n",
    "print(\"\\nParams:\")\n",
    "for k, v in record.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nMetrics (last values):\")\n",
    "for k, v in record.metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nTags:\")\n",
    "for k, v in record.tags.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Fingerprints\n",
    "fingerprints = getattr(record, \"fingerprints\", {})\n",
    "run_fp = fingerprints.get(\"run\") or getattr(record, \"run_fingerprint\", None)\n",
    "prog_fp = fingerprints.get(\"program\") or getattr(record, \"program_fingerprint\", None)\n",
    "\n",
    "print(\"\\nFingerprints:\")\n",
    "print(f\"  run:     {run_fp}\")\n",
    "print(f\"  program: {prog_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcbfcf",
   "metadata": {},
   "source": [
    "## 3. Artifacts: list + retrieve\n",
    "\n",
    "Artifacts are referenced by digest in the object store. The helper functions below make it easy to\n",
    "list or find artifacts by role/kind, then download their bytes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d21e02",
   "metadata": {},
   "source": [
    "### Artifact roles (why they matter)\n",
    "\n",
    "Artifacts are not just \"files\": their **role** tells devqubit how to interpret them.\n",
    "\n",
    "Common roles:\n",
    "\n",
    "- `program` — circuits/programs (important for reproducibility + comparisons)\n",
    "- `results` — counts, expectation values, measurements\n",
    "- `config` — compile/execute options, hyperparameters, settings\n",
    "- `device_snapshot` — device calibration snapshots\n",
    "- `documentation` — notes, reports, metadata for humans\n",
    "\n",
    "In practice: **always** log your circuit/QASM/QPY under `role=\"program\"` whenever possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67492372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All artifacts:\")\n",
    "for art in record.artifacts:\n",
    "    print(f\"  [{art.role}] {art.kind} ({art.media_type}) -> {art.digest}\")\n",
    "\n",
    "# Find config artifact by role\n",
    "config_art = next((a for a in record.artifacts if a.role == \"config\"), None)\n",
    "if config_art:\n",
    "    config_bytes = store.get_bytes(config_art.digest)\n",
    "    print(\"\\nConfig artifact (decoded):\")\n",
    "    print(config_bytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5c288",
   "metadata": {},
   "source": [
    "## 4. Parameter sweep with grouped runs\n",
    "\n",
    "When you sweep a parameter (shots, learning rate, depth, etc.), it's helpful to group runs:\n",
    "\n",
    "- `group_id`: stable identifier you can query later\n",
    "- `group_name`: human-readable label\n",
    "\n",
    "This keeps your registry tidy and makes comparisons easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_run_ids: list[str] = []\n",
    "\n",
    "for lr in CFG.lr_sweep:\n",
    "    with track(\n",
    "        project=CFG.project,\n",
    "        store=store,\n",
    "        registry=registry,\n",
    "        group_id=CFG.sweep_group_id,\n",
    "        group_name=CFG.sweep_group_name,\n",
    "    ) as run:\n",
    "        run.log_param(\"learning_rate\", lr)\n",
    "        run.log_param(\"max_iterations\", CFG.iterations)\n",
    "        run.set_tag(\"kind\", \"sweep\")\n",
    "\n",
    "        history = mock_optimization(\n",
    "            learning_rate=lr,\n",
    "            iterations=CFG.iterations,\n",
    "            seed=CFG.seed,\n",
    "        )\n",
    "        run.log_metric(\"final_loss\", history[-1])\n",
    "        run.log_metric(\"best_loss\", float(min(history)))\n",
    "\n",
    "        sweep_run_ids.append(run.run_id)\n",
    "\n",
    "print(\"Sweep runs:\")\n",
    "for rid in sweep_run_ids:\n",
    "    print(\" \", rid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149512e",
   "metadata": {},
   "source": [
    "### List groups and their runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f88a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = registry.list_groups()\n",
    "\n",
    "print(\"Groups:\")\n",
    "for g in groups:\n",
    "    print(f\"  {g['group_id']} - {g['group_name']} ({g['project']})\")\n",
    "\n",
    "print(\"\\nRuns in sweep group:\")\n",
    "runs_in_group = registry.list_runs_in_group(CFG.sweep_group_id)\n",
    "for r in runs_in_group:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af4d6a",
   "metadata": {},
   "source": [
    "## 5. Parent → child run lineage (multi-stage experiments)\n",
    "\n",
    "For workflows like:\n",
    "- coarse search → fine search\n",
    "- baseline → candidate tuning\n",
    "- training → evaluation\n",
    "\n",
    "...you can link runs via `parent_run_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with track(\n",
    "    project=CFG.project,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    "    run_name=\"coarse\",\n",
    ") as parent:\n",
    "    parent.log_param(\"stage\", \"coarse\")\n",
    "    parent_id = parent.run_id\n",
    "\n",
    "with track(\n",
    "    project=CFG.project,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    "    run_name=\"fine\",\n",
    "    parent_run_id=parent_id,\n",
    ") as child:\n",
    "    child.log_param(\"stage\", \"fine\")\n",
    "    child.log_param(\"parent_run_id\", parent_id)\n",
    "\n",
    "print(f\"Parent: {parent_id}\")\n",
    "print(f\"Child:  {child.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe24ca",
   "metadata": {},
   "source": [
    "## 6. Search (filter runs by params/metrics/tags)\n",
    "\n",
    "The registry supports simple query expressions to find runs of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2948dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: parameter-based search\n",
    "results = registry.search_runs(\"params.learning_rate = 0.1\")\n",
    "print(f\"params.learning_rate = 0.1 -> {len(results)} runs\")\n",
    "\n",
    "# Example 2: metric-based search (final_loss exists on sweep runs)\n",
    "results = registry.search_runs(\"metric.final_loss > 0\")\n",
    "print(f\"metric.final_loss > 0 -> {len(results)} runs\")\n",
    "\n",
    "# Example 3: sort by a metric\n",
    "results = registry.search_runs(\n",
    "    \"metric.final_loss > 0\",\n",
    "    sort_by=\"metric.final_loss\",\n",
    "    limit=5,\n",
    ")\n",
    "print(\"\\nTop 5 by final_loss:\")\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"  {r.run_id} | lr={r.params.get('learning_rate')} | final_loss={r.metrics.get('final_loss')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aaf3c3",
   "metadata": {},
   "source": [
    "## 7. Baselines, comparison, and verification\n",
    "\n",
    "devqubit supports two related workflows:\n",
    "\n",
    "- **Compare**: diff two runs and interpret result differences (e.g., shot-noise-aware TVD).\n",
    "- **Verify**: check a candidate run against the project's baseline using a policy (useful in CI).\n",
    "\n",
    "> In quantum workloads, distributions often vary run-to-run due to sampling noise.\n",
    "> devqubit uses statistics (e.g., expected noise) to help interpret TVD and avoid false alarms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3552e62",
   "metadata": {},
   "source": [
    "### A note on TVD (Total Variation Distance)\n",
    "\n",
    "When you compare two measured distributions (e.g., bitstring counts), a common distance is **TVD**:\n",
    "\n",
    "\\[\n",
    "\\mathrm{TVD}(p, q) = \\frac{1}{2} \\sum_x |p(x) - q(x)|\n",
    "\\]\n",
    "\n",
    "- **0.0** means identical distributions\n",
    "- small values are often consistent with **shot noise**\n",
    "- larger values may indicate a real change (code/config/device drift)\n",
    "\n",
    "devqubit uses this (plus an expected-noise model) to avoid false alarms in CI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae12cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set the baseline for this project.\"\"\"\n",
    "\n",
    "registry.set_baseline(CFG.project, baseline_id)\n",
    "baseline_info = registry.get_baseline(CFG.project)\n",
    "print(\"Baseline set:\")\n",
    "print(baseline_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ca85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a candidate run and compare it to baseline.\"\"\"\n",
    "\n",
    "with track(\n",
    "    project=CFG.project,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    "    run_name=\"candidate\",\n",
    ") as run:\n",
    "    run.log_params(\n",
    "        {\n",
    "            \"learning_rate\": 0.12,\n",
    "            \"max_iterations\": CFG.iterations,\n",
    "            \"seed\": CFG.seed,\n",
    "        }\n",
    "    )\n",
    "    run.set_tag(\"kind\", \"candidate\")\n",
    "\n",
    "    history = mock_optimization(\n",
    "        learning_rate=0.12,\n",
    "        iterations=CFG.iterations,\n",
    "        seed=CFG.seed,\n",
    "    )\n",
    "    run.log_metric(\"final_loss\", history[-1])\n",
    "    candidate_id = run.run_id\n",
    "\n",
    "print(f\"Candidate run_id: {candidate_id}\")\n",
    "\n",
    "# Compare baseline vs candidate\n",
    "comparison = diff(\n",
    "    baseline_id,\n",
    "    candidate_id,\n",
    "    registry=registry,\n",
    "    store=store,\n",
    ")\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Verify the candidate against the project's baseline.\"\"\"\n",
    "\n",
    "policy = VerifyPolicy(\n",
    "    params_must_match=False,  # candidate may differ in params\n",
    "    program_must_match=False,  # this toy example has no program artifact\n",
    "    tvd_max=CFG.tvd_max,\n",
    "    noise_factor=CFG.noise_factor,\n",
    ")\n",
    "\n",
    "verify_result = verify_against_baseline(\n",
    "    candidate=registry.load(candidate_id),\n",
    "    project=CFG.project,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    "    policy=policy,\n",
    ")\n",
    "\n",
    "print(verify_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9df94",
   "metadata": {},
   "source": [
    "## 8. Bundles: share runs portably\n",
    "\n",
    "Bundles are zip files containing:\n",
    "\n",
    "- the run record (params/metrics/tags/artifacts metadata)\n",
    "- all referenced objects (by digest)\n",
    "\n",
    "This makes it easy to share runs across machines or attach them to CI artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_path = CFG.workspace_dir / f\"{baseline_id[:8]}.devqubit.zip\"\n",
    "\n",
    "pack_result = pack_run(\n",
    "    run_id=baseline_id,\n",
    "    output_path=bundle_path,\n",
    "    store=store,\n",
    "    registry=registry,\n",
    ")\n",
    "\n",
    "print(f\"Bundle created: {bundle_path.name}\")\n",
    "print(f\"  Size:      {bundle_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"  Artifacts: {pack_result.artifact_count}\")\n",
    "print(f\"  Objects:   {pack_result.object_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c04898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inspect bundle contents without extracting.\"\"\"\n",
    "\n",
    "with Bundle(bundle_path) as b:\n",
    "    print(\"Bundle manifest:\")\n",
    "    print(json.dumps(b.manifest, indent=2)[:800] + \"\\n...\")\n",
    "\n",
    "    print(\"\\nFirst few objects:\")\n",
    "    for obj in b.list_objects()[:5]:\n",
    "        print(\" \", obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e078c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unpack the bundle into a new workspace and sanity-check it.\"\"\"\n",
    "\n",
    "import_dir = CFG.workspace_dir / \"imported\"\n",
    "import_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import_store = create_store(f\"file://{import_dir}/objects\")\n",
    "import_registry = create_registry(f\"file://{import_dir}\")\n",
    "\n",
    "unpack_result = unpack_bundle(\n",
    "    bundle_path=bundle_path,\n",
    "    dest_store=import_store,\n",
    "    dest_registry=import_registry,\n",
    ")\n",
    "\n",
    "print(f\"Unpacked run: {unpack_result.run_id}\")\n",
    "\n",
    "# Quick integrity checks\n",
    "imported = import_registry.load(unpack_result.run_id)\n",
    "original = registry.load(baseline_id)\n",
    "\n",
    "print(f\"Params match:  {imported.params == original.params}\")\n",
    "print(f\"Tags match:    {imported.tags == original.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dee5e8",
   "metadata": {},
   "source": [
    "## 9. Clean up (optional)\n",
    "\n",
    "Delete the local demo workspace created by this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this out if you want to keep the demo workspace around.\n",
    "shutil.rmtree(CFG.workspace_dir)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e44a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
