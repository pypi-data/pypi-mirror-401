{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-bestfit API Demo (LocalBackend)\n",
    "\n",
    "This notebook demonstrates the complete API for the `spark-bestfit` library using the **LocalBackend** - no Spark required!\n",
    "\n",
    "The LocalBackend uses Python's `ThreadPoolExecutor` for parallel distribution fitting, making it perfect for:\n",
    "- Development and testing without Spark\n",
    "- Small to medium datasets (< 1M rows)\n",
    "- Quick prototyping before scaling to Spark\n",
    "\n",
    "**Topics covered:**\n",
    "\n",
    "1. **Setup** - LocalBackend initialization and data preparation\n",
    "2. **Excluded Distributions** - Customizing which distributions to fit\n",
    "3. **Distribution Fitting** - Basic and advanced fitting options\n",
    "4. **FitterConfig Builder** - Fluent configuration API (v2.2.0+)\n",
    "5. **Progress Tracking** - Monitor long-running fits with callbacks\n",
    "6. **Working with Results** - FitResults and DistributionFitResult objects\n",
    "7. **Lazy Metrics** - Skip KS/AD computation for faster model selection (v1.5.0+)\n",
    "8. **Pre-filtering** - Skip incompatible distributions for faster fitting (v1.6.0+)\n",
    "9. **Plotting** - Visualization with Q-Q and P-P plots\n",
    "10. **Multi-Column Fitting** - Fit multiple columns in one operation\n",
    "11. **Serialization** - Save and load fitted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The LocalBackend works with **pandas DataFrames** and uses Python threads for parallelism.\n",
    "No Spark installation required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\n\n# Import spark-bestfit components\nfrom spark_bestfit import (\n    DistributionFitter,\n    LocalBackend,\n    FitterConfig,\n    FitterConfigBuilder,\n    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n)\n\n# Create LocalBackend - specify max_workers for thread parallelism\nbackend = LocalBackend(max_workers=4)\nprint(f\"LocalBackend created with {backend.max_workers} workers\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = pd.DataFrame({\"value\": normal_data})\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = pd.DataFrame({\"value\": exp_data})\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = pd.DataFrame({\"value\": gamma_data})\n",
    "\n",
    "print(f\"Normal data: {len(df_normal):,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {len(df_exp):,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {len(df_gamma):,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Excluded Distributions\n",
    "\n",
    "spark-bestfit excludes some slow distributions by default. You can customize this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DEFAULT_EXCLUDED_DISTRIBUTIONS\n",
    "\n",
    "Some distributions are excluded by default because they are very slow to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View default excluded distributions\n",
    "print(f\"Default excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\n",
    "for dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n",
    "    print(f\"  - {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a specific distribution that's excluded by default\n",
    "custom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n",
    "\n",
    "fitter_with_wald = DistributionFitter(backend=backend, excluded_distributions=custom_exclusions)\n",
    "print(f\"Now fitting 'wald' distribution (removed from exclusions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fitter with LocalBackend\n",
    "fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fitting with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only non-negative distributions using support_at_zero=True\n",
    "fitter_nonneg = DistributionFitter(backend=backend)\n",
    "\n",
    "print(\"Fitting non-negative distributions to exponential data...\")\n",
    "results_exp = fitter_nonneg.fit(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    bins=100,\n",
    "    support_at_zero=True,  # Only fit non-negative distributions\n",
    "    enable_sampling=True,\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_exp.count()} non-negative distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 FitterConfig Builder (v2.2.0+)\n",
    "\n",
    "For complex configurations with many parameters, use the **fluent builder pattern**. This provides:\n",
    "- **Cleaner code**: Grouped, readable configuration\n",
    "- **Reusability**: Same config works across multiple fits  \n",
    "- **IDE-friendly**: Better autocomplete and discoverability\n",
    "- **Immutable**: Frozen dataclass prevents accidental mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reusable configuration with the fluent builder\n",
    "config = (FitterConfigBuilder()\n",
    "    .with_bins(100)                           # Histogram bins\n",
    "    .with_support_at_zero()                   # Non-negative distributions only\n",
    "    .with_sampling(fraction=0.5)              # Sample 50% of data\n",
    "    .with_max_distributions(15)               # Limit to 15 distributions\n",
    "    .with_lazy_metrics()                      # Defer KS/AD computation\n",
    "    .build())\n",
    "\n",
    "print(\"FitterConfig created:\")\n",
    "print(f\"  bins: {config.bins}\")\n",
    "print(f\"  support_at_zero: {config.support_at_zero}\")\n",
    "print(f\"  sample_fraction: {config.sample_fraction}\")\n",
    "print(f\"  max_distributions: {config.max_distributions}\")\n",
    "print(f\"  lazy_metrics: {config.lazy_metrics}\")\n",
    "\n",
    "# Use config with fitter\n",
    "fitter_config = DistributionFitter(backend=backend)\n",
    "results_config = fitter_config.fit(df_exp, column=\"value\", config=config)\n",
    "\n",
    "print(f\"\\nFitted {results_config.count()} distributions using FitterConfig\")\n",
    "\n",
    "# Config is reusable - use same config for different data\n",
    "results_gamma_config = fitter_config.fit(df_gamma, column=\"value\", config=config)\n",
    "print(f\"Fitted {results_gamma_config.count()} distributions (reused same config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FitterConfig with bounded fitting and prefilter\n",
    "bounded_config = (FitterConfigBuilder()\n",
    "    .with_bounds(lower=0, upper=100)          # Explicit bounds\n",
    "    .with_prefilter()                          # Skip incompatible distributions\n",
    "    .with_lazy_metrics()                       # Fast model selection\n",
    "    .with_max_distributions(20)\n",
    "    .build())\n",
    "\n",
    "print(\"Bounded FitterConfig:\")\n",
    "print(f\"  bounded: {bounded_config.bounded}\")\n",
    "print(f\"  lower_bound: {bounded_config.lower_bound}\")\n",
    "print(f\"  upper_bound: {bounded_config.upper_bound}\")\n",
    "print(f\"  prefilter: {bounded_config.prefilter}\")\n",
    "\n",
    "# You can also create FitterConfig directly (without builder)\n",
    "direct_config = FitterConfig(\n",
    "    bins=50,\n",
    "    lazy_metrics=True,\n",
    "    max_distributions=10,\n",
    ")\n",
    "print(f\"\\nDirect FitterConfig: bins={direct_config.bins}, lazy={direct_config.lazy_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Progress Tracking\n",
    "\n",
    "For long-running fits, you can monitor progress with a callback. The easiest way is to use the built-in `console_progress()` utility:\n",
    "\n",
    "```python\n",
    "from spark_bestfit import console_progress\n",
    "\n",
    "results = fitter.fit(df, column=\"value\", progress_callback=console_progress())\n",
    "```\n",
    "\n",
    "For custom callbacks, pass any function matching `(completed: int, total: int, percent: float) -> None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit import console_progress\n",
    "\n",
    "# Simple approach: use built-in console_progress()\n",
    "print(\"Fitting with console_progress()...\")\n",
    "fitter_progress = DistributionFitter(backend=backend)\n",
    "results_progress = fitter_progress.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=25,\n",
    "    progress_callback=console_progress(\"Fitting\"),  # Built-in utility\n",
    ")\n",
    "print()  # Newline after progress\n",
    "print(f\"Fitted {results_progress.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best distribution (by K-S statistic, the default)\n",
    "best = results_normal.best(n=1)[0]\n",
    "print(f\"Best by K-S statistic: {best.distribution}\")\n",
    "print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "print(f\"  A-D statistic: {best.ad_statistic:.6f}\")\n",
    "print(f\"  A-D p-value: {best.ad_pvalue:.4f}\" if best.ad_pvalue else f\"  A-D p-value: N/A (not available for {best.distribution})\")\n",
    "print(f\"  SSE: {best.sse:.6f}\")\n",
    "print(f\"  AIC: {best.aic:.2f}\")\n",
    "print(f\"  BIC: {best.bic:.2f}\")\n",
    "print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 by different metrics\n",
    "print(\"Top 5 by K-S statistic (default):\")\n",
    "for i, r in enumerate(results_normal.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 by A-D statistic:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"ad_statistic\"), 1):\n",
    "    ad_p = f\"{r.ad_pvalue:.4f}\" if r.ad_pvalue else \"N/A\"\n",
    "    print(f\"  {i}. {r.distribution:20s} AD={r.ad_statistic:.6f} p={ad_p}\")\n",
    "\n",
    "print(\"\\nTop 5 by SSE:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 by AIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by K-S statistic threshold\n",
    "good_fits = results_normal.filter(ks_threshold=0.05)\n",
    "print(f\"Distributions with K-S statistic < 0.05: {good_fits.count()}\")\n",
    "\n",
    "for r in good_fits.best(n=10):\n",
    "    print(f\"  {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "# Filter by p-value threshold (keep distributions with p-value > 0.05)\n",
    "significant = results_normal.filter(pvalue_threshold=0.05)\n",
    "print(f\"\\nDistributions with p-value > 0.05: {significant.count()}\")\n",
    "\n",
    "# Filter by A-D statistic threshold\n",
    "good_ad = results_normal.filter(ad_threshold=2.0)\n",
    "print(f\"\\nDistributions with A-D statistic < 2.0: {good_ad.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting to Pandas\n",
    "\n",
    "With LocalBackend, results are already in pandas format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame is already pandas with LocalBackend\n",
    "df_results = results_normal.df\n",
    "print(\"Results as pandas DataFrame:\")\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DistributionFitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Parameter Confidence Intervals\n",
    "\n",
    "Compute bootstrap confidence intervals for fitted distribution parameters. This is useful for understanding the uncertainty in your parameter estimates.\n",
    "\n",
    "**Note**: CI width depends on sample size and distribution identifiability. Highly flexible distributions (like beta with 4 parameters) may have wider CIs due to parameter identifiability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exponential fit for CI demo (simpler distribution = more stable CIs)\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "\n",
    "print(f\"Distribution: {best_exp.distribution}\")\n",
    "print(f\"Parameter names: {best_exp.get_param_names()}\")\n",
    "print(f\"Fitted values: {[f'{p:.4f}' for p in best_exp.parameters]}\")\n",
    "\n",
    "# Compute 95% bootstrap confidence intervals\n",
    "print(\"\\nComputing 95% confidence intervals (this may take a few seconds)...\")\n",
    "ci = best_exp.confidence_intervals(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    alpha=0.05,           # 95% CI\n",
    "    n_bootstrap=500,      # Number of bootstrap samples (use 1000+ for production)\n",
    "    random_seed=42,       # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nParameter confidence intervals:\")\n",
    "for param, (lower, upper) in ci.items():\n",
    "    print(f\"  {param}: [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Lazy Metrics for Fast Model Selection (v1.5.0+)\n",
    "\n",
    "When fitting ~100 distributions, computing KS and AD statistics for all of them can be slow. With **lazy metrics**, these expensive computations are skipped during fitting and only computed on-demand when you actually need them.\n",
    "\n",
    "**Key benefits:**\n",
    "- Fast initial fitting (skip KS/AD computation)\n",
    "- On-demand computation only for distributions you access\n",
    "- Ideal for model selection workflows using AIC/BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with lazy metrics - KS/AD statistics are NOT computed during fitting\n",
    "print(\"Fitting with lazy_metrics=True (fast)...\")\n",
    "fitter_lazy = DistributionFitter(backend=backend)\n",
    "results_lazy = fitter_lazy.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=20,\n",
    "    lazy_metrics=True,  # Skip KS/AD computation!\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_lazy.count()} distributions\")\n",
    "print(f\"Is lazy: {results_lazy.is_lazy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by AIC - fast! No KS/AD computation needed\n",
    "best_aic = results_lazy.best(n=1, metric=\"aic\")[0]\n",
    "\n",
    "print(f\"Best by AIC: {best_aic.distribution}\")\n",
    "print(f\"  AIC: {best_aic.aic:.2f}\")\n",
    "print(f\"  BIC: {best_aic.bic:.2f}\")\n",
    "print(f\"  KS statistic: {best_aic.ks_statistic}\")  # None - not computed yet!\n",
    "print(f\"  AD statistic: {best_aic.ad_statistic}\")  # None - not computed yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by KS statistic - triggers ON-DEMAND computation!\n",
    "# Only computes KS/AD for top candidates, not all distributions\n",
    "best_ks = results_lazy.best(n=1, metric=\"ks_statistic\")[0]\n",
    "\n",
    "print(f\"Best by KS: {best_ks.distribution}\")\n",
    "print(f\"  KS statistic: {best_ks.ks_statistic:.6f}\")  # Computed on-demand!\n",
    "print(f\"  p-value: {best_ks.pvalue:.4f}\")\n",
    "print(f\"  AD statistic: {best_ks.ad_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need all metrics computed (e.g., before unpersisting source data),\n",
    "# use materialize() to force computation of all KS/AD statistics\n",
    "materialized = results_lazy.materialize()\n",
    "\n",
    "print(f\"Is lazy after materialize: {materialized.is_lazy}\")  # False\n",
    "\n",
    "# Now all distributions have KS/AD computed\n",
    "top_3 = materialized.best(n=3, metric=\"ks_statistic\")\n",
    "print(\"\\nTop 3 distributions (all metrics available):\")\n",
    "for i, r in enumerate(top_3, 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Pre-filtering Distributions (v1.6.0+)\n",
    "\n",
    "When you know something about your data, you can skip distributions that are mathematically incompatible. Pre-filtering uses data characteristics (support bounds, skewness, kurtosis) to eliminate distributions before the expensive fitting step.\n",
    "\n",
    "**Filtering layers:**\n",
    "1. **Support bounds (100% reliable)**: Skips distributions whose support doesn't contain your data range\n",
    "2. **Skewness sign (95% reliable)**: Skips positive-skew-only distributions for left-skewed data  \n",
    "3. **Kurtosis (aggressive mode, ~80% reliable)**: Skips low-kurtosis distributions for heavy-tailed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative data (will filter out non-negative distributions like expon, gamma)\n",
    "np.random.seed(42)\n",
    "negative_data = np.random.normal(loc=-50, scale=10, size=10_000)\n",
    "df_negative = pd.DataFrame({\"value\": negative_data})\n",
    "\n",
    "print(f\"Data range: [{negative_data.min():.1f}, {negative_data.max():.1f}]\")\n",
    "print(f\"All values are negative - expon/gamma distributions cannot fit this data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITHOUT prefilter (baseline)\n",
    "print(\"Fitting WITHOUT prefilter...\")\n",
    "fitter_no_prefilter = DistributionFitter(backend=backend)\n",
    "results_no_prefilter = fitter_no_prefilter.fit(\n",
    "    df_negative, \n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=False,  # Default - fit all distributions\n",
    ")\n",
    "print(f\"Fitted {results_no_prefilter.count()} distributions (no prefilter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITH prefilter (safe mode) - skips incompatible distributions\n",
    "print(\"\\nFitting WITH prefilter=True (safe mode)...\")\n",
    "fitter_prefilter = DistributionFitter(backend=backend)\n",
    "results_prefilter = fitter_prefilter.fit(\n",
    "    df_negative,\n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=True,  # Enable pre-filtering!\n",
    ")\n",
    "print(f\"Fitted {results_prefilter.count()} distributions (with prefilter)\")\n",
    "print(\"\\n-> Pre-filter skipped distributions incompatible with negative data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best fits - both should find similar results for normal data\n",
    "best_no_prefilter = results_no_prefilter.best(n=1)[0]\n",
    "best_prefilter = results_prefilter.best(n=1)[0]\n",
    "\n",
    "print(\"Best distribution comparison:\")\n",
    "print(f\"  Without prefilter: {best_no_prefilter.distribution} (KS={best_no_prefilter.ks_statistic:.6f})\")\n",
    "print(f\"  With prefilter:    {best_prefilter.distribution} (KS={best_prefilter.ks_statistic:.6f})\")\n",
    "print(\"\\n-> Same best fit, but prefilter was faster by skipping incompatible distributions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Plot with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plot with direct parameters\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=12,\n",
    "    grid_alpha=0.4,\n",
    "    title=\"Distribution Fit with Custom Styling\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best fit for exponential data\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "print(f\"Best fit for exponential data: {best_exp.distribution}\")\n",
    "print(f\"  K-S statistic: {best_exp.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best_exp.pvalue:.4f}\")\n",
    "\n",
    "fig, ax = fitter_nonneg.plot(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Q-Q Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A Q-Q (quantile-quantile) plot is a powerful visual tool for assessing how well a distribution fits your data. It plots sample quantiles against theoretical quantiles from the fitted distribution. If the fit is good, points will fall approximately along the diagonal reference line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_qq(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: Q-Q plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_qq(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 P-P Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A P-P (probability-probability) plot compares the empirical cumulative distribution function (CDF) of the sample data against the theoretical CDF of the fitted distribution. It is particularly useful for assessing the fit in the center of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-P plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_pp(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: P-P plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_pp(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Multi-Column Fitting\n",
    "\n",
    "Fit multiple columns efficiently in a single operation. This shares overhead (sampling, broadcasting) across all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create Multi-Column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with multiple columns (different distributions)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_rows = 20_000\n",
    "df_multi = pd.DataFrame({\n",
    "    \"normal_col\": np.random.normal(50, 10, n_rows),\n",
    "    \"exp_col\": np.random.exponential(5, n_rows),\n",
    "    \"gamma_col\": np.random.gamma(2.0, 2.0, n_rows),\n",
    "})\n",
    "\n",
    "print(f\"Created DataFrame with {len(df_multi):,} rows and columns: {list(df_multi.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fit Multiple Columns in One Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions to all columns in a single operation\n",
    "# This is more efficient than fitting each column separately\n",
    "print(\"Fitting distributions to 3 columns simultaneously...\")\n",
    "\n",
    "fitter_multi = DistributionFitter(backend=backend)\n",
    "results_multi = fitter_multi.fit(\n",
    "    df_multi,\n",
    "    columns=[\"normal_col\", \"exp_col\", \"gamma_col\"],  # Multi-column fitting!\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal results: {results_multi.count()}\")\n",
    "print(f\"Columns fitted: {results_multi.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Get Best Distribution Per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best distribution for each column\n",
    "best_per_col = results_multi.best_per_column(n=1)\n",
    "\n",
    "print(\"Best distribution per column:\")\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    print(f\"  Distribution: {best.distribution}\")\n",
    "    print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "    print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "    print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Filter Results by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for a specific column\n",
    "exp_results = results_multi.for_column(\"exp_col\")\n",
    "\n",
    "print(f\"Results for 'exp_col': {exp_results.count()} distributions\")\n",
    "print(\"\\nTop 5 by K-S statistic:\")\n",
    "for i, r in enumerate(exp_results.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Plot Results for Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit for each column\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    fig, ax = fitter_multi.plot(\n",
    "        best,\n",
    "        df_multi,\n",
    "        col_name,\n",
    "        title=f\"{col_name}: {best.distribution} (KS={best.ks_statistic:.4f})\",\n",
    "        xlabel=\"Value\",\n",
    "        ylabel=\"Density\",\n",
    "        figsize=(10, 6),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Serialization\n",
    "\n",
    "Save fitted distributions to disk and reload them later for inference without re-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Save and Load\n",
    "\n",
    "Save a fitted distribution to JSON (default) or pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the best fit from normal data\n",
    "best = results_normal.best(n=1)[0]\n",
    "print(f\"Saving distribution: {best.distribution}\")\n",
    "print(f\"Parameters: {best.parameters}\")\n",
    "\n",
    "# Save to a temporary directory\n",
    "model_dir = Path(tempfile.mkdtemp())\n",
    "json_path = model_dir / \"best_model.json\"\n",
    "pkl_path = model_dir / \"best_model.pkl\"\n",
    "\n",
    "# Save as JSON (human-readable, default)\n",
    "best.save(json_path)\n",
    "print(f\"\\nSaved to JSON: {json_path}\")\n",
    "print(f\"File size: {json_path.stat().st_size:,} bytes\")\n",
    "\n",
    "# Save as pickle (binary, faster)\n",
    "best.save(pkl_path, format=\"pickle\")\n",
    "print(f\"\\nSaved to pickle: {pkl_path}\")\n",
    "print(f\"File size: {pkl_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit import DistributionFitResult\n",
    "\n",
    "# Load the saved model\n",
    "loaded = DistributionFitResult.load(json_path)\n",
    "\n",
    "print(f\"Loaded distribution: {loaded.distribution}\")\n",
    "print(f\"Parameters: {loaded.parameters}\")\n",
    "print(f\"K-S statistic: {loaded.ks_statistic:.6f}\")\n",
    "print(f\"p-value: {loaded.pvalue:.4f}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "samples = loaded.sample(size=1000, random_state=42)\n",
    "print(f\"\\nGenerated {len(samples)} samples from loaded model\")\n",
    "print(f\"Sample mean: {samples.mean():.2f}\")\n",
    "print(f\"Sample std: {samples.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Data Summary\n",
    "\n",
    "When fitting with `DistributionFitter`, the result includes data statistics\n",
    "about the fitted data. This provides lightweight provenance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data statistics from the loaded model (v2.0+ flat field API)\n",
    "if loaded.data_count is not None:\n",
    "    print(\"Data Statistics (from fitting):\")\n",
    "    print(f\"  Sample size: {loaded.data_count:,.0f}\")\n",
    "    print(f\"  Min: {loaded.data_min:.4f}\")\n",
    "    print(f\"  Max: {loaded.data_max:.4f}\")\n",
    "    print(f\"  Mean: {loaded.data_mean:.4f}\")\n",
    "    print(f\"  Std: {loaded.data_stddev:.4f}\")\n",
    "else:\n",
    "    print(\"No data statistics available (result may have been created manually)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 JSON Format\n",
    "\n",
    "The JSON format is human-readable and includes version metadata for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the JSON content\n",
    "with open(json_path) as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"JSON file content:\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(model_dir)\n",
    "print(f\"Cleaned up temporary directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete spark-bestfit API using the **LocalBackend**:\n",
    "\n",
    "### LocalBackend Benefits\n",
    "- **No Spark required** - Works with pure Python and pandas\n",
    "- **Thread-based parallelism** - Uses `ThreadPoolExecutor` for parallel fitting\n",
    "- **Same API** - Identical to SparkBackend for easy migration\n",
    "- **Fast for small data** - Lower overhead than Spark for < 1M rows\n",
    "\n",
    "### Key Features Covered\n",
    "\n",
    "1. **Setup**: `LocalBackend(max_workers=4)` for thread-based parallelism\n",
    "\n",
    "2. **Excluded Distributions**: `DEFAULT_EXCLUDED_DISTRIBUTIONS` and custom exclusions\n",
    "\n",
    "3. **Fitting**: `DistributionFitter(backend=backend).fit(df, column=\"value\", ...)`\n",
    "\n",
    "4. **FitterConfig Builder (v2.2.0+)**: Fluent API for complex configurations\n",
    "\n",
    "5. **Progress Tracking**: `progress_callback=console_progress(\"Fitting\")`\n",
    "\n",
    "6. **Results**:\n",
    "   - `results.best(n, metric)` - Get top N by K-S, A-D, SSE, AIC, or BIC\n",
    "   - `results.filter(ks_threshold, pvalue_threshold, ad_threshold)`\n",
    "   - `results.df` - Already a pandas DataFrame with LocalBackend!\n",
    "\n",
    "7. **Using Fitted Distributions**:\n",
    "   - `result.sample(size, random_state)` - Generate random samples\n",
    "   - `result.pdf(x)`, `result.cdf(x)` - Evaluate density/CDF\n",
    "   - `result.confidence_intervals(df, column, ...)` - Bootstrap CIs\n",
    "\n",
    "8. **Lazy Metrics (v1.5.0+)**: `lazy_metrics=True` for fast model selection\n",
    "\n",
    "9. **Pre-filtering (v1.6.0+)**: `prefilter=True` to skip incompatible distributions\n",
    "\n",
    "10. **Plotting**:\n",
    "    - `fitter.plot()` - Histogram + PDF overlay\n",
    "    - `fitter.plot_qq()` - Q-Q plot for goodness-of-fit\n",
    "    - `fitter.plot_pp()` - P-P plot for fit assessment\n",
    "\n",
    "11. **Multi-Column Fitting**:\n",
    "    - `fitter.fit(df, columns=[...])`\n",
    "    - `results.best_per_column(n)`\n",
    "    - `results.for_column(name)`\n",
    "\n",
    "12. **Serialization**:\n",
    "    - `result.save(path)` - JSON or pickle format\n",
    "    - `DistributionFitResult.load(path)` - Reload saved models\n",
    "\n",
    "### When to Use LocalBackend vs SparkBackend\n",
    "\n",
    "| Use Case | Recommended Backend |\n",
    "|----------|--------------------|\n",
    "| Development/testing | LocalBackend |\n",
    "| < 1M rows | LocalBackend |\n",
    "| > 10M rows | SparkBackend |\n",
    "| Production cluster | SparkBackend |\n",
    "| Quick prototyping | LocalBackend |\n",
    "| No Spark available | LocalBackend |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
