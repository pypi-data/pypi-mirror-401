{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Insurance Claims Modeling with spark-bestfit (LocalBackend)\n",
    "\n",
    "This notebook demonstrates **actuarial modeling** of insurance claims using\n",
    "statistical distribution fitting.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Model claim frequency** with discrete distributions (Poisson, Negative Binomial)\n",
    "2. **Model claim severity** with heavy-tailed distributions (Pareto, Lognormal)\n",
    "3. **Handle policy limits** with bounded distribution fitting\n",
    "4. **Validate tail behavior** using Q-Q plots\n",
    "5. **Calculate aggregate loss** distributions\n",
    "\n",
    "## Actuarial Context\n",
    "\n",
    "Insurance pricing and reserving require understanding two key distributions:\n",
    "\n",
    "- **Frequency**: How many claims per policy/period? (discrete: Poisson, NB)\n",
    "- **Severity**: How large is each claim? (continuous: Pareto, lognormal)\n",
    "\n",
    "The **aggregate loss** is the sum of all claim amounts:\n",
    "$$S = X_1 + X_2 + ... + X_N$$\n",
    "\n",
    "where $N$ is the random claim count and $X_i$ are individual claim amounts.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install spark-bestfit pandas numpy matplotlib scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from spark_bestfit import DistributionFitter, DiscreteDistributionFitter\n",
    "from spark_bestfit.backends.local import LocalBackend\n",
    "\n",
    "# Create LocalBackend\n",
    "backend = LocalBackend()\n",
    "print(f\"LocalBackend initialized with {backend.max_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Simulate Insurance Claims Data\n",
    "\n",
    "We'll simulate a property insurance portfolio:\n",
    "\n",
    "- **10,000 policies** over one year\n",
    "- **Claim frequency**: Negative Binomial (overdispersed Poisson)\n",
    "- **Claim severity**: Pareto distribution (heavy-tailed for large losses)\n",
    "- **Policy limit**: $500,000 maximum per claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Portfolio parameters\n",
    "n_policies = 10000\n",
    "policy_limit = 500000  # $500K per-claim limit\n",
    "\n",
    "# Claim frequency: Negative Binomial (accounts for heterogeneity)\n",
    "# Mean ~0.15 claims/policy, with overdispersion\n",
    "freq_n = 3        # Number of successes parameter\n",
    "freq_p = 0.95     # Probability parameter\n",
    "# This gives mean = n(1-p)/p = 3*0.05/0.95 ~ 0.158\n",
    "\n",
    "claim_counts = np.random.negative_binomial(freq_n, freq_p, n_policies)\n",
    "\n",
    "print(f\"Claim Frequency Statistics:\")\n",
    "print(f\"  Total policies: {n_policies:,}\")\n",
    "print(f\"  Policies with claims: {(claim_counts > 0).sum():,} ({(claim_counts > 0).mean():.1%})\")\n",
    "print(f\"  Total claims: {claim_counts.sum():,}\")\n",
    "print(f\"  Mean claims/policy: {claim_counts.mean():.3f}\")\n",
    "print(f\"  Variance: {claim_counts.var():.3f} (overdispersion ratio: {claim_counts.var()/claim_counts.mean():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate individual claim amounts (Pareto distribution)\n",
    "# Pareto is ideal for insurance: P(X > x) = (x_m/x)^alpha\n",
    "# alpha (shape) < 2 means infinite variance (very heavy tail)\n",
    "# alpha between 2-3 is common for property insurance\n",
    "\n",
    "total_claims = claim_counts.sum()\n",
    "pareto_alpha = 2.5  # Shape parameter\n",
    "pareto_scale = 5000  # Minimum claim amount ($5K deductible met)\n",
    "\n",
    "# Generate raw Pareto claims\n",
    "raw_claims = (np.random.pareto(pareto_alpha, total_claims) + 1) * pareto_scale\n",
    "\n",
    "# Apply policy limit (censoring)\n",
    "claim_amounts = np.minimum(raw_claims, policy_limit)\n",
    "n_capped = (raw_claims > policy_limit).sum()\n",
    "\n",
    "print(f\"\\nClaim Severity Statistics:\")\n",
    "print(f\"  Total claims: {total_claims:,}\")\n",
    "print(f\"  Claims hitting limit: {n_capped:,} ({n_capped/total_claims:.1%})\")\n",
    "print(f\"  Mean claim: ${claim_amounts.mean():,.0f}\")\n",
    "print(f\"  Median claim: ${np.median(claim_amounts):,.0f}\")\n",
    "print(f\"  Max claim: ${claim_amounts.max():,.0f}\")\n",
    "print(f\"  Total losses: ${claim_amounts.sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy-level DataFrame\n",
    "policy_data = []\n",
    "claim_idx = 0\n",
    "\n",
    "for policy_id in range(n_policies):\n",
    "    n_claims = claim_counts[policy_id]\n",
    "    if n_claims > 0:\n",
    "        policy_claims = claim_amounts[claim_idx:claim_idx + n_claims]\n",
    "        total_loss = policy_claims.sum()\n",
    "        max_claim = policy_claims.max()\n",
    "        claim_idx += n_claims\n",
    "    else:\n",
    "        total_loss = 0\n",
    "        max_claim = 0\n",
    "    \n",
    "    policy_data.append({\n",
    "        'policy_id': policy_id,\n",
    "        'num_claims': n_claims,\n",
    "        'total_loss': total_loss,\n",
    "        'max_claim': max_claim\n",
    "    })\n",
    "\n",
    "policy_df = pd.DataFrame(policy_data)\n",
    "\n",
    "# Create individual claims DataFrame\n",
    "claims_df = pd.DataFrame({'claim_amount': claim_amounts})\n",
    "\n",
    "print(\"Policy DataFrame:\")\n",
    "print(policy_df.head())\n",
    "\n",
    "print(\"\\nClaims DataFrame:\")\n",
    "print(claims_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Claim frequency distribution\n",
    "max_claims = min(10, claim_counts.max())\n",
    "axes[0].hist(claim_counts, bins=range(max_claims + 2), density=True, alpha=0.7, \n",
    "             color='steelblue', edgecolor='black', align='left')\n",
    "axes[0].set_xlabel('Number of Claims')\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_title(f'Claim Frequency\\n(Mean: {claim_counts.mean():.3f})')\n",
    "axes[0].set_xticks(range(max_claims + 1))\n",
    "\n",
    "# Claim severity distribution (log scale)\n",
    "axes[1].hist(claim_amounts, bins=50, density=True, alpha=0.7, \n",
    "             color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Claim Amount ($)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title(f'Claim Severity\\n(Mean: ${claim_amounts.mean():,.0f})')\n",
    "axes[1].axvline(policy_limit, color='red', linestyle='--', label=f'Policy Limit: ${policy_limit:,}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Log-log plot (Pareto tail)\n",
    "sorted_claims = np.sort(claim_amounts)[::-1]\n",
    "ranks = np.arange(1, len(sorted_claims) + 1)\n",
    "survival_prob = ranks / len(sorted_claims)\n",
    "\n",
    "axes[2].loglog(sorted_claims, survival_prob, 'o', markersize=2, alpha=0.5)\n",
    "axes[2].set_xlabel('Claim Amount ($)')\n",
    "axes[2].set_ylabel('P(X > x)')\n",
    "axes[2].set_title('Survival Function (Log-Log)\\nLinear = Pareto Tail')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 2: Fit Claim Frequency Distribution\n",
    "\n",
    "For claim counts, we'll compare:\n",
    "\n",
    "- **Poisson**: Assumes variance = mean (rarely true in practice)\n",
    "- **Negative Binomial**: Allows overdispersion (variance > mean)\n",
    "- **Geometric**: Special case of NB with n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit discrete distributions to claim counts\n",
    "disc_fitter = DiscreteDistributionFitter(backend=backend)\n",
    "\n",
    "freq_results = disc_fitter.fit(\n",
    "    policy_df,\n",
    "    column='num_claims'\n",
    ")\n",
    "\n",
    "print(f\"Fitted {freq_results.count()} frequency distributions\")\n",
    "\n",
    "# Show top candidates\n",
    "print(\"\\nBest Frequency Distributions (by AIC):\")\n",
    "for fit in freq_results.best(n=5, metric='aic'):\n",
    "    print(f\"  {fit.distribution}: AIC={fit.aic:.1f}, BIC={fit.bic:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best frequency fit (explicitly use AIC metric)\n",
    "best_freq = freq_results.best(n=1, metric='aic')[0]\n",
    "\n",
    "print(f\"Best Frequency Model: {best_freq.distribution}\")\n",
    "print(f\"  Parameters: {best_freq.parameters}\")\n",
    "print(f\"  AIC: {best_freq.aic:.2f}\")\n",
    "\n",
    "# Compare with Poisson (often used but may not fit well)\n",
    "poisson_fit = None\n",
    "for fit in freq_results.best(n=20, metric='aic'):\n",
    "    if fit.distribution == 'poisson':\n",
    "        poisson_fit = fit\n",
    "        break\n",
    "\n",
    "if poisson_fit:\n",
    "    print(f\"\\nPoisson Model (for comparison):\")\n",
    "    print(f\"  Parameters: {poisson_fit.parameters}\")\n",
    "    print(f\"  AIC: {poisson_fit.aic:.2f}\")\n",
    "    print(f\"  AIC Difference: {poisson_fit.aic - best_freq.aic:.1f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frequency fit\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Observed frequencies\n",
    "max_k = min(8, claim_counts.max())\n",
    "observed_freq = np.array([np.sum(claim_counts == k) for k in range(max_k + 1)]) / n_policies\n",
    "\n",
    "x = np.arange(max_k + 1)\n",
    "width = 0.35\n",
    "\n",
    "# Plot observed\n",
    "bars1 = ax.bar(x - width/2, observed_freq, width, label='Observed', color='steelblue', edgecolor='black')\n",
    "\n",
    "# Plot fitted (best model) - use get_scipy_dist() for frozen distribution\n",
    "freq_dist = best_freq.get_scipy_dist()\n",
    "fitted_freq = freq_dist.pmf(x)\n",
    "bars2 = ax.bar(x + width/2, fitted_freq, width, label=f'Fitted ({best_freq.distribution})', \n",
    "               color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Number of Claims')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Claim Frequency: Observed vs Fitted')\n",
    "ax.set_xticks(x)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 3: Fit Claim Severity Distribution\n",
    "\n",
    "For claim amounts, heavy-tailed distributions are essential:\n",
    "\n",
    "- **Pareto**: Power-law tail, common for catastrophic losses\n",
    "- **Lognormal**: Multiplicative processes, moderate tail\n",
    "- **Gamma/Weibull**: Lighter tails, good for attritional losses\n",
    "\n",
    "The choice matters enormously for extreme quantiles (99th percentile)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit severity distributions\n",
    "cont_fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "# Fit with limited distributions for speed\n",
    "# Includes heavy-tailed options: pareto, lognorm, burr, gamma, weibull, etc.\n",
    "severity_results = cont_fitter.fit(\n",
    "    claims_df,\n",
    "    column='claim_amount',\n",
    "    max_distributions=20,  # Includes heavy-tailed distributions\n",
    "    lazy_metrics=True\n",
    ")\n",
    "\n",
    "print(f\"Fitted {severity_results.count()} severity distributions\")\n",
    "\n",
    "# Show rankings\n",
    "print(\"\\nBest Severity Distributions (by AIC):\")\n",
    "for fit in severity_results.best(n=5, metric='aic'):\n",
    "    print(f\"  {fit.distribution}: AIC={fit.aic:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best severity fit (explicitly use AIC metric)\n",
    "best_severity = severity_results.best(n=1, metric='aic')[0]\n",
    "\n",
    "print(f\"Best Severity Model: {best_severity.distribution}\")\n",
    "print(f\"  Parameters: {best_severity.parameters}\")\n",
    "print(f\"  AIC: {best_severity.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize severity fit\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PDF comparison\n",
    "x = np.linspace(5000, 200000, 500)\n",
    "\n",
    "axes[0].hist(claim_amounts[claim_amounts < 200000], bins=50, density=True, \n",
    "             alpha=0.6, color='coral', edgecolor='black', label='Observed')\n",
    "\n",
    "# Plot fitted PDF using get_scipy_dist()\n",
    "severity_dist = best_severity.get_scipy_dist()\n",
    "fitted_pdf = severity_dist.pdf(x)\n",
    "axes[0].plot(x, fitted_pdf, 'b-', lw=2, label=f'{best_severity.distribution} fit')\n",
    "\n",
    "axes[0].set_xlabel('Claim Amount ($)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Claim Severity Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Q-Q Plot for tail validation\n",
    "theoretical_quantiles = severity_dist.ppf(np.linspace(0.01, 0.99, 100))\n",
    "empirical_quantiles = np.percentile(claim_amounts, np.linspace(1, 99, 100))\n",
    "\n",
    "axes[1].scatter(theoretical_quantiles, empirical_quantiles, alpha=0.6, s=20)\n",
    "max_val = max(theoretical_quantiles.max(), empirical_quantiles.max())\n",
    "axes[1].plot([0, max_val], [0, max_val], 'r--', label='Perfect fit')\n",
    "axes[1].set_xlabel(f'Theoretical Quantiles ({best_severity.distribution})')\n",
    "axes[1].set_ylabel('Empirical Quantiles')\n",
    "axes[1].set_title('Q-Q Plot: Tail Behavior Validation')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 4: Bounded Fitting for Capped Claims\n",
    "\n",
    "Our data has claims capped at the policy limit ($500K). For reserving purposes,\n",
    "we might want to model the **unlimited** severity (what claims would have been\n",
    "without the cap).\n",
    "\n",
    "We'll fit to uncapped claims only, then extrapolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to uncapped claims for \"ground-up\" severity fitting\n",
    "uncapped_claims = claim_amounts[claim_amounts < policy_limit]\n",
    "uncapped_df = pd.DataFrame({'claim_amount': uncapped_claims})\n",
    "\n",
    "print(f\"Uncapped claims: {len(uncapped_claims):,} ({len(uncapped_claims)/len(claim_amounts):.1%} of total)\")\n",
    "\n",
    "# Fit to uncapped data with heavy-tailed distributions\n",
    "uncapped_results = cont_fitter.fit(\n",
    "    uncapped_df,\n",
    "    column='claim_amount',\n",
    "    max_distributions=15,  # Includes pareto, lognorm, gamma, burr, etc.\n",
    "    lazy_metrics=True\n",
    ")\n",
    "\n",
    "best_uncapped = uncapped_results.best(n=1, metric='aic')[0]\n",
    "print(f\"\\nBest Uncapped Model: {best_uncapped.distribution}\")\n",
    "print(f\"  Parameters: {best_uncapped.parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare expected unlimited loss vs limited loss\n",
    "# Use get_scipy_dist() to get the frozen distribution\n",
    "uncapped_dist = best_uncapped.get_scipy_dist()\n",
    "\n",
    "# Expected unlimited claim (from fitted distribution)\n",
    "expected_unlimited = uncapped_dist.mean()\n",
    "\n",
    "# Expected limited claim (truncated at policy limit)\n",
    "# E[min(X, L)] = integral from 0 to L of S(x) dx where S(x) = 1 - F(x)\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def survival(x):\n",
    "    return 1 - uncapped_dist.cdf(x)\n",
    "\n",
    "expected_limited, _ = quad(survival, 0, policy_limit)\n",
    "\n",
    "print(f\"Expected Claim Amounts:\")\n",
    "print(f\"  Unlimited (model): ${expected_unlimited:,.0f}\")\n",
    "print(f\"  Limited at ${policy_limit:,}: ${expected_limited:,.0f}\")\n",
    "print(f\"  Observed mean: ${claim_amounts.mean():,.0f}\")\n",
    "print(f\"\\n  Loss due to limit: ${expected_unlimited - expected_limited:,.0f} per claim ({(expected_unlimited - expected_limited)/expected_unlimited:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Part 5: Aggregate Loss Distribution\n",
    "\n",
    "The aggregate loss $S = \\sum_{i=1}^N X_i$ combines frequency and severity.\n",
    "We'll use Monte Carlo simulation with our fitted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_aggregate_loss(freq_fit, sev_fit, n_policies, policy_limit, n_simulations=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate aggregate loss distribution using fitted frequency and severity.\n",
    "    \n",
    "    Returns array of simulated annual aggregate losses.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get frozen scipy distributions using get_scipy_dist()\n",
    "    freq_dist = freq_fit.get_scipy_dist()\n",
    "    sev_dist = sev_fit.get_scipy_dist()\n",
    "    \n",
    "    aggregate_losses = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Simulate claim counts for all policies\n",
    "        counts = freq_dist.rvs(size=n_policies)\n",
    "        # Ensure non-negative integer counts\n",
    "        counts = np.maximum(0, np.round(counts)).astype(int)\n",
    "        total_claims = counts.sum()\n",
    "        \n",
    "        if total_claims > 0:\n",
    "            # Simulate claim amounts\n",
    "            amounts = sev_dist.rvs(size=total_claims)\n",
    "            # Apply policy limit\n",
    "            amounts = np.minimum(amounts, policy_limit)\n",
    "            aggregate_loss = amounts.sum()\n",
    "        else:\n",
    "            aggregate_loss = 0\n",
    "        \n",
    "        aggregate_losses.append(aggregate_loss)\n",
    "    \n",
    "    return np.array(aggregate_losses)\n",
    "\n",
    "# Run simulation\n",
    "print(\"Simulating aggregate losses...\")\n",
    "aggregate_losses = simulate_aggregate_loss(\n",
    "    best_freq, \n",
    "    best_severity,  \n",
    "    n_policies=n_policies,\n",
    "    policy_limit=policy_limit,\n",
    "    n_simulations=10000\n",
    ")\n",
    "\n",
    "print(f\"\\nAggregate Loss Distribution (Annual):\")\n",
    "print(f\"  Mean: ${aggregate_losses.mean():,.0f}\")\n",
    "print(f\"  Std Dev: ${aggregate_losses.std():,.0f}\")\n",
    "print(f\"  Median: ${np.median(aggregate_losses):,.0f}\")\n",
    "print(f\"  Observed (actual): ${claim_amounts.sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate risk metrics\n",
    "var_95 = np.percentile(aggregate_losses, 95)\n",
    "var_99 = np.percentile(aggregate_losses, 99)\n",
    "var_995 = np.percentile(aggregate_losses, 99.5)\n",
    "\n",
    "# TVaR (Expected Shortfall)\n",
    "tvar_95 = aggregate_losses[aggregate_losses >= var_95].mean()\n",
    "tvar_99 = aggregate_losses[aggregate_losses >= var_99].mean()\n",
    "\n",
    "print(\"Risk Metrics:\")\n",
    "print(f\"\\n  Value-at-Risk (VaR):\")\n",
    "print(f\"    95%: ${var_95:,.0f}\")\n",
    "print(f\"    99%: ${var_99:,.0f}\")\n",
    "print(f\"    99.5%: ${var_995:,.0f}\")\n",
    "\n",
    "print(f\"\\n  Tail Value-at-Risk (TVaR/ES):\")\n",
    "print(f\"    95%: ${tvar_95:,.0f}\")\n",
    "print(f\"    99%: ${tvar_99:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aggregate loss distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution\n",
    "axes[0].hist(aggregate_losses / 1e6, bins=50, density=True, alpha=0.7, \n",
    "             color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(aggregate_losses.mean() / 1e6, color='darkblue', linestyle='-', \n",
    "                lw=2, label=f'Mean: ${aggregate_losses.mean()/1e6:.1f}M')\n",
    "axes[0].axvline(var_99 / 1e6, color='red', linestyle='--', \n",
    "                lw=2, label=f'99% VaR: ${var_99/1e6:.1f}M')\n",
    "axes[0].axvline(claim_amounts.sum() / 1e6, color='green', linestyle=':', \n",
    "                lw=2, label=f'Observed: ${claim_amounts.sum()/1e6:.1f}M')\n",
    "axes[0].set_xlabel('Aggregate Loss ($ Millions)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Annual Aggregate Loss Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Exceedance probability curve\n",
    "sorted_losses = np.sort(aggregate_losses)\n",
    "exceedance_prob = 1 - np.arange(1, len(sorted_losses) + 1) / len(sorted_losses)\n",
    "\n",
    "axes[1].semilogy(sorted_losses / 1e6, exceedance_prob, 'b-', lw=1)\n",
    "axes[1].axhline(0.05, color='orange', linestyle='--', label='5% exceedance')\n",
    "axes[1].axhline(0.01, color='red', linestyle='--', label='1% exceedance')\n",
    "axes[1].set_xlabel('Aggregate Loss ($ Millions)')\n",
    "axes[1].set_ylabel('Exceedance Probability')\n",
    "axes[1].set_title('Exceedance Probability Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 6: Premium Calculation\n",
    "\n",
    "Use the fitted distributions to calculate risk-based premiums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premium calculation\n",
    "expected_loss = aggregate_losses.mean()\n",
    "loss_std = aggregate_losses.std()\n",
    "\n",
    "# Loading factors\n",
    "expense_loading = 0.25      # 25% for expenses\n",
    "profit_loading = 0.05       # 5% target profit\n",
    "risk_loading_factor = 0.10  # Risk load = 10% of std dev\n",
    "\n",
    "# Technical premium\n",
    "pure_premium = expected_loss\n",
    "risk_load = risk_loading_factor * loss_std\n",
    "technical_premium = pure_premium + risk_load\n",
    "\n",
    "# Gross premium\n",
    "gross_premium = technical_premium / (1 - expense_loading - profit_loading)\n",
    "\n",
    "# Per-policy premium\n",
    "per_policy = gross_premium / n_policies\n",
    "\n",
    "print(\"Premium Calculation (Annual Portfolio):\")\n",
    "print(f\"\\n  Expected Loss (Pure Premium): ${expected_loss:,.0f}\")\n",
    "print(f\"  Risk Loading (10% x sigma): ${risk_load:,.0f}\")\n",
    "print(f\"  Technical Premium: ${technical_premium:,.0f}\")\n",
    "print(f\"\\n  Expense Loading (25%): ${gross_premium * expense_loading:,.0f}\")\n",
    "print(f\"  Profit Loading (5%): ${gross_premium * profit_loading:,.0f}\")\n",
    "print(f\"\\n  Gross Premium: ${gross_premium:,.0f}\")\n",
    "print(f\"  Per Policy: ${per_policy:,.0f}\")\n",
    "\n",
    "# Loss ratio analysis\n",
    "print(f\"\\n  Expected Loss Ratio: {expected_loss / gross_premium:.1%}\")\n",
    "print(f\"  99% VaR Loss Ratio: {var_99 / gross_premium:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated actuarial modeling with spark-bestfit using LocalBackend:\n",
    "\n",
    "1. **Claim frequency modeling** with discrete distributions (Negative Binomial captures overdispersion)\n",
    "2. **Claim severity modeling** with heavy-tailed distributions (Pareto, Burr)\n",
    "3. **Q-Q plots** for validating tail behavior - critical for extreme loss estimation\n",
    "4. **Bounded fitting** for policy limits and censored data\n",
    "5. **Aggregate loss simulation** combining frequency and severity\n",
    "6. **Risk metrics** (VaR, TVaR) for capital and reserving\n",
    "\n",
    "### Key spark-bestfit Features Used\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|----------|\n",
    "| `LocalBackend` | Local parallel processing |\n",
    "| `DiscreteDistributionFitter` | Claim frequency (Poisson, NB) |\n",
    "| Heavy-tail distributions | Claim severity (Pareto, Burr) |\n",
    "| `lazy_metrics=True` | Fast model selection |\n",
    "| `results.best(n=5)` | Compare distribution candidates |\n",
    "\n",
    "### Actuarial Extensions\n",
    "\n",
    "- **Credibility weighting**: Blend with prior for small portfolios\n",
    "- **Covariate modeling**: GLM for frequency/severity by risk factors\n",
    "- **Reinsurance pricing**: Layer excess-of-loss with fitted severity\n",
    "- **IBNR reserves**: Project development patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup complete\n",
    "print(\"Insurance claims modeling complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
