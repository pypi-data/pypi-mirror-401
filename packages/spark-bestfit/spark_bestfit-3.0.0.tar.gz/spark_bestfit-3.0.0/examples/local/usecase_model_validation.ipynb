{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Risk Model Validation with spark-bestfit (LocalBackend)\n",
    "\n",
    "This notebook demonstrates **statistical model validation** using goodness-of-fit tests,\n",
    "focusing on the Kolmogorov-Smirnov (KS) and Anderson-Darling (AD) statistics.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **When to use KS vs AIC** for distribution selection\n",
    "2. **Backtest VaR models** against realized losses\n",
    "3. **Interpret KS p-values** for regulatory compliance\n",
    "4. **Compare KS vs Anderson-Darling** for tail sensitivity\n",
    "\n",
    "## Business Context\n",
    "\n",
    "Financial regulators (Basel III, Solvency II) require formal statistical validation\n",
    "of risk models. Unlike model *selection* (where AIC helps choose the best predictor),\n",
    "model *validation* asks: \"Does this model adequately describe the data?\"\n",
    "\n",
    "**Key Distinction:**\n",
    "- **AIC**: \"Which model predicts best?\" (relative comparison)\n",
    "- **KS/AD**: \"Does the data come from this distribution?\" (hypothesis test)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install spark-bestfit pandas numpy matplotlib scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from spark_bestfit import DistributionFitter\n",
    "from spark_bestfit.backends.local import LocalBackend\n",
    "\n",
    "# Create LocalBackend\n",
    "backend = LocalBackend()\n",
    "print(f\"LocalBackend initialized with {backend.max_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Understanding KS vs AIC\n",
    "\n",
    "Let's illustrate when each metric is appropriate with a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate data from a known distribution (normal)\n",
    "true_mu, true_sigma = 100, 15\n",
    "n_samples = 1000\n",
    "data = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "\n",
    "# Create pandas DataFrame\n",
    "df = pd.DataFrame({'value': data})\n",
    "\n",
    "print(f\"Generated {n_samples} samples from Normal(mu={true_mu}, sigma={true_sigma})\")\n",
    "print(f\"Sample mean: {data.mean():.2f}, Sample std: {data.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions - note we DON'T use lazy_metrics because we need KS/AD\n",
    "fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "results = fitter.fit(\n",
    "    df,\n",
    "    column='value',\n",
    "    max_distributions=15,\n",
    "    lazy_metrics=False  # IMPORTANT: Compute KS and AD statistics\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results.count()} distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare rankings by different metrics\n",
    "print(\"=\"*70)\n",
    "print(\"METRIC COMPARISON: Same data, different rankings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best by AIC (prediction-focused)\n",
    "print(\"\\nBest by AIC (model selection for prediction):\")\n",
    "for i, fit in enumerate(results.best(n=5, metric='aic'), 1):\n",
    "    print(f\"  {i}. {fit.distribution}: AIC={fit.aic:.1f}\")\n",
    "\n",
    "# Best by KS (hypothesis testing)\n",
    "print(\"\\nBest by KS statistic (goodness-of-fit testing):\")\n",
    "for i, fit in enumerate(results.best(n=5, metric='ks_statistic'), 1):\n",
    "    ks_pval = fit.pvalue if fit.pvalue else 0\n",
    "    print(f\"  {i}. {fit.distribution}: KS={fit.ks_statistic:.4f}, p={ks_pval:.4f}\")\n",
    "\n",
    "# Best by AD (tail-sensitive testing)\n",
    "print(\"\\nBest by Anderson-Darling (tail-sensitive testing):\")\n",
    "for i, fit in enumerate(results.best(n=5, metric='ad_statistic'), 1):\n",
    "    print(f\"  {i}. {fit.distribution}: AD={fit.ad_statistic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-xz0",
   "metadata": {},
   "source": [
    "**Why doesn't Normal win?** We generated data from `Normal(100, 15)`, yet other distributions\n",
    "may rank higher. This is expected behavior with finite samples:\n",
    "\n",
    "1. **Flexible distributions adapt better**: Some distributions have more parameters, allowing them\n",
    "   to capture sample-specific quirks (slight skewness, kurtosis variations) that arise from\n",
    "   random sampling.\n",
    "\n",
    "2. **AIC penalizes complexity, but not enough**: With 1000 samples, the likelihood gain from\n",
    "   extra parameters often exceeds the AIC penalty.\n",
    "\n",
    "3. **All top models pass KS test**: Notice p-values > 0.05 for all top distributions. The\n",
    "   KS test says \"we cannot reject that the data comes from this distribution\" - this is true\n",
    "   for multiple distributions.\n",
    "\n",
    "**Key insight**: With finite samples, the \"true\" generating distribution isn't always the\n",
    "best-fitting one. This is normal - focus on whether your chosen model passes validation (KS p > 0.05),\n",
    "not on recovering the exact generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 2: Interpreting KS Test Results\n",
    "\n",
    "The KS test answers: \"What is the probability of observing this data if it truly\n",
    "came from the fitted distribution?\"\n",
    "\n",
    "- **KS statistic**: Maximum distance between empirical and theoretical CDFs\n",
    "- **p-value > 0.05**: Cannot reject that data comes from this distribution\n",
    "- **p-value < 0.05**: Evidence that data does NOT come from this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best fit by KS statistic\n",
    "best_ks = results.best(n=1, metric='ks_statistic')[0]\n",
    "\n",
    "print(f\"Best KS Fit: {best_ks.distribution}\")\n",
    "print(f\"  KS Statistic: {best_ks.ks_statistic:.4f}\")\n",
    "print(f\"  KS p-value: {best_ks.pvalue:.4f}\")\n",
    "print(f\"  Parameters: {best_ks.parameters}\")\n",
    "\n",
    "# Interpretation\n",
    "if best_ks.pvalue > 0.05:\n",
    "    print(f\"\\n  INTERPRETATION: p={best_ks.pvalue:.3f} > 0.05\")\n",
    "    print(f\"  Cannot reject H0: Data comes from {best_ks.distribution} distribution\")\n",
    "    print(f\"  Model PASSES validation at 95% confidence level\")\n",
    "else:\n",
    "    print(f\"\\n  INTERPRETATION: p={best_ks.pvalue:.3f} < 0.05\")\n",
    "    print(f\"  Reject H0: Data likely does NOT come from {best_ks.distribution}\")\n",
    "    print(f\"  Model FAILS validation at 95% confidence level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KS test\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CDF comparison (what KS measures)\n",
    "sorted_data = np.sort(data)\n",
    "empirical_cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "\n",
    "# Get fitted distribution\n",
    "fitted_dist = best_ks.get_scipy_dist()\n",
    "theoretical_cdf = fitted_dist.cdf(sorted_data)\n",
    "\n",
    "axes[0].plot(sorted_data, empirical_cdf, 'b-', lw=2, label='Empirical CDF')\n",
    "axes[0].plot(sorted_data, theoretical_cdf, 'r--', lw=2, label=f'Fitted {best_ks.distribution} CDF')\n",
    "\n",
    "# Show max distance (KS statistic)\n",
    "diff = np.abs(empirical_cdf - theoretical_cdf)\n",
    "max_idx = np.argmax(diff)\n",
    "axes[0].vlines(sorted_data[max_idx], theoretical_cdf[max_idx], empirical_cdf[max_idx],\n",
    "               colors='green', linestyles='-', lw=3, label=f'KS={best_ks.ks_statistic:.4f}')\n",
    "\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Cumulative Probability')\n",
    "axes[0].set_title('KS Test: CDF Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PDF comparison\n",
    "x = np.linspace(data.min(), data.max(), 200)\n",
    "axes[1].hist(data, bins=40, density=True, alpha=0.6, color='steelblue', \n",
    "             edgecolor='black', label='Observed')\n",
    "axes[1].plot(x, fitted_dist.pdf(x), 'r-', lw=2, label=f'{best_ks.distribution} fit')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title(f'Distribution Fit (p-value={best_ks.pvalue:.3f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: VaR Model Backtesting\n",
    "\n",
    "A practical application: validating that a Value-at-Risk model is correctly calibrated.\n",
    "\n",
    "**Regulatory Requirement**: If a 99% VaR model is correct, we expect ~1% of days\n",
    "to have losses exceeding VaR. Too many exceptions = model underestimates risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Simulate historical portfolio returns (slightly fat-tailed)\n",
    "n_days = 500  # ~2 years of trading days\n",
    "\n",
    "# True distribution: t-distribution with 5 degrees of freedom (fat tails)\n",
    "true_returns = stats.t.rvs(df=5, loc=0.0005, scale=0.015, size=n_days)\n",
    "\n",
    "returns_df = pd.DataFrame({'daily_return': true_returns})\n",
    "\n",
    "print(f\"Historical returns: {n_days} days\")\n",
    "print(f\"Mean daily return: {true_returns.mean():.4%}\")\n",
    "print(f\"Volatility: {true_returns.std():.4%}\")\n",
    "print(f\"Empirical 1% VaR: {np.percentile(true_returns, 1):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions to returns - we need KS for validation\n",
    "returns_results = fitter.fit(\n",
    "    returns_df,\n",
    "    column='daily_return',\n",
    "    max_distributions=20,\n",
    "    lazy_metrics=False  # Need KS/AD for validation\n",
    ")\n",
    "\n",
    "# Best by KS\n",
    "best_var_model = returns_results.best(n=1, metric='ks_statistic')[0]\n",
    "\n",
    "print(\"Model Comparison for VaR:\\n\")\n",
    "print(f\"Best KS Model: {best_var_model.distribution}\")\n",
    "print(f\"  KS statistic: {best_var_model.ks_statistic:.4f}\")\n",
    "print(f\"  KS p-value: {best_var_model.pvalue:.4f}\")\n",
    "print(f\"  Validation: {'PASS' if best_var_model.pvalue > 0.05 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest: Compare VaR predictions from different models\n",
    "confidence_level = 0.99\n",
    "alpha = 1 - confidence_level\n",
    "\n",
    "# Get fitted distributions\n",
    "best_dist = best_var_model.get_scipy_dist()\n",
    "\n",
    "# Also fit a normal for comparison\n",
    "normal_mu, normal_sigma = true_returns.mean(), true_returns.std()\n",
    "normal_dist = stats.norm(loc=normal_mu, scale=normal_sigma)\n",
    "\n",
    "# Calculate VaR from each model\n",
    "var_best = best_dist.ppf(alpha)\n",
    "var_normal = normal_dist.ppf(alpha)\n",
    "var_empirical = np.percentile(true_returns, alpha * 100)\n",
    "\n",
    "print(f\"99% VaR Estimates (1-day):\")\n",
    "print(f\"  Empirical: {var_empirical:.4%}\")\n",
    "print(f\"  Normal assumption: {var_normal:.4%}\")\n",
    "print(f\"  Best KS model ({best_var_model.distribution}): {var_best:.4%}\")\n",
    "\n",
    "# Count exceptions (days where loss exceeded VaR)\n",
    "exceptions_normal = (true_returns < var_normal).sum()\n",
    "exceptions_best = (true_returns < var_best).sum()\n",
    "expected_exceptions = n_days * alpha\n",
    "\n",
    "print(f\"\\nBacktest Results ({n_days} days):\")\n",
    "print(f\"  Expected exceptions (1%): {expected_exceptions:.1f}\")\n",
    "print(f\"  Normal model exceptions: {exceptions_normal} ({exceptions_normal/n_days:.1%})\")\n",
    "print(f\"  Best KS model exceptions: {exceptions_best} ({exceptions_best/n_days:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize backtest\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series with VaR breaches\n",
    "days = np.arange(n_days)\n",
    "axes[0].plot(days, true_returns * 100, 'b-', alpha=0.6, lw=0.5, label='Daily returns')\n",
    "axes[0].axhline(var_best * 100, color='green', linestyle='-', lw=2, \n",
    "                label=f'99% VaR ({best_var_model.distribution}): {var_best:.2%}')\n",
    "axes[0].axhline(var_normal * 100, color='red', linestyle='--', lw=2,\n",
    "                label=f'99% VaR (Normal): {var_normal:.2%}')\n",
    "\n",
    "# Mark exceptions\n",
    "breach_mask = true_returns < var_best\n",
    "axes[0].scatter(days[breach_mask], true_returns[breach_mask] * 100, \n",
    "                color='red', s=50, zorder=5, label='VaR breach')\n",
    "\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Daily Return (%)')\n",
    "axes[0].set_title('VaR Backtest: Daily Returns vs Model Predictions')\n",
    "axes[0].legend(loc='lower left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tail comparison\n",
    "x = np.linspace(true_returns.min(), np.percentile(true_returns, 10), 100)\n",
    "axes[1].hist(true_returns[true_returns < np.percentile(true_returns, 10)], \n",
    "             bins=30, density=True, alpha=0.6, color='coral', \n",
    "             edgecolor='black', label='Observed (left tail)')\n",
    "axes[1].plot(x, best_dist.pdf(x), 'g-', lw=2, label=f'{best_var_model.distribution}')\n",
    "axes[1].plot(x, normal_dist.pdf(x), 'r--', lw=2, label='Normal')\n",
    "axes[1].axvline(var_best, color='green', linestyle=':', lw=2)\n",
    "axes[1].axvline(var_normal, color='red', linestyle=':', lw=2)\n",
    "\n",
    "axes[1].set_xlabel('Daily Return')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Left Tail Comparison (VaR Region)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 4: KS vs Anderson-Darling\n",
    "\n",
    "Both are goodness-of-fit tests, but with different sensitivities:\n",
    "\n",
    "- **KS**: Sensitive to differences in the middle of the distribution\n",
    "- **AD**: More sensitive to differences in the **tails**\n",
    "\n",
    "For risk management (VaR, ES), tail behavior matters most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare KS vs AD rankings\n",
    "print(\"KS vs Anderson-Darling Comparison:\\n\")\n",
    "print(f\"{'Distribution':<15} {'KS Stat':<12} {'AD Stat':<12} {'KS Rank':<10} {'AD Rank'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get top 10 by each metric\n",
    "ks_ranking = {fit.distribution: (i+1, fit.ks_statistic) \n",
    "              for i, fit in enumerate(returns_results.best(n=10, metric='ks_statistic'))}\n",
    "ad_ranking = {fit.distribution: (i+1, fit.ad_statistic) \n",
    "              for i, fit in enumerate(returns_results.best(n=10, metric='ad_statistic'))}\n",
    "\n",
    "# Show distributions that appear in both top 10s\n",
    "all_dists = set(ks_ranking.keys()) | set(ad_ranking.keys())\n",
    "for dist in sorted(all_dists):\n",
    "    ks_rank, ks_stat = ks_ranking.get(dist, ('-', None))\n",
    "    ad_rank, ad_stat = ad_ranking.get(dist, ('-', None))\n",
    "    ks_stat_str = f\"{ks_stat:.4f}\" if ks_stat else \"-\"\n",
    "    ad_stat_str = f\"{ad_stat:.4f}\" if ad_stat else \"-\"\n",
    "    print(f\"{dist:<15} {ks_stat_str:<12} {ad_stat_str:<12} {str(ks_rank):<10} {ad_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by AD for tail-sensitive validation\n",
    "best_ad = returns_results.best(n=1, metric='ad_statistic')[0]\n",
    "\n",
    "print(\"Tail-Sensitive Model Selection (Anderson-Darling):\")\n",
    "print(f\"\\nBest AD Model: {best_ad.distribution}\")\n",
    "print(f\"  AD Statistic: {best_ad.ad_statistic:.4f}\")\n",
    "print(f\"  KS Statistic: {best_ad.ks_statistic:.4f}\")\n",
    "print(f\"  Parameters: {best_ad.parameters}\")\n",
    "\n",
    "# Compare VaR from KS-best vs AD-best\n",
    "ad_dist = best_ad.get_scipy_dist()\n",
    "var_ad = ad_dist.ppf(alpha)\n",
    "\n",
    "print(f\"\\n99% VaR Comparison:\")\n",
    "print(f\"  KS-optimal ({best_var_model.distribution}): {var_best:.4%}\")\n",
    "print(f\"  AD-optimal ({best_ad.distribution}): {var_ad:.4%}\")\n",
    "print(f\"  Empirical: {var_empirical:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 5: Regulatory Validation Report\n",
    "\n",
    "Generate a summary suitable for regulatory documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regulatory-style validation report\n",
    "print(\"=\" * 70)\n",
    "print(\"RISK MODEL VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. DATA SUMMARY\")\n",
    "print(f\"   Observation period: {n_days} trading days\")\n",
    "print(f\"   Mean return: {true_returns.mean():.4%}\")\n",
    "print(f\"   Volatility: {true_returns.std():.4%}\")\n",
    "print(f\"   Skewness: {stats.skew(true_returns):.3f}\")\n",
    "print(f\"   Excess Kurtosis: {stats.kurtosis(true_returns):.3f}\")\n",
    "\n",
    "print(f\"\\n2. DISTRIBUTION FIT RESULTS\")\n",
    "print(f\"   Selected Model: {best_var_model.distribution}\")\n",
    "print(f\"   Selection Criterion: Kolmogorov-Smirnov statistic (lower is better)\")\n",
    "\n",
    "print(f\"\\n3. GOODNESS-OF-FIT TESTS\")\n",
    "print(f\"   KS Statistic: {best_var_model.ks_statistic:.4f}\")\n",
    "print(f\"   KS p-value: {best_var_model.pvalue:.4f}\")\n",
    "print(f\"   KS Test Result: {'PASS' if best_var_model.pvalue > 0.05 else 'FAIL'} (alpha=0.05)\")\n",
    "print(f\"   AD Statistic: {best_var_model.ad_statistic:.4f}\")\n",
    "ad_pval_str = f\"{best_var_model.ad_pvalue:.4f}\" if best_var_model.ad_pvalue else \"N/A (AD tables limited)\"\n",
    "print(f\"   AD p-value: {ad_pval_str}\")\n",
    "\n",
    "print(f\"\\n4. VAR BACKTESTING\")\n",
    "print(f\"   Confidence Level: {confidence_level:.0%}\")\n",
    "print(f\"   Expected Exceptions: {expected_exceptions:.1f} ({alpha:.1%} of {n_days})\")\n",
    "print(f\"   Observed Exceptions: {exceptions_best}\")\n",
    "exception_ratio = exceptions_best / expected_exceptions\n",
    "print(f\"   Exception Ratio: {exception_ratio:.2f}x expected\")\n",
    "\n",
    "# Basel traffic light test\n",
    "if exceptions_best <= expected_exceptions * 1.5:\n",
    "    zone = \"GREEN\"\n",
    "    recommendation = \"Model is well-calibrated\"\n",
    "elif exceptions_best <= expected_exceptions * 2.5:\n",
    "    zone = \"YELLOW\" \n",
    "    recommendation = \"Model may underestimate risk - monitoring required\"\n",
    "else:\n",
    "    zone = \"RED\"\n",
    "    recommendation = \"Model significantly underestimates risk - recalibration required\"\n",
    "\n",
    "print(f\"   Basel Zone: {zone}\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATION\")\n",
    "print(f\"   {recommendation}\")\n",
    "print(f\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated model validation with goodness-of-fit statistics using LocalBackend:\n",
    "\n",
    "1. **AIC vs KS**: AIC for model selection, KS for hypothesis testing\n",
    "2. **KS interpretation**: p-value > 0.05 means data is consistent with distribution\n",
    "3. **VaR backtesting**: Validate risk models against realized losses\n",
    "4. **KS vs AD**: AD is more tail-sensitive (important for risk)\n",
    "\n",
    "### Key spark-bestfit Features Used\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|----------|\n",
    "| `LocalBackend` | Local parallel processing |\n",
    "| `lazy_metrics=False` | Compute KS and AD statistics |\n",
    "| `metric='ks_statistic'` | Select by goodness-of-fit |\n",
    "| `metric='ad_statistic'` | Tail-sensitive selection |\n",
    "| `fit.pvalue` | KS hypothesis test p-value |\n",
    "\n",
    "### When to Use Each Metric\n",
    "\n",
    "| Metric | Use When | Example |\n",
    "|--------|----------|--------|\n",
    "| AIC | Choosing best predictor | Forecasting, simulation |\n",
    "| KS | Testing distribution assumption | Model validation, compliance |\n",
    "| AD | Testing tail behavior | VaR, extreme value analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup complete\n",
    "print(\"Model validation analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
