{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML Synthetic Data Generation with spark-bestfit (LocalBackend)\n",
    "\n",
    "This notebook demonstrates how to generate **synthetic data** for machine learning workflows\n",
    "by fitting statistical distributions to real data.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Fit distributions** to production data features\n",
    "2. **Handle mixed types** (continuous + discrete columns)\n",
    "3. **Save and load** fitted models for reproducibility\n",
    "4. **Generate synthetic data** that matches original statistics\n",
    "5. **Validate** synthetic data quality\n",
    "\n",
    "## Business Context\n",
    "\n",
    "Synthetic data is valuable for:\n",
    "\n",
    "- **Privacy**: Share data patterns without exposing real records\n",
    "- **Testing**: Generate test data that matches production characteristics\n",
    "- **Augmentation**: Expand small datasets for model training\n",
    "- **Development**: Work with realistic data in non-production environments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install spark-bestfit pandas numpy matplotlib scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spark_bestfit import DistributionFitter, DiscreteDistributionFitter\n",
    "from spark_bestfit.backends.local import LocalBackend\n",
    "\n",
    "# Create LocalBackend\n",
    "backend = LocalBackend()\n",
    "print(f\"LocalBackend initialized with {backend.max_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Create Sample \"Production\" Dataset\n",
    "\n",
    "We'll simulate a customer dataset with mixed feature types:\n",
    "\n",
    "- **Continuous**: age, income, account_balance, credit_score\n",
    "- **Discrete**: num_products, num_transactions, tenure_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_customers = 5000\n",
    "\n",
    "# Generate realistic customer data\n",
    "data = {\n",
    "    # Continuous features\n",
    "    'age': np.clip(np.random.normal(42, 15, n_customers), 18, 85).astype(int),\n",
    "    'income': np.random.lognormal(10.8, 0.6, n_customers),  # Skewed income distribution\n",
    "    'account_balance': np.abs(np.random.normal(5000, 3000, n_customers)),\n",
    "    'credit_score': np.clip(np.random.normal(700, 80, n_customers), 300, 850).astype(int),\n",
    "    \n",
    "    # Discrete features\n",
    "    'num_products': np.random.poisson(2.5, n_customers),\n",
    "    'num_transactions': np.random.negative_binomial(5, 0.3, n_customers),\n",
    "    'tenure_months': np.random.geometric(0.02, n_customers)\n",
    "}\n",
    "\n",
    "# Create Pandas DataFrame\n",
    "original_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Original dataset: {len(original_df)} customers\")\n",
    "print(original_df.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(original_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "columns = list(data.keys())\n",
    "for i, col in enumerate(columns):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(original_df[col], bins=40, density=True, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{col}\\nmean={original_df[col].mean():.1f}')\n",
    "        axes[i].set_xlabel(col)\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Original Data Distributions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Fit Distributions to Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous and discrete columns\n",
    "continuous_cols = ['age', 'income', 'account_balance', 'credit_score']\n",
    "discrete_cols = ['num_products', 'num_transactions', 'tenure_months']\n",
    "\n",
    "# Fit continuous distributions using LocalBackend\n",
    "cont_fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "cont_results = cont_fitter.fit(\n",
    "    original_df,\n",
    "    columns=continuous_cols,\n",
    "    lazy_metrics=True,\n",
    "    max_distributions=25  # Focus on common distributions\n",
    ")\n",
    "\n",
    "print(f\"Fitted {cont_results.count()} continuous distribution-column combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best continuous fits\n",
    "best_continuous = cont_results.best_per_column(n=1, metric='aic')\n",
    "\n",
    "print(\"Best Continuous Distributions:\\n\")\n",
    "for col, fits in best_continuous.items():\n",
    "    fit = fits[0]\n",
    "    print(f\"  {col}: {fit.distribution} (AIC={fit.aic:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 3: Fit Distributions to Discrete Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit discrete distributions\n",
    "disc_fitter = DiscreteDistributionFitter(backend=backend)\n",
    "\n",
    "disc_results = disc_fitter.fit(\n",
    "    original_df,\n",
    "    columns=discrete_cols\n",
    ")\n",
    "\n",
    "print(f\"Fitted {disc_results.count()} discrete distribution-column combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best discrete fits\n",
    "best_discrete = disc_results.best_per_column(n=1, metric='aic')\n",
    "\n",
    "print(\"Best Discrete Distributions:\\n\")\n",
    "for col, fits in best_discrete.items():\n",
    "    fit = fits[0]\n",
    "    print(f\"  {col}: {fit.distribution} (AIC={fit.aic:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 4: Save Fitted Models\n",
    "\n",
    "Save the fitted distributions for reproducibility and later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temp directory for models\n",
    "model_dir = tempfile.mkdtemp(prefix='synthetic_models_')\n",
    "\n",
    "# Save best fits for each column\n",
    "cont_model_path = os.path.join(model_dir, 'continuous_fits')\n",
    "os.makedirs(cont_model_path, exist_ok=True)\n",
    "\n",
    "print(\"Saved continuous models:\")\n",
    "for col, fits in best_continuous.items():\n",
    "    fit = fits[0]\n",
    "    path = os.path.join(cont_model_path, f'{col}.json')\n",
    "    fit.save(path)\n",
    "    print(f\"  {col}: {path}\")\n",
    "\n",
    "disc_model_path = os.path.join(model_dir, 'discrete_fits')\n",
    "os.makedirs(disc_model_path, exist_ok=True)\n",
    "\n",
    "print(\"\\nSaved discrete models:\")\n",
    "for col, fits in best_discrete.items():\n",
    "    fit = fits[0]\n",
    "    path = os.path.join(disc_model_path, f'{col}.json')\n",
    "    fit.save(path)\n",
    "    print(f\"  {col}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 5: Generate Synthetic Data\n",
    "\n",
    "Now we'll generate synthetic data by sampling from the fitted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def generate_synthetic_data(best_continuous, best_discrete, n_samples, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data by sampling from fitted distributions.\n",
    "    \n",
    "    Args:\n",
    "        best_continuous: Dict of column -> DistributionFitResult for continuous\n",
    "        best_discrete: Dict of column -> DistributionFitResult for discrete\n",
    "        n_samples: Number of synthetic records to generate\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with synthetic data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    synthetic_data = {}\n",
    "    \n",
    "    # Generate continuous columns using get_scipy_dist()\n",
    "    for col, fits in best_continuous.items():\n",
    "        fit = fits[0]\n",
    "        frozen_dist = fit.get_scipy_dist()\n",
    "        \n",
    "        # Sample from distribution\n",
    "        samples = frozen_dist.rvs(size=n_samples)\n",
    "        synthetic_data[col] = samples\n",
    "        print(f\"  Generated {col}: {fit.distribution}\")\n",
    "    \n",
    "    # Generate discrete columns using get_scipy_dist()\n",
    "    for col, fits in best_discrete.items():\n",
    "        fit = fits[0]\n",
    "        frozen_dist = fit.get_scipy_dist()\n",
    "        \n",
    "        # Sample from distribution\n",
    "        samples = frozen_dist.rvs(size=n_samples)\n",
    "        synthetic_data[col] = samples\n",
    "        print(f\"  Generated {col}: {fit.distribution}\")\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Generate synthetic dataset (same size as original)\n",
    "print(\"Generating synthetic data...\")\n",
    "synthetic_df = generate_synthetic_data(\n",
    "    best_continuous,\n",
    "    best_discrete,\n",
    "    n_samples=n_customers,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_df)} synthetic records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 6: Validate Synthetic Data Quality\n",
    "\n",
    "Compare synthetic data statistics to original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare summary statistics\n",
    "all_cols = continuous_cols + discrete_cols\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Original Mean': original_df[all_cols].mean(),\n",
    "    'Synthetic Mean': synthetic_df[all_cols].mean(),\n",
    "    'Original Std': original_df[all_cols].std(),\n",
    "    'Synthetic Std': synthetic_df[all_cols].std(),\n",
    "})\n",
    "\n",
    "comparison['Mean Diff %'] = ((comparison['Synthetic Mean'] - comparison['Original Mean']) \n",
    "                              / comparison['Original Mean'] * 100).round(1)\n",
    "comparison['Std Diff %'] = ((comparison['Synthetic Std'] - comparison['Original Std']) \n",
    "                             / comparison['Original Std'] * 100).round(1)\n",
    "\n",
    "print(\"Statistical Comparison:\\n\")\n",
    "print(comparison.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(all_cols):\n",
    "    if i < len(axes):\n",
    "        # Original\n",
    "        axes[i].hist(original_df[col], bins=40, density=True, alpha=0.5, \n",
    "                     label='Original', color='blue', edgecolor='black')\n",
    "        # Synthetic\n",
    "        axes[i].hist(synthetic_df[col], bins=40, density=True, alpha=0.5,\n",
    "                     label='Synthetic', color='orange', edgecolor='black')\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].legend(fontsize=8)\n",
    "\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Original vs Synthetic Data Distributions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test for distribution similarity\n",
    "print(\"Kolmogorov-Smirnov Tests (Original vs Synthetic):\\n\")\n",
    "print(f\"{'Column':<20} {'KS Statistic':<15} {'p-value':<15} {'Match?'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in all_cols:\n",
    "    ks_stat, p_value = stats.ks_2samp(original_df[col], synthetic_df[col])\n",
    "    match = \"Yes\" if p_value > 0.05 else \"No\"\n",
    "    print(f\"{col:<20} {ks_stat:<15.4f} {p_value:<15.4f} {match}\")\n",
    "\n",
    "print(\"\\n(p-value > 0.05 suggests distributions are similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Part 7: Using Synthetic Data for ML\n",
    "\n",
    "Demonstrate using synthetic data to train a model, then evaluate on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "try:\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, classification_report\n    SKLEARN_AVAILABLE = True\nexcept ImportError:\n    SKLEARN_AVAILABLE = False\n    print(\"Note: scikit-learn not installed. Skipping ML validation.\")\n    print(\"Install with: pip install scikit-learn\")\n\nif SKLEARN_AVAILABLE:\n    # Create a binary target (e.g., high-value customer: income > median)\n    median_income = original_df['income'].median()\n\n    original_df['high_value'] = (original_df['income'] > median_income).astype(int)\n    synthetic_df['high_value'] = (synthetic_df['income'] > median_income).astype(int)\n\n    # Features (exclude income since it defines the target)\n    feature_cols = ['age', 'account_balance', 'credit_score', 'num_products', 'num_transactions', 'tenure_months']\n\n    # Split original data for testing\n    X_original = original_df[feature_cols]\n    y_original = original_df['high_value']\n\n    X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n        X_original, y_original, test_size=0.3, random_state=42\n    )\n\n    # Train on synthetic data\n    X_synthetic = synthetic_df[feature_cols]\n    y_synthetic = synthetic_df['high_value']\n\n    print(f\"Training set (synthetic): {len(X_synthetic)} samples\")\n    print(f\"Test set (real): {len(X_test)} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "if SKLEARN_AVAILABLE:\n    # Model 1: Train on original, test on original\n    rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_original.fit(X_train_orig, y_train_orig)\n    acc_original = accuracy_score(y_test, rf_original.predict(X_test))\n\n    # Model 2: Train on synthetic, test on original (real)\n    rf_synthetic = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_synthetic.fit(X_synthetic, y_synthetic)\n    acc_synthetic = accuracy_score(y_test, rf_synthetic.predict(X_test))\n\n    print(\"Model Performance on Real Test Data:\\n\")\n    print(f\"  Trained on Original:  {acc_original:.1%} accuracy\")\n    print(f\"  Trained on Synthetic: {acc_synthetic:.1%} accuracy\")\n    print(f\"  \\nDifference: {(acc_original - acc_synthetic):.1%}\")\nelse:\n    print(\"Skipping ML comparison (scikit-learn not installed)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 8: Scaling Up - Generate Large Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger synthetic dataset (10x original)\n",
    "print(\"Generating 50,000 synthetic records...\")\n",
    "large_synthetic = generate_synthetic_data(\n",
    "    best_continuous,\n",
    "    best_discrete,\n",
    "    n_samples=50000,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "print(f\"\\nLarge synthetic dataset: {len(large_synthetic)} records\")\n",
    "print(large_synthetic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete synthetic data generation workflow using LocalBackend:\n",
    "\n",
    "1. **Fit distributions** to continuous features with `DistributionFitter`\n",
    "2. **Fit distributions** to discrete features with `DiscreteDistributionFitter`\n",
    "3. **Save models** for reproducibility with `results.save()`\n",
    "4. **Generate synthetic data** by sampling from fitted distributions\n",
    "5. **Validate quality** using statistical tests and visual comparison\n",
    "6. **ML validation** - model trained on synthetic achieves similar accuracy\n",
    "\n",
    "### Key spark-bestfit Features Used\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|----------|\n",
    "| `LocalBackend` | Local parallel processing |\n",
    "| Multi-column fitting | Fit all features efficiently |\n",
    "| `DiscreteDistributionFitter` | Handle count/categorical data |\n",
    "| `results.save()` | Persist fitted models |\n",
    "| `lazy_metrics=True` | Fast fitting when only AIC needed |\n",
    "\n",
    "### Limitations & Extensions\n",
    "\n",
    "- **Correlations**: This example generates columns independently. For correlated features, use `GaussianCopula` (see Monte Carlo notebook).\n",
    "- **Constraints**: Real data may have business constraints (e.g., age > 18). Add post-processing validation.\n",
    "- **Privacy**: For formal privacy guarantees, consider differential privacy techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
