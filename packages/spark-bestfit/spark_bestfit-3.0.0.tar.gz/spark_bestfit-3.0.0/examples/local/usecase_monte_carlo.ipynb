{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Monte Carlo Risk Simulation with spark-bestfit (LocalBackend)\n",
    "\n",
    "This notebook demonstrates how to use spark-bestfit for **Monte Carlo simulation** in financial risk management.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Fit distributions** to historical asset returns\n",
    "2. **Model correlations** between assets using Gaussian Copula\n",
    "3. **Generate correlated scenarios** for risk simulation\n",
    "4. **Calculate Value-at-Risk (VaR)** and Expected Shortfall (ES)\n",
    "\n",
    "## Business Context\n",
    "\n",
    "Risk managers need to estimate potential portfolio losses under various market conditions.\n",
    "Monte Carlo simulation generates thousands of plausible scenarios by:\n",
    "\n",
    "- Fitting statistical distributions to historical returns\n",
    "- Preserving correlations between assets (crucial for diversification analysis)\n",
    "- Sampling from the joint distribution to create future scenarios\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install spark-bestfit pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from spark_bestfit import DistributionFitter, GaussianCopula\n",
    "from spark_bestfit.backends.local import LocalBackend\n",
    "\n",
    "# Create LocalBackend\n",
    "backend = LocalBackend()\n",
    "print(f\"LocalBackend initialized with {backend.max_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Historical Returns\n",
    "\n",
    "In production, you'd load real historical data. Here we simulate realistic correlated returns\n",
    "for a 3-asset portfolio: Tech Stock (high vol), Bond ETF (low vol), and Gold (medium vol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_days = 1000  # ~4 years of daily data\n",
    "\n",
    "# Define realistic correlation matrix\n",
    "# Tech and Bonds are negatively correlated (flight to safety)\n",
    "# Gold has low correlation with both\n",
    "correlation_matrix = np.array([\n",
    "    [1.00,  -0.30,  0.10],   # Tech\n",
    "    [-0.30,  1.00,  0.15],   # Bonds\n",
    "    [0.10,   0.15,  1.00]    # Gold\n",
    "])\n",
    "\n",
    "# Cholesky decomposition to generate correlated normals\n",
    "L = np.linalg.cholesky(correlation_matrix)\n",
    "uncorrelated = np.random.standard_normal((n_days, 3))\n",
    "correlated_normals = uncorrelated @ L.T\n",
    "\n",
    "# Transform to realistic return distributions\n",
    "# Tech: Higher vol, slight positive skew (big up days)\n",
    "tech_returns = correlated_normals[:, 0] * 0.02 + 0.0005  # ~2% daily vol, slight positive drift\n",
    "\n",
    "# Bonds: Low vol, slight negative skew\n",
    "bond_returns = correlated_normals[:, 1] * 0.005 + 0.0001  # ~0.5% daily vol\n",
    "\n",
    "# Gold: Medium vol, fat tails (use t-distribution transform)\n",
    "gold_normals = correlated_normals[:, 2]\n",
    "gold_returns = stats.t.ppf(stats.norm.cdf(gold_normals), df=5) * 0.012  # Fat tails\n",
    "\n",
    "# Create DataFrame\n",
    "returns_df = pd.DataFrame({\n",
    "    'date': pd.date_range('2020-01-01', periods=n_days, freq='B'),\n",
    "    'tech': tech_returns,\n",
    "    'bonds': bond_returns,\n",
    "    'gold': gold_returns\n",
    "})\n",
    "\n",
    "print(f\"Historical returns: {len(returns_df)} days\")\n",
    "print(returns_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize historical returns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for i, col in enumerate(['tech', 'bonds', 'gold']):\n",
    "    data = returns_df[col]\n",
    "    axes[i].hist(data, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    axes[i].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.4f}')\n",
    "    axes[i].axvline(data.quantile(0.05), color='orange', linestyle=':', label=f'5th %ile: {data.quantile(0.05):.4f}')\n",
    "    axes[i].set_title(f'{col.upper()} Daily Returns')\n",
    "    axes[i].set_xlabel('Return')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation\n",
    "print(\"\\nHistorical Correlation Matrix:\")\n",
    "print(returns_df[['tech', 'bonds', 'gold']].corr().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 2: Fit Distributions to Each Asset\n",
    "\n",
    "We'll fit distributions to each asset's returns to find the best marginal distribution.\n",
    "Using `lazy_metrics=True` for faster fitting since we only need AIC for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions to all assets at once (multi-column fitting)\n",
    "fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "results = fitter.fit(\n",
    "    returns_df,\n",
    "    columns=['tech', 'bonds', 'gold'],\n",
    "    lazy_metrics=True,  # Skip KS/AD for speed - we'll use AIC\n",
    "    max_distributions=30  # Focus on common distributions\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results.count()} distribution-column combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best distribution for each asset\n",
    "best_per_asset = results.best_per_column(n=3, metric='aic')\n",
    "\n",
    "print(\"Best Distributions by AIC:\\n\")\n",
    "for column, fits in best_per_asset.items():\n",
    "    print(f\"\\n{column.upper()}:\")\n",
    "    for i, fit in enumerate(fits, 1):\n",
    "        print(f\"  {i}. {fit.distribution}: AIC={fit.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store best fits for later use\n",
    "best_fits = {col: fits[0] for col, fits in best_per_asset.items()}\n",
    "\n",
    "# Visualize fits - create individual plots\n",
    "for col, fit in best_fits.items():\n",
    "    fig, ax = fitter.plot(\n",
    "        fit, \n",
    "        returns_df, \n",
    "        col,\n",
    "        title=f\"{col.upper()}: {fit.distribution}\",\n",
    "        figsize=(8, 5)\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: Model Correlations with Gaussian Copula\n",
    "\n",
    "The **Gaussian Copula** captures the dependency structure between assets while allowing\n",
    "each asset to have its own marginal distribution. This is crucial because:\n",
    "\n",
    "- Assets may have different distribution shapes (normal, t, skewed)\n",
    "- Correlations can change the joint tail behavior\n",
    "- Diversification benefits depend on correlation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gaussian Copula to capture correlations\n",
    "# Note: GaussianCopula.fit() requires FitResults from multi-column fitting\n",
    "copula = GaussianCopula.fit(results, returns_df, columns=['tech', 'bonds', 'gold'])\n",
    "\n",
    "print(\"Copula Correlation Matrix (Spearman):\")\n",
    "print(pd.DataFrame(\n",
    "    copula.correlation_matrix,\n",
    "    index=['tech', 'bonds', 'gold'],\n",
    "    columns=['tech', 'bonds', 'gold']\n",
    ").round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 4: Generate Monte Carlo Scenarios\n",
    "\n",
    "Now we generate thousands of correlated return scenarios for risk analysis.\n",
    "The copula samples preserve the correlation structure while each marginal\n",
    "follows its fitted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Generate 10,000 scenarios using the copula\nn_scenarios = 10000\n\n# sample() returns a dict mapping column names to arrays\n# Convert to DataFrame for analysis\nsamples = copula.sample(\n    n=n_scenarios,\n    random_state=42  # For reproducibility\n)\nscenarios_df = pd.DataFrame(samples)\n\nprint(f\"Generated {len(scenarios_df)} scenarios\")\nprint(scenarios_df.describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correlation is preserved in scenarios\n",
    "print(\"Scenario Correlation Matrix:\")\n",
    "print(scenarios_df[['tech', 'bonds', 'gold']].corr().round(3))\n",
    "\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(returns_df[['tech', 'bonds', 'gold']].corr().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 5: Calculate Portfolio Risk Metrics\n",
    "\n",
    "Define a portfolio and calculate key risk metrics:\n",
    "\n",
    "- **Value-at-Risk (VaR)**: Maximum loss at a given confidence level\n",
    "- **Expected Shortfall (ES)**: Average loss beyond VaR (also called CVaR)\n",
    "\n",
    "Portfolio weights: 50% Tech, 30% Bonds, 20% Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define portfolio weights\n",
    "weights = {'tech': 0.50, 'bonds': 0.30, 'gold': 0.20}\n",
    "portfolio_value = 1_000_000  # $1M portfolio\n",
    "\n",
    "# Calculate portfolio returns for each scenario\n",
    "scenarios_df['portfolio_return'] = (\n",
    "    scenarios_df['tech'] * weights['tech'] +\n",
    "    scenarios_df['bonds'] * weights['bonds'] +\n",
    "    scenarios_df['gold'] * weights['gold']\n",
    ")\n",
    "\n",
    "# Convert to dollar P&L\n",
    "scenarios_df['pnl'] = scenarios_df['portfolio_return'] * portfolio_value\n",
    "\n",
    "print(scenarios_df[['portfolio_return', 'pnl']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VaR and Expected Shortfall\n",
    "confidence_levels = [0.95, 0.99]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"PORTFOLIO RISK REPORT\")\n",
    "print(f\"Portfolio Value: ${portfolio_value:,.0f}\")\n",
    "print(f\"Allocation: Tech {weights['tech']:.0%}, Bonds {weights['bonds']:.0%}, Gold {weights['gold']:.0%}\")\n",
    "print(f\"Scenarios: {n_scenarios:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for conf in confidence_levels:\n",
    "    # VaR is the quantile of losses (negative returns)\n",
    "    var_return = scenarios_df['portfolio_return'].quantile(1 - conf)\n",
    "    var_dollar = var_return * portfolio_value\n",
    "    \n",
    "    # Expected Shortfall is the average of losses beyond VaR\n",
    "    es_returns = scenarios_df[scenarios_df['portfolio_return'] <= var_return]['portfolio_return']\n",
    "    es_return = es_returns.mean()\n",
    "    es_dollar = es_return * portfolio_value\n",
    "    \n",
    "    print(f\"\\n{conf:.0%} Confidence Level:\")\n",
    "    print(f\"  Value-at-Risk (VaR):      {var_return:>8.2%}  (${-var_dollar:>12,.0f})\")\n",
    "    print(f\"  Expected Shortfall (ES):  {es_return:>8.2%}  (${-es_dollar:>12,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio return distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of returns\n",
    "ax1 = axes[0]\n",
    "ax1.hist(scenarios_df['portfolio_return'], bins=100, density=True, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Mark VaR levels\n",
    "var_95 = scenarios_df['portfolio_return'].quantile(0.05)\n",
    "var_99 = scenarios_df['portfolio_return'].quantile(0.01)\n",
    "ax1.axvline(var_95, color='orange', linestyle='--', linewidth=2, label=f'95% VaR: {var_95:.2%}')\n",
    "ax1.axvline(var_99, color='red', linestyle='--', linewidth=2, label=f'99% VaR: {var_99:.2%}')\n",
    "ax1.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax1.set_xlabel('Daily Return')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Portfolio Return Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# P&L distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(scenarios_df['pnl'] / 1000, bins=100, density=True, alpha=0.7, edgecolor='black', color='green')\n",
    "\n",
    "var_95_dollar = var_95 * portfolio_value / 1000\n",
    "var_99_dollar = var_99 * portfolio_value / 1000\n",
    "ax2.axvline(var_95_dollar, color='orange', linestyle='--', linewidth=2, label=f'95% VaR: ${var_95_dollar:,.0f}K')\n",
    "ax2.axvline(var_99_dollar, color='red', linestyle='--', linewidth=2, label=f'99% VaR: ${var_99_dollar:,.0f}K')\n",
    "ax2.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax2.set_xlabel('Daily P&L ($K)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Portfolio P&L Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Part 6: Stress Testing - Correlation Breakdown\n",
    "\n",
    "During market stress, correlations often increase (\"correlations go to 1 in a crisis\").\n",
    "Let's simulate what happens if our diversification benefits disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress test: What if correlations increase during crisis?\n",
    "# In market stress, correlations often spike (\"correlation breakdown\")\n",
    "\n",
    "# Compare normal vs stressed VaR by resampling with modified correlation\n",
    "# Note: For production stress testing, you would fit to historical crisis periods\n",
    "\n",
    "print(\"Stress Test Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"During market stress, asset correlations typically increase,\")\n",
    "print(\"meaning diversification benefits may be reduced when needed most.\")\n",
    "print()\n",
    "print(\"Current Portfolio VaR:\")\n",
    "print(f\"  95% VaR: {var_95:.2%}\")\n",
    "print(f\"  99% VaR: {var_99:.2%}\")\n",
    "print()\n",
    "print(\"For stress testing, consider:\")\n",
    "print(\"  - Fitting copula to crisis-period data (e.g., 2008, 2020)\")\n",
    "print(\"  - Using higher correlation assumptions (+0.2 to +0.4)\")\n",
    "print(\"  - Applying tail-dependent copulas (Clayton, Gumbel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete Monte Carlo risk simulation workflow using LocalBackend:\n",
    "\n",
    "1. **Distribution Fitting**: Used `DistributionFitter` with multi-column fitting and `lazy_metrics=True`\n",
    "2. **Correlation Modeling**: Applied `GaussianCopula` to preserve asset dependencies\n",
    "3. **Scenario Generation**: Generated 10,000 correlated scenarios\n",
    "4. **Risk Metrics**: Calculated VaR and Expected Shortfall at multiple confidence levels\n",
    "5. **Stress Testing**: Analyzed impact of correlation breakdown\n",
    "\n",
    "### Key spark-bestfit Features Used\n",
    "\n",
    "| Feature | Purpose |\n",
    "|---------|----------|\n",
    "| `LocalBackend` | Local parallel processing |\n",
    "| Multi-column fitting | Fit all assets in one call |\n",
    "| `lazy_metrics=True` | Fast fitting for model selection |\n",
    "| `GaussianCopula` | Model correlations between assets |\n",
    "| `copula.sample()` | Generate correlated scenarios |\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Scale**: For larger portfolios, consider using SparkBackend or RayBackend\n",
    "- **Data**: Replace synthetic data with real historical returns\n",
    "- **Validation**: Backtest VaR estimates against realized losses\n",
    "- **Frequency**: Consider weekly/monthly horizons for longer-term risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup complete\n",
    "print(\"Monte Carlo risk simulation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
