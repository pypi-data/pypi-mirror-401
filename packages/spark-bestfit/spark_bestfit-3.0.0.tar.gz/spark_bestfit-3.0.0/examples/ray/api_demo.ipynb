{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# spark-bestfit API Demo (Ray Backend)\n",
    "\n",
    "This notebook demonstrates the complete API for the `spark-bestfit` library using the **RayBackend**, including:\n",
    "\n",
    "1. **Distribution Fitting** - Using DistributionFitter with RayBackend\n",
    "2. **FitterConfig Builder** - Fluent configuration API for complex setups (v2.2.0+)\n",
    "3. **Progress Tracking** - Monitor long-running fits with callbacks\n",
    "4. **Working with Results** - FitResults and DistributionFitResult objects\n",
    "5. **Lazy Metrics** - Skip KS/AD computation for faster model selection (v1.5.0+)\n",
    "6. **Pre-filtering** - Skip incompatible distributions for faster fitting (v1.6.0+)\n",
    "7. **Confidence Intervals** - Bootstrap confidence intervals for fitted parameters\n",
    "8. **Plotting** - Visualization with customizable parameters\n",
    "9. **Excluding Distributions** - Customizing which distributions to fit\n",
    "10. **Serialization** - Save and load fitted distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's initialize a RayBackend. Unlike Spark, Ray can auto-initialize locally.\n",
    "\n",
    "**RayBackend initialization options:**\n",
    "- `RayBackend()` - Auto-initialize Ray locally (simplest)\n",
    "- `RayBackend(num_cpus=4)` - Limit CPU usage\n",
    "- `RayBackend(address=\"auto\")` - Connect to existing Ray cluster\n",
    "- `RayBackend(address=\"ray://cluster:10001\")` - Connect to specific cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spark_bestfit import RayBackend\n",
    "\n",
    "# Create RayBackend (auto-initializes Ray locally)\n",
    "backend = RayBackend()\n",
    "\n",
    "print(f\"RayBackend initialized with {backend.get_parallelism()} CPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spark-bestfit components\n",
    "from spark_bestfit import (\n",
    "    DistributionFitter,\n",
    "    FitterConfig,\n",
    "    FitterConfigBuilder,\n",
    "    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = pd.DataFrame({\"value\": normal_data})\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = pd.DataFrame({\"value\": exp_data})\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = pd.DataFrame({\"value\": gamma_data})\n",
    "\n",
    "print(f\"Normal data: {len(df_normal):,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {len(df_exp):,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {len(df_gamma):,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Excluded Distributions\n",
    "\n",
    "spark-bestfit excludes some slow distributions by default. You can customize this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 1.1 DEFAULT_EXCLUDED_DISTRIBUTIONS\n",
    "\n",
    "Some distributions are excluded by default because they are very slow to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View default excluded distributions\n",
    "print(f\"Default excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\n",
    "for dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n",
    "    print(f\"  - {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a specific distribution that's excluded by default\n",
    "custom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n",
    "\n",
    "fitter_with_wald = DistributionFitter(backend=backend, excluded_distributions=custom_exclusions)\n",
    "print(f\"Now fitting 'wald' distribution (removed from exclusions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 2.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fitter with RayBackend\n",
    "fitter = DistributionFitter(backend=backend)\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 2.2 Fitting with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only non-negative distributions using support_at_zero=True\n",
    "fitter_nonneg = DistributionFitter(backend=backend)\n",
    "\n",
    "print(\"Fitting non-negative distributions to exponential data...\")\n",
    "results_exp = fitter_nonneg.fit(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    bins=100,\n",
    "    support_at_zero=True,  # Only fit non-negative distributions\n",
    "    enable_sampling=True,\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_exp.count()} non-negative distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 2.3 FitterConfig Builder (v2.2.0+)\n",
    "\n",
    "For complex configurations with many parameters, use the **fluent builder pattern**. This provides:\n",
    "- **Cleaner code**: Grouped, readable configuration\n",
    "- **Reusability**: Same config works across multiple fits  \n",
    "- **IDE-friendly**: Better autocomplete and discoverability\n",
    "- **Immutable**: Frozen dataclass prevents accidental mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a reusable configuration with the fluent builder\n",
    "config = (FitterConfigBuilder()\n",
    "    .with_bins(100)                           # Histogram bins\n",
    "    .with_support_at_zero()                   # Non-negative distributions only\n",
    "    .with_sampling(fraction=0.5)              # Sample 50% of data\n",
    "    .with_max_distributions(15)               # Limit to 15 distributions\n",
    "    .with_lazy_metrics()                      # Defer KS/AD computation\n",
    "    .build())\n",
    "\n",
    "print(\"FitterConfig created:\")\n",
    "print(f\"  bins: {config.bins}\")\n",
    "print(f\"  support_at_zero: {config.support_at_zero}\")\n",
    "print(f\"  sample_fraction: {config.sample_fraction}\")\n",
    "print(f\"  max_distributions: {config.max_distributions}\")\n",
    "print(f\"  lazy_metrics: {config.lazy_metrics}\")\n",
    "\n",
    "# Use config with fitter\n",
    "fitter_config = DistributionFitter(backend=backend)\n",
    "results_config = fitter_config.fit(df_exp, column=\"value\", config=config)\n",
    "\n",
    "print(f\"\\nFitted {results_config.count()} distributions using FitterConfig\")\n",
    "\n",
    "# Config is reusable - use same config for different data\n",
    "results_gamma_config = fitter_config.fit(df_gamma, column=\"value\", config=config)\n",
    "print(f\"Fitted {results_gamma_config.count()} distributions (reused same config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FitterConfig with bounded fitting and prefilter\n",
    "bounded_config = (FitterConfigBuilder()\n",
    "    .with_bounds(lower=0, upper=100)          # Explicit bounds\n",
    "    .with_prefilter()                          # Skip incompatible distributions\n",
    "    .with_lazy_metrics()                       # Fast model selection\n",
    "    .with_max_distributions(20)\n",
    "    .build())\n",
    "\n",
    "print(\"Bounded FitterConfig:\")\n",
    "print(f\"  bounded: {bounded_config.bounded}\")\n",
    "print(f\"  lower_bound: {bounded_config.lower_bound}\")\n",
    "print(f\"  upper_bound: {bounded_config.upper_bound}\")\n",
    "print(f\"  prefilter: {bounded_config.prefilter}\")\n",
    "\n",
    "# You can also create FitterConfig directly (without builder)\n",
    "direct_config = FitterConfig(\n",
    "    bins=50,\n",
    "    lazy_metrics=True,\n",
    "    max_distributions=10,\n",
    ")\n",
    "print(f\"\\nDirect FitterConfig: bins={direct_config.bins}, lazy={direct_config.lazy_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 2.4 Progress Tracking\n",
    "\n",
    "For long-running fits, you can monitor progress with a callback. The easiest way is to use the built-in `console_progress()` utility:\n",
    "\n",
    "```python\n",
    "from spark_bestfit.progress import console_progress\n",
    "\n",
    "results = fitter.fit(df, column=\"value\", progress_callback=console_progress())\n",
    "```\n",
    "\n",
    "For custom callbacks, pass any function matching `(completed: int, total: int, percent: float) -> None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit.progress import console_progress\n",
    "\n",
    "# Simple approach: use built-in console_progress()\n",
    "print(\"Fitting with console_progress()...\")\n",
    "fitter_progress = DistributionFitter(backend=backend)\n",
    "results_progress = fitter_progress.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=25,\n",
    "    progress_callback=console_progress(\"Fitting\"),  # Built-in utility\n",
    ")\n",
    "print()  # Newline after progress\n",
    "print(f\"Fitted {results_progress.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 2.5 Using Ray Dataset (Distributed)\n",
    "\n",
    "For larger datasets, you can use Ray Dataset for distributed aggregation. This avoids collecting raw data to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Create a larger dataset\n",
    "large_data = np.random.normal(loc=100, scale=25, size=100_000)\n",
    "large_df = pd.DataFrame({\"value\": large_data})\n",
    "\n",
    "# Convert to Ray Dataset\n",
    "ds = ray.data.from_pandas(large_df)\n",
    "print(f\"Ray Dataset created with {ds.count()} rows\")\n",
    "\n",
    "# Fit using Ray Dataset - distributed histogram and sampling\n",
    "results_ray = fitter.fit(ds, column=\"value\", max_distributions=15)\n",
    "\n",
    "best_ray = results_ray.best(n=1)[0]\n",
    "print(f\"Best fit: {best_ray.distribution}\")\n",
    "print(f\"  KS statistic: {best_ray.ks_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 3.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best distribution (by K-S statistic, the default)\n",
    "best = results_normal.best(n=1)[0]\n",
    "print(f\"Best by K-S statistic: {best.distribution}\")\n",
    "print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "print(f\"  A-D statistic: {best.ad_statistic:.6f}\")\n",
    "print(f\"  A-D p-value: {best.ad_pvalue:.4f}\" if best.ad_pvalue else f\"  A-D p-value: N/A (not available for {best.distribution})\")\n",
    "print(f\"  SSE: {best.sse:.6f}\")\n",
    "print(f\"  AIC: {best.aic:.2f}\")\n",
    "print(f\"  BIC: {best.bic:.2f}\")\n",
    "print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 by different metrics\n",
    "print(\"Top 5 by K-S statistic (default):\")\n",
    "for i, r in enumerate(results_normal.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 by A-D statistic:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"ad_statistic\"), 1):\n",
    "    ad_p = f\"{r.ad_pvalue:.4f}\" if r.ad_pvalue else \"N/A\"\n",
    "    print(f\"  {i}. {r.distribution:20s} AD={r.ad_statistic:.6f} p={ad_p}\")\n",
    "\n",
    "print(\"\\nTop 5 by SSE:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 by AIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 3.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by K-S statistic threshold\n",
    "good_fits = results_normal.filter(ks_threshold=0.05)\n",
    "print(f\"Distributions with K-S statistic < 0.05: {good_fits.count()}\")\n",
    "\n",
    "for r in good_fits.best(n=10):\n",
    "    print(f\"  {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n",
    "\n",
    "# Filter by p-value threshold (keep distributions with p-value > 0.05)\n",
    "significant = results_normal.filter(pvalue_threshold=0.05)\n",
    "print(f\"\\nDistributions with p-value > 0.05: {significant.count()}\")\n",
    "\n",
    "# Filter by A-D statistic threshold\n",
    "good_ad = results_normal.filter(ad_threshold=2.0)\n",
    "print(f\"\\nDistributions with A-D statistic < 2.0: {good_ad.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 3.3 Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# Convert to pandas DataFrame for further analysis\n# For RayBackend, results.df is already a pandas DataFrame\ndf_results = results_normal.df\nprint(\"Results as pandas DataFrame:\")\ndf_results.head(10)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 3.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DistributionFitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 3.5 Parameter Confidence Intervals\n",
    "\n",
    "Compute bootstrap confidence intervals for fitted distribution parameters. This is useful for understanding the uncertainty in your parameter estimates.\n",
    "\n",
    "**Note**: CI width depends on sample size and distribution identifiability. Highly flexible distributions (like beta with 4 parameters) may have wider CIs due to parameter identifiability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exponential fit for CI demo (simpler distribution = more stable CIs)\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "\n",
    "print(f\"Distribution: {best_exp.distribution}\")\n",
    "print(f\"Parameter names: {best_exp.get_param_names()}\")\n",
    "print(f\"Fitted values: {[f'{p:.4f}' for p in best_exp.parameters]}\")\n",
    "\n",
    "# Compute 95% bootstrap confidence intervals\n",
    "print(\"\\nComputing 95% confidence intervals (this may take a few seconds)...\")\n",
    "ci = best_exp.confidence_intervals(\n",
    "    df_exp,\n",
    "    column=\"value\",\n",
    "    alpha=0.05,           # 95% CI\n",
    "    n_bootstrap=500,      # Number of bootstrap samples (use 1000+ for production)\n",
    "    random_seed=42,       # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nParameter confidence intervals:\")\n",
    "for param, (lower, upper) in ci.items():\n",
    "    print(f\"  {param}: [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## 3.6 Lazy Metrics for Fast Model Selection (v1.5.0+)\n",
    "\n",
    "When fitting ~100 distributions, computing KS and AD statistics for all of them can be slow. With **lazy metrics**, these expensive computations are skipped during fitting and only computed on-demand when you actually need them.\n",
    "\n",
    "**Key benefits:**\n",
    "- Fast initial fitting (skip KS/AD computation)\n",
    "- On-demand computation only for distributions you access\n",
    "- Ideal for model selection workflows using AIC/BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with lazy metrics - KS/AD statistics are NOT computed during fitting\n",
    "print(\"Fitting with lazy_metrics=True (fast)...\")\n",
    "fitter_lazy = DistributionFitter(backend=backend)\n",
    "results_lazy = fitter_lazy.fit(\n",
    "    df_normal,\n",
    "    column=\"value\",\n",
    "    max_distributions=20,\n",
    "    lazy_metrics=True,  # Skip KS/AD computation!\n",
    ")\n",
    "\n",
    "print(f\"Fitted {results_lazy.count()} distributions\")\n",
    "print(f\"Is lazy: {results_lazy.is_lazy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by AIC - fast! No KS/AD computation needed\n",
    "best_aic = results_lazy.best(n=1, metric=\"aic\")[0]\n",
    "\n",
    "print(f\"Best by AIC: {best_aic.distribution}\")\n",
    "print(f\"  AIC: {best_aic.aic:.2f}\")\n",
    "print(f\"  BIC: {best_aic.bic:.2f}\")\n",
    "print(f\"  KS statistic: {best_aic.ks_statistic}\")  # None - not computed yet!\n",
    "print(f\"  AD statistic: {best_aic.ad_statistic}\")  # None - not computed yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best by KS statistic - triggers ON-DEMAND computation!\n",
    "# Only computes KS/AD for top candidates, not all distributions\n",
    "best_ks = results_lazy.best(n=1, metric=\"ks_statistic\")[0]\n",
    "\n",
    "print(f\"Best by KS: {best_ks.distribution}\")\n",
    "print(f\"  KS statistic: {best_ks.ks_statistic:.6f}\")  # Computed on-demand!\n",
    "print(f\"  p-value: {best_ks.pvalue:.4f}\")\n",
    "print(f\"  AD statistic: {best_ks.ad_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need all metrics computed (e.g., before unpersisting source data),\n",
    "# use materialize() to force computation of all KS/AD statistics\n",
    "materialized = results_lazy.materialize()\n",
    "\n",
    "print(f\"Is lazy after materialize: {materialized.is_lazy}\")  # False\n",
    "\n",
    "# Now all distributions have KS/AD computed\n",
    "top_3 = materialized.best(n=3, metric=\"ks_statistic\")\n",
    "print(\"\\nTop 3 distributions (all metrics available):\")\n",
    "for i, r in enumerate(top_3, 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## 3.7 Pre-filtering Distributions (v1.6.0+)\n",
    "\n",
    "When you know something about your data, you can skip distributions that are mathematically incompatible. Pre-filtering uses data characteristics (support bounds, skewness, kurtosis) to eliminate distributions before the expensive fitting step.\n",
    "\n",
    "**Filtering layers:**\n",
    "1. **Support bounds (100% reliable)**: Skips distributions whose support doesn't contain your data range\n",
    "2. **Skewness sign (95% reliable)**: Skips positive-skew-only distributions for left-skewed data  \n",
    "3. **Kurtosis (aggressive mode, ~80% reliable)**: Skips low-kurtosis distributions for heavy-tailed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative data (will filter out non-negative distributions like expon, gamma)\n",
    "np.random.seed(42)\n",
    "negative_data = np.random.normal(loc=-50, scale=10, size=10_000)\n",
    "df_negative = pd.DataFrame({\"value\": negative_data})\n",
    "\n",
    "print(f\"Data range: [{negative_data.min():.1f}, {negative_data.max():.1f}]\")\n",
    "print(f\"All values are negative - expon/gamma distributions cannot fit this data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITHOUT prefilter (baseline)\n",
    "print(\"Fitting WITHOUT prefilter...\")\n",
    "fitter_no_prefilter = DistributionFitter(backend=backend)\n",
    "results_no_prefilter = fitter_no_prefilter.fit(\n",
    "    df_negative, \n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=False,  # Default - fit all distributions\n",
    ")\n",
    "print(f\"Fitted {results_no_prefilter.count()} distributions (no prefilter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit WITH prefilter (safe mode) - skips incompatible distributions\n",
    "print(\"\\nFitting WITH prefilter=True (safe mode)...\")\n",
    "fitter_prefilter = DistributionFitter(backend=backend)\n",
    "results_prefilter = fitter_prefilter.fit(\n",
    "    df_negative,\n",
    "    column=\"value\", \n",
    "    max_distributions=20,\n",
    "    prefilter=True,  # Enable pre-filtering!\n",
    ")\n",
    "print(f\"Fitted {results_prefilter.count()} distributions (with prefilter)\")\n",
    "print(\"\\n-> Pre-filter skipped distributions incompatible with negative data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best fits - both should find norm as best for normal data\n",
    "best_no_prefilter = results_no_prefilter.best(n=1)[0]\n",
    "best_prefilter = results_prefilter.best(n=1)[0]\n",
    "\n",
    "print(\"Best distribution comparison:\")\n",
    "print(f\"  Without prefilter: {best_no_prefilter.distribution} (KS={best_no_prefilter.ks_statistic:.6f})\")\n",
    "print(f\"  With prefilter:    {best_prefilter.distribution} (KS={best_prefilter.ks_statistic:.6f})\")\n",
    "print(\"\\n-> Same best fit, but prefilter was faster by skipping incompatible distributions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "## 4.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "best = results_normal.best(n=1)[0]\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "## 4.2 Plot with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plot with direct parameters\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=12,\n",
    "    grid_alpha=0.4,\n",
    "    title=\"Distribution Fit with Custom Styling\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "## 4.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best fit for exponential data\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "print(f\"Best fit for exponential data: {best_exp.distribution}\")\n",
    "print(f\"  K-S statistic: {best_exp.ks_statistic:.6f}\")\n",
    "print(f\"  p-value: {best_exp.pvalue:.4f}\")\n",
    "\n",
    "fig, ax = fitter_nonneg.plot(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## 4.4 Q-Q Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A Q-Q (quantile-quantile) plot is a powerful visual tool for assessing how well a distribution fits your data. It plots sample quantiles against theoretical quantiles from the fitted distribution. If the fit is good, points will fall approximately along the diagonal reference line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_qq(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: Q-Q plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_qq(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"Q-Q Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "## 4.5 P-P Plots for Goodness-of-Fit Assessment\n",
    "\n",
    "A P-P (probability-probability) plot compares the empirical cumulative distribution function (CDF) of the sample data against the theoretical CDF of the fitted distribution. It is particularly useful for assessing the fit in the center of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-P plot for the best fit on normal data\n",
    "fig, ax = fitter.plot_pp(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Normal Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Compare: P-P plot for exponential data\n",
    "fig, ax = fitter_nonneg.plot_pp(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    max_points=1000,\n",
    "    figsize=(10, 10),\n",
    "    title=\"P-P Plot: Exponential Data vs Fitted Distribution\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Multi-Column Fitting\n",
    "\n",
    "Fit multiple columns efficiently in a single operation. This shares overhead across all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-58",
   "metadata": {},
   "source": [
    "## 5.1 Create Multi-Column DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with multiple columns (different distributions)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data from different distributions\n",
    "n_rows = 20_000\n",
    "df_multi = pd.DataFrame({\n",
    "    \"normal_col\": np.random.normal(50, 10, n_rows),\n",
    "    \"exp_col\": np.random.exponential(5, n_rows),\n",
    "    \"gamma_col\": np.random.gamma(2.0, 2.0, n_rows),\n",
    "})\n",
    "\n",
    "print(f\"Created DataFrame with {len(df_multi):,} rows and columns: {df_multi.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "## 5.2 Fit Multiple Columns in One Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions to all columns in a single operation\n",
    "# This is more efficient than fitting each column separately\n",
    "print(\"Fitting distributions to 3 columns simultaneously...\")\n",
    "\n",
    "fitter_multi = DistributionFitter(backend=backend)\n",
    "results_multi = fitter_multi.fit(\n",
    "    df_multi,\n",
    "    columns=[\"normal_col\", \"exp_col\", \"gamma_col\"],  # Multi-column fitting!\n",
    "    max_distributions=15,\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal results: {results_multi.count()}\")\n",
    "print(f\"Columns fitted: {results_multi.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "## 5.3 Get Best Distribution Per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best distribution for each column\n",
    "best_per_col = results_multi.best_per_column(n=1)\n",
    "\n",
    "print(\"Best distribution per column:\")\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    print(f\"  Distribution: {best.distribution}\")\n",
    "    print(f\"  K-S statistic: {best.ks_statistic:.6f}\")\n",
    "    print(f\"  p-value: {best.pvalue:.4f}\")\n",
    "    print(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-64",
   "metadata": {},
   "source": [
    "## 5.4 Filter Results by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for a specific column\n",
    "exp_results = results_multi.for_column(\"exp_col\")\n",
    "\n",
    "print(f\"Results for 'exp_col': {exp_results.count()} distributions\")\n",
    "print(\"\\nTop 5 by K-S statistic:\")\n",
    "for i, r in enumerate(exp_results.best(n=5), 1):\n",
    "    print(f\"  {i}. {r.distribution:15} KS={r.ks_statistic:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-66",
   "metadata": {},
   "source": [
    "## 5.5 Plot Results for Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit for each column\n",
    "for col_name, fits in best_per_col.items():\n",
    "    best = fits[0]\n",
    "    fig, ax = fitter_multi.plot(\n",
    "        best,\n",
    "        df_multi,\n",
    "        col_name,\n",
    "        title=f\"{col_name}: {best.distribution} (KS={best.ks_statistic:.4f})\",\n",
    "        xlabel=\"Value\",\n",
    "        ylabel=\"Density\",\n",
    "        figsize=(10, 6),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Complete Workflow Example\n",
    "\n",
    "Putting it all together - a complete production-style workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-69",
   "metadata": {},
   "outputs": [],
   "source": "# Complete workflow with all parameters\nfitter_gamma = DistributionFitter(backend=backend, random_seed=42)\n\n# Fit distributions\nprint(\"Fitting gamma distribution data...\")\nresults = fitter_gamma.fit(\n    df_gamma,\n    column=\"value\",\n    bins=100,\n    use_rice_rule=False,\n    enable_sampling=True,\n    max_sample_size=1_000_000,\n    max_distributions=25,\n)\n\n# Get best result\nbest = results.best(n=1)[0]\nprint(f\"\\nBest distribution: {best.distribution}\")\nprint(f\"K-S statistic: {best.ks_statistic:.6f}\")\nprint(f\"p-value: {best.pvalue:.4f}\")\nprint(f\"SSE: {best.sse:.6f}\")\nprint(f\"Parameters: {[f'{p:.4f}' for p in best.parameters]}\")\n\n# Plot with custom parameters\nfig, ax = fitter_gamma.plot(\n    best,\n    df_gamma,\n    \"value\",\n    figsize=(14, 9),\n    dpi=150,\n    histogram_alpha=0.6,\n    pdf_linewidth=3,\n    title_fontsize=16,\n    title=f\"Gamma Data - Best Fit: {best.distribution.capitalize()}\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()\n\n# Show top 5 results (sorted by K-S statistic for meaningful ranking)\n# For RayBackend, results.df is already a pandas DataFrame\nprint(\"\\nTop 5 distributions:\")\ndf_top5 = results.df.sort_values(\"ks_statistic\").head(5)\ndf_top5[[\"distribution\", \"ks_statistic\", \"pvalue\", \"sse\", \"aic\", \"bic\"]]"
  },
  {
   "cell_type": "markdown",
   "id": "cell-70",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Serialization\n",
    "\n",
    "Save fitted distributions to disk and reload them later for inference without re-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-71",
   "metadata": {},
   "source": [
    "## 7.1 Save and Load\n",
    "\n",
    "Save a fitted distribution to JSON (default) or pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the best fit from Part 6\n",
    "print(f\"Saving distribution: {best.distribution}\")\n",
    "print(f\"Parameters: {best.parameters}\")\n",
    "\n",
    "# Save to a temporary directory\n",
    "model_dir = Path(tempfile.mkdtemp())\n",
    "json_path = model_dir / \"best_model.json\"\n",
    "pkl_path = model_dir / \"best_model.pkl\"\n",
    "\n",
    "# Save as JSON (human-readable, default)\n",
    "best.save(json_path)\n",
    "print(f\"\\nSaved to JSON: {json_path}\")\n",
    "print(f\"File size: {json_path.stat().st_size:,} bytes\")\n",
    "\n",
    "# Save as pickle (binary, faster)\n",
    "best.save(pkl_path, format=\"pickle\")\n",
    "print(f\"\\nSaved to pickle: {pkl_path}\")\n",
    "print(f\"File size: {pkl_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit import DistributionFitResult\n",
    "\n",
    "# Load the saved model\n",
    "loaded = DistributionFitResult.load(json_path)\n",
    "\n",
    "print(f\"Loaded distribution: {loaded.distribution}\")\n",
    "print(f\"Parameters: {loaded.parameters}\")\n",
    "print(f\"K-S statistic: {loaded.ks_statistic:.6f}\")\n",
    "print(f\"p-value: {loaded.pvalue:.4f}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "samples = loaded.sample(size=1000, random_state=42)\n",
    "print(f\"\\nGenerated {len(samples)} samples from loaded model\")\n",
    "print(f\"Sample mean: {samples.mean():.2f}\")\n",
    "print(f\"Sample std: {samples.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-74",
   "metadata": {},
   "source": [
    "## 7.2 Data Summary\n",
    "\n",
    "When fitting with `DistributionFitter`, the result includes data statistics\n",
    "about the fitted data. This provides lightweight provenance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data statistics from the loaded model (v2.0+ flat field API)\n",
    "if loaded.data_count is not None:\n",
    "    print(\"Data Statistics (from fitting):\")\n",
    "    print(f\"  Sample size: {loaded.data_count:,.0f}\")\n",
    "    print(f\"  Min: {loaded.data_min:.4f}\")\n",
    "    print(f\"  Max: {loaded.data_max:.4f}\")\n",
    "    print(f\"  Mean: {loaded.data_mean:.4f}\")\n",
    "    print(f\"  Std: {loaded.data_stddev:.4f}\")\n",
    "else:\n",
    "    print(\"No data statistics available (result may have been created manually)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-76",
   "metadata": {},
   "source": [
    "## 7.3 JSON Format\n",
    "\n",
    "The JSON format is human-readable and includes version metadata for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the JSON content\n",
    "with open(json_path) as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"JSON file content:\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(model_dir)\n",
    "print(f\"Cleaned up temporary directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Ray-Specific Features\n",
    "\n",
    "These features are unique to the RayBackend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-80",
   "metadata": {},
   "source": [
    "## 8.1 Discrete Distribution Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit import DiscreteDistributionFitter\n",
    "\n",
    "# Generate count data\n",
    "count_data = np.random.poisson(lam=12, size=3000)\n",
    "count_df = pd.DataFrame({\"events\": count_data})\n",
    "\n",
    "# Convert to Ray Dataset\n",
    "count_ds = ray.data.from_pandas(count_df)\n",
    "\n",
    "# Fit discrete distributions\n",
    "discrete_fitter = DiscreteDistributionFitter(backend=backend)\n",
    "discrete_results = discrete_fitter.fit(count_ds, column=\"events\", max_distributions=5)\n",
    "\n",
    "# Best by AIC (recommended for discrete)\n",
    "best_discrete = discrete_results.best(n=1, metric=\"aic\")[0]\n",
    "print(f\"Best discrete: {best_discrete.distribution}\")\n",
    "print(f\"  Parameters: {dict(zip(best_discrete.get_param_names(), best_discrete.parameters))}\")\n",
    "print(f\"  AIC: {best_discrete.aic:.2f}\")\n",
    "print(f\"  Data mean: {count_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-82",
   "metadata": {},
   "source": [
    "## 8.2 Gaussian Copula with RayBackend\n",
    "\n",
    "The Gaussian Copula uses RayBackend for distributed correlation computation and sample generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_bestfit import GaussianCopula\n",
    "\n",
    "# Create correlated data\n",
    "n = 5000\n",
    "x = np.random.normal(0, 1, n)\n",
    "correlated_data = pd.DataFrame({\n",
    "    \"feature_a\": x * 10 + 50,  # Normal-like\n",
    "    \"feature_b\": np.exp(0.5 * x + np.random.normal(0, 0.3, n)),  # Log-normal-like\n",
    "    \"feature_c\": np.abs(x) * 20 + np.random.exponential(5, n),  # Right-skewed\n",
    "})\n",
    "\n",
    "# Fit marginal distributions\n",
    "marginal_results = fitter.fit(\n",
    "    correlated_data, \n",
    "    columns=[\"feature_a\", \"feature_b\", \"feature_c\"],\n",
    "    max_distributions=10\n",
    ")\n",
    "\n",
    "print(\"Marginal fits:\")\n",
    "for col, fits in marginal_results.best_per_column(n=1).items():\n",
    "    print(f\"  {col}: {fits[0].distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit copula (correlation computed via RayBackend)\n",
    "copula = GaussianCopula.fit(marginal_results, correlated_data, backend=backend)\n",
    "\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(copula.correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated samples\n",
    "samples = copula.sample(n=1000)\n",
    "\n",
    "# Verify correlation is preserved\n",
    "sample_df = pd.DataFrame(samples)\n",
    "print(\"Sample correlation matrix:\")\n",
    "print(sample_df.corr().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-86",
   "metadata": {},
   "source": [
    "## 8.3 Distributed Sample Generation\n",
    "\n",
    "RayBackend's `generate_samples` distributes sample generation across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large sample using RayBackend\n",
    "best_fit = results_multi.for_column(\"normal_col\").best(n=1)[0]\n",
    "\n",
    "# Local sampling (small scale)\n",
    "local_samples = best_fit.sample(size=10000)\n",
    "print(f\"Local samples: mean={local_samples.mean():.2f}, std={local_samples.std():.2f}\")\n",
    "\n",
    "# For very large samples, use the backend's generate_samples\n",
    "def sample_generator(n_samples, partition_id, seed):\n",
    "    np.random.seed(seed)\n",
    "    return {\"value\": best_fit.sample(size=n_samples)}\n",
    "\n",
    "large_samples = backend.generate_samples(\n",
    "    n=100000,\n",
    "    generator_func=sample_generator,\n",
    "    column_names=[\"value\"],\n",
    "    num_partitions=4,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDistributed samples: {len(large_samples)} rows\")\n",
    "print(f\"  mean={large_samples['value'].mean():.2f}, std={large_samples['value'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-88",
   "metadata": {},
   "source": [
    "## 8.4 Performance Comparison: pandas vs Ray Dataset\n",
    "\n",
    "For small datasets, pandas is faster (no serialization overhead). For large datasets, Ray Dataset provides distributed aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Small dataset comparison\n",
    "small_data = pd.DataFrame({\"value\": np.random.exponential(5, 1000)})\n",
    "small_ds = ray.data.from_pandas(small_data)\n",
    "\n",
    "# pandas DataFrame\n",
    "start = time.time()\n",
    "_ = fitter.fit(small_data, column=\"value\", max_distributions=5)\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "# Ray Dataset\n",
    "start = time.time()\n",
    "_ = fitter.fit(small_ds, column=\"value\", max_distributions=5)\n",
    "ray_time = time.time() - start\n",
    "\n",
    "print(f\"Small dataset (1K rows):\")\n",
    "print(f\"  pandas: {pandas_time:.2f}s\")\n",
    "print(f\"  Ray Dataset: {ray_time:.2f}s\")\n",
    "print(f\"  Recommendation: Use pandas for small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-90",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Summary\n\nThis notebook demonstrated:\n\n1. **Excluded Distributions**:\n   - `DEFAULT_EXCLUDED_DISTRIBUTIONS` - Slow distributions excluded by default\n   - Pass custom `excluded_distributions` to `DistributionFitter()` to include/exclude\n\n2. **RayBackend Initialization**:\n   - `RayBackend()` - Auto-initialize locally\n   - `RayBackend(num_cpus=N)` - Limit CPU usage\n   - `RayBackend(address=\"auto\")` - Connect to existing cluster\n\n3. **Fitting**:\n   - `DistributionFitter.fit()` - Fit distributions to data\n   - Parameters: `bins`, `use_rice_rule`, `support_at_zero`, `enable_sampling`, etc.\n   - `max_distributions` parameter to limit fitting scope\n   - `progress_callback` parameter to monitor long-running fits\n\n4. **FitterConfig Builder (v2.2.0+)**:\n   - `FitterConfigBuilder()` - Fluent API for complex configurations\n   - Chain methods: `.with_bins()`, `.with_bounds()`, `.with_lazy_metrics()`, etc.\n   - `.build()` returns immutable `FitterConfig` dataclass\n   - Pass `config=config` to `fit()` for reusable configurations\n\n5. **Progress Tracking**:\n   - Pass `progress_callback=fn` to `fit()` to receive progress updates\n   - Callback receives `(completed_tasks, total_tasks, percent_complete)`\n   - Works with both `DistributionFitter` and `DiscreteDistributionFitter`\n\n6. **Results**:\n   - `results.best(n, metric)` - Get top N by K-S statistic (default), A-D statistic, SSE, AIC, or BIC\n   - `results.filter(ks_threshold, pvalue_threshold, ad_threshold)` - Filter by goodness-of-fit\n   - `results.df` - Access underlying pandas DataFrame (RayBackend uses pandas)\n   - `DistributionFitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n   - `DistributionFitResult.get_param_names()` - Get parameter names\n   - `DistributionFitResult.confidence_intervals()` - Bootstrap confidence intervals\n\n7. **Lazy Metrics (v1.5.0+)**:\n   - `lazy_metrics=True` - Skip KS/AD computation during fitting for faster iteration\n   - `results.is_lazy` - Check if results have lazy metrics\n   - `results.best(metric=\"ks_statistic\")` - Triggers on-demand computation for top candidates only\n   - `results.materialize()` - Force computation of all KS/AD statistics\n\n8. **Pre-filtering (v1.6.0+)**:\n   - `prefilter=True` - Skip distributions incompatible with your data (safe mode)\n   - `prefilter=\"aggressive\"` - Also filter by kurtosis for heavy-tailed data\n   - Uses support bounds, skewness sign, and kurtosis to eliminate distributions\n   - 30-70% fewer distributions to fit with automatic fallback\n\n9. **Multi-Column Fitting**:\n   - `fitter.fit(df, columns=[...])` - Fit multiple columns in one call\n   - `results.column_names` - List all fitted columns\n   - `results.for_column(name)` - Filter results to one column\n   - `results.best_per_column(n, metric)` - Get top N per column\n\n10. **Plotting**:\n    - `fitter.plot()` - Visualize fitted distribution with data histogram\n    - `fitter.plot_qq()` - Q-Q plot for visual goodness-of-fit assessment\n    - `fitter.plot_pp()` - P-P plot for assessing fit in the center of distribution\n    - Customizable with `figsize`, `dpi`, `histogram_alpha`, `pdf_linewidth`, etc.\n\n11. **Serialization**:\n    - `result.save(path)` - Save to JSON (default) or pickle format\n    - `DistributionFitResult.load(path)` - Load a saved result\n    - `result.data_min`, `result.data_max`, etc. - Access fitting statistics for provenance\n    - JSON format includes version metadata for compatibility\n\n12. **Goodness-of-Fit Metrics**:\n    - **K-S statistic** (default) - Lower is better, measures max distance from empirical CDF\n    - **A-D statistic** - Lower is better, more sensitive to tails than K-S\n    - **p-value** - Higher is better (>0.05 suggests good fit)\n    - **A-D p-value** - Only available for norm, expon, logistic, gumbel_r, gumbel_l\n    - **SSE** - Sum of squared errors between histogram and fitted PDF\n    - **AIC/BIC** - Information criteria for model comparison\n\n**When to use RayBackend:**\n\n| Scenario | Recommended Backend |\n|----------|---------------------|\n| Local development/testing | LocalBackend |\n| Spark cluster available | SparkBackend |\n| Ray cluster or Ray-based ML pipeline | RayBackend |\n| Data already in Ray Dataset | RayBackend |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray (optional - useful in notebooks)\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
