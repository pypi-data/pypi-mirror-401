[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "spark-bestfit"
version = "2.7.1"
description = "Modern distribution fitting library with Spark, Ray, and local parallel backends"
readme = "README.md"
requires-python = ">=3.11,<3.14"
license = {text = "MIT"}
authors = [
    {name = "Dustin Smith", email = "dustin.william.smith@gmail.com"},
]
keywords = ["spark", "pyspark", "distribution", "fitting", "statistics", "scipy"]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

dependencies = [
    "pandas>=1.5.0,<3.0.0",
    "numpy>=1.24.0,<3.0.0",
    "scipy>=1.11.0,<2.0.0",
]

[project.urls]
Documentation = "https://spark-bestfit.readthedocs.io/en/latest/"
Homepage = "https://github.com/dwsmith1983/spark-bestfit"
Repository = "https://github.com/dwsmith1983/spark-bestfit"
Issues = "https://github.com/dwsmith1983/spark-bestfit/issues"

[project.optional-dependencies]
# Runtime optional: PySpark (for users without a managed Spark environment)
spark = [
    "pyspark>=3.5.0,<5.0.0",
    "pyarrow>=12.0.0,<19.0.0",
]

# Runtime optional: Ray (for Ray cluster environments)
# Note: PyArrow is required by ray.data (Ray Datasets)
ray = [
    "ray[default]>=2.9.0,<3.0.0",
    "pyarrow>=12.0.0,<19.0.0",
]

# Runtime optional: Visualization (for built-in plotting)
plotting = [
    "matplotlib>=3.7.0,<4.0.0",
]

dev = [
    "pyspark>=3.5.0,<5.0.0",
    "pyarrow>=12.0.0,<19.0.0",
    "ray[default]>=2.9.0,<3.0.0",
    "matplotlib>=3.7.0,<4.0.0",
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "pytest-benchmark>=5.0.0",
    "hypothesis>=6.100.0",
    "mutmut>=2.4.0,<3.0.0",  # v3 trampoline mode causes segfaults with scipy
    "ruff>=0.8.0",
    "black>=24.10.0",
    "isort>=5.13.0",
    "mypy>=1.13.0",
    "pre-commit>=4.5.0",
    "scipy-stubs>=1.14.0",
    "pandas-stubs>=2.2.0",
]

# Test dependencies (subset of dev for CI)
test = [
    "pyspark>=3.5.0,<5.0.0",
    "pyarrow>=12.0.0,<19.0.0",
    "ray[default]>=2.9.0,<3.0.0",
    "matplotlib>=3.7.0,<4.0.0",
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "hypothesis>=6.100.0",
]

# Base test dependencies without pyspark/pyarrow
test-base = [
    "matplotlib>=3.7.0,<4.0.0",
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "hypothesis>=6.100.0",
]

# Documentation dependencies
docs = [
    "pyspark>=3.5.0,<5.0.0",
    "pyarrow>=12.0.0,<19.0.0",
    "sphinx>=8.0.0",
    "furo",
    "sphinx-copybutton",
]

[tool.hatch.build.targets.sdist]
include = ["/src", "/tests", "/examples", "/README.md"]

[tool.hatch.build.targets.wheel]
packages = ["src/spark_bestfit"]

[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["src"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
# Exclude benchmarks from normal test runs (run with: pytest tests/benchmarks/)
norecursedirs = ["tests/benchmarks"]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "benchmark: performance benchmarks (run with: make benchmark)",
    "spark: tests requiring Spark (skip for mutation testing with '-m \"not spark\"')",
]

[tool.coverage.run]
source = ["src/spark_bestfit"]
branch = true

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "if TYPE_CHECKING:",
    "if __name__ == .__main__.:",
]

# Hatch test environments for cross-version testing
[tool.hatch.envs.test]
dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
]

[tool.hatch.envs.test.scripts]
run = "pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-fail-under=85 -v {args}"

# Spark 3.5 with Python 3.11
[[tool.hatch.envs.test.matrix]]
python = ["3.11"]
spark = ["3.5"]

[tool.hatch.envs.test.overrides]
matrix.spark.dependencies = [
    { value = "pyspark>=3.5.0,<4.0.0", if = ["3.5"] },
    { value = "pyspark>=4.0.0,<5.0.0", if = ["4.0"] },
]

# Separate environments with explicit dependencies for each combo
[tool.hatch.envs.spark35-py311]
python = "3.11"
dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "pyspark>=3.5.0,<4.0.0",
    "numpy>=1.24.0,<2.0.0",
    "pandas>=1.5.0,<3.0.0",
    "pyarrow>=12.0.0,<17.0.0",
]

[tool.hatch.envs.spark35-py311.scripts]
test = "pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-fail-under=85 -v {args}"

[tool.hatch.envs.spark35-py312]
python = "3.12"
dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "pyspark>=3.5.0,<4.0.0",
    "numpy>=1.26.0,<2.0.0",
    "pandas>=2.0.0,<3.0.0",
    "pyarrow>=14.0.0,<17.0.0",
    "setuptools",  # Provides distutils for PySpark 3.5 on Python 3.12+
]

[tool.hatch.envs.spark35-py312.scripts]
test = "pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-fail-under=85 -v {args}"

[tool.hatch.envs.spark40-py312]
python = "3.12"
dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "pyspark>=4.0.0,<5.0.0",
    "numpy>=2.0.0,<3.0.0",
    "pandas>=2.2.0,<3.0.0",
    "pyarrow>=17.0.0,<19.0.0",
]

[tool.hatch.envs.spark40-py312.scripts]
test = "pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-fail-under=85 -v {args}"

[tool.hatch.envs.spark40-py313]
python = "3.13"
dependencies = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "pytest-spark>=0.6.0",
    "pyspark>=4.0.0,<5.0.0",
    "numpy>=2.0.0,<3.0.0",
    "pandas>=2.2.0,<3.0.0",
    "pyarrow>=17.0.0,<19.0.0",
]

[tool.hatch.envs.spark40-py313.scripts]
test = "pytest --cov=src/spark_bestfit --cov-report=term-missing --cov-fail-under=85 -v {args}"

# Semantic Release configuration
[tool.semantic_release]
version_toml = ["pyproject.toml:project.version"]
version_variables = ["src/spark_bestfit/_version.py:__version__"]
branch = "main"
allow_zero_version = true
build_command = "pip install hatch && hatch build"
commit_message = "chore(release): {version}\n\nAutomatically generated by python-semantic-release"

[tool.semantic_release.changelog]
exclude_commit_patterns = [
    "^chore.*",
    "^ci.*",
    "^docs.*",
    "^style.*",
]

[tool.semantic_release.changelog.default_templates]
changelog_file = "CHANGELOG.md"

[tool.semantic_release.branches.main]
match = "main"
prerelease = false

[tool.semantic_release.commit_parser_options]
allowed_tags = ["feat", "fix", "perf", "refactor", "build"]
minor_tags = ["feat"]
patch_tags = ["fix", "perf", "refactor", "build"]

# Mutation testing configuration
# See: https://mutmut.readthedocs.io/
# Note: Uses LocalBackend tests only (mutmut 3's trampoline is incompatible with distributed workers)
[tool.mutmut]
# Paths to mutate (core fitting logic)
paths_to_mutate = ["src/spark_bestfit/"]
# Exclude files with Spark-specific code that can't be tested via LocalBackend
do_not_mutate = [
    "src/spark_bestfit/backends/spark.py",
    "src/spark_bestfit/backends/ray.py",
    "src/spark_bestfit/progress.py",
    "src/spark_bestfit/plotting.py",
    "src/spark_bestfit/sampling.py",   # sample_spark deprecated wrapper uses Spark
    "src/spark_bestfit/protocols.py",  # Abstract ExecutionBackend protocol methods
    "src/spark_bestfit/utils.py",      # _get_spark_session and Spark utilities
]
# Test selection: ALL tests that work without Spark/Ray
pytest_add_cli_args_test_selection = [
    # Pure unit tests (numpy/scipy only)
    "tests/test_fitting.py",
    "tests/test_discrete_fitting.py",
    "tests/test_distributions.py",
    "tests/test_serialization.py",
    "tests/test_results.py",
    "tests/test_numerical_stability.py",
    "tests/test_config.py",
    # Note: test_property_based.py excluded - hypothesis incompatible with mutmut trampoline
    # Using LocalBackend
    "tests/test_core.py",
    "tests/test_histogram.py",
    "tests/test_backends.py",
    "tests/test_bounded_fitting.py",
    "tests/test_copula.py",
]
# Additional pytest args: disable spark plugin, skip spark tests, quiet output
pytest_add_cli_args = ["-p", "no:spark", "-m", "not spark", "-q", "--tb=no"]
