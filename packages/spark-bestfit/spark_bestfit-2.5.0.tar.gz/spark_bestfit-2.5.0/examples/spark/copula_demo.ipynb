{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d13be88-bf77-420b-9047-26d0f08f8878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:48:52.042395Z",
     "iopub.status.busy": "2026-01-02T15:48:52.042239Z",
     "iopub.status.idle": "2026-01-02T15:48:52.572854Z",
     "shell.execute_reply": "2026-01-02T15:48:52.572267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /Users/dustin/.venvs/base/lib/python3.13/site-packages (0.14.6)\r\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from statsmodels) (2.4.0)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from statsmodels) (1.16.3)\r\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from statsmodels) (2.3.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from statsmodels) (1.0.2)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from statsmodels) (25.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dustin/.venvs/base/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gaussian Copula Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Why Copulas Matter** - Preserving correlation structure\n",
    "2. **Multi-Column Fitting** - Fit distributions to multiple columns\n",
    "3. **Copula Fitting** - Capture correlation via Spark ML (scales to billions)\n",
    "4. **Correlated Sampling** - Local and distributed sampling\n",
    "5. **Serialization** - Save and load copulas\n",
    "6. **Performance Benchmark** - spark-bestfit vs statsmodels at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:48:52.574885Z",
     "iopub.status.busy": "2026-01-02T15:48:52.574741Z",
     "iopub.status.idle": "2026-01-02T15:48:53.070941Z",
     "shell.execute_reply": "2026-01-02T15:48:53.070472Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from spark_bestfit import DistributionFitter, GaussianCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:48:53.072181Z",
     "iopub.status.busy": "2026-01-02T15:48:53.072079Z",
     "iopub.status.idle": "2026-01-02T15:48:55.019701Z",
     "shell.execute_reply": "2026-01-02T15:48:55.019139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/02 22:48:53 WARN Utils: Your hostname, 2025m5.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.201 instead (on interface en0)\n",
      "26/01/02 22:48:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/02 22:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .appName(\"CopulaDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Why Copulas Matter: The Correlation Problem\n",
    "\n",
    "When you fit distributions to columns independently, the correlation between columns is lost. Let's demonstrate this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:48:55.021042Z",
     "iopub.status.busy": "2026-01-02T15:48:55.020898Z",
     "iopub.status.idle": "2026-01-02T15:48:55.353122Z",
     "shell.execute_reply": "2026-01-02T15:48:55.352728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Spearman correlation: -0.691\n",
      "\n",
      "Sample statistics:\n",
      "          price  quantity\n",
      "count  10000.00  10000.00\n",
      "mean     113.68    100.37\n",
      "std       62.39     45.14\n",
      "min       15.92     10.62\n",
      "25%       71.22     67.19\n",
      "50%      100.19     93.54\n",
      "75%      139.65    126.72\n",
      "max      801.15    372.41\n"
     ]
    }
   ],
   "source": [
    "# Generate correlated data: price and quantity with negative correlation\n",
    "# (higher price -> lower quantity, like demand curves)\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "\n",
    "# Create correlated normal samples\n",
    "correlation = -0.7  # Strong negative correlation\n",
    "cov_matrix = [[1.0, correlation], [correlation, 1.0]]\n",
    "correlated_normals = np.random.multivariate_normal([0, 0], cov_matrix, n_samples)\n",
    "\n",
    "# Transform to different distributions (price: lognormal, quantity: gamma)\n",
    "from scipy import stats as st\n",
    "price = st.lognorm.ppf(st.norm.cdf(correlated_normals[:, 0]), s=0.5, scale=100)\n",
    "quantity = st.gamma.ppf(st.norm.cdf(correlated_normals[:, 1]), a=5, scale=20)\n",
    "\n",
    "# Create DataFrame\n",
    "pdf = pd.DataFrame({\"price\": price, \"quantity\": quantity})\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show the original correlation\n",
    "original_corr = pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original Spearman correlation: {original_corr:.3f}\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(pdf.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. The Problem: Independent Sampling Loses Correlation\n",
    "\n",
    "If we fit each column independently and sample separately, correlation is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:48:55.354136Z",
     "iopub.status.busy": "2026-01-02T15:48:55.354071Z",
     "iopub.status.idle": "2026-01-02T15:49:00.539480Z",
     "shell.execute_reply": "2026-01-02T15:49:00.538907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit for price: betaprime\n",
      "Best fit for quantity: beta\n"
     ]
    }
   ],
   "source": [
    "# Fit distributions independently\n",
    "fitter = DistributionFitter(spark, random_seed=42)\n",
    "results = fitter.fit(df, columns=[\"price\", \"quantity\"], max_distributions=10)\n",
    "\n",
    "# Get best fits\n",
    "best_price = results.for_column(\"price\").best(n=1)[0]\n",
    "best_quantity = results.for_column(\"quantity\").best(n=1)[0]\n",
    "\n",
    "print(f\"Best fit for price: {best_price.distribution}\")\n",
    "print(f\"Best fit for quantity: {best_quantity.distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:00.541338Z",
     "iopub.status.busy": "2026-01-02T15:49:00.541214Z",
     "iopub.status.idle": "2026-01-02T15:49:00.546236Z",
     "shell.execute_reply": "2026-01-02T15:49:00.545927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original correlation:    -0.691\n",
      "Independent correlation: 0.003  <- LOST!\n"
     ]
    }
   ],
   "source": [
    "# Sample independently - correlation is LOST!\n",
    "independent_price = best_price.sample(size=10_000, random_state=42)\n",
    "independent_quantity = best_quantity.sample(size=10_000, random_state=42)\n",
    "\n",
    "independent_pdf = pd.DataFrame({\n",
    "    \"price\": independent_price,\n",
    "    \"quantity\": independent_quantity\n",
    "})\n",
    "\n",
    "independent_corr = independent_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original correlation:    {original_corr:.3f}\")\n",
    "print(f\"Independent correlation: {independent_corr:.3f}  <- LOST!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. The Solution: Gaussian Copula\n",
    "\n",
    "A Gaussian copula preserves both:\n",
    "- **Marginal distributions**: Each column follows its fitted distribution\n",
    "- **Correlation structure**: Columns maintain their original relationships\n",
    "\n",
    "**Big Data Advantage**: Unlike standard libraries that require `.toPandas()`, spark-bestfit computes correlation via Spark ML - scaling to billions of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:00.547214Z",
     "iopub.status.busy": "2026-01-02T15:49:00.547156Z",
     "iopub.status.idle": "2026-01-02T15:49:01.872212Z",
     "shell.execute_reply": "2026-01-02T15:49:01.871759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['price', 'quantity']\n",
      "\n",
      "Correlation matrix (computed via Spark ML):\n",
      "          price  quantity\n",
      "price     1.000    -0.691\n",
      "quantity -0.691     1.000\n"
     ]
    }
   ],
   "source": [
    "# Fit the copula - correlation computed via Spark ML (no .toPandas()!)\n",
    "copula = GaussianCopula.fit(results, df)\n",
    "\n",
    "print(f\"Columns: {copula.column_names}\")\n",
    "print(f\"\\nCorrelation matrix (computed via Spark ML):\")\n",
    "print(pd.DataFrame(\n",
    "    copula.correlation_matrix,\n",
    "    index=copula.column_names,\n",
    "    columns=copula.column_names\n",
    ").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:01.873261Z",
     "iopub.status.busy": "2026-01-02T15:49:01.873187Z",
     "iopub.status.idle": "2026-01-02T15:49:01.888643Z",
     "shell.execute_reply": "2026-01-02T15:49:01.888245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original correlation:    -0.691\n",
      "Independent correlation: 0.003  <- Lost\n",
      "Copula correlation:      -0.657  <- Preserved!\n"
     ]
    }
   ],
   "source": [
    "# Sample with correlation preserved!\n",
    "copula_samples = copula.sample(n=10_000, random_state=42)\n",
    "copula_pdf = pd.DataFrame(copula_samples)\n",
    "\n",
    "copula_corr = copula_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "print(f\"Original correlation:    {original_corr:.3f}\")\n",
    "print(f\"Independent correlation: {independent_corr:.3f}  <- Lost\")\n",
    "print(f\"Copula correlation:      {copula_corr:.3f}  <- Preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Distributed Sampling with `sample_spark()`\n",
    "\n",
    "For large-scale sampling (millions of correlated samples), use `sample_spark()` to leverage Spark's distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:01.889731Z",
     "iopub.status.busy": "2026-01-02T15:49:01.889665Z",
     "iopub.status.idle": "2026-01-02T15:49:01.975013Z",
     "shell.execute_reply": "2026-01-02T15:49:01.973832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: StructType([StructField('price', DoubleType(), False), StructField('quantity', DoubleType(), False)])\n",
      "\n",
      "Sample preview:\n",
      "+-----------------+------------------+\n",
      "|            price|          quantity|\n",
      "+-----------------+------------------+\n",
      "|52.47441364126707|166.50310551152634|\n",
      "|107.2929674701062| 130.1509506077366|\n",
      "|76.66989812274993|143.40923006683462|\n",
      "|97.19551367162858| 64.72617297162276|\n",
      "|76.01941400258045|100.14161006341728|\n",
      "+-----------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate 100,000 correlated samples using Spark\n",
    "samples_df = copula.sample_spark(n=100_000, random_seed=42)\n",
    "\n",
    "print(f\"Schema: {samples_df.schema}\")\n",
    "print(f\"\\nSample preview:\")\n",
    "samples_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:01.976921Z",
     "iopub.status.busy": "2026-01-02T15:49:01.976832Z",
     "iopub.status.idle": "2026-01-02T15:49:02.622605Z",
     "shell.execute_reply": "2026-01-02T15:49:02.622196Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dustin/.venvs/base/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "  File \"/Users/dustin/.venvs/base/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "    outfile.flush()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original correlation:     -0.691\n",
      "Spark sample correlation: -0.678\n",
      "Difference:               0.012\n"
     ]
    }
   ],
   "source": [
    "# Verify correlation is preserved at scale\n",
    "spark_samples_pdf = samples_df.toPandas()\n",
    "spark_corr = spark_samples_pdf.corr(method=\"spearman\").iloc[0, 1]\n",
    "\n",
    "print(f\"Original correlation:     {original_corr:.3f}\")\n",
    "print(f\"Spark sample correlation: {spark_corr:.3f}\")\n",
    "print(f\"Difference:               {abs(original_corr - spark_corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Verifying Marginal Distributions\n",
    "\n",
    "The copula preserves marginal distributions - each column still follows its fitted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:02.623722Z",
     "iopub.status.busy": "2026-01-02T15:49:02.623659Z",
     "iopub.status.idle": "2026-01-02T15:49:02.630441Z",
     "shell.execute_reply": "2026-01-02T15:49:02.630109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price:\n",
      "  Distribution: betaprime\n",
      "  K-S statistic: 0.0089\n",
      "  P-value: 0.4035 (good fit!)\n",
      "\n",
      "quantity:\n",
      "  Distribution: beta\n",
      "  K-S statistic: 0.0098\n",
      "  P-value: 0.2944 (good fit!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-S test: verify samples match the fitted marginal distributions\n",
    "for col in copula.column_names:\n",
    "    marginal = copula.marginals[col]\n",
    "    frozen_dist = marginal.get_scipy_dist()  # Already frozen with parameters\n",
    "    samples = copula_pdf[col].values\n",
    "    \n",
    "    # K-S test against fitted distribution (frozen dist doesn't need extra params)\n",
    "    ks_stat, p_value = st.kstest(samples, frozen_dist.cdf)\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Distribution: {marginal.distribution}\")\n",
    "    print(f\"  K-S statistic: {ks_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4f} {'(good fit!)' if p_value > 0.05 else '(poor fit)'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Serialization: Save and Load Copulas\n",
    "\n",
    "Save fitted copulas for later use - great for production pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:02.631417Z",
     "iopub.status.busy": "2026-01-02T15:49:02.631351Z",
     "iopub.status.idle": "2026-01-02T15:49:02.635803Z",
     "shell.execute_reply": "2026-01-02T15:49:02.635449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON structure:\n",
      "  schema_version: 1.1\n",
      "  type: gaussian_copula\n",
      "  columns: ['price', 'quantity']\n",
      "  marginals: ['price', 'quantity']\n",
      "\n",
      "Loaded copula works! Generated 1000 samples.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Save to JSON (human-readable, recommended)\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    json_path = os.path.join(tmpdir, \"copula.json\")\n",
    "    copula.save(json_path)\n",
    "    \n",
    "    # Show the JSON contents\n",
    "    with open(json_path, \"r\") as f:\n",
    "        import json\n",
    "        data = json.load(f)\n",
    "        print(\"JSON structure:\")\n",
    "        print(f\"  schema_version: {data['schema_version']}\")\n",
    "        print(f\"  type: {data['type']}\")\n",
    "        print(f\"  columns: {data['column_names']}\")\n",
    "        print(f\"  marginals: {list(data['marginals'].keys())}\")\n",
    "    \n",
    "    # Load and verify\n",
    "    loaded = GaussianCopula.load(json_path)\n",
    "    loaded_samples = loaded.sample(n=1000, random_state=42)\n",
    "    print(f\"\\nLoaded copula works! Generated {len(loaded_samples['price'])} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Three-Column Example\n",
    "\n",
    "Copulas work with any number of columns (minimum 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:02.636753Z",
     "iopub.status.busy": "2026-01-02T15:49:02.636698Z",
     "iopub.status.idle": "2026-01-02T15:49:02.711678Z",
     "shell.execute_reply": "2026-01-02T15:49:02.711292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original correlation matrix:\n",
      "          price  quantity  revenue\n",
      "price     1.000    -0.584    0.301\n",
      "quantity -0.584     1.000    0.467\n",
      "revenue   0.301     0.467    1.000\n"
     ]
    }
   ],
   "source": [
    "# Generate correlated data: price, quantity, revenue\n",
    "np.random.seed(123)\n",
    "n = 10_000\n",
    "\n",
    "# Create correlation structure\n",
    "corr_3d = [\n",
    "    [1.0,  -0.6, 0.3],   # price: neg corr with quantity, pos with revenue\n",
    "    [-0.6, 1.0,  0.5],   # quantity: neg corr with price, pos with revenue\n",
    "    [0.3,  0.5,  1.0],   # revenue: pos corr with both\n",
    "]\n",
    "correlated_normals_3d = np.random.multivariate_normal([0, 0, 0], corr_3d, n)\n",
    "\n",
    "# Transform to different distributions\n",
    "price_3d = st.lognorm.ppf(st.norm.cdf(correlated_normals_3d[:, 0]), s=0.3, scale=50)\n",
    "quantity_3d = st.gamma.ppf(st.norm.cdf(correlated_normals_3d[:, 1]), a=10, scale=5)\n",
    "revenue_3d = st.norm.ppf(st.norm.cdf(correlated_normals_3d[:, 2]), loc=1000, scale=200)\n",
    "\n",
    "pdf_3d = pd.DataFrame({\n",
    "    \"price\": price_3d,\n",
    "    \"quantity\": quantity_3d,\n",
    "    \"revenue\": revenue_3d\n",
    "})\n",
    "df_3d = spark.createDataFrame(pdf_3d)\n",
    "\n",
    "print(\"Original correlation matrix:\")\n",
    "print(pdf_3d.corr(method=\"spearman\").round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:02.712653Z",
     "iopub.status.busy": "2026-01-02T15:49:02.712594Z",
     "iopub.status.idle": "2026-01-02T15:49:06.078731Z",
     "shell.execute_reply": "2026-01-02T15:49:06.078301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 185:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copula correlation matrix (should match original):\n",
      "          price  quantity  revenue\n",
      "price      1.00    -0.570    0.280\n",
      "quantity  -0.57     1.000    0.453\n",
      "revenue    0.28     0.453    1.000\n"
     ]
    }
   ],
   "source": [
    "# Fit distributions and copula\n",
    "results_3d = fitter.fit(df_3d, columns=[\"price\", \"quantity\", \"revenue\"], max_distributions=10)\n",
    "copula_3d = GaussianCopula.fit(results_3d, df_3d)\n",
    "\n",
    "# Sample and compare correlations\n",
    "samples_3d = copula_3d.sample(n=10_000, random_state=42)\n",
    "samples_3d_pdf = pd.DataFrame(samples_3d)\n",
    "\n",
    "print(\"Copula correlation matrix (should match original):\")\n",
    "print(samples_3d_pdf.corr(method=\"spearman\").round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i9z4an1lm7",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmark: When to Use spark-bestfit\n",
    "\n",
    "**Important context**: spark-bestfit is NOT faster than statsmodels for small data on a single machine. The value is **scale** - handling data that doesn't fit in memory and generating samples across a cluster.\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Data fits in memory (< 10M rows) | statsmodels is faster |\n",
    "| Data exceeds memory (100M+ rows) | spark-bestfit (only option) |\n",
    "| Data already in Spark | spark-bestfit (avoid `.toPandas()`) |\n",
    "| Need 100M+ samples | `sample_spark()` (distributed) |\n",
    "\n",
    "> **Note**: This section requires `statsmodels` for comparison: `pip install statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ud5fu34se",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:06.079899Z",
     "iopub.status.busy": "2026-01-02T15:49:06.079829Z",
     "iopub.status.idle": "2026-01-02T15:49:21.207804Z",
     "shell.execute_reply": "2026-01-02T15:49:21.207443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation-Only Benchmark (apples-to-apples)\n",
      "======================================================================\n",
      "      N Rows |    pandas (ms) |  Spark ML (ms) |                  Notes\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10,000 |            1.6 |          205.9 | Spark 133x slower (overhead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     100,000 |           19.0 |          470.6 | Spark 25x slower (overhead)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 358:>                                                        (0 + 7) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 360:=======>                                                 (1 + 7) / 8]\r",
      "\r",
      "[Stage 360:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 379:>                                                        (0 + 8) / 8]\r",
      "\r",
      "[Stage 379:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1,000,000 |          244.4 |         3257.0 | Spark 13x slower (overhead)\n",
      "\n",
      "** Spark overhead is expected for local mode. The value is MEMORY scale, not speed. **\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from statsmodels.distributions.copula.api import GaussianCopula as StatsmodelsGaussianCopula\n",
    "from scipy.stats import norm, gamma, lognorm\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "def benchmark_correlation_only(n_rows: int, n_runs: int = 3):\n",
    "    \"\"\"Benchmark ONLY correlation computation: pandas vs Spark ML.\"\"\"\n",
    "    \n",
    "    # Generate test data with VALID correlation matrix\n",
    "    np.random.seed(42)\n",
    "    # Valid positive-semidefinite correlation matrix\n",
    "    corr_matrix = [[1.0, 0.6, 0.3], [0.6, 1.0, 0.4], [0.3, 0.4, 1.0]]\n",
    "    data = np.random.multivariate_normal([0, 0, 0], corr_matrix, n_rows)\n",
    "    \n",
    "    pdf_data = pd.DataFrame({\"col1\": data[:, 0], \"col2\": data[:, 1], \"col3\": data[:, 2]})\n",
    "    spark_df = spark.createDataFrame(pdf_data)\n",
    "    \n",
    "    # Benchmark pandas (what statsmodels uses internally)\n",
    "    pandas_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        corr = pdf_data.corr(method=\"spearman\").values\n",
    "        pandas_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark Spark ML correlation ONLY (no distribution fitting)\n",
    "    spark_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        assembler = VectorAssembler(inputCols=[\"col1\", \"col2\", \"col3\"], outputCol=\"features\", handleInvalid=\"skip\")\n",
    "        vector_df = assembler.transform(spark_df).select(\"features\")\n",
    "        corr_result = Correlation.corr(vector_df, \"features\", method=\"spearman\")\n",
    "        _ = corr_result.head()[0].toArray()  # Force execution\n",
    "        spark_times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"pandas_ms\": np.mean(pandas_times) * 1000,\n",
    "        \"spark_ml_ms\": np.mean(spark_times) * 1000,\n",
    "    }\n",
    "\n",
    "print(\"Correlation-Only Benchmark (apples-to-apples)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'N Rows':>12} | {'pandas (ms)':>14} | {'Spark ML (ms)':>14} | {'Notes':>22}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for n in [10_000, 100_000, 1_000_000]:\n",
    "    result = benchmark_correlation_only(n, n_runs=2)\n",
    "    ratio = result[\"spark_ml_ms\"] / result[\"pandas_ms\"]\n",
    "    notes = f\"Spark {ratio:.0f}x slower (overhead)\"\n",
    "    print(f\"{result['n_rows']:>12,} | {result['pandas_ms']:>14.1f} | {result['spark_ml_ms']:>14.1f} | {notes:>22}\")\n",
    "\n",
    "print(\"\\n** Spark overhead is expected for local mode. The value is MEMORY scale, not speed. **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "iwh6rzmn81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:49:21.209081Z",
     "iopub.status.busy": "2026-01-02T15:49:21.209015Z",
     "iopub.status.idle": "2026-01-02T15:52:45.960799Z",
     "shell.execute_reply": "2026-01-02T15:52:45.960326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling Benchmark (Optimized)\n",
      "====================================================================================================\n",
      "   N Samples |    statsmodels | return_uniform | with transform |        Fastest\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1,000,000 |           77.6 |           59.7 |         1548.1 | return_uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10,000,000 |          740.6 |          556.7 |        15485.3 | return_uniform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50,000,000 |         3647.7 |         2810.9 |        77447.0 | return_uniform\n",
      "\n",
      "** return_uniform=True matches statsmodels speed (both skip marginal PPF transforms) **\n",
      "** 'with transform' includes PPF to convert to fitted marginal distributions **\n"
     ]
    }
   ],
   "source": [
    "def benchmark_sampling(n_samples: int, n_runs: int = 3):\n",
    "    \"\"\"Benchmark sample generation: statsmodels vs spark-bestfit.\"\"\"\n",
    "    \n",
    "    # Use the copula we already fitted (copula_3d from section 7)\n",
    "    corr_matrix_np = copula_3d.correlation_matrix\n",
    "    \n",
    "    # Create statsmodels copula with same correlation\n",
    "    sm_copula = StatsmodelsGaussianCopula(corr=corr_matrix_np, k_dim=3)\n",
    "    \n",
    "    # Benchmark statsmodels sampling (single-node, returns uniform only)\n",
    "    sm_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        sm_samples = sm_copula.rvs(n_samples)\n",
    "        sm_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark spark-bestfit with return_uniform=True (apples-to-apples with statsmodels)\n",
    "    uniform_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples = copula_3d.sample(n=n_samples, random_state=42, return_uniform=True)\n",
    "        uniform_times.append(time.time() - start)\n",
    "    \n",
    "    # Benchmark spark-bestfit local sampling (includes marginal transform)\n",
    "    local_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples = copula_3d.sample(n=n_samples, random_state=42)\n",
    "        local_times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"statsmodels_ms\": np.mean(sm_times) * 1000,\n",
    "        \"uniform_ms\": np.mean(uniform_times) * 1000,\n",
    "        \"local_ms\": np.mean(local_times) * 1000,\n",
    "    }\n",
    "\n",
    "print(\"\\nSampling Benchmark (Optimized)\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'N Samples':>12} | {'statsmodels':>14} | {'return_uniform':>14} | {'with transform':>14} | {'Fastest':>14}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for n in [1_000_000, 10_000_000, 50_000_000]:\n",
    "    result = benchmark_sampling(n, n_runs=2)\n",
    "    times = [(\"statsmodels\", result[\"statsmodels_ms\"]), \n",
    "             (\"return_uniform\", result[\"uniform_ms\"]), \n",
    "             (\"with_transform\", result[\"local_ms\"])]\n",
    "    winner = min(times, key=lambda x: x[1])[0]\n",
    "    print(f\"{result['n_samples']:>12,} | {result['statsmodels_ms']:>14.1f} | {result['uniform_ms']:>14.1f} | {result['local_ms']:>14.1f} | {winner:>14}\")\n",
    "\n",
    "print(\"\\n** return_uniform=True matches statsmodels speed (both skip marginal PPF transforms) **\")\n",
    "print(\"** 'with transform' includes PPF to convert to fitted marginal distributions **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j349sd10n5",
   "metadata": {},
   "source": [
    "### Benchmark Takeaways\n",
    "\n",
    "**Correlation Computation:**\n",
    "- pandas/statsmodels is **faster** for data that fits in memory (expected!)\n",
    "- Spark ML has overhead but can handle **billions of rows** that would crash pandas\n",
    "\n",
    "**Sampling:**\n",
    "- Local methods (statsmodels, `sample()`) are faster for small-medium samples\n",
    "- `sample_spark()` distributes work across the cluster for massive scale\n",
    "\n",
    "**When spark-bestfit copula makes sense:**\n",
    "1. **Data already in Spark** - avoid expensive `.toPandas()` to compute correlation\n",
    "2. **Data exceeds memory** - 100M+ rows won't fit in pandas, Spark is the only option\n",
    "3. **Massive sample generation** - 100M+ correlated samples for Monte Carlo simulation\n",
    "4. **Production pipelines** - integrate with existing Spark ETL workflows\n",
    "\n",
    "**When to use statsmodels instead:**\n",
    "- Data fits comfortably in memory (< 10M rows)\n",
    "- You're not already using Spark\n",
    "- You need fastest possible local performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `GaussianCopula` class enables correlated multi-column sampling **at scale**:\n",
    "\n",
    "| Scenario | statsmodels | spark-bestfit |\n",
    "|----------|-------------|---------------|\n",
    "| Data < 10M rows | **Faster** (use this) | Slower (Spark overhead) |\n",
    "| Data > 100M rows | Crashes (OOM) | **Works** (distributed) |\n",
    "| Data in Spark | Requires `.toPandas()` | **Native** (no conversion) |\n",
    "| 100M+ samples | May OOM | **`sample_spark()`** distributed |\n",
    "\n",
    "**Use spark-bestfit copula when:**\n",
    "- Data is already in a Spark DataFrame\n",
    "- Data exceeds single-node memory (100M+ rows)\n",
    "- You need 100M+ correlated samples for simulation\n",
    "- You're building production Spark pipelines\n",
    "\n",
    "**Key Methods:**\n",
    "- `GaussianCopula.fit(results, df)` - Fit copula from multi-column results\n",
    "- `copula.sample(n)` - Local sampling for small scale\n",
    "- `copula.sample_spark(n)` - Distributed sampling for massive scale\n",
    "- `copula.save(path)` / `GaussianCopula.load(path)` - Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:52:45.961989Z",
     "iopub.status.busy": "2026-01-02T15:52:45.961924Z",
     "iopub.status.idle": "2026-01-02T15:52:46.420115Z",
     "shell.execute_reply": "2026-01-02T15:52:46.419236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
