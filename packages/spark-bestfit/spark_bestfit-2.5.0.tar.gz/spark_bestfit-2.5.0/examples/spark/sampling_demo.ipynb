{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Sampling & Fit Quality Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Distributed Sampling** - Generate millions of samples using Spark parallelism\n",
    "2. **Fit Quality Warnings** - Automatic detection of poor fits\n",
    "3. **Performance Comparison** - Local vs distributed sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:49.424257Z",
     "iopub.status.busy": "2026-01-02T15:53:49.423995Z",
     "iopub.status.idle": "2026-01-02T15:53:49.947354Z",
     "shell.execute_reply": "2026-01-02T15:53:49.947009Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from spark_bestfit import DistributionFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:49.948635Z",
     "iopub.status.busy": "2026-01-02T15:53:49.948549Z",
     "iopub.status.idle": "2026-01-02T15:53:52.253390Z",
     "shell.execute_reply": "2026-01-02T15:53:52.252991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/02 22:53:50 WARN Utils: Your hostname, 2025m5.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.201 instead (on interface en0)\n",
      "26/01/02 22:53:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/02 22:53:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"SamplingDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit a Distribution\n",
    "\n",
    "First, let's generate some sample data and fit distributions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:52.254706Z",
     "iopub.status.busy": "2026-01-02T15:53:52.254553Z",
     "iopub.status.idle": "2026-01-02T15:53:56.936714Z",
     "shell.execute_reply": "2026-01-02T15:53:56.936227Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 4]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(4 + 0) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit: chi2\n",
      "Parameters: [4.071206092834473, 0.009233290329575539, 2.472316026687622]\n",
      "K-S statistic: 0.0051\n",
      "P-value: 0.9572\n"
     ]
    }
   ],
   "source": [
    "# Generate gamma-distributed data\n",
    "np.random.seed(42)\n",
    "data = np.random.gamma(shape=2.0, scale=5.0, size=10_000)\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame([(float(x),) for x in data], [\"value\"])\n",
    "\n",
    "# Fit distributions\n",
    "fitter = DistributionFitter(spark, random_seed=42)\n",
    "results = fitter.fit(df, column=\"value\", max_distributions=20)\n",
    "\n",
    "# Get best fit\n",
    "best = results.best(n=1)[0]\n",
    "print(f\"Best fit: {best.distribution}\")\n",
    "print(f\"Parameters: {best.parameters}\")\n",
    "print(f\"K-S statistic: {best.ks_statistic:.4f}\")\n",
    "print(f\"P-value: {best.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Sampling with `sample_spark()`\n",
    "\n",
    "Generate samples using Spark's distributed computing. This is ideal for generating millions of samples efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:56.938010Z",
     "iopub.status.busy": "2026-01-02T15:53:56.937937Z",
     "iopub.status.idle": "2026-01-02T15:53:57.082854Z",
     "shell.execute_reply": "2026-01-02T15:53:57.082166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: StructType([StructField('sample', DoubleType(), False)])\n",
      "Sample count: 100,000\n",
      "+------------------+\n",
      "|            sample|\n",
      "+------------------+\n",
      "| 21.62513549334104|\n",
      "|10.715187326200923|\n",
      "|14.942145266139688|\n",
      "|6.2094943252486114|\n",
      "| 11.09381252827893|\n",
      "+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate 100,000 samples using Spark\n",
    "samples_df = best.sample_spark(\n",
    "    n=100_000,\n",
    "    spark=spark,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Schema: {samples_df.schema}\")\n",
    "print(f\"Sample count: {samples_df.count():,}\")\n",
    "samples_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:57.084209Z",
     "iopub.status.busy": "2026-01-02T15:53:57.084123Z",
     "iopub.status.idle": "2026-01-02T15:53:57.134963Z",
     "shell.execute_reply": "2026-01-02T15:53:57.134178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|  generated_values|\n",
      "+------------------+\n",
      "| 10.83504621526457|\n",
      "| 4.173895196887788|\n",
      "|22.627553500211572|\n",
      "|14.512516565715536|\n",
      "| 7.652403897328518|\n",
      "+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Custom column name and explicit partitions\n",
    "samples_df = best.sample_spark(\n",
    "    n=50_000,\n",
    "    spark=spark,\n",
    "    num_partitions=8,\n",
    "    column_name=\"generated_values\",\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "samples_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison: Local vs Distributed\n",
    "\n",
    "Compare the performance of local sampling (`sample()`) vs distributed sampling (`sample_spark()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:53:57.137052Z",
     "iopub.status.busy": "2026-01-02T15:53:57.136952Z",
     "iopub.status.idle": "2026-01-02T15:54:03.354397Z",
     "shell.execute_reply": "2026-01-02T15:54:03.354016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N Samples |   Local (ms) |   Spark (ms) |     Winner\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1,000 |          0.4 |        369.9 |      Local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1,000,000 |         16.5 |         64.9 |      Local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10,000,000 |        154.3 |        152.3 |      Spark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50,000,000 |        774.2 |        538.9 |      Spark\n"
     ]
    }
   ],
   "source": [
    "def benchmark_sampling(n_samples: int, n_runs: int = 3):\n",
    "    \"\"\"Benchmark local vs distributed sampling.\"\"\"\n",
    "    \n",
    "    # Local sampling\n",
    "    local_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples = best.sample(size=n_samples, random_state=42)\n",
    "        local_times.append(time.time() - start)\n",
    "    \n",
    "    # Distributed sampling\n",
    "    spark_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        samples_df = best.sample_spark(n=n_samples, spark=spark, random_seed=42)\n",
    "        _ = samples_df.count()  # Force execution\n",
    "        spark_times.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"n_samples\": n_samples,\n",
    "        \"local_avg_ms\": np.mean(local_times) * 1000,\n",
    "        \"spark_avg_ms\": np.mean(spark_times) * 1000,\n",
    "    }\n",
    "\n",
    "# Run benchmarks for different sizes\n",
    "print(f\"{'N Samples':>12} | {'Local (ms)':>12} | {'Spark (ms)':>12} | {'Winner':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for n in [1_000, 1_000_000, 10_000_000, 50_000_000]:\n",
    "    result = benchmark_sampling(n)\n",
    "    winner = \"Local\" if result[\"local_avg_ms\"] < result[\"spark_avg_ms\"] else \"Spark\"\n",
    "    print(f\"{result['n_samples']:>12,} | {result['local_avg_ms']:>12.1f} | {result['spark_avg_ms']:>12.1f} | {winner:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaway:** Local sampling is faster for smaller sample sizes due to Spark overhead. Distributed sampling becomes advantageous for very large samples (millions) where memory and parallelism matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit Quality Warnings\n",
    "\n",
    "Use `warn_if_poor=True` to automatically detect poor fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:03.355781Z",
     "iopub.status.busy": "2026-01-02T15:54:03.355698Z",
     "iopub.status.idle": "2026-01-02T15:54:03.945760Z",
     "shell.execute_reply": "2026-01-02T15:54:03.945435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning raised!\n",
      "  Message: Best fit 'arcsine' has p-value 0.0000 < 0.05, indicating a potentially poor fit. Consider using quality_report() for detailed diagnostics.\n"
     ]
    }
   ],
   "source": [
    "# Create data that doesn't fit well with common distributions\n",
    "# (bimodal distribution)\n",
    "np.random.seed(42)\n",
    "bimodal_data = np.concatenate([\n",
    "    np.random.normal(10, 2, 5000),\n",
    "    np.random.normal(30, 2, 5000)\n",
    "])\n",
    "\n",
    "bimodal_df = spark.createDataFrame([(float(x),) for x in bimodal_data], [\"value\"])\n",
    "\n",
    "# Fit only 3 distributions for speed (bimodal data is slow to fit)\n",
    "results = fitter.fit(bimodal_df, column=\"value\", max_distributions=3)\n",
    "\n",
    "# Get best with warning enabled\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    best_bimodal = results.best(n=1, warn_if_poor=True)[0]\n",
    "    \n",
    "    # Filter for UserWarning only (ignore ResourceWarning from Spark sockets)\n",
    "    user_warnings = [x for x in w if issubclass(x.category, UserWarning)]\n",
    "    \n",
    "    if user_warnings:\n",
    "        print(\"Warning raised!\")\n",
    "        print(f\"  Message: {user_warnings[0].message}\")\n",
    "    else:\n",
    "        print(f\"Best fit: {best_bimodal.distribution} (p-value: {best_bimodal.pvalue:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Report\n",
    "\n",
    "Get a comprehensive quality assessment with `quality_report()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:03.946769Z",
     "iopub.status.busy": "2026-01-02T15:54:03.946710Z",
     "iopub.status.idle": "2026-01-02T15:54:04.118054Z",
     "shell.execute_reply": "2026-01-02T15:54:04.117470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "QUALITY REPORT\n",
      "==================================================\n",
      "\n",
      "Total distributions fitted: 3\n",
      "Distributions meeting thresholds: 0\n",
      "\n",
      "Top 5 Fits:\n",
      "  1. arcsine: KS=0.1940, p=0.0000\n",
      "  2. alpha: KS=0.2374, p=0.0000\n",
      "  3. anglit: KS=0.2381, p=0.0000\n",
      "\n",
      "Warnings:\n",
      "  - Best fit 'arcsine' has low p-value (0.0000 < 0.05)\n",
      "  - Best fit 'arcsine' has high K-S statistic (0.1940 > 0.1)\n",
      "  - Best fit 'arcsine' has high A-D statistic (775.4061 > 2.0)\n",
      "  - No distributions meet all quality thresholds\n"
     ]
    }
   ],
   "source": [
    "# Get quality report\n",
    "report = results.quality_report(n=5)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal distributions fitted: {report['summary']['total_distributions']}\")\n",
    "print(f\"Distributions meeting thresholds: {report['n_acceptable']}\")\n",
    "\n",
    "print(\"\\nTop 5 Fits:\")\n",
    "for i, fit in enumerate(report['top_fits'], 1):\n",
    "    print(f\"  {i}. {fit.distribution}: KS={fit.ks_statistic:.4f}, p={fit.pvalue:.4f}\")\n",
    "\n",
    "print(\"\\nWarnings:\")\n",
    "if report['warnings']:\n",
    "    for warning in report['warnings']:\n",
    "        print(f\"  - {warning}\")\n",
    "else:\n",
    "    print(\"  None - all fits look good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:04.119176Z",
     "iopub.status.busy": "2026-01-02T15:54:04.119091Z",
     "iopub.status.idle": "2026-01-02T15:54:04.252710Z",
     "shell.execute_reply": "2026-01-02T15:54:04.252259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With strict thresholds: 0 acceptable fits\n",
      "Warnings: [\"Best fit 'arcsine' has low p-value (0.0000 < 0.1)\", \"Best fit 'arcsine' has high K-S statistic (0.1940 > 0.05)\", \"Best fit 'arcsine' has high A-D statistic (775.4061 > 1.0)\", 'No distributions meet all quality thresholds']\n"
     ]
    }
   ],
   "source": [
    "# Custom thresholds for stricter quality assessment\n",
    "strict_report = results.quality_report(\n",
    "    n=3,\n",
    "    pvalue_threshold=0.10,  # Stricter p-value\n",
    "    ks_threshold=0.05,      # Stricter KS\n",
    "    ad_threshold=1.0        # Stricter A-D\n",
    ")\n",
    "\n",
    "print(f\"With strict thresholds: {strict_report['n_acceptable']} acceptable fits\")\n",
    "print(f\"Warnings: {strict_report['warnings']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Workflow Example\n",
    "\n",
    "Putting it all together: fit, validate, sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:04.254344Z",
     "iopub.status.busy": "2026-01-02T15:54:04.254197Z",
     "iopub.status.idle": "2026-01-02T15:54:05.292586Z",
     "shell.execute_reply": "2026-01-02T15:54:05.292172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality issues detected - review before using\n",
      "\n",
      "Best distribution: beta\n",
      "Parameters: {'a': 0.9914222955703735, 'b': 77.39903259277344, 'loc': 5.8174115110887215e-05, 'scale': 386.3881530761719}\n",
      "\n",
      "Generated 50,000 synthetic samples\n",
      "Sample mean: 4.89 (expected ~5.0)\n",
      "Sample std: 4.86 (expected ~5.0)\n"
     ]
    }
   ],
   "source": [
    "# Generate well-behaved data\n",
    "np.random.seed(42)\n",
    "good_data = np.random.exponential(scale=5.0, size=10_000)\n",
    "good_df = spark.createDataFrame([(float(x),) for x in good_data], [\"value\"])\n",
    "\n",
    "# Fit (using fewer distributions for demo speed)\n",
    "results = fitter.fit(good_df, column=\"value\", max_distributions=5)\n",
    "\n",
    "# Validate with quality report\n",
    "report = results.quality_report()\n",
    "if report['warnings']:\n",
    "    print(\"Quality issues detected - review before using\")\n",
    "else:\n",
    "    print(\"Quality check passed!\")\n",
    "\n",
    "# Get best fit\n",
    "best = results.best(n=1, warn_if_poor=True)[0]\n",
    "print(f\"\\nBest distribution: {best.distribution}\")\n",
    "print(f\"Parameters: {dict(zip(best.get_param_names(), best.parameters))}\")\n",
    "\n",
    "# Generate samples for downstream use\n",
    "synthetic_df = best.sample_spark(n=50_000, spark=spark, random_seed=42)\n",
    "print(f\"\\nGenerated {synthetic_df.count():,} synthetic samples\")\n",
    "\n",
    "# Verify statistical properties\n",
    "samples = synthetic_df.toPandas()[\"sample\"].values\n",
    "print(f\"Sample mean: {samples.mean():.2f} (expected ~5.0)\")\n",
    "print(f\"Sample std: {samples.std():.2f} (expected ~5.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T15:54:05.293617Z",
     "iopub.status.busy": "2026-01-02T15:54:05.293555Z",
     "iopub.status.idle": "2026-01-02T15:54:06.258845Z",
     "shell.execute_reply": "2026-01-02T15:54:06.258030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
