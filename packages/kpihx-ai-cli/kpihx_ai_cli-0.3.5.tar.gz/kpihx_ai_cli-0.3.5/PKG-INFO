Metadata-Version: 2.4
Name: kpihx-ai-cli
Version: 0.3.5
Summary: Un client CLI intelligent pour Ollama avec gestion d'historique et mÃ©moire persistante
Project-URL: Homepage, https://github.com/KpihX/ai-cli
Project-URL: Repository, https://github.com/KpihX/ai-cli
Project-URL: Issues, https://github.com/KpihX/ai-cli/issues
Author-email: KAMDEM POUOKAM Ivann Harold <kapoivha@gmail.com>
License-Expression: MIT
License-File: LICENSE
Keywords: ai,chatbot,cli,llm,ollama,terminal
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.11
Requires-Dist: prompt-toolkit>=3.0.52
Requires-Dist: pyyaml>=6.0.3
Requires-Dist: requests>=2.32.5
Requires-Dist: rich>=14.2.0
Requires-Dist: typer>=0.21.1
Description-Content-Type: text/markdown

<p align="center">
  <img src="docs/assets/banner.png" alt="AI-CLI Banner" width="600"/>
</p>

<h1 align="center">AI-CLI ğŸ¤–</h1>

<p align="center">
  <strong>Un client CLI intelligent, modulaire et puissant pour interagir avec Ollama</strong>
</p>

<p align="center">
  <a href="#-installation">Installation</a> â€¢
  <a href="#-fonctionnalitÃ©s">FonctionnalitÃ©s</a> â€¢
  <a href="#-utilisation">Utilisation</a> â€¢
  <a href="#-configuration">Configuration</a> â€¢
  <a href="#-architecture">Architecture</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11+-blue.svg" alt="Python 3.11+"/>
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="MIT License"/>
  <img src="https://img.shields.io/badge/tests-46%20passed-brightgreen.svg" alt="Tests"/>
  <img src="https://img.shields.io/badge/ollama-compatible-orange.svg" alt="Ollama"/>
</p>

---

## âœ¨ FonctionnalitÃ©s

<table>
<tr>
<td width="50%">

### ğŸ§  MÃ©moire Persistante
L'agent apprend de vous et se souvient entre les sessions grÃ¢ce Ã  `AI_CLI.md`.

### ğŸ“ Historique Complet
Toutes vos discussions sauvegardÃ©es automatiquement en JSON avec support UTF-8.

### âœ‚ï¸ RÃ©sumÃ© Intelligent
Compressez les longues discussions sans perdre le contexte avec `/resume`.

</td>
<td width="50%">

### âŒ¨ï¸ Interface AvancÃ©e
Auto-complÃ©tion, suggestions, rendu Markdown en temps rÃ©el avec Rich.

### âš™ï¸ 100% Configurable
Personnalisez chaque prompt et paramÃ¨tre dans `config.yaml`.

### ğŸ”„ Auto-gestion Ollama
DÃ©tecte, dÃ©marre et tÃ©lÃ©charge les modÃ¨les automatiquement.

</td>
</tr>
</table>

---

## ğŸ“¦ Installation

### PrÃ©requis

- **Python 3.11+**
- **[Ollama](https://ollama.ai/)** installÃ© et fonctionnel

### ğŸš€ Installation Rapide

<table>
<tr>
<td>

**Depuis PyPI (RecommandÃ©)**

```bash
pipx install ai-cli
# ou
pip install ai-cli
```

</td>
<td>

**Depuis les sources**

```bash
git clone https://github.com/KpihX/ai-cli.git
cd ai-cli
pipx install .
# ou: uv tool install .
```

</td>
</tr>
</table>

### ğŸ› ï¸ Mode DÃ©veloppement

```bash
# Cloner et installer en mode dÃ©veloppement
git clone https://github.com/KpihX/ai-cli.git
cd ai-cli
uv sync --dev

# ExÃ©cuter les tests (43 tests)
uv run pytest tests/ -v
```

---

## ğŸš€ Utilisation

### Commandes de Base

| Commande | Description |
|----------|-------------|
| `ai-cli -i` | Mode interactif (conversation continue) |
| `ai-cli -p "Question"` | Question rapide (one-shot) |
| `ai-cli -l` | Lister les modÃ¨les disponibles |
| `ai-cli -d llama3.2` | Changer le modÃ¨le par dÃ©faut |
| `ai-cli -m mistral -i` | Utiliser un modÃ¨le spÃ©cifique |

### Commandes Interactives (Slash Commands)

Une fois en mode interactif, vous avez accÃ¨s Ã  ces commandes :

| Commande | Description | Exemple |
|----------|-------------|---------|
| `/help` | ğŸ“š Affiche l'aide complÃ¨te | `/help` |
| `/new` | ğŸ†• Sauvegarde et nouvelle discussion | `/new` |
| `/old` | ğŸ“‚ Charger ou supprimer une discussion | `/old` â†’ `d 1` pour supprimer |
| `/memory` | ğŸ§  GÃ©rer la mÃ©moire (voir/add/delete) | `/memory add J'aime Python` |
| `/resume` | ğŸ“ RÃ©sume l'historique | `/resume` |
| `/settings` | âš™ï¸ ParamÃ¨tres (modÃ¨le, tempÃ©rature) | `/settings` |
| `/clear` | ğŸ§¹ Efface l'Ã©cran | `/clear` |
| `/exit` | ğŸ‘‹ Sauvegarde et quitte | `/exit` ou `Ctrl+D` |

### ğŸ’¬ Exemple de Session

```
$ ai-cli -i

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚              Bienvenue dans AI-CLI !                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Index â”‚ Titre                         â”‚ Date               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1     â”‚ Discussion sur Python         â”‚ 20260110_143052    â”‚
â”‚ 2     â”‚ Aide DevOps avec Docker       â”‚ 20260109_221015    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Utilisez /old pour charger une ancienne discussion.

Mode Interactif avec phi3.5. Tapez /exit pour quitter.

Vous > Quelle est la diffÃ©rence entre Docker et Podman ?

â•­â”€â”€â”€ AI (phi3.5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ## Docker vs Podman                                                    â”‚
â”‚                                                                        â”‚
â”‚ **1. Architecture**                                                    â”‚
â”‚ - Docker: daemon central (`dockerd`) qui gÃ¨re tous les conteneurs      â”‚
â”‚ - Podman: **daemonless**, chaque conteneur est un processus enfant     â”‚
â”‚                                                                        â”‚
â”‚ **2. SÃ©curitÃ©**                                                        â”‚
â”‚ - Docker: nÃ©cessite root par dÃ©faut                                    â”‚
â”‚ - Podman: **rootless** par dÃ©faut, plus sÃ©curisÃ©                       â”‚
â”‚                                                                        â”‚
â”‚ **3. CompatibilitÃ©**                                                   â”‚
â”‚ - Les deux utilisent les mÃªmes images OCI                              â”‚
â”‚ - `alias docker=podman` fonctionne dans la plupart des cas             â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Vous > /save L'utilisateur s'intÃ©resse Ã  Docker et Podman
[green]Information sauvegardÃ©e dans AI_CLI.md.[/green]

Vous > /exit
Sauvegarde de la discussion...
Proposition de titre : Docker vs Podman Comparison
Session enregistrÃ©e sous : Docker vs Podman Comparison
```

---

## âš™ï¸ Configuration

Le fichier `config.yaml` permet de tout personnaliser :

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    CONFIGURATION AI-CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ModÃ¨le LLM utilisÃ© par dÃ©faut
default_model: phi3.5

# URL du serveur Ollama
ollama_url: http://localhost:11434

# Nombre de messages Ã  garder avant rÃ©sumÃ© automatique
summary_threshold: 5

# Timeout des requÃªtes HTTP (secondes)
request_timeout: 30

# RÃ©pertoire de stockage des donnÃ©es
history_dir: ~/.ai-cli

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#                    PROMPTS PERSONNALISABLES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
prompts:
  # GÃ©nÃ©ration automatique de titre pour les sessions
  title_generation: >
    GÃ©nÃ¨re un titre court (3-5 mots max) pour cette discussion.
    RÃ©ponds UNIQUEMENT avec le titre:
    {content}

  # RÃ©sumÃ© de l'historique pour /resume
  summarization: >
    RÃ©sume de faÃ§on concise les points clÃ©s de cet Ã©change
    pour servir de contexte historique:
    {content}

  # Injection de la mÃ©moire utilisateur
  memory_prefix: "Voici ce que tu sais sur l'utilisateur: {memory}"

  # Message d'accueil
  welcome_message: "Bienvenue dans AI-CLI !"
  
  # Info mode interactif
  interactive_info: "Mode Interactif avec {model}. Tapez /exit pour quitter."
```

---

## ğŸ—ï¸ Architecture

```
ai-cli/
â”œâ”€â”€ ğŸ“ src/ai_cli/
â”‚   â”œâ”€â”€ main.py           # ğŸ¯ Point d'entrÃ©e CLI + boucle REPL
â”‚   â”œâ”€â”€ ollama_client.py  # ğŸ¦™ Client API Ollama (stream/sync)
â”‚   â””â”€â”€ storage.py        # ğŸ’¾ Persistance (sessions + mÃ©moire)
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ test_main.py      # âœ… Tests CLI (12 tests)
â”‚   â”œâ”€â”€ test_client.py    # âœ… Tests Ollama (16 tests)
â”‚   â””â”€â”€ test_storage.py   # âœ… Tests Storage (15 tests)
â”œâ”€â”€ ğŸ“ docs/assets/
â”‚   â””â”€â”€ banner.png        # ğŸ–¼ï¸ Banner du projet
â”œâ”€â”€ config.yaml           # âš™ï¸ Configuration par dÃ©faut
â”œâ”€â”€ pyproject.toml        # ğŸ“¦ MÃ©tadonnÃ©es du package
â””â”€â”€ README.md             # ğŸ“š Documentation
```

### Flux de DonnÃ©es

```mermaid
flowchart LR
    subgraph User["ğŸ‘¤ Utilisateur"]
        A[Terminal]
    end
    
    subgraph CLI["ğŸ–¥ï¸ AI-CLI"]
        B[main.py<br/>REPL Loop]
        C[ollama_client.py<br/>Stream/Sync]
        D[storage.py<br/>Persistence]
    end
    
    subgraph Backend["ğŸ¦™ Ollama"]
        E[LLM API]
        F[phi3.5 / llama3.2 / ...]
    end
    
    subgraph Storage["ğŸ“ ~/.ai-cli/"]
        G[sessions/*.json]
        H[AI_CLI.md]
    end
    
    A -->|prompt| B
    B -->|API call| C
    C -->|HTTP POST| E
    E -->|inference| F
    F -->|stream| E
    E -->|chunks| C
    C -->|content| B
    B -->|Rich render| A
    B <-->|save/load| D
    D <-->|read/write| G
    D <-->|memory| H
```

### Modules DÃ©taillÃ©s

<table>
<tr>
<th>Module</th>
<th>Classes/Fonctions</th>
<th>ResponsabilitÃ©</th>
</tr>
<tr>
<td><code>ollama_client.py</code></td>
<td>

- `OllamaClient`
- `OllamaConnectionError`
- `chat_stream()` / `chat_sync()`
- `generate_title()` / `summarize()`

</td>
<td>Communication avec l'API Ollama, gestion du streaming et des erreurs rÃ©seau</td>
</tr>
<tr>
<td><code>storage.py</code></td>
<td>

- `StorageManager`
- `StorageError`
- `save_session()` / `load_session()`
- `save_memory()` / `get_memory()`

</td>
<td>Persistance JSON des sessions et gestion de la mÃ©moire Markdown</td>
</tr>
<tr>
<td><code>main.py</code></td>
<td>

- `SessionState`
- `load_config()`
- `run_interactive()`
- `handle_save_and_exit()`

</td>
<td>Point d'entrÃ©e, boucle REPL, routage des commandes slash</td>
</tr>
</table>

---

## ğŸ“Š Tests

Le projet est couvert par **46 tests unitaires** organisÃ©s en classes :

```bash
# ExÃ©cuter tous les tests
uv run pytest tests/ -v

# Avec couverture de code
uv run pytest tests/ --cov=ai_cli --cov-report=html

# Tests spÃ©cifiques
uv run pytest tests/test_client.py -v  # Tests Ollama
uv run pytest tests/test_storage.py -v # Tests Storage
```

### Couverture des Tests

| Module | Tests | CatÃ©gories |
|--------|-------|------------|
| `main.py` | 12 | Config, SessionState, Slash commands, Input handling |
| `ollama_client.py` | 16 | Stream, Sync, is_running, Erreurs rÃ©seau, Backward compat |
| `storage.py` | 18 | CRUD, Delete, Unicode, JSON corrompu, Memory modes, Init |

---

## ğŸ“ Fichiers de DonnÃ©es

| Chemin | Description | Format |
|--------|-------------|--------|
| `~/.ai-cli/sessions/` | Historique des discussions | JSON |
| `~/.ai-cli/AI_CLI.md` | MÃ©moire persistante de l'agent | Markdown |
| `~/.ai-cli/config.yaml` | Configuration utilisateur (optionnel) | YAML |

### Structure d'une Session

```json
{
    "title": "Discussion sur Python",
    "timestamp": "20260110_143052",
    "messages": [
        {"role": "system", "content": "Voici ce que tu sais..."},
        {"role": "user", "content": "Explique les dÃ©corateurs"},
        {"role": "assistant", "content": "Les dÃ©corateurs en Python..."}
    ]
}
```

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Pour contribuer :

1. **Fork** le projet
2. **CrÃ©er** une branche (`git checkout -b feature/ma-feature`)
3. **Committer** vos changements (`git commit -m 'Add: ma feature'`)
4. **Pousser** (`git push origin feature/ma-feature`)
5. **Ouvrir** une Pull Request

### Guidelines

- Respecter le style de code existant
- Ajouter des tests pour les nouvelles fonctionnalitÃ©s
- Mettre Ã  jour la documentation si nÃ©cessaire

---

## ğŸ“œ Licence

```
MIT License

Copyright (c) 2026 KAMDEM POUOKAM Ivann Harold

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

Voir le fichier [LICENSE](LICENSE) pour le texte complet.

---

<p align="center">
  <strong>Fait avec â¤ï¸ et ğŸ Python</strong>
  <br/>
  <a href="https://github.com/KpihX">@KpihX</a>
</p>
