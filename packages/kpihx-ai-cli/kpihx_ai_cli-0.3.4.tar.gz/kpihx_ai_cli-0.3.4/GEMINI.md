# Gemini Context: ai-cli

## 1. üë§ Identit√© du Projet
- **Nom :** ai-cli
- **Version :** 0.3.0
- **Auteur :** KAMDEM POUOKAM Ivann Harold (@KpihX)
- **Objectif :** Fournir une interface CLI robuste, intelligente et hautement configurable pour interagir avec des mod√®les LLM locaux via Ollama.

## 2. üíª Profil Technique & Architecture
- **Langage :** Python 3.11+
- **Gestionnaire de d√©pendances :** `uv` (utilis√© pour l'initialisation, le packaging et les tests).
- **Stack logicielle :**
  - `typer` : Gestion de la CLI et des arguments.
  - `rich` : Rendu visuel (Markdown, Panels, Live, Tables).
  - `prompt_toolkit` : Saisie interactive avanc√©e (auto-suggestions, historique, compl√©tion Tab).
  - `requests` : Communication avec l'API REST d'Ollama.
  - `pyyaml` : Gestion de la configuration externalis√©e.
- **Structure Modulaire :**
  - `main.py` : Point d'entr√©e, boucle REPL et routage des commandes slash.
  - `storage.py` : Gestion de la persistance (JSON sessions et m√©moire Markdown).
  - `ollama_client.py` : Abstraction de l'API Ollama avec m√©thodes s√©par√©es `chat_stream()` / `chat_sync()`.

## 3. ‚öôÔ∏è Fonctionnalit√©s Cl√©s
- **Z√©ro Hardcoding :** Tous les prompts syst√®me (g√©n√©ration de titre, r√©sum√©, m√©moire) sont dans `config.yaml`.
- **Gestion d'Historique :** Stockage automatique dans `~/.ai-cli/sessions/` au format JSON avec support UTF-8.
- **M√©moire Persistante :** Utilisation de `~/.ai-cli/AI_CLI.md` pour injecter des connaissances sur l'utilisateur dans chaque session.
- **Optimisation de Contexte :** Commande `/resume` pour compresser les anciens messages via un r√©sum√© LLM sans perdre le fil de la discussion.
- **Auto-gestion d'Ollama :** D√©tection automatique du serveur, d√©marrage si n√©cessaire, et t√©l√©chargement (`pull`) transparent des mod√®les manquants.
- **Gestion d'Erreurs Robuste :** Exceptions personnalis√©es (`OllamaConnectionError`, `StorageError`) avec logging appropri√©.
- **Options CLI avanc√©es :** `-l` pour lister les mod√®les, `-d` pour changer le mod√®le par d√©faut, `-m` pour utiliser un mod√®le sp√©cifique.

## 4. üõ† Workflow de D√©veloppement
- **Installation :** `uv tool install .` ou `pipx install .`
- **Tests :** 46 tests unitaires avec `pytest` et `pytest-mock` situ√©s dans le dossier `tests/`.
  - `test_main.py` : 12 tests (config, SessionState, slash commands)
  - `test_client.py` : 16 tests (stream, sync, erreurs r√©seau)
  - `test_storage.py` : 18 tests (CRUD, Delete, Unicode, edge cases)
- **Conventions :** Adh√©sion stricte aux standards de modularit√© et de s√©paration des pr√©occupations (SOC).

## 5. üß† Commandes Slash Support√©es
- `/new` : Archive la session actuelle et en d√©marre une nouvelle.
- `/old` : Liste, charge ou supprime des discussions pr√©c√©dentes.
- `/save <info>` : Enregistre un fait dans `AI_CLI.md`.
- `/resume` : R√©sume l'historique au-del√† du `summary_threshold`.
- `/settings` : Modifie les param√®tres de session (mod√®le, temp√©rature, etc.).
- `/clear` : Nettoie l'interface.
- `/help` : Affiche l'aide des commandes disponibles.
- `/exit` : Quitte proprement avec sauvegarde.

## 6. üèóÔ∏è Architecture des Modules

### ollama_client.py
- `OllamaClient` : Classe principale avec m√©thodes :
  - `chat_stream()` : G√©n√©rateur pour r√©ponses en streaming
  - `chat_sync()` : Appel synchrone retournant la r√©ponse compl√®te
  - `chat()` : Wrapper backward-compatible
  - `generate_title()` / `summarize()` : Utilitaires LLM (titre am√©lior√© avec multi-contexte)
  - `is_running()` / `list_models()` / `model_exists()` : Gestion mod√®les
- `OllamaConnectionError` : Exception personnalis√©e

### storage.py
- `StorageManager` : Gestion fichiers avec :
  - `save_session()` / `load_session()` / `list_sessions()` / `delete_session()`
  - `save_memory()` / `get_memory()`
  - `ensure_default_config()` / `get_config_path()`
  - Support UTF-8, gestion fichiers corrompus
- `StorageError` : Exception personnalis√©e

### main.py
- `SessionState` : √âtat de la conversation avec param√®tres modifiables
- `load_config()` : Chargement config YAML avec fallback
- `run_interactive()` : Boucle REPL principale avec Ctrl+C handling
- `handle_save_and_exit()` : Sauvegarde avec suggestion de titre am√©lior√©e
- `display_models_list()` / `update_default_model_in_config()` : Gestion mod√®les CLI
