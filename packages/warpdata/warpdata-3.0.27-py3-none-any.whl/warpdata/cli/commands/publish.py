"""Publish CLI command."""

from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path


def _find_in_local_cache(uri: str, settings) -> Path | None:
    """Find a content-addressed S3 object in local cache.

    Searches for the file by content hash in:
    1. workspace_root/data/**/objects/
    2. cache_dir/blobs/
    3. cache_dir/objects/

    Args:
        uri: S3 URI with content-addressed path pattern
        settings: Settings object with workspace_root and cache_dir

    Returns:
        Path to local file if found, None otherwise
    """
    import re

    # Extract content hash from URI pattern: objects/{2}/{2}/{64char hash}
    match = re.search(r'/objects/([0-9a-f]{2})/([0-9a-f]{2})/([0-9a-f]{64})(?:\?|$)', uri)
    if not match:
        return None

    prefix1, prefix2, content_hash = match.groups()
    relative_path = f"objects/{prefix1}/{prefix2}/{content_hash}"

    # 1. Check workspace data directories
    data_dir = settings.workspace_root / "data"
    if data_dir.exists():
        for candidate in data_dir.glob(f"**/{relative_path}"):
            if candidate.is_file():
                return candidate

    # 2. Check cache_dir/blobs (by content hash as filename)
    blobs_dir = settings.cache_dir / "blobs"
    if blobs_dir.exists():
        blob_path = blobs_dir / content_hash
        if blob_path.is_file():
            return blob_path

    # 3. Check cache_dir/objects
    cache_objects = settings.cache_dir / relative_path
    if cache_objects.is_file():
        return cache_objects

    return None


def _normalize_ref_value(v: str | None, artifact_root: Path) -> str | None:
    """Convert absolute path to relative tar member path.

    Args:
        v: Original ref value (may be absolute path)
        artifact_root: Root directory of the artifact

    Returns:
        Relative path suitable for tar member lookup
    """
    if v is None:
        return None
    v = str(v)

    # Normalize separators early
    v_norm = v.replace("\\", "/")

    # If absolute path, make relative to artifact root
    if os.path.isabs(v_norm):
        p = Path(v_norm).resolve()
        root = artifact_root.resolve()
        try:
            rel = p.relative_to(root)
        except ValueError:
            raise ValueError(
                f"Ref value points outside artifact root.\n"
                f"  value: {v}\n"
                f"  artifact_root: {artifact_root}"
            )
        return rel.as_posix()

    # Already relative â†’ keep but normalize
    return Path(v_norm).as_posix().lstrip("./")


def _rewrite_parquet_ref_columns(
    parquet_path: Path,
    column_to_root: dict[str, Path],
    output_path: Path,
) -> None:
    """Rewrite ref columns in parquet to use relative paths.

    Args:
        parquet_path: Source parquet file
        column_to_root: Mapping of column name to artifact root
        output_path: Destination for rewritten parquet
    """
    import pyarrow as pa
    import pyarrow.parquet as pq

    table = pq.read_table(parquet_path)

    for col, root in column_to_root.items():
        if col not in table.column_names:
            continue
        values = table[col].to_pylist()
        new_values = [_normalize_ref_value(v, root) for v in values]
        idx = table.schema.get_field_index(col)
        table = table.set_column(idx, col, pa.array(new_values, type=pa.string()))

    pq.write_table(table, output_path)


def parse_size(size_str: str) -> int:
    """Parse human-readable size string to bytes."""
    size_str = size_str.strip().upper()

    multipliers = {
        "B": 1,
        "KB": 1024,
        "K": 1024,
        "MB": 1024 * 1024,
        "M": 1024 * 1024,
        "GB": 1024 * 1024 * 1024,
        "G": 1024 * 1024 * 1024,
    }

    for suffix, mult in multipliers.items():
        if size_str.endswith(suffix):
            num_str = size_str[: -len(suffix)].strip()
            return int(float(num_str) * mult)

    return int(size_str)


def parse_binding(binding_str: str) -> tuple[str, str, str, str, str]:
    """Parse a binding string.

    Format: table.column=artifact:media_type:ref_type

    Returns:
        (table, column, artifact, media_type, ref_type)
    """
    if "=" not in binding_str:
        raise ValueError(f"Invalid binding format: {binding_str}")

    left, right = binding_str.split("=", 1)

    if "." not in left:
        raise ValueError(f"Invalid binding format (need table.column): {binding_str}")

    table, column = left.split(".", 1)

    parts = right.split(":")
    if len(parts) < 2:
        raise ValueError(f"Invalid binding format (need artifact:media_type): {binding_str}")

    artifact = parts[0]
    media_type = parts[1]
    ref_type = parts[2] if len(parts) > 2 else "tar_member_path"

    return table, column, artifact, media_type, ref_type


def _save_local_sources(settings, dataset_id: str, artifact_roots: dict[str, Path]) -> None:
    """Save local source paths so they can be used after publish.

    This allows the resolver to use local files instead of downloading from S3,
    even after the manifest points to S3 URIs.
    """
    import json

    sources_path = settings.workspace_root / "local_sources.json"

    # Load existing sources
    if sources_path.exists():
        try:
            with open(sources_path) as f:
                sources = json.load(f)
        except (json.JSONDecodeError, IOError):
            sources = {}
    else:
        sources = {}

    # Add/update mappings for this dataset's artifacts
    for artifact_name, local_path in artifact_roots.items():
        key = f"{dataset_id}/{artifact_name}"
        sources[key] = str(local_path)

    # Save
    sources_path.parent.mkdir(parents=True, exist_ok=True)
    with open(sources_path, "w") as f:
        json.dump(sources, f, indent=2)


def _publish_existing_dataset(args: argparse.Namespace, settings, base_uri: str) -> int:
    """Publish an existing local dataset to S3.

    This uploads local files to S3 and rewrites the manifest with S3 URIs.
    """
    import hashlib
    import json
    import boto3

    dataset_id = args.dataset

    # Load existing manifest
    manifest_dir = settings.workspace_root / "manifests"
    workspace, name = dataset_id.split("/", 1)
    dataset_manifest_dir = manifest_dir / workspace / name

    latest_path = dataset_manifest_dir / "latest.json"
    if not latest_path.exists():
        print(f"Error: Dataset '{dataset_id}' not found locally", file=sys.stderr)
        print(f"Expected manifest at: {dataset_manifest_dir}", file=sys.stderr)
        return 1

    with open(latest_path) as f:
        latest = json.load(f)
    version = latest["version"]

    manifest_path = dataset_manifest_dir / f"{version}.json"
    with open(manifest_path) as f:
        manifest = json.load(f)

    # Note: We do NOT migrate URIs here - we handle legacy bucket references
    # during the upload logic by checking if content already exists on target

    print(f"Publishing existing dataset: {dataset_id}")
    print(f"  Version: {version[:12]}")
    print(f"  Destination: {base_uri}")

    # Parse S3 bucket/prefix from base_uri
    if not base_uri.startswith("s3://"):
        print(f"Error: Only S3 publishing is supported. Got: {base_uri}", file=sys.stderr)
        return 1

    from urllib.parse import urlparse
    parsed = urlparse(base_uri)
    bucket = parsed.netloc
    base_prefix = parsed.path.lstrip("/")
    objects_prefix = f"{base_prefix}/objects" if base_prefix else "objects"

    # Create S3 client with custom endpoint if configured (for B2, MinIO, etc.)
    s3_kwargs = {}
    if settings.s3_endpoint_url:
        s3_kwargs["endpoint_url"] = settings.s3_endpoint_url
    if settings.s3_region:
        s3_kwargs["region_name"] = settings.s3_region
    s3 = boto3.client("s3", **s3_kwargs)

    # Track upload failures - publish will abort if any data can't be uploaded
    upload_failures: list[str] = []
    upload_stats = {"uploaded": 0, "skipped": 0, "bytes_uploaded": 0}

    def upload_file_to_s3(local_path: Path, content_type: str = "application/octet-stream") -> str:
        """Upload a file to S3 with content-addressable path. Returns S3 URI."""
        from tqdm import tqdm

        # Compute SHA256 hash
        file_size = local_path.stat().st_size
        sha256 = hashlib.sha256()
        with open(local_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        file_hash = sha256.hexdigest()

        # S3 key: objects/ab/cd/abcd1234...
        s3_key = f"{objects_prefix}/{file_hash[:2]}/{file_hash[2:4]}/{file_hash}"
        s3_uri = f"s3://{bucket}/{s3_key}"

        # Check if already exists
        try:
            s3.head_object(Bucket=bucket, Key=s3_key)
            size_mb = file_size / (1024 * 1024)
            print(f"    [skip] {local_path.name} ({size_mb:.1f} MB, already exists)")
            upload_stats["skipped"] += 1
            return s3_uri
        except s3.exceptions.ClientError:
            pass

        # Upload with progress bar
        size_mb = file_size / (1024 * 1024)

        class ProgressCallback:
            def __init__(self, total_size, filename):
                self.pbar = tqdm(
                    total=total_size,
                    unit='B',
                    unit_scale=True,
                    unit_divisor=1024,
                    desc=f"    {filename[:40]}",
                    bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{rate_fmt}]'
                )

            def __call__(self, bytes_transferred):
                self.pbar.update(bytes_transferred)

            def close(self):
                self.pbar.close()

        callback = ProgressCallback(file_size, local_path.name)
        try:
            s3.upload_file(
                str(local_path),
                bucket,
                s3_key,
                ExtraArgs={"ContentType": content_type},
                Callback=callback,
            )
            upload_stats["uploaded"] += 1
            upload_stats["bytes_uploaded"] += file_size
        finally:
            callback.close()

        return s3_uri

    def resolve_local_path(uri: str) -> Path | None:
        """Resolve a local:// or file:// URI to a Path."""
        if uri.startswith("local://"):
            rel_path = uri[8:]
            return settings.workspace_root / rel_path
        elif uri.startswith("file://"):
            return Path(uri[7:])
        return None

    # Build mapping: artifact_name -> local root dir (only for directory artifacts)
    artifact_roots: dict[str, Path] = {}
    for artifact_name, artifact_data in manifest.get("artifacts", {}).items():
        if artifact_data.get("kind") != "directory":
            continue
        if not artifact_data.get("shards"):
            continue
        base_uri = artifact_data["shards"][0]["uri"]
        local_root = resolve_local_path(base_uri)
        if local_root and local_root.exists() and local_root.is_dir():
            artifact_roots[artifact_name] = local_root

    # Save local sources so resolver can use them after publish
    # This is the key fix: local paths are preserved alongside S3 URIs
    if artifact_roots:
        _save_local_sources(settings, dataset_id, artifact_roots)
        print(f"\nSaved local source mappings for {len(artifact_roots)} artifact(s)")

    # Determine which table columns must be rewritten: column -> artifact_root
    column_to_root: dict[str, Path] = {}
    for b in manifest.get("bindings", []):
        # Only rewrite columns bound to directory artifacts we are converting
        art = b["artifact"]
        if art in artifact_roots:
            column_to_root[b["column"]] = artifact_roots[art]

    # Rewrite parquet shards locally (only if needed)
    rewritten_parquet: dict[str, list[Path]] = {}  # table_name -> rewritten shard paths
    tmp_dir = None

    if column_to_root:
        import tempfile
        tmp_dir = Path(tempfile.mkdtemp(prefix="warpdata_rewrite_"))
        print(f"\nRewriting ref columns: {list(column_to_root.keys())}")

        for table_name, table_data in manifest.get("tables", {}).items():
            new_paths = []
            for i, shard in enumerate(table_data.get("shards", [])):
                uri = shard.get("uri") or shard.get("key")
                local_path = resolve_local_path(uri) if uri else None

                if not local_path or not local_path.exists():
                    # If it's already remote and you can't rewrite, error
                    raise RuntimeError(f"Cannot rewrite refs; shard not local: {uri}")

                out_path = tmp_dir / f"{table_name}--shard-{i:05d}.parquet"
                _rewrite_parquet_ref_columns(local_path, column_to_root, out_path)
                new_paths.append(out_path)
                print(f"    Rewrote {local_path.name}")

            rewritten_parquet[table_name] = new_paths

    # Upload table shards
    print("\nUploading table shards...")
    new_tables = {}
    for table_name, table_data in manifest.get("tables", {}).items():
        new_shards = []
        for i, shard in enumerate(table_data.get("shards", [])):
            uri = shard.get("uri") or shard.get("key")

            # Use rewritten parquet if available
            if table_name in rewritten_parquet:
                local_path = rewritten_parquet[table_name][i]
            else:
                local_path = resolve_local_path(uri) if uri else None

            if local_path and local_path.exists():
                new_uri = upload_file_to_s3(local_path, "application/octet-stream")
                new_shards.append({**shard, "uri": new_uri})
            elif uri and uri.startswith("s3://"):
                # Check if already on the TARGET bucket
                if uri.startswith(f"s3://{bucket}/"):
                    # Already on target bucket, keep as-is
                    new_shards.append(shard)
                else:
                    # Different bucket - check if content hash already exists on target
                    # Content-addressed URIs: .../objects/{2}/{2}/{64-char hash}
                    import re
                    hash_match = re.search(r'/objects/([0-9a-f]{2})/([0-9a-f]{2})/([0-9a-f]{64})(?:\?|$)', uri)
                    if hash_match:
                        p1, p2, content_hash = hash_match.groups()
                        target_key = f"{objects_prefix}/{p1}/{p2}/{content_hash}"
                        target_uri = f"s3://{bucket}/{target_key}"
                        # Check if it exists on target bucket
                        try:
                            s3.head_object(Bucket=bucket, Key=target_key)
                            # Already exists on target - use target URI
                            print(f"    [skip] {content_hash[:12]}... (already on target)")
                            upload_stats["skipped"] += 1
                            new_shards.append({**shard, "uri": target_uri})
                            continue
                        except s3.exceptions.ClientError:
                            pass  # Not on target, continue to check local cache

                    # Check local cache
                    cache_path = _find_in_local_cache(uri, settings)
                    if cache_path:
                        new_uri = upload_file_to_s3(cache_path, "application/octet-stream")
                        new_shards.append({**shard, "uri": new_uri})
                    else:
                        print(f"    [FAIL] Cannot migrate table shard from different bucket: {uri}", file=sys.stderr)
                        print(f"           Data not found in local cache. Run 'warp sync {dataset_id}' first.", file=sys.stderr)
                        upload_failures.append(f"table:{table_name}:shard:{i}")
                        new_shards.append(shard)
            else:
                print(f"    [FAIL] Cannot resolve table shard: {uri}", file=sys.stderr)
                upload_failures.append(f"table:{table_name}:shard:{i}")
                new_shards.append(shard)

        new_tables[table_name] = {**table_data, "shards": new_shards}

    # Upload artifacts (if they have local paths)
    print("\nUploading artifacts...")
    new_artifacts = {}
    for artifact_name, artifact_data in manifest.get("artifacts", {}).items():
        kind = artifact_data.get("kind", "")

        if kind == "directory":
            # Need to pack directory into tar and upload
            new_shards = []
            all_tar_paths = []  # Collect for index building
            for shard in artifact_data.get("shards", []):
                uri = shard.get("uri") or shard.get("key")
                local_path = resolve_local_path(uri) if uri else None

                if local_path and local_path.exists() and local_path.is_dir():
                    # Pack directory to tar
                    print(f"    Packing {artifact_name} from {local_path}...")
                    from warpdata.publish.packer import pack_directory_to_tar_shards

                    # Use workspace cache for temp files (more space than /tmp)
                    tar_output = settings.cache_dir / "publish_tmp" / f".warp_tar_{artifact_name}"
                    tar_shards = pack_directory_to_tar_shards(
                        local_path,
                        output_dir=tar_output,
                        shard_size_bytes=4 * 1024 * 1024 * 1024,  # 4GB shards
                    )

                    # Upload tar shards
                    for tar_shard in tar_shards:
                        tar_uri = upload_file_to_s3(tar_shard.path, "application/x-tar")
                        new_shards.append({
                            "uri": tar_uri,
                            "byte_size": tar_shard.size_bytes,
                        })
                        all_tar_paths.append(tar_shard.path)
                else:
                    new_shards.append(shard)

            # Build artifact index for tar shards
            index_info = None
            if all_tar_paths:
                import tempfile
                from warpdata.artifacts.tar.index_builder import build_tar_index, write_index_parquet

                print(f"    Building index for {artifact_name}...")
                entries = build_tar_index(all_tar_paths)

                index_tmp_dir = Path(tempfile.mkdtemp(prefix="warpdata_art_index_"))
                index_path = index_tmp_dir / f"{artifact_name}_index.parquet"
                write_index_parquet(entries, index_path)

                # Upload index
                index_uri = upload_file_to_s3(index_path, "application/octet-stream")
                index_info = {
                    "uri": index_uri,
                    "byte_size": index_path.stat().st_size,
                }

                # Clean up index temp file
                index_path.unlink()
                index_tmp_dir.rmdir()

            # Clean up temp tar files
            for tar_path in all_tar_paths:
                if tar_path.exists():
                    tar_path.unlink()

            # Change kind from directory to tar_shards, include index
            artifact_dict = {
                "kind": "tar_shards",
                "shards": new_shards,
            }
            if index_info:
                artifact_dict["index"] = index_info
            new_artifacts[artifact_name] = artifact_dict

        elif kind in ("tar", "tar_shards"):
            # Already tar, just upload if local
            import re
            new_shards = []
            for shard in artifact_data.get("shards", []):
                uri = shard.get("uri") or shard.get("key")
                local_path = resolve_local_path(uri) if uri else None

                if local_path and local_path.exists():
                    new_uri = upload_file_to_s3(local_path, "application/x-tar")
                    new_shards.append({**shard, "uri": new_uri})
                elif uri and uri.startswith("s3://"):
                    # Check if already on the TARGET bucket
                    if uri.startswith(f"s3://{bucket}/"):
                        new_shards.append(shard)
                    else:
                        # Different bucket - check if content hash already exists on target
                        hash_match = re.search(r'/objects/([0-9a-f]{2})/([0-9a-f]{2})/([0-9a-f]{64})(?:\?|$)', uri)
                        if hash_match:
                            p1, p2, content_hash = hash_match.groups()
                            target_key = f"{objects_prefix}/{p1}/{p2}/{content_hash}"
                            target_uri = f"s3://{bucket}/{target_key}"
                            try:
                                s3.head_object(Bucket=bucket, Key=target_key)
                                print(f"    [skip] {content_hash[:12]}... (already on target)")
                                upload_stats["skipped"] += 1
                                new_shards.append({**shard, "uri": target_uri})
                                continue
                            except s3.exceptions.ClientError:
                                pass

                        # Check local cache
                        cache_path = _find_in_local_cache(uri, settings)
                        if cache_path:
                            new_uri = upload_file_to_s3(cache_path, "application/x-tar")
                            new_shards.append({**shard, "uri": new_uri})
                        else:
                            print(f"    [FAIL] Cannot migrate artifact shard from different bucket: {uri}", file=sys.stderr)
                            print(f"           Data not found in local cache. Run 'warp sync {dataset_id}' first.", file=sys.stderr)
                            upload_failures.append(f"artifact:{artifact_name}:shard")
                            new_shards.append(shard)
                else:
                    print(f"    [FAIL] Cannot resolve artifact shard: {uri}", file=sys.stderr)
                    upload_failures.append(f"artifact:{artifact_name}:shard")
                    new_shards.append(shard)

            # Also migrate the index if present and on different bucket
            new_index = artifact_data.get("index")
            if new_index:
                index_uri = new_index.get("uri")
                if index_uri and index_uri.startswith("s3://"):
                    if not index_uri.startswith(f"s3://{bucket}/"):
                        # Check if content hash already exists on target
                        hash_match = re.search(r'/objects/([0-9a-f]{2})/([0-9a-f]{2})/([0-9a-f]{64})(?:\?|$)', index_uri)
                        if hash_match:
                            p1, p2, content_hash = hash_match.groups()
                            target_key = f"{objects_prefix}/{p1}/{p2}/{content_hash}"
                            target_uri = f"s3://{bucket}/{target_key}"
                            try:
                                s3.head_object(Bucket=bucket, Key=target_key)
                                print(f"    [skip] index {content_hash[:12]}... (already on target)")
                                upload_stats["skipped"] += 1
                                new_index = {**new_index, "uri": target_uri}
                            except s3.exceptions.ClientError:
                                pass  # Not on target, continue to check local cache

                        if new_index.get("uri") == index_uri:  # Still original URI, need to migrate
                            cache_path = _find_in_local_cache(index_uri, settings)
                            if cache_path:
                                new_index_uri = upload_file_to_s3(cache_path, "application/octet-stream")
                                new_index = {**new_index, "uri": new_index_uri}
                            else:
                                print(f"    [FAIL] Cannot migrate artifact index from different bucket: {index_uri}", file=sys.stderr)
                                print(f"           Data not found in local cache. Run 'warp sync {dataset_id}' first.", file=sys.stderr)
                                upload_failures.append(f"artifact:{artifact_name}:index")

            artifact_result = {**artifact_data, "shards": new_shards}
            if new_index:
                artifact_result["index"] = new_index
            new_artifacts[artifact_name] = artifact_result
        else:
            # Unknown kind, keep as is
            new_artifacts[artifact_name] = artifact_data

    # Update bindings to match artifact kind changes (directory -> tar_shards)
    new_bindings = []
    for b in manifest.get("bindings", []):
        art = b["artifact"]
        if art in new_artifacts and new_artifacts[art]["kind"] == "tar_shards":
            # Ensure correct ref_type for tar_shards
            b = {**b, "ref_type": "tar_member_path"}
        new_bindings.append(b)

    # FAIL if any data couldn't be uploaded
    if upload_failures:
        print(f"\n{'='*60}", file=sys.stderr)
        print(f"PUBLISH FAILED: {len(upload_failures)} data file(s) could not be uploaded", file=sys.stderr)
        print(f"{'='*60}", file=sys.stderr)
        print(f"\nThe following items were not available locally:", file=sys.stderr)
        for failure in upload_failures:
            print(f"  - {failure}", file=sys.stderr)
        print(f"\nTo fix this:", file=sys.stderr)
        print(f"  1. Run 'warp sync {dataset_id}' to download the data locally", file=sys.stderr)
        print(f"  2. Then run 'warp publish {dataset_id}' again", file=sys.stderr)
        print(f"\nNO manifest was published. Your remote dataset is unchanged.", file=sys.stderr)
        return 1

    # Build new manifest
    new_manifest = {
        **manifest,
        "tables": new_tables,
    }
    if new_artifacts:
        new_manifest["artifacts"] = new_artifacts
    if new_bindings:
        new_manifest["bindings"] = new_bindings

    # Compute new version hash
    manifest_json = json.dumps(new_manifest, sort_keys=True, separators=(",", ":"))
    new_version = hashlib.sha256(manifest_json.encode()).hexdigest()[:12]

    print(f"\nNew version: {new_version}")

    # Save new manifest locally
    new_manifest_path = dataset_manifest_dir / f"{new_version}.json"
    with open(new_manifest_path, "w") as f:
        json.dump(new_manifest, f, indent=2)

    # Update latest.json
    with open(latest_path, "w") as f:
        json.dump({"version": new_version}, f)

    print(f"  Saved local manifest: {new_manifest_path}")

    # Push to S3 manifests
    manifests_prefix = f"{base_prefix}/manifests" if base_prefix else "manifests"

    manifest_s3_key = f"{manifests_prefix}/{workspace}/{name}/{new_version}.json"
    latest_s3_key = f"{manifests_prefix}/{workspace}/{name}/latest.json"

    print(f"\nPushing manifest to S3...")
    s3.put_object(
        Bucket=bucket,
        Key=manifest_s3_key,
        Body=json.dumps(new_manifest, indent=2).encode("utf-8"),
        ContentType="application/json",
    )
    print(f"  Uploaded: s3://{bucket}/{manifest_s3_key}")

    s3.put_object(
        Bucket=bucket,
        Key=latest_s3_key,
        Body=json.dumps({"version": new_version}).encode("utf-8"),
        ContentType="application/json",
    )
    print(f"  Updated latest pointer")

    # Format bytes
    def _format_bytes(b: int) -> str:
        if b >= 1024 * 1024 * 1024:
            return f"{b / (1024*1024*1024):.1f} GB"
        elif b >= 1024 * 1024:
            return f"{b / (1024*1024):.1f} MB"
        elif b >= 1024:
            return f"{b / 1024:.1f} KB"
        return f"{b} B"

    print(f"\nPublish complete!")
    print(f"  Dataset: {dataset_id}")
    print(f"  Version: {new_version}")
    print(f"  Files uploaded: {upload_stats['uploaded']} ({_format_bytes(upload_stats['bytes_uploaded'])})")
    print(f"  Files skipped:  {upload_stats['skipped']} (already existed)")

    return 0


def _validate_canonical_bucket(base_uri: str, force: bool) -> bool:
    """Validate that publish target is the canonical B2 bucket.

    Publishing to non-canonical locations can cause data to be orphaned
    when manifests reference buckets that don't exist or aren't accessible.

    Args:
        base_uri: The S3 URI to publish to
        force: If True, bypass this check with a warning

    Returns:
        True if validation passes, False otherwise
    """
    from warpdata.config.settings import DEFAULT_MANIFEST_BASE, DEFAULT_S3_BUCKET

    # Parse the target bucket from base_uri
    if not base_uri.startswith("s3://"):
        # Non-S3 destinations (file://, etc.) require --force
        if force:
            print(f"Warning: Publishing to non-S3 destination: {base_uri}", file=sys.stderr)
            print(f"         This is allowed with --force but may cause issues.", file=sys.stderr)
            return True
        print(f"Error: Publishing to non-S3 destinations is not allowed.", file=sys.stderr)
        print(f"       Target: {base_uri}", file=sys.stderr)
        print(f"\nUse --force to override this check (not recommended).", file=sys.stderr)
        return False

    from urllib.parse import urlparse
    parsed = urlparse(base_uri)
    target_bucket = parsed.netloc

    # Check if it's the canonical bucket
    if target_bucket != DEFAULT_S3_BUCKET:
        if force:
            print(f"Warning: Publishing to non-canonical bucket: {target_bucket}", file=sys.stderr)
            print(f"         Canonical bucket is: {DEFAULT_S3_BUCKET}", file=sys.stderr)
            print(f"         This is allowed with --force but may cause data accessibility issues.", file=sys.stderr)
            print(file=sys.stderr)
            return True

        print(f"Error: Cannot publish to non-canonical bucket.", file=sys.stderr)
        print(f"", file=sys.stderr)
        print(f"  Target bucket:    {target_bucket}", file=sys.stderr)
        print(f"  Canonical bucket: {DEFAULT_S3_BUCKET}", file=sys.stderr)
        print(f"", file=sys.stderr)
        print(f"Publishing to non-canonical buckets can cause data to become", file=sys.stderr)
        print(f"inaccessible if the bucket is deleted or credentials change.", file=sys.stderr)
        print(f"", file=sys.stderr)
        print(f"To fix this:", file=sys.stderr)
        print(f"  1. Remove --base-uri to use the default bucket", file=sys.stderr)
        print(f"  2. Or unset WARPDATASETS_MANIFEST_BASE environment variable", file=sys.stderr)
        print(f"  3. Or use --force to override this check (not recommended)", file=sys.stderr)
        return False

    return True


def _run_preflight_check(settings) -> bool:
    """Run pre-flight connectivity check before publish.

    Returns:
        True if checks pass, False otherwise
    """
    from warpdata.tools.doctor.checks import (
        CheckStatus,
        check_storage_config,
        check_connectivity,
    )

    # Check storage configuration
    config_result = check_storage_config(settings)
    if config_result.status == CheckStatus.FAIL:
        print(f"Pre-flight check failed: {config_result.message}", file=sys.stderr)
        if config_result.details:
            print(f"  {config_result.details}", file=sys.stderr)
        if config_result.suggestion:
            print(f"\nHow to fix:\n  {config_result.suggestion}", file=sys.stderr)
        print(f"\nRun 'warp doctor --repair' for interactive help.", file=sys.stderr)
        return False

    # Check connectivity
    conn_result = check_connectivity(settings)
    if conn_result.status == CheckStatus.FAIL:
        print(f"Pre-flight check failed: {conn_result.message}", file=sys.stderr)
        if conn_result.details:
            print(f"  {conn_result.details}", file=sys.stderr)
        if conn_result.suggestion:
            print(f"\nHow to fix:\n  {conn_result.suggestion}", file=sys.stderr)
        print(f"\nRun 'warp doctor' for detailed diagnostics.", file=sys.stderr)
        return False

    return True


def _list_local_datasets(settings) -> list[str]:
    """List all locally registered datasets.

    Returns:
        List of dataset IDs (workspace/name)
    """
    manifest_dir = settings.workspace_root / "manifests"
    datasets = []

    if not manifest_dir.exists():
        return datasets

    for workspace_dir in manifest_dir.iterdir():
        if not workspace_dir.is_dir() or workspace_dir.name.startswith("."):
            continue
        workspace = workspace_dir.name

        for dataset_dir in workspace_dir.iterdir():
            if not dataset_dir.is_dir() or dataset_dir.name.startswith("."):
                continue
            name = dataset_dir.name

            # Check if it has a latest.json
            if (dataset_dir / "latest.json").exists():
                datasets.append(f"{workspace}/{name}")

    return sorted(datasets)


def _is_already_published(dataset_id: str, settings, s3) -> bool:
    """Check if a dataset's current version is already published on S3.

    Args:
        dataset_id: Dataset ID (workspace/name)
        settings: Settings object
        s3: boto3 S3 client

    Returns:
        True if current local version exists on S3
    """
    import json
    from urllib.parse import urlparse

    workspace, name = dataset_id.split("/", 1)

    # Get local version
    manifest_dir = settings.workspace_root / "manifests"
    latest_path = manifest_dir / workspace / name / "latest.json"

    if not latest_path.exists():
        return False

    with open(latest_path) as f:
        latest = json.load(f)
    local_version = latest["version"]

    # Check S3 for this version
    base_uri = settings.manifest_base
    if not base_uri:
        from warpdata.config.settings import DEFAULT_MANIFEST_BASE
        base_uri = DEFAULT_MANIFEST_BASE

    parsed = urlparse(base_uri)
    bucket = parsed.netloc
    base_prefix = parsed.path.lstrip("/")
    manifests_prefix = f"{base_prefix}/manifests" if base_prefix else "manifests"

    manifest_key = f"{manifests_prefix}/{workspace}/{name}/{local_version}.json"

    try:
        s3.head_object(Bucket=bucket, Key=manifest_key)
        return True
    except s3.exceptions.ClientError:
        return False


def _publish_all(args: argparse.Namespace, settings, base_uri: str) -> int:
    """Publish all locally registered datasets that aren't already published.

    Returns:
        Exit code (0 for success)
    """
    import boto3

    datasets = _list_local_datasets(settings)

    if not datasets:
        print("No local datasets found to publish.")
        return 0

    print(f"Found {len(datasets)} local dataset(s)")
    print()

    # Create S3 client
    s3_kwargs = {}
    if settings.s3_endpoint_url:
        s3_kwargs["endpoint_url"] = settings.s3_endpoint_url
    if settings.s3_region:
        s3_kwargs["region_name"] = settings.s3_region
    s3 = boto3.client("s3", **s3_kwargs)

    # Track results
    published = []
    skipped = []
    failed = []

    for dataset_id in datasets:
        print(f"[{dataset_id}]")

        # Check if already published
        if _is_already_published(dataset_id, settings, s3):
            print(f"  Already published, skipping")
            skipped.append(dataset_id)
            print()
            continue

        # Publish this dataset
        print(f"  Publishing...")
        # Create args copy with this dataset
        import copy
        dataset_args = copy.copy(args)
        dataset_args.dataset = dataset_id

        try:
            result = _publish_existing_dataset(dataset_args, settings, base_uri)
            if result == 0:
                published.append(dataset_id)
            else:
                failed.append(dataset_id)
        except Exception as e:
            print(f"  ERROR: {e}", file=sys.stderr)
            failed.append(dataset_id)

        print()

    # Summary
    print("=" * 60)
    print("Publish All Summary:")
    print(f"  Published: {len(published)}")
    print(f"  Skipped:   {len(skipped)} (already up-to-date)")
    print(f"  Failed:    {len(failed)}")

    if failed:
        print()
        print("Failed datasets:")
        for ds in failed:
            print(f"  - {ds}")
        return 1

    return 0


def run(args: argparse.Namespace) -> int:
    """Run publish command.

    Args:
        args: Parsed command line arguments

    Returns:
        Exit code (0 for success)
    """
    from warpdata.publish.builder import ManifestBuilder
    from warpdata.publish.packer import pack_directory_to_tar_shards
    from warpdata.publish.storage import create_storage
    from warpdata.publish.uploader import Uploader
    from warpdata.publish.verify import verify_publish
    from warpdata.config.settings import get_settings

    settings = get_settings()

    # Pre-flight connectivity check (unless --skip-check is specified)
    if not getattr(args, "skip_check", False):
        if not _run_preflight_check(settings):
            return 1

    # Get base URI - use default S3 bucket if not specified
    base_uri = getattr(args, "base_uri", None) or settings.manifest_base
    if not base_uri:
        # Use default warp bucket
        from warpdata.config.settings import DEFAULT_MANIFEST_BASE
        base_uri = DEFAULT_MANIFEST_BASE

    # Validate that we're publishing to the canonical bucket
    force = getattr(args, "force", False)
    if not _validate_canonical_bucket(base_uri, force):
        return 1

    # Handle --all flag
    if getattr(args, "publish_all", False):
        return _publish_all(args, settings, base_uri)

    # Require dataset if not --all
    if not args.dataset:
        print("Error: dataset is required (or use --all to publish all)", file=sys.stderr)
        return 1

    # Check if this is publishing an existing local dataset (no --table specified)
    if not getattr(args, "table", None):
        return _publish_existing_dataset(args, settings, base_uri)

    # Parse table inputs
    table_inputs = {}
    if args.table:
        for table_spec in args.table:
            if "=" not in table_spec:
                print(f"Error: Invalid table format: {table_spec}", file=sys.stderr)
                print("Expected: name=path/to/shards/*.parquet", file=sys.stderr)
                return 1

            name, pattern = table_spec.split("=", 1)

            # Expand glob pattern
            base_path = Path(pattern).parent
            glob_pattern = Path(pattern).name
            shard_paths = list(base_path.glob(glob_pattern))

            if not shard_paths:
                print(f"Error: No files found matching: {pattern}", file=sys.stderr)
                return 1

            table_inputs[name] = shard_paths

    if not table_inputs:
        print("Error: At least one --table is required", file=sys.stderr)
        return 1

    # Parse artifact inputs
    artifact_dirs = {}
    if args.artifact:
        for artifact_spec in args.artifact:
            if "=" not in artifact_spec:
                print(f"Error: Invalid artifact format: {artifact_spec}", file=sys.stderr)
                print("Expected: name=path/to/directory", file=sys.stderr)
                return 1

            name, dir_path = artifact_spec.split("=", 1)
            artifact_dirs[name] = Path(dir_path)

            if not artifact_dirs[name].is_dir():
                print(f"Error: Artifact directory not found: {dir_path}", file=sys.stderr)
                return 1

    # Parse bindings
    bindings = []
    if args.bind:
        for bind_str in args.bind:
            try:
                bindings.append(parse_binding(bind_str))
            except ValueError as e:
                print(f"Error: {e}", file=sys.stderr)
                return 1

    # Parse shard size
    shard_size = parse_size(args.shard_size)

    print(f"Publishing: {args.dataset}")
    print(f"Base URI: {base_uri}")

    # Build manifest
    builder = ManifestBuilder(
        dataset_id=args.dataset,
        base_uri=base_uri,
    )

    # Add tables
    for name, shard_paths in table_inputs.items():
        print(f"  Table '{name}': {len(shard_paths)} shards")
        builder.add_table(name, shard_paths)

    # Pack and add artifacts
    artifact_shards = {}
    for name, dir_path in artifact_dirs.items():
        print(f"  Packing artifact '{name}' from {dir_path}...")

        tar_output = Path(args.temp_dir or ".") / f".warp_tar_{name}"
        shards = pack_directory_to_tar_shards(
            dir_path,
            output_dir=tar_output,
            shard_size_bytes=shard_size,
        )

        print(f"    -> {len(shards)} tar shards")
        artifact_shards[name] = shards
        builder.add_artifact(name, shards)

    # Save local sources so resolver can use them after publish
    if artifact_dirs:
        _save_local_sources(settings, args.dataset, artifact_dirs)
        print(f"\nSaved local source mappings for {len(artifact_dirs)} artifact(s)")

    # Add bindings
    for table, column, artifact, media_type, ref_type in bindings:
        print(f"  Binding: {table}.{column} -> {artifact}:{media_type}")
        builder.add_binding(table, column, artifact, media_type, ref_type)

    # Build plan
    plan = builder.build_plan()

    print()
    print(f"Version hash: {plan.manifest.version_hash}")
    print(f"Total shards: {plan.shard_count}")
    print(f"Total bytes: {plan._format_bytes(plan.total_bytes)}")

    if args.dry_run:
        print()
        print("DRY RUN - would upload:")
        for upload in plan.all_uploads:
            print(f"  {upload.source_path.name} -> {upload.target_uri}")
        print(f"  manifest -> {plan.manifest_uri}")
        if args.set_latest:
            print(f"  latest -> {plan.latest_uri}")
        return 0

    # Create storage and uploader
    storage = create_storage(
        base_uri,
        region=settings.s3_region,
        endpoint_url=settings.s3_endpoint_url,
    )

    uploader = Uploader(
        storage=storage,
        concurrency=args.concurrency,
    )

    # Execute publish
    print()
    print("Uploading...")

    result = uploader.execute_plan(
        plan,
        skip_existing=not args.force,
        update_latest=args.set_latest,
    )

    if not result.success:
        print()
        print("Publish FAILED:", file=sys.stderr)
        for error in result.errors:
            print(f"  {error}", file=sys.stderr)
        return 1

    print()
    print(f"Uploaded: {result.shards_uploaded} shards")
    print(f"Skipped:  {result.shards_skipped} shards (already exist)")

    if result.latest_updated:
        print(f"Updated latest pointer")

    # Verify if requested
    if args.verify:
        print()
        print("Verifying...")
        verify_result = verify_publish(plan, storage, check_schema=True)

        if verify_result.success:
            print("Verification: PASSED")
        else:
            print("Verification: FAILED", file=sys.stderr)
            for error in verify_result.errors:
                print(f"  {error}", file=sys.stderr)
            return 1

    print()
    print("Publish complete!")
    print(f"  Dataset: {args.dataset}")
    print(f"  Version: {result.version_hash}")
    print(f"  Manifest: {result.manifest_uri}")

    return 0
