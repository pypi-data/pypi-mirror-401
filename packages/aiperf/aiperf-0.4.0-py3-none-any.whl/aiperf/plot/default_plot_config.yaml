# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

################################################################################
# AIPerf Plot Configuration
################################################################################
#
# This file defines which plots are generated by 'aiperf plot'.
#
# CUSTOMIZATION:
#   • First run: Auto-creates ~/.aiperf/plot_config.yaml
#   • Edit ~/.aiperf/plot_config.yaml to customize
#   • Changes take effect on next run
#
# QUICK START:
#   • Enable/disable plots: Edit the *_defaults lists below
#   • Customize plots: Modify preset definitions
#   • Add new plots: Add to presets, then reference in defaults
#
################################################################################

visualization:
  # =============================================================================
  # MULTI-RUN COMPARISON: Default plots when comparing multiple runs
  # =============================================================================
  multi_run_defaults:
    - pareto_curve_throughput_per_gpu_vs_latency
    - pareto_curve_throughput_per_gpu_vs_interactivity
    - ttft_vs_throughput

  # =============================================================================
  # SINGLE-RUN ANALYSIS: Default plots for analyzing one run over time
  # =============================================================================
  single_run_defaults:
    # AIPerf client-side metrics
    - ttft_over_time
    # - itl_over_time
    # - latency_over_time
    # - dispersed_throughput_over_time
    - ttft_timeline
    - timeslices_ttft
    - timeslices_itl
    # - timeslices_throughput
    # - timeslices_latency
    - gpu_utilization_and_throughput_over_time

  # =============================================================================
  # MULTI-RUN COMPARISON PRESETS
  # =============================================================================
  multi_run_plots:
  # CONFIGURATION GUIDE:
  #   labels: Controls what text appears at each data point
  #     - Accepts a single column name as a list: [model] or [concurrency]
  #   groups: Controls how runs are grouped into lines/series (determines colors)
  #     - Accepts a single column name as a list: [model] or [concurrency]
  #   autoscale: Controls which axes to autoscale (reduces whitespace)
  #     - Values: "none" (default), "x", "y", "both"
  #   x_label: Custom x-axis label (optional, auto-generated from metric name if omitted)
  #   y_label: Custom y-axis label (optional, auto-generated from metric name if omitted)
  #   description: Human-readable description of what this plot shows
    pareto_curve_throughput_per_gpu_vs_latency:
      type: pareto
      description: "Trade-off frontier showing optimal throughput vs latency configurations"
      x:
        metric: request_latency
        stat: avg
      y:
        metric: output_token_throughput_per_gpu
        stat: avg
      labels: [concurrency]
      groups: [model]
      title: "Pareto Curve: Token Throughput per GPU vs Latency"

    pareto_curve_throughput_per_gpu_vs_interactivity:
      type: pareto
      description: "Trade-off frontier balancing system efficiency vs user experience"
      x:
        metric: output_token_throughput_per_user
        stat: avg
      y:
        metric: output_token_throughput_per_gpu
        stat: avg
      labels: [concurrency]
      groups: [model]
      title: "Pareto Curve: Token Throughput per GPU vs Interactivity"

    ttft_vs_throughput:
      type: scatter_line
      description: "How time-to-first-token changes as throughput increases across configs"
      x:
        metric: time_to_first_token
        stat: avg
      y:
        metric: request_throughput
        stat: avg
      labels: [concurrency]
      groups: [model]
      title: "TTFT vs Throughput"

  # =============================================================================
  # SINGLE-RUN PRESETS (time-series over duration of profiling run)
  # =============================================================================
  # x_label and y_label are also supported here for custom axis labels
  single_run_plots:
    ttft_over_time:
      type: scatter
      description: "Individual TTFT values for each request over the run duration"
      x: request_number
      y: time_to_first_token
      title: "TTFT Per Request Over Time"

    itl_over_time:
      type: scatter
      description: "Individual inter-token latency for each request over the run"
      x: request_number
      y: inter_token_latency
      title: "Inter-Token Latency Per Request Over Time"

    latency_over_time:
      type: scatter_with_percentiles
      description: "Request latency with rolling p50/p95/p99 trend lines to spot degradation"
      x: timestamp
      y: request_latency
      title: "Request Latency Over Time with Percentiles"

    dispersed_throughput_over_time:
      type: area
      description: "Filled area showing token throughput distribution over time"
      x: timestamp_s
      y: throughput_tokens_per_sec
      title: "Dispersed Output Token Throughput Over Time"

    ttft_timeline:
      type: request_timeline
      description: "Gantt-style view showing TTFT vs generation time per request"
      x: request_start_ns
      y: time_to_first_token
      title: "Request Timeline: TTFT Phases Over Time"

    timeslices_ttft:
      type: timeslice
      description: "Average TTFT per time window to assess stability over time"
      x: Timeslice
      y: Time to First Token
      stat: avg
      source: timeslices
      title: "Time to First Token Across Time Slices"
      use_slice_duration: true

    timeslices_itl:
      type: timeslice
      description: "Average inter-token latency per time window"
      x: Timeslice
      y: Inter Token Latency
      stat: avg
      source: timeslices
      title: "Inter Token Latency Across Time Slices"
      use_slice_duration: true

    timeslices_throughput:
      type: timeslice
      description: "Request throughput per time window to track capacity changes"
      x: Timeslice
      y: Request Throughput
      stat: avg
      source: timeslices
      title: "Request Throughput Across Time Slices"
      use_slice_duration: true

    timeslices_latency:
      type: timeslice
      description: "Average request latency per time window"
      x: Timeslice
      y: Request Latency
      stat: avg
      source: timeslices
      title: "Request Latency Across Time Slices"
      use_slice_duration: true

    gpu_utilization_and_throughput_over_time:
      type: dual_axis
      description: "Token throughput overlaid with GPU utilization to correlate performance"
      x: timestamp_s
      y: throughput_tokens_per_sec
      y2: gpu_utilization
      title: "Output Token Throughput with GPU Utilization"
      primary_style:
        mode: lines
        line_shape: hv
      secondary_style:
        mode: lines
        fill: tozeroy
      supplementary_col: active_requests


# =============================================================================
# SETTINGS: General plot configuration and data processing
# =============================================================================
settings:
  # Server metrics downsampling configuration
  server_metrics_downsampling:
    # Enable/disable downsampling (default: true)
    # Downsampling reduces high-frequency Parquet data for efficient rendering
    enabled: true

    # Window size in seconds for aggregating data points (default: 5.0)
    # Larger values = fewer points, faster rendering, less detail
    # Smaller values = more points, slower rendering, more detail
    window_size_seconds: 5.0

    # Aggregation method for combining points within each window (default: "mean")
    # Options: "mean" (average), "max" (maximum), "min" (minimum), "median" (50th percentile)
    # - "mean": Best for smooth trends (recommended for most metrics)
    # - "max": Preserves peak values (useful for utilization spikes)
    # - "min": Preserves minimum values (useful for latency floors)
    # - "median": Robust to outliers
    aggregation_method: "mean"

# ==============================================================================
# EXPERIMENT CLASSIFICATION (Optional)
# ==============================================================================
#
# Classify runs as "baseline" or "treatment" for semantic color assignment:
#   • Baselines: Grey shades | Treatments: NVIDIA green shades
#   • Legend: Baselines first, then treatments (alphabetically sorted)
#
# BEHAVIOR:
#   When enabled, ALL multi-run plots automatically group by experiment_type
#   (overrides groups: [model] setting above for clean baseline vs treatment comparison)
#
# PRIORITY: Pattern matching > Default fallback
#   Patterns: Glob patterns (e.g., "*baseline*" matches baseline, my_baseline_1)
#   Default: Fallback when no patterns match
#
### UNCOMMENT BELOW TO ENABLE EXPERIMENT CLASSIFICATION ###
# experiment_classification:
#   baselines:
#     - "*_agg_*"
#     - "*baseline*"
#   treatments:
#     - "*_disagg_*"
#     - "*kvrouter*"
#     - "*treatment*"
#   default: treatment
