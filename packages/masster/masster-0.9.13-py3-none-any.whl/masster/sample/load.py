# mypy: disable-error-code="union-attr"
"""
_import.py

This module provides data import functionality for mass spectrometry files.
It handles loading and processing of various mass spectrometry file formats
including mzML, vendor formats (WIFF, RAW).

Key Features:
- **Multi-Format Support**: Load mzML, WIFF (SCIEX), and RAW (Thermo) files.
- **File Validation**: Check file existence and format compatibility.
- **Memory Management**: Support for on-disk and in-memory data handling.
- **Metadata Extraction**: Extract acquisition parameters and instrument information.
- **Error Handling**: Comprehensive error reporting for file loading issues.
- **Raw Data Processing**: Handle centroided and profile data with signal smoothing.

Functions:
- `load()`: Main file loading function with format detection.
- `_load_mzML()`: Specialized mzML file loader.
- `_load_wiff()`: SCIEX WIFF file loader.
- `_load_raw()`: Thermo RAW file loader.

Supported File Formats:
- mzML (open standard format)
- WIFF (SCIEX vendor format)
- RAW (Thermo proprietary format)

See Also:
- `parameters._import_parameters`: For import-specific parameter configuration.
- `_export.py`: For data export functionality.
- `single.py`: For using imported data with ddafile class.

"""

from datetime import datetime
import os
import warnings

import numpy as np
import pandas as pd
import polars as pl
from tqdm import tqdm

from masster.chromatogram import Chromatogram
from masster.exceptions import (
    ConfigurationError,
    FileFormatError,
    InvalidPolarityError,
)
from masster.spectrum import Spectrum

from .h5 import _load_sample5

# Suppress pyOpenMS warnings globally
warnings.filterwarnings("ignore", message=".*OPENMS_DATA_PATH.*", category=UserWarning)
warnings.filterwarnings(
    "ignore",
    message="Warning: OPENMS_DATA_PATH.*",
    category=UserWarning,
)

# Import pyopenms with suppressed warnings
with warnings.catch_warnings():
    warnings.filterwarnings(
        "ignore",
        message=".*OPENMS_DATA_PATH environment variable already exists.*",
        category=UserWarning,
    )
    warnings.filterwarnings(
        "ignore",
        message="Warning: OPENMS_DATA_PATH.*",
        category=UserWarning,
    )
    import pyopenms as oms


def load(
    self,
    filename: str | None = None,
    ondisk: bool = False,
    type: str | None = None,
    label: str | None = None,
    sample_idx: int | None = None,
    interface: str | None = None,
    cachelevel: list[int] | None = [1],
    mslevel: list[int] | None = None,
) -> None:
    """
    Load file content from a specified filename.

    Parameters:
        filename: The path to the file or folder to load. Supports:
                 - .mzML (open format)
                 - .wiff/.wiff2 (SCIEX)
                 - .raw (Thermo Fisher)
                 - .d (Agilent/Bruker, requires rawreader)
                 - .sample5 (masster native format)
        ondisk: Indicates whether the file should be treated as on disk. Defaults to False.
        type: Specifies the type of file. If provided and set to 'ztscan' (case-insensitive), the type
             attribute will be adjusted accordingly. Defaults to None.
        label: An optional label to associate with the loaded file. Defaults to None.
        sample_idx: For WIFF files with multiple samples, specifies which sample to load (0-indexed).
                   Defaults to None (loads the last sample if multiple exist).
        interface: Specifies which loader interface to use:
                  - None (default): Auto-detects best available loader
                    * For .wiff files: rawreader > masster > alpharaw
                    * For .raw files: rawreader > alpharaw
                    * For .mzML files: oms
                  - "rawreader": Forces rawreader (for .wiff, .raw, .d)
                  - "masster": Forces masster's native reader (for .wiff only)
                  - "alpharaw": Forces alpharaw (for .wiff, .raw)
                  - "oms": Forces OpenMS (for .mzML)
        cachelevel: For files using rawreader/masster (.wiff, .raw, .d), specifies which MS levels to cache
                   spectrum data for. Examples:
                   - [1]: Cache MS1 only
                   - [2]: Cache MS2 only
                   - [1, 2]: Cache both MS1 and MS2
                   - None (default): Cache all MS levels
                   Use this to reduce memory usage when only specific MS levels are needed.
        mslevel: For files using rawreader/masster (.wiff, .raw, .d), specifies which MS levels to load
                from the file. Examples:
                - [1]: Load MS1 scans only
                - [2]: Load MS2 scans only
                - [1, 2]: Load both MS1 and MS2 scans
                - None (default): Load all MS levels
                Use this to reduce memory usage and loading time when only specific MS levels are needed.

    Raises:
        FileNotFoundError:
            If the file specified by filename does not exist.
        FileFormatError:
            If the file extension is not one of the supported types, or if .d files are
            loaded without rawreader installed.
        ConfigurationError:
            If the type parameter has an invalid value.

    Notes:
        The function determines the appropriate internal loading mechanism based on file extension
        and the interface parameter:

        **File Format Support:**
            - ".mzml": OpenMS loader (oms) via _load_mzML()
            - ".wiff/.wiff2": rawreader/masster/alpharaw via _load_wiff_v2()/_load_wiff_masster()/_load_wiff()
            - ".raw": rawreader or alpharaw via _load_raw_v2()/_load_raw()
            - ".d": rawreader only (Agilent/Bruker) via _load_d_v2()
            - ".sample5": masster native format via _load_sample5()

        **Interface Selection:**
            When interface=None (default):
                - Attempts to use rawreader for .wiff/.raw/.d files
                - Falls back to masster for .wiff files (if rawreader unavailable)
                - Falls back to alpharaw if neither rawreader nor masster available
                - Uses oms for .mzML files

            When interface is specified:
                - Forces the specified loader (rawreader, alpharaw, or oms)
                - Raises error if the specified interface is not available or not compatible

        **Post-Loading:**
            - The type attribute defaults to 'dda' unless 'ztscan' is detected or specified
            - For rawreader-based loaders, acquisition metadata and method parameters are
              automatically stored in sample.history for reproducibility
            - The file_interface attribute is set to indicate which loader was used
    """
    # Validate filename
    if filename is None:
        filename = self.file_path

    if filename is None:
        raise FileNotFoundError(
            "No filename provided and sample.file_path is not set.\n"
            "Usage: sample.load('path/to/file.mzML')",
        )

    filename = os.path.abspath(filename)

    if not os.path.exists(filename):
        raise FileNotFoundError(
            f"Sample file not found: {filename}\n"
            f"Supported formats: .raw (Thermo), .wiff/.wiff2 (SCIEX), .mzML (open format), .sample5 (masster format)",
        )

    # Validate type parameter if provided
    if type is not None and type.lower() not in ["ztscan", "dda"]:
        raise ConfigurationError(
            f"Invalid type parameter: '{type}'\n"
            f"Valid options: 'ztscan', 'dda', or None (auto-detect)",
        )

    self.ondisk = ondisk

    # Determine file format and load accordingly
    file_lower = filename.lower()
    if file_lower.endswith(".mzml"):
        _load_mzML(self, filename)
    elif file_lower.endswith(".wiff") or file_lower.endswith(".wiff2"):
        # Determine which loader to use based on interface parameter
        selected_interface = None

        if interface == "alpharaw":
            selected_interface = "alpharaw"
        elif interface == "rawreader":
            selected_interface = "rawreader"
        elif interface == "masster":
            selected_interface = "masster"
        elif interface is None:  # Auto-detect: rawreader > masster > alpharaw
            try:
                import rawreader

                selected_interface = "rawreader"
            except ImportError:
                # Try masster as fallback
                try:
                    from masster.sample.sciex_v2 import SciexOndemandReader

                    selected_interface = "masster"
                except ImportError:
                    selected_interface = "alpharaw"
        else:
            raise ValueError(
                f"Invalid interface '{interface}' for .wiff files. "
                f"Valid options: None (auto), 'rawreader', 'masster', 'alpharaw'",
            )

        if selected_interface == "rawreader":
            metadata, method = _load_wiff_v2(
                self,
                filename,
                sample_idx=sample_idx,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
                # Check if type or sample_type indicates ztscan (case-insensitive)
                if ("type" in metadata and "ztscan" in metadata["type"].lower()) or (
                    "sample_type" in metadata
                    and "ztscan" in metadata["sample_type"].lower()
                ):
                    self.type = "ztscan"
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        elif selected_interface == "masster":
            metadata, method = _load_wiff_masster(
                self,
                filename,
                sample_idx=sample_idx,
                cachelevel=cachelevel if cachelevel is not None else [1],
                mslevel=mslevel if mslevel is not None else [1, 2],
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
                # Check if type or sample_type indicates ztscan (case-insensitive)
                if ("type" in metadata and "ztscan" in metadata["type"].lower()) or (
                    "sample_type" in metadata
                    and "ztscan" in metadata["sample_type"].lower()
                ):
                    self.type = "ztscan"
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        else:  # alpharaw
            _load_wiff(self, filename, sample_idx=sample_idx)
    elif file_lower.endswith(".raw"):
        # Determine which loader to use based on interface parameter
        use_rawreader = False

        if interface == "alpharaw":
            use_rawreader = False
        elif interface == "rawreader":
            use_rawreader = True
        elif interface is None:  # Default to rawreader
            try:
                import rawreader

                use_rawreader = True
            except ImportError:
                use_rawreader = False
        else:
            raise ValueError(
                f"Invalid interface '{interface}' for .raw files. "
                f"Valid options: None (auto), 'rawreader', 'alpharaw'",
            )

        if use_rawreader:
            metadata, method = _load_raw_v2(
                self,
                filename,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        else:
            _load_raw(self, filename)
    elif file_lower.endswith(".d"):
        # Agilent and Bruker .d folders - only available with rawreader
        try:
            import rawreader

            metadata, method = _load_d_v2(
                self,
                filename,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        except ImportError:
            raise FileFormatError(
                "Loading .d files requires rawreader to be installed.\n"
                "Install with: pip install rawreader",
            )
    elif file_lower.endswith(".sample5"):
        _load_sample5(self, filename)
    else:
        # Extract file extension for error message
        _, ext = os.path.splitext(filename)
        raise FileFormatError(
            f"Unsupported file format: {ext if ext else '(no extension)'}\n"
            f"Supported formats:\n"
            f"  - .raw (Thermo Fisher)\n"
            f"  - .wiff/.wiff2 (SCIEX)\n"
            f"  - .d (Agilent/Bruker, requires rawreader)\n"
            f"  - .mzML (open standard)\n"
            f"  - .sample5 (masster format)",
        )

    # Only override type if explicitly provided, otherwise preserve existing value
    if type is not None and type.lower() in ["ztscan"]:
        self.type = "ztscan"
    elif type is None and not hasattr(self, "type"):
        # Set default only if type was never initialized
        self.type = "dda"

    if label is not None:
        self.label = label


def load_noms1(
    self,
    filename: str | None = None,
    ondisk: bool = False,
    type: str | None = None,
    label: str | None = None,
    sample_idx: int | None = None,
    version: int | None = None,
    cachelevel: list[int] | None = None,
    mslevel: list[int] | None = None,
) -> None:
    """
    Optimized load method that skips loading ms1_df for better performance.

    This method is identical to load() but uses _load_sample5_study() for .sample5 files,
    which skips reading the potentially large ms1_df dataset to improve throughput when
    adding samples to studies or when MS1 spectral data is not needed.

    Args:
        filename: The path to the file or folder to load. Supports:
                 - .mzML, .wiff/.wiff2, .raw, .d (Agilent/Bruker), .sample5
        ondisk: Whether to load on-disk or in-memory. Defaults to False.
        type: Override file type detection. Can be "ztscan". Defaults to None.
        label: Override sample label. Defaults to None.
        sample_idx: For WIFF files with multiple samples, specifies which sample to load (0-indexed).
                   Defaults to None (loads the last sample if multiple exist).
        version: For WIFF and RAW files, specifies which loader version to use:
                - None (default): Uses rawreader if available, otherwise falls back to alpharaw
                - 1: Forces use of alpharaw (legacy loader)
        cachelevel: For files using rawreader (.wiff, .raw, .d), specifies which MS levels to cache.
                   Use [1] for MS1 only, [2] for MS2 only, or None for all levels.

    Raises:
        FileNotFoundError: If the specified file doesn't exist.
        ValueError: If the file format is not supported.

    Notes:
        - Only affects .sample5 files (uses _load_sample5_study instead of _load_sample5)
        - Other file formats (.mzML, .wiff, .raw) are loaded normally
        - Sets ms1_df = None for .sample5 files to save memory and loading time
        - Recommended when MS1 spectral data is not needed (e.g., study workflows, feature-only analysis)
    """
    if filename is None:
        filename = self.file_path
    filename = os.path.abspath(filename)
    if not os.path.exists(filename):
        raise FileNotFoundError("Filename not valid. Provide a valid file path.")
    self.ondisk = ondisk

    # check if file is mzML
    if filename.lower().endswith(".mzml"):
        _load_mzML(self, filename)
    elif filename.lower().endswith(".wiff") or filename.lower().endswith(".wiff2"):
        # Check if rawreader is available and decide which loader to use
        use_v2 = False
        if version != 1:  # If version is not explicitly set to 1 (old version)
            try:
                import rawreader

                use_v2 = True
            except ImportError:
                use_v2 = False

        if use_v2:
            metadata, method = _load_wiff_v2(
                self,
                filename,
                sample_idx=sample_idx,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        else:
            _load_wiff(self, filename, sample_idx=sample_idx)
    elif filename.lower().endswith(".raw"):
        # Check if rawreader is available and decide which loader to use
        use_v2 = False
        if version != 1:  # If version is not explicitly set to 1 (old version)
            try:
                import rawreader

                use_v2 = True
            except ImportError:
                use_v2 = False

        if use_v2:
            metadata, method = _load_raw_v2(
                self,
                filename,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        else:
            _load_raw(self, filename)
    elif filename.lower().endswith(".d"):
        # Agilent and Bruker .d folders - only available with rawreader
        try:
            import rawreader

            metadata, method = _load_d_v2(
                self,
                filename,
                cachelevel=cachelevel,
                mslevel=mslevel,
            )
            if metadata is not None:
                self.update_history(["acquisition_sample"], metadata)
            if method is not None and isinstance(method, dict):
                for key, value in method.items():
                    self.update_history([f"acquisition_{key}"], value)
        except ImportError:
            raise FileFormatError(
                "Loading .d files requires rawreader to be installed.\\n"
                "Install with: pip install rawreader",
            )
    elif filename.lower().endswith(".sample5"):
        from masster.sample.h5 import _load_sample5_study

        _load_sample5_study(self, filename)  # Use optimized version for study loading
    else:
        raise ValueError("File must be .mzML, .wiff, *.raw, or .sample5")

    # Only override type if explicitly provided, otherwise preserve existing value
    if type is not None and type.lower() in ["ztscan"]:
        self.type = "ztscan"
    elif type is None and not hasattr(self, "type"):
        # Set default only if type was never initialized
        self.type = "dda"

    if label is not None:
        self.label = label


# Renamed for clarity and internal use
def _load_ms1(
    self,
    filename=None,
    ondisk=False,
    type=None,
    label=None,
):
    """
    Load MS1-only data (renamed from load_study for clarity).
    Optimized version for study loading that excludes MS2 data.

    This method is deprecated. Use load_noms1() instead.
    """
    return self.load_noms1(filename=filename, ondisk=ondisk, type=type, label=label)


def _load_mzML(
    self,
    filename=None,
):
    """
    Load an mzML file and process its spectra.
    This method loads an mzML file (if a filename is provided, it will update the internal file path) using either an on-disk or in-memory MS experiment depending on the object's "ondisk" flag. It then iterates over all the spectra in the experiment:
        - For MS level 1 spectra, it increments a cycle counter and creates a polars DataFrame containing the retention time, m/z values, and intensity values.
        - For higher MS level spectra, it processes precursor-related information such as precursor m/z, isolation window offsets, intensity, and activation energy.
    Each spectrum is further processed by computing its baseline, denoising based on the baseline, and extracting various scan properties (such as TIC, minimum/maximum intensity, m/z bounds, etc.). This scan information is appended to a list.
    After processing all spectra, the method consolidates the collected scan data into a polars DataFrame with an explicit schema. It also assigns the on-disk/in-memory experiment object and corresponding file interface to instance attributes. The method sets a label based on the file basename, and, unless the scan type is 'ztscan', calls an additional analysis routine (analyze_dda).
    Parameters:
        filename (str, optional): The path to the mzML file to load. If None, the existing file path attribute is used.
    Returns:
        None
    Side Effects:
        - Updates self.file_path if a new filename is provided.
        - Loads and stores the MS experiment in self.file_obj.
        - Sets self.file_interface to the string 'oms'.
        - Stores the processed scan data in self.scans_df.
        - Maintains MS1-specific data in self.ms1_df.
        - Updates the instance label based on the loaded file's basename.
        - Invokes the analyze_dda method if the scan type is not 'ztscan'.
    """
    # check if filename exists
    if filename is None:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")
    if filename is not None:
        self.file_path = filename
        self.file_source = filename

    self.logger.info(f"Loading {filename}")

    omsexp: oms.OnDiscMSExperiment | oms.MSExperiment
    if self.ondisk:
        omsexp = oms.OnDiscMSExperiment()
        self.file_obj = omsexp
    else:
        omsexp = oms.MSExperiment()
        oms.MzMLFile().load(self.file_path, omsexp)
        self.file_obj = omsexp

    scans = []
    cycle = 0
    schema = {
        "cycle": pl.Int32,
        "scan_id": pl.Int64,
        "rt": pl.Float64,
        "mz": pl.Float64,
        "inty": pl.Float64,
    }
    # create a polars DataFrame with explicit schema: cycle: int, rt: float, mz: float, intensity: float
    ms1_df = pl.DataFrame(
        {"cycle": [], "scan_id": [], "rt": [], "mz": [], "inty": []},
        schema=schema,
    )

    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    polarity = None
    # iterate over all spectra
    for i, s in tqdm(
        enumerate(omsexp.getSpectra()),
        total=omsexp.getNrSpectra(),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        # try to get polarity
        if polarity is None:
            try:
                pol = s.getInstrumentSettings().getPolarity()
                if pol == 1:
                    polarity = "positive"
                elif pol == 2:
                    polarity = "negative"
            except Exception:
                pass
        # create a dict
        if s.getMSLevel() == 1:
            cycle += 1
            prec_mz = None
            precursorIsolationWindowLowerMZ = None
            precursorIsolationWindowUpperMZ = None
            prec_inty = None
            energy = None
        else:
            prec_mz = s.getPrecursors()
            if len(prec_mz) == 0:
                continue
            prec_mz = prec_mz[0].getMZ()
            precursorIsolationWindowLowerMZ = s.getPrecursors()[
                0
            ].getIsolationWindowLowerOffset()
            precursorIsolationWindowUpperMZ = s.getPrecursors()[
                0
            ].getIsolationWindowUpperOffset()
            prec_inty = s.getPrecursors()[0].getIntensity()
            # Try to get collision energy from meta values first, fallback to getActivationEnergy()
            try:
                energy = s.getPrecursors()[0].getMetaValue("collision energy")
                if energy is None or energy == 0.0:
                    energy = s.getPrecursors()[0].getActivationEnergy()
            except Exception:
                energy = s.getPrecursors()[0].getActivationEnergy()

        peaks = s.get_peaks()
        spect = Spectrum(mz=peaks[0], inty=peaks[1], ms_level=s.getMSLevel())

        bl = spect.baseline()
        spect = spect.denoise(threshold=bl)

        if spect.ms_level == 1:
            mz = np.array(spect.mz)
            median_diff = np.median(np.diff(np.sort(mz))) if mz.size > 1 else None

            if median_diff is not None and median_diff < 0.01:
                spect = spect.centroid(
                    tolerance=self.parameters.mz_tol_ms1_da,
                    ppm=self.parameters.mz_tol_ms1_ppm,
                    min_points=self.parameters.centroid_min_points_ms1,
                    refine_window=self.parameters.centroid_refine_mz_tol,
                )

        newscan = {
            "scan_id": i,
            "cycle": cycle,
            "ms_level": int(s.getMSLevel()),
            "rt": s.getRT(),
            "inty_tot": spect.tic,
            "inty_min": spect.inty_min,
            "inty_max": spect.inty_max,
            "bl": bl,
            "mz_min": spect.mz_min,
            "mz_max": spect.mz_max,
            "comment": s.getComment(),
            "name": s.getName(),
            "id": s.getNativeID(),
            "prec_mz": prec_mz,
            "prec_mz_min": precursorIsolationWindowLowerMZ,
            "prec_mz_max": precursorIsolationWindowUpperMZ,
            "prec_inty": prec_inty,
            "energy": energy,
            "feature_id": -1,
        }

        scans.append(newscan)

        if s.getMSLevel() == 1 and len(peaks) > 0:
            newms1_df = pl.DataFrame(
                {
                    "cycle": cycle,
                    "scan_id": i,
                    "rt": s.getRT(),
                    "mz": spect.mz,
                    "inty": spect.inty,
                },
                schema=schema,
            )
            ms1_df = pl.concat([ms1_df, newms1_df])

    # convert to polars DataFrame with explicit schema and store in self.scans_df
    self.scans_df = pl.DataFrame(
        scans,
        schema={
            "scan_id": pl.Int64,
            "cycle": pl.Int64,
            "ms_level": pl.Int64,
            "rt": pl.Float64,
            "inty_tot": pl.Float64,
            "inty_min": pl.Float64,
            "inty_max": pl.Float64,
            "bl": pl.Float64,
            "mz_min": pl.Float64,
            "mz_max": pl.Float64,
            "comment": pl.Utf8,
            "name": pl.Utf8,
            "id": pl.Utf8,
            "prec_mz": pl.Float64,
            "prec_mz_min": pl.Float64,
            "prec_mz_max": pl.Float64,
            "prec_inty": pl.Float64,
            "energy": pl.Float64,
            "feature_id": pl.Int64,
        },
        infer_schema_length=None,
    )
    self.polarity = polarity
    self.file_interface = "oms"
    self.ms1_df = ms1_df
    self.label = os.path.basename(filename)
    if self.type != "ztscan":
        self.analyze_dda()


def _load_raw(
    self,
    filename=None,
):
    """
    Load and process raw spectral data from the given file.
    This method reads a Thermo raw file (with '.raw' extension) by utilizing the ThermoRawData class from
    the alpharaw.thermo module. It validates the filename, checks for file existence, and then imports and processes
    the raw data. The method performs the following tasks:
        - Converts retention times (rt) from minutes to seconds and rounds them to 4 decimal places.
        - Iterates over each spectrum in the raw data and constructs a list of scan dictionaries.
        - For MS level 1 scans, performs centroiding if peaks with intensities > 0 after denoising.
        - Creates a Polars DataFrame for all scans (self.scans_df) with detailed spectrum information.
        - Aggregates MS1 spectrum peak data into a separate Polars DataFrame (self.ms1_df).
        - Sets additional attributes such as file path, raw data object, interface label, and file label.
        - Calls the analyze_dda method for further processed data analysis.
    Parameters:
        filename (str): The path to the raw data file. Must end with ".raw".
    Raises:
        ValueError: If the provided filename does not end with ".raw".
        FileNotFoundError: If the file specified by filename does not exist.
    Side Effects:
        - Populates self.scans_df with scan data in a Polars DataFrame.
        - Populates self.ms1_df with MS1 scan data.
        - Updates instance attributes including self.file_path, self.file_obj, self.file_interface, and self.label.
        - Initiates further analysis by invoking analyze_dda().
    """
    # from alpharaw.thermo import ThermoRawData
    from masster.sample.thermo import ThermoRawData

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    raw_data = ThermoRawData(centroided=False)
    raw_data.keep_k_peaks_per_spec = self.parameters.max_points_per_spectrum
    # check thatupdat filename ends with .raw
    if not filename.endswith(".raw"):
        raise ValueError("filename must end with .raw")
    # check that the file exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")
    self.logger.info(f"Loading {filename} (alpharaw)")
    raw_data.import_raw(filename)
    specs = raw_data.spectrum_df
    # convert rt from minutes to seconds, round to 4 decimal places
    specs = specs.with_columns((pl.col("rt") * 60).round(4).alias("rt"))

    scans = []
    cycle = 0
    schema = {
        "cycle": pl.Int32,
        "scan_id": pl.Int64,
        "rt": pl.Float64,
        "mz": pl.Float64,
        "inty": pl.Float64,
    }
    # create a polars DataFrame with explicit schema: cycle: int, rt: float, mz: float, intensity: float
    ms1_df = pl.DataFrame(
        {"cycle": [], "scan_id": [], "rt": [], "mz": [], "inty": []},
        schema=schema,
    )
    # iterate over rows of specs
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    for i, s in tqdm(
        enumerate(specs.iter_rows(named=True)),
        total=len(specs),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        # create a dict
        if s["ms_level"] == 1:
            cycle += 1
            prec_mz = None
            precursorIsolationWindowLowerMZ = None
            precursorIsolationWindowUpperMZ = None
            prec_intyensity = None
            energy = None
        else:
            prec_mz = s["precursor_mz"]
            precursorIsolationWindowLowerMZ = s["isolation_lower_mz"]
            precursorIsolationWindowUpperMZ = s["isolation_upper_mz"]
            prec_intyensity = None
            energy = s["nce"]

        # try to get polarity
        if self.polarity is None:
            if s["polarity"] == "positive":
                self.polarity = "positive"
            elif s["polarity"] == "negative":
                self.polarity = "negative"

        peak_start_idx = s["peak_start_idx"]
        peak_stop_idx = s["peak_stop_idx"]
        peaks = raw_data.peak_df.slice(peak_start_idx, peak_stop_idx - peak_start_idx)
        spect = Spectrum(
            mz=peaks["mz"].to_numpy(),
            inty=peaks["intensity"].to_numpy(),
            ms_level=s["ms_level"],
        )
        # remove peaks with intensity <= 0

        bl = spect.baseline()
        spect = spect.denoise(threshold=bl)

        if spect.ms_level == 1:
            # Use the same logic as mzML loading
            mz = np.array(spect.mz)
            median_diff = np.median(np.diff(np.sort(mz))) if mz.size > 1 else None

            if median_diff is not None and median_diff < 0.01:
                spect = spect.centroid(
                    tolerance=self.parameters.mz_tol_ms1_da,
                    ppm=self.parameters.mz_tol_ms1_ppm,
                    min_points=self.parameters.centroid_min_points_ms1,
                    refine_window=self.parameters.centroid_refine_mz_tol,
                )
        newscan = {
            "scan_id": i,
            "cycle": cycle,
            "ms_level": int(s["ms_level"]),
            "rt": s["rt"],
            "inty_tot": spect.tic,
            "inty_min": spect.inty_min,
            "inty_max": spect.inty_max,
            "bl": bl,
            "mz_min": spect.mz_min,
            "mz_max": spect.mz_max,
            "comment": "",
            "name": "",
            "id": "",
            "prec_mz": prec_mz,
            "prec_mz_min": precursorIsolationWindowLowerMZ,
            "prec_mz_max": precursorIsolationWindowUpperMZ,
            "prec_inty": prec_intyensity,
            "energy": energy,
            "feature_id": -1,
        }

        scans.append(newscan)

        if s["ms_level"] == 1 and len(peaks) > 0:
            newms1_df = pl.DataFrame(
                {
                    "cycle": cycle,
                    "scan_id": i,
                    "rt": s["rt"],
                    "mz": spect.mz,
                    "inty": spect.inty,
                },
                schema=schema,
            )
            ms1_df = pl.concat([ms1_df, newms1_df])

    # convert to polars DataFrame with explicit schema and store in self.scans_df
    self.scans_df = pl.DataFrame(
        scans,
        schema={
            "scan_id": pl.Int64,
            "cycle": pl.Int64,
            "ms_level": pl.Int64,
            "rt": pl.Float64,
            "inty_tot": pl.Float64,
            "inty_min": pl.Float64,
            "inty_max": pl.Float64,
            "bl": pl.Float64,
            "mz_min": pl.Float64,
            "mz_max": pl.Float64,
            "comment": pl.Utf8,
            "name": pl.Utf8,
            "id": pl.Utf8,
            "prec_mz": pl.Float64,
            "prec_mz_min": pl.Float64,
            "prec_mz_max": pl.Float64,
            "prec_inty": pl.Float64,
            "energy": pl.Float64,
            "feature_id": pl.Int64,
        },
        infer_schema_length=None,
    )
    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_data
    self.file_interface = "alpharaw"
    self.label = os.path.basename(filename)
    self.ms1_df = ms1_df
    self.analyze_dda()


def _load_wiff(
    self,
    filename=None,
    sample_idx=None,
):
    # Use masster's own implementation first
    from masster.sample.sciex import SciexWiffData, count_samples

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    raw_data = SciexWiffData(centroided=False)
    raw_data.keep_k_peaks_per_spec = self.parameters.max_points_per_spectrum

    # replace wiff2 with wiff for compatibility
    if filename.endswith(".wiff2"):
        filename = filename[:-1]
    if not filename.endswith(".wiff"):
        raise ValueError("filename must end with .wiff")
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    # Check number of samples in the WIFF file
    num_samples = count_samples(filename)
    if num_samples == 0:
        raise ValueError("No samples found in the WIFF file.")
    if sample_idx is None:
        sample_idx = num_samples - 1  # Default to last sample
    if sample_idx > num_samples - 1:
        raise ValueError(
            f"sample_idx {sample_idx} out of range. File contains {num_samples} samples.",
        )
    if num_samples > 1:
        self.logger.warning(
            f"WIFF file contains {num_samples} samples. Only loading sample index {sample_idx}.",
        )
        raw_data.sample_id = sample_idx

    self.logger.info(f"Loading {filename} (alpharaw)")
    raw_data.import_raw(filename)

    specs = raw_data.spectrum_df
    specs = specs.with_columns((pl.col("rt") * 60).round(4).alias("rt"))

    algo = self.parameters.centroid_algo

    scans = []
    ms1_df_records = []
    cycle = 0
    schema = {
        "cycle": pl.Int32,
        "scan_id": pl.Int64,
        "rt": pl.Float64,
        "mz": pl.Float64,
        "inty": pl.Float64,
    }
    polarity = None
    # iterate over rows of specs
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    scan_idx = 0  # Track scan index for scan_id
    for s in tqdm(
        specs.iter_rows(named=True),
        total=len(specs),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        ms_level = s["ms_level"]
        # try to get polarity
        if polarity is None:
            if s["polarity"] == "positive":
                polarity = "positive"
            elif s["polarity"] == "negative":
                polarity = "negative"

        if ms_level == 1:
            cycle += 1
            prec_mz = None
            precursorIsolationWindowLowerMZ = None
            precursorIsolationWindowUpperMZ = None
            prec_intyensity = None
            energy = None
        else:
            prec_mz = s["precursor_mz"]
            precursorIsolationWindowLowerMZ = s["isolation_lower_mz"]
            precursorIsolationWindowUpperMZ = s["isolation_upper_mz"]
            prec_intyensity = None
            energy = s["nce"]

        peak_start_idx = s["peak_start_idx"]
        peak_stop_idx = s["peak_stop_idx"]
        peaks = raw_data.peak_df.slice(peak_start_idx, peak_stop_idx - peak_start_idx)
        spect = Spectrum(
            mz=peaks["mz"].to_numpy(),
            inty=peaks["intensity"].to_numpy(),
            ms_level=ms_level,
            centroided=False,  # WIFF files always contain profile data
        )
        bl = spect.baseline()
        spect = spect.denoise(threshold=bl)
        if ms_level == 1 and algo is not None:
            spect = spect.centroid(
                algo=algo,
                tolerance=self.parameters.mz_tol_ms1_da,
                ppm=self.parameters.mz_tol_ms1_ppm,
                min_points=self.parameters.centroid_min_points_ms1,
                refine_window=self.parameters.centroid_refine_mz_tol,
            )
        scans.append(
            {
                "scan_id": scan_idx,
                "cycle": cycle,
                "ms_level": int(ms_level),
                "rt": s["rt"],
                "inty_tot": spect.tic,
                "inty_min": spect.inty_min,
                "inty_max": spect.inty_max,
                "bl": bl,
                "mz_min": spect.mz_min,
                "mz_max": spect.mz_max,
                "comment": "",
                "name": "",
                "id": "",
                "prec_mz": prec_mz,
                "prec_mz_min": precursorIsolationWindowLowerMZ,
                "prec_mz_max": precursorIsolationWindowUpperMZ,
                "prec_inty": prec_intyensity,
                "energy": energy,
                "feature_id": -1,
            },
        )

        scan_idx += 1  # Increment scan counter

        if ms_level == 1 and len(peaks) > 0:
            # Use extend for all mz/int pairs at once
            ms1_df_records.extend(
                [
                    {
                        "cycle": cycle,
                        "scan_id": scan_idx - 1,  # Use the scan_idx we just assigned
                        "rt": s["rt"],
                        "mz": mz,
                        "inty": inty,
                    }
                    for mz, inty in zip(spect.mz, spect.inty, strict=False)
                ],
            )

    # Create DataFrames in one go
    self.scans_df = pl.DataFrame(
        scans,
        schema={
            "scan_id": pl.Int64,
            "cycle": pl.Int64,
            "ms_level": pl.Int64,
            "rt": pl.Float64,
            "inty_tot": pl.Float64,
            "inty_min": pl.Float64,
            "inty_max": pl.Float64,
            "bl": pl.Float64,
            "mz_min": pl.Float64,
            "mz_max": pl.Float64,
            "comment": pl.Utf8,
            "name": pl.Utf8,
            "id": pl.Utf8,
            "prec_mz": pl.Float64,
            "prec_mz_min": pl.Float64,
            "prec_mz_max": pl.Float64,
            "prec_inty": pl.Float64,
            "energy": pl.Float64,
            "feature_id": pl.Int64,
        },
        infer_schema_length=None,
    )
    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_data
    self.file_interface = "alpharaw"
    self.polarity = polarity
    self.label = os.path.basename(filename)
    self.ms1_df = pl.DataFrame(ms1_df_records, schema=schema)
    if self.type != "ztscan":
        self.analyze_dda()


def _load_wiff_v2(
    self,
    filename=None,
    sample_idx=None,
    cachelevel=None,
    mslevel=None,
):
    """Load WIFF files using rawreader module instead of alpharaw.

    This version uses the sciex_reader from rawreader.RawReader which provides
    a more direct interface to the WIFF/WIFF2 files via ProteoWizard DLLs.

    Args:
        filename: Path to the WIFF/WIFF2 file
        sample_idx: Sample index for multi-sample files
        cachelevel: List of MS levels to cache spectrum data for (e.g., [1, 2]).
                   If None, all MS levels are cached. Use [1] for MS1 only, [2] for MS2 only.
        mslevel: List of MS levels to load from file (e.g., [1, 2]).
                If None, all MS levels are loaded. Use [1] for MS1 only, [2] for MS2 only.
    """
    from rawreader import RawReader

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    if not (filename.endswith(".wiff") or filename.endswith(".wiff2")):
        raise ValueError("filename must end with .wiff or .wiff2")

    self.logger.info(f"Loading {filename} (rawreader)")

    if sample_idx is not None and sample_idx < 0:
        raise ValueError("sample_idx must be >= 0 or None")

    # Create RawReader instance - it automatically detects SCIEX format
    raw_reader = RawReader(
        filename,
        sample_id=sample_idx,
        centroid=False,
        mslevel=mslevel,
        cachelevel=cachelevel,
        silent=True,
    )

    # Get scan and peak dataframes - using new API
    scan_df = raw_reader.scans
    peak_df = raw_reader.cache

    # Note: scan_id is now provided by rawreader and is invariant across mslevel filters
    # No need to reassign it here - rawreader preserves scan_id from the original file

    # Filter scans by mslevel if specified
    if mslevel is not None:
        scan_df = scan_df.filter(pl.col("mslevel").is_in(mslevel))

    # Get metadata
    metadata = raw_reader.metadata
    sample_count = int(metadata.get("sample_count", 1) or 1)

    # Determine which sample was actually selected by rawreader
    resolved_sample_idx = sample_idx
    if resolved_sample_idx is None:
        resolved_sample_idx = (
            raw_reader.params.get("sample_id")
            if hasattr(raw_reader, "params") and isinstance(raw_reader.params, dict)
            else None
        )
    if resolved_sample_idx is None:
        resolved_sample_idx = metadata.get("sample_id", 0)
    resolved_sample_idx = int(resolved_sample_idx)

    # Validate requested/selected sample index against metadata (if available)
    if resolved_sample_idx > sample_count - 1:
        raise ValueError(
            f"sample_idx {resolved_sample_idx} out of range. File contains {sample_count} samples.",
        )
    if sample_count > 1:
        self.logger.warning(
            f"WIFF file contains {sample_count} samples. Only loading sample index {resolved_sample_idx}.",
        )

    # Persist selection so later operations (e.g., index_raw) can reuse it
    self.sample_idx = resolved_sample_idx

    algo = self.parameters.centroid_algo

    # Remove the filter - rawreader with mslevel parameter ensures all returned scans are valid
    # No need to filter by spectrum_start/spectrum_end
    # scan_df = scan_df.filter(
    #     pl.col("spectrum_start").is_not_null()
    #     & pl.col("spectrum_end").is_not_null()
    #     & (pl.col("spectrum_end") > pl.col("spectrum_start"))
    # )

    # scan_id already added before filtering for consistency

    # Convert RT from minutes to seconds (rawreader returns RT in minutes)
    scan_df = scan_df.with_columns(
        (pl.col("rt") * 60.0).alias("rt"),
    )

    # Determine polarity from first non-null value
    polarity = None
    first_polarity = scan_df.select(pl.col("polarity")).to_series()[0]
    if first_polarity == "positive":
        polarity = "positive"
    elif first_polarity == "negative":
        polarity = "negative"

    # Calculate cycle numbers (cumulative count of MS1 scans)
    scan_df = scan_df.with_columns(
        pl.when(pl.col("mslevel") == 1).then(1).otherwise(0).cum_sum().alias("cycle"),
    )

    # Process spectra with progress bar
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    scans = []
    ms1_peaks_list = []

    for row in tqdm(
        scan_df.iter_rows(named=True),
        total=len(scan_df),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        scan_id = row["scan_id"]
        cycle = row["cycle"]
        ms_level = row["mslevel"]
        rt = row["rt"]
        fill_time = row.get("fill_time", None)

        # Get peaks for this scan
        cache_start = row["cache_start"]
        cache_end = row["cache_end"]

        peaks = None
        spect = None
        bl = None

        if cache_start is not None and cache_end is not None:
            peaks = peak_df.slice(cache_start, cache_end - cache_start)

            # Create spectrum and process
            spect = Spectrum(
                mz=peaks["mz"].to_numpy(),
                inty=peaks["inty"].to_numpy(),
                ms_level=ms_level,
                centroided=False,
            )

            bl = spect.baseline()
            spect = spect.denoise(threshold=bl)
            if ms_level == 1 and algo is not None and len(spect.mz) > 0:
                spect = spect.centroid(
                    algo=algo,
                    tolerance=self.parameters.mz_tol_ms1_da,
                    ppm=self.parameters.mz_tol_ms1_ppm,
                    min_points=self.parameters.centroid_min_points_ms1,
                    refine_window=self.parameters.centroid_refine_mz_tol,
                )

        # Build scan record even if spectrum was not cached (e.g., cachelevel excludes this ms_level)
        scans.append(
            {
                "scan_id": scan_id,
                "cycle": cycle,
                "ms_level": int(ms_level),
                "rt": rt,
                "time_fill": fill_time,
                "inty_tot": spect.tic if spect is not None else None,
                "inty_min": spect.inty_min if spect is not None else None,
                "inty_max": spect.inty_max if spect is not None else None,
                "bl": bl if spect is not None else None,
                "mz_min": spect.mz_min if spect is not None else None,
                "mz_max": spect.mz_max if spect is not None else None,
                "comment": "",
                "name": "",
                "id": "",
                "prec_mz": row["precursor_mz"] if ms_level > 1 else None,
                "prec_mz_min": row["isolation_lower_mz"] if ms_level > 1 else None,
                "prec_mz_max": row["isolation_upper_mz"] if ms_level > 1 else None,
                "prec_inty": None,
                "energy": row["collision_energy"] if ms_level > 1 else None,
                "feature_id": -1,
            },
        )

        # Collect MS1 peaks
        if ms_level == 1 and spect is not None and len(spect.mz) > 0:
            n_peaks = len(spect.mz)
            ms1_peaks_list.append(
                pl.DataFrame(
                    {
                        "cycle": [cycle] * n_peaks,
                        "scan_id": [scan_id] * n_peaks,
                        "rt": [rt] * n_peaks,
                        "mz": spect.mz,
                        "inty": spect.inty,
                    },
                ),
            )

    # Create DataFrames
    self.scans_df = pl.DataFrame(
        scans,
        schema={
            "scan_id": pl.Int64,
            "cycle": pl.Int64,
            "ms_level": pl.Int64,
            "rt": pl.Float64,
            "time_fill": pl.Float64,
            "inty_tot": pl.Float64,
            "inty_min": pl.Float64,
            "inty_max": pl.Float64,
            "bl": pl.Float64,
            "mz_min": pl.Float64,
            "mz_max": pl.Float64,
            "comment": pl.Utf8,
            "name": pl.Utf8,
            "id": pl.Utf8,
            "prec_mz": pl.Float64,
            "prec_mz_min": pl.Float64,
            "prec_mz_max": pl.Float64,
            "prec_inty": pl.Float64,
            "energy": pl.Float64,
            "feature_id": pl.Int64,
        },
        infer_schema_length=None,
    )

    # Concatenate MS1 peaks efficiently
    if ms1_peaks_list:
        self.ms1_df = pl.concat(ms1_peaks_list, rechunk=True)
    else:
        # Empty DataFrame with correct schema
        self.ms1_df = pl.DataFrame(
            schema={
                "cycle": pl.Int32,
                "scan_id": pl.Int64,
                "rt": pl.Float64,
                "mz": pl.Float64,
                "inty": pl.Float64,
            },
        )

    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_reader
    self.file_interface = "rawreader"
    self.polarity = polarity
    self.label = os.path.basename(filename)
    if self.type != "ztscan":
        self.analyze_dda()

    # Extract metadata and method from raw_reader
    metadata = raw_reader.metadata
    method = raw_reader.method
    return metadata, method


def _load_wiff_masster(
    self,
    filename=None,
    sample_idx=None,
    cachelevel=None,
    mslevel=None,
):
    """Load WIFF files using masster's native SciexOndemandReader.

    This version uses masster.sample.sciex_v2.SciexOndemandReader which provides
    a RawReader-compatible interface using only the AnalystDataProvider DLL (no wiff2sqlite).

    Args:
        filename: Path to the WIFF/WIFF2 file
        sample_idx: Sample index for multi-sample files
        cachelevel: List of MS levels to cache spectrum data for (e.g., [1, 2]).
                   If None, all MS levels are cached. Use [1] for MS1 only, [2] for MS2 only.
        mslevel: List of MS levels to load from file (e.g., [1, 2]).
                If None, all MS levels are loaded. Use [1] for MS1 only, [2] for MS2 only.
    """
    from masster.sample.sciex_v2 import SciexOndemandReader

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    if not (filename.endswith(".wiff") or filename.endswith(".wiff2")):
        raise ValueError("filename must end with .wiff or .wiff2")

    self.logger.info(f"Loading {filename} (masster)")

    if sample_idx is not None and sample_idx < 0:
        raise ValueError("sample_idx must be >= 0 or None")

    # Create SciexOndemandReader instance
    raw_reader = SciexOndemandReader(
        filename,
        sample_id=sample_idx,
        mslevel=mslevel,
        cachelevel=cachelevel,
        keep_k_peaks=100000,
    )

    # Get scan and peak dataframes
    scan_df = raw_reader.scans
    peak_df = raw_reader.cache

    # Filter scans by mslevel if specified
    if mslevel is not None:
        scan_df = scan_df.filter(pl.col("mslevel").is_in(mslevel))

    # Get metadata
    metadata = raw_reader.metadata
    sample_count = int(metadata.get("sample_count", 1) or 1)

    # Determine which sample was actually selected
    resolved_sample_idx = sample_idx
    if resolved_sample_idx is None:
        resolved_sample_idx = metadata.get("sample_id", 0)
    resolved_sample_idx = int(resolved_sample_idx)

    # Validate requested/selected sample index against metadata
    if resolved_sample_idx > sample_count - 1:
        raise ValueError(
            f"sample_idx {resolved_sample_idx} out of range. File contains {sample_count} samples.",
        )
    if sample_count > 1:
        self.logger.warning(
            f"WIFF file contains {sample_count} samples. Only loading sample index {resolved_sample_idx}.",
        )

    # Persist selection so later operations (e.g., index_raw) can reuse it
    self.sample_idx = resolved_sample_idx

    algo = self.parameters.centroid_algo

    # Convert RT from minutes to seconds (masster reader returns RT in minutes)
    scan_df = scan_df.with_columns(
        (pl.col("rt") * 60.0).alias("rt"),
    )

    # Determine polarity from first non-null value
    polarity = None
    first_polarity = scan_df.select(pl.col("polarity")).to_series()[0]
    if first_polarity == "positive":
        polarity = "positive"
    elif first_polarity == "negative":
        polarity = "negative"

    # Calculate cycle numbers (cumulative count of MS1 scans)
    scan_df = scan_df.with_columns(
        pl.when(pl.col("mslevel") == 1).then(1).otherwise(0).cum_sum().alias("cycle"),
    )

    # Process spectra with progress bar
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    scans = []
    ms1_peaks_list = []

    for row in tqdm(
        scan_df.iter_rows(named=True),
        total=len(scan_df),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        scan_id = row["scan_id"]
        cycle = row["cycle"]
        ms_level = row["mslevel"]
        rt = row["rt"]
        fill_time = row.get("fill_time", None)

        # Get peaks for this scan
        cache_start = row["cache_start"]
        cache_end = row["cache_end"]

        peaks = None
        spect = None
        bl = None

        if cache_start is not None and cache_end is not None:
            peaks = peak_df.slice(cache_start, cache_end - cache_start)

            # Create spectrum and process
            spect = Spectrum(
                mz=peaks["mz"].to_numpy(),
                inty=peaks["inty"].to_numpy(),
                ms_level=ms_level,
                centroided=False,
            )

            bl = spect.baseline()
            spect = spect.denoise(threshold=bl)
            if ms_level == 1 and algo is not None and len(spect.mz) > 0:
                spect = spect.centroid(
                    algo=algo,
                    tolerance=self.parameters.mz_tol_ms1_da,
                    ppm=self.parameters.mz_tol_ms1_ppm,
                    min_points=self.parameters.centroid_min_points_ms1,
                    refine_window=self.parameters.centroid_refine_mz_tol,
                )

        # Build scan record even if spectrum was not cached
        scans.append(
            {
                "scan_id": scan_id,
                "cycle": cycle,
                "ms_level": int(ms_level),
                "rt": rt,
                "time_fill": fill_time,
                "inty_tot": spect.tic if spect is not None else None,
                "inty_min": spect.inty_min if spect is not None else None,
                "inty_max": spect.inty_max if spect is not None else None,
                "bl": bl if spect is not None else None,
                "mz_min": spect.mz_min if spect is not None else None,
                "mz_max": spect.mz_max if spect is not None else None,
                "comment": "",
                "name": "",
                "id": "",
                "prec_mz": row["precursor_mz"] if ms_level > 1 else None,
                "prec_mz_min": row["isolation_lower_mz"] if ms_level > 1 else None,
                "prec_mz_max": row["isolation_upper_mz"] if ms_level > 1 else None,
                "prec_inty": None,
                "energy": row["collision_energy"] if ms_level > 1 else None,
                "feature_id": -1,
            },
        )

        # Collect MS1 peaks
        if ms_level == 1 and spect is not None and len(spect.mz) > 0:
            n_peaks = len(spect.mz)
            ms1_peaks_list.append(
                pl.DataFrame(
                    {
                        "cycle": [cycle] * n_peaks,
                        "scan_id": [scan_id] * n_peaks,
                        "rt": [rt] * n_peaks,
                        "mz": spect.mz,
                        "inty": spect.inty,
                    },
                ),
            )

    # Create DataFrames
    self.scans_df = pl.DataFrame(
        scans,
        schema={
            "scan_id": pl.Int64,
            "cycle": pl.Int64,
            "ms_level": pl.Int64,
            "rt": pl.Float64,
            "time_fill": pl.Float64,
            "inty_tot": pl.Float64,
            "inty_min": pl.Float64,
            "inty_max": pl.Float64,
            "bl": pl.Float64,
            "mz_min": pl.Float64,
            "mz_max": pl.Float64,
            "comment": pl.Utf8,
            "name": pl.Utf8,
            "id": pl.Utf8,
            "prec_mz": pl.Float64,
            "prec_mz_min": pl.Float64,
            "prec_mz_max": pl.Float64,
            "prec_inty": pl.Float64,
            "energy": pl.Float64,
            "feature_id": pl.Int64,
        },
        infer_schema_length=None,
    )

    # Concatenate MS1 peaks efficiently
    if ms1_peaks_list:
        self.ms1_df = pl.concat(ms1_peaks_list, rechunk=True)
    else:
        # Empty DataFrame with correct schema
        self.ms1_df = pl.DataFrame(
            schema={
                "cycle": pl.Int32,
                "scan_id": pl.Int64,
                "rt": pl.Float64,
                "mz": pl.Float64,
                "inty": pl.Float64,
            },
        )

    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_reader
    self.file_interface = "masster"
    self.polarity = polarity
    self.label = os.path.basename(filename)
    if self.type != "ztscan":
        self.analyze_dda()

    # Extract metadata and method from raw_reader
    metadata = raw_reader.metadata
    method = raw_reader.method
    return metadata, method


def _load_raw_v2(
    self,
    filename=None,
    cachelevel=None,
    mslevel=None,
):
    """Load Thermo RAW files using rawreader module instead of alpharaw.

    This version uses rawreader.RawReader which provides a more direct interface
    to RAW files via ProteoWizard DLLs.

    Args:
        filename: Path to the RAW file
        cachelevel: List of MS levels to cache spectrum data for (e.g., [1, 2]).
                   If None, all MS levels are cached. Use [1] for MS1 only, [2] for MS2 only.
        mslevel: List of MS levels to load from file (e.g., [1, 2]).
                If None, all MS levels are loaded. Use [1] for MS1 only, [2] for MS2 only.
    """
    from rawreader import RawReader

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    if not filename.lower().endswith(".raw"):
        raise ValueError("filename must end with .raw")

    self.logger.info(f"Loading {filename} (rawreader)")

    # Create RawReader instance - it automatically detects Thermo format
    raw_reader = RawReader(
        filename,
        centroid=False,
        mslevel=mslevel,
        cachelevel=cachelevel,
    )

    # Get scan and peak dataframes - using new API
    scan_df = raw_reader.scans
    peak_df = raw_reader.cache

    # Note: scan_id is now provided by rawreader and is invariant across mslevel filters

    # Filter scans by mslevel if specified
    if mslevel is not None:
        scan_df = scan_df.filter(pl.col("mslevel").is_in(mslevel))

    # Get metadata
    metadata = raw_reader.metadata

    # scan_id already provided by rawreader with correct invariant numbering

    # Convert RT from minutes to seconds (rawreader returns RT in minutes)
    scan_df = scan_df.with_columns(
        (pl.col("rt") * 60.0).alias("rt"),
    )

    # Determine polarity from first non-null value
    polarity = None
    first_polarity = scan_df.select(pl.col("polarity")).to_series()[0]
    if first_polarity == "positive":
        polarity = "positive"
    elif first_polarity == "negative":
        polarity = "negative"

    # Calculate cycle numbers (cumulative count of MS1 scans)
    scan_df = scan_df.with_columns(
        pl.when(pl.col("mslevel") == 1).then(1).otherwise(0).cum_sum().alias("cycle"),
    )

    # Process spectra with progress bar
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    scans = []
    ms1_peaks_list = []

    for row in tqdm(
        scan_df.iter_rows(named=True),
        total=len(scan_df),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        scan_id = row["scan_id"]
        cycle = row["cycle"]
        ms_level = row["mslevel"]
        rt = row["rt"]
        fill_time = row.get("fill_time", None)

        # Get peaks for this scan
        cache_start = row["cache_start"]
        cache_end = row["cache_end"]

        peaks = None
        spect = None
        bl = None

        if cache_start is not None and cache_end is not None:
            peaks = peak_df.slice(cache_start, cache_end - cache_start)

            # Create spectrum and process
            spect = Spectrum(
                mz=peaks["mz"].to_numpy(),
                inty=peaks["inty"].to_numpy(),
                ms_level=ms_level,
                centroided=False,
            )

            algo = self.parameters.centroid_algo
            bl = spect.baseline()
            spect = spect.denoise(threshold=bl)
            if ms_level == 1 and algo is not None and len(spect.mz) > 0:
                spect = spect.centroid(
                    algo=algo,
                    tolerance=self.parameters.mz_tol_ms1_da,
                )

        # Create scan record even if spectrum was not cached (e.g., cachelevel excludes this ms_level)
        scan_record = {
            "scan_id": scan_id,
            "cycle": cycle,
            "ms_level": ms_level,
            "rt": rt,
            "time_fill": fill_time if spect is not None else None,
            "time_cycle": None,
            "prec_mz": row.get("precursor_mz"),
            "prec_z": row.get("precursor_z"),
            "prec_inty": None,
            "isolationWindowTargetMZ": row.get("precursor_mz"),
            "isolationWindowLowerOffset": None,
            "isolationWindowUpperOffset": None,
            "precursorIsolationWindowLowerMZ": row.get("isolation_lower_mz"),
            "precursorIsolationWindowUpperMZ": row.get("isolation_upper_mz"),
            "energy": row.get("collision_energy"),
        }
        scans.append(scan_record)

        # For MS1, store peaks for ms1_df
        if ms_level == 1 and spect is not None and len(spect.mz) > 0:
            ms1_peaks_list.append(
                pl.DataFrame(
                    {
                        "cycle": [cycle] * len(spect.mz),
                        "scan_id": [scan_id] * len(spect.mz),
                        "rt": [rt] * len(spect.mz),
                        "mz": spect.mz,
                        "inty": spect.inty,
                    },
                ),
            )

    # Create scans_df
    self.scans_df = pl.DataFrame(scans)

    # Create ms1_df
    if ms1_peaks_list:
        self.ms1_df = pl.concat(ms1_peaks_list)
    else:
        # Empty DataFrame with correct schema
        self.ms1_df = pl.DataFrame(
            schema={
                "cycle": pl.Int32,
                "scan_id": pl.Int64,
                "rt": pl.Float64,
                "mz": pl.Float64,
                "inty": pl.Float64,
            },
        )

    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_reader
    self.file_interface = "rawreader"
    self.polarity = polarity
    self.label = os.path.basename(filename)
    if self.type != "ztscan":
        self.analyze_dda()

    # Extract metadata and method from raw_reader
    metadata = raw_reader.metadata
    method = raw_reader.method
    return metadata, method


def _load_d_v2(
    self,
    filename=None,
    cachelevel=None,
    mslevel=None,
):
    """Load Agilent and Bruker .d folders using rawreader module.

    This loader uses rawreader.RawReader which provides access to .d folders
    via ProteoWizard DLLs.

    Args:
        filename: Path to the .d folder
        cachelevel: List of MS levels to cache spectrum data for (e.g., [1, 2]).
                   If None, all MS levels are cached. Use [1] for MS1 only, [2] for MS2 only.
        mslevel: List of MS levels to load from file (e.g., [1, 2]).
                If None, all MS levels are loaded. Use [1] for MS1 only, [2] for MS2 only.
    """
    from rawreader import RawReader

    if not filename:
        raise ValueError("Filename must be provided.")

    filename = os.path.abspath(filename)
    # check if it exists
    if not os.path.exists(filename):
        raise FileNotFoundError(f"File {filename} not found.")

    if not filename.lower().endswith(".d"):
        raise ValueError("filename must end with .d")

    self.logger.info(f"Loading {filename} (rawreader)")

    # Create RawReader instance - it automatically detects Agilent/Bruker format
    raw_reader = RawReader(
        filename,
        centroid=False,
        mslevel=mslevel,
        cachelevel=cachelevel,
    )

    # Get scan and peak dataframes - using new API
    scan_df = raw_reader.scans
    peak_df = raw_reader.cache

    # Note: scan_id is now provided by rawreader and is invariant across mslevel filters

    # Filter scans by mslevel if specified
    if mslevel is not None:
        scan_df = scan_df.filter(pl.col("mslevel").is_in(mslevel))

    # Get metadata
    metadata = raw_reader.metadata

    # scan_id already provided by rawreader with correct invariant numbering

    # Convert RT from minutes to seconds (rawreader returns RT in minutes)
    scan_df = scan_df.with_columns(
        (pl.col("rt") * 60.0).alias("rt"),
    )

    # Determine polarity from first non-null value
    polarity = None
    first_polarity = scan_df.select(pl.col("polarity")).to_series()[0]
    if first_polarity == "positive":
        polarity = "positive"
    elif first_polarity == "negative":
        polarity = "negative"

    # Calculate cycle numbers (cumulative count of MS1 scans)
    scan_df = scan_df.with_columns(
        pl.when(pl.col("mslevel") == 1).then(1).otherwise(0).cum_sum().alias("cycle"),
    )

    # Process spectra with progress bar
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    scans = []
    ms1_peaks_list = []

    for row in tqdm(
        scan_df.iter_rows(named=True),
        total=len(scan_df),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Scans",
        disable=tdqm_disable,
    ):
        scan_id = row["scan_id"]
        cycle = row["cycle"]
        ms_level = row["mslevel"]
        rt = row["rt"]
        fill_time = row.get("fill_time", None)

        # Get peaks for this scan
        cache_start = row["cache_start"]
        cache_end = row["cache_end"]

        peaks = None
        spect = None
        bl = None

        if cache_start is not None and cache_end is not None:
            peaks = peak_df.slice(cache_start, cache_end - cache_start)

            # Create spectrum and process
            spect = Spectrum(
                mz=peaks["mz"].to_numpy(),
                inty=peaks["inty"].to_numpy(),
                ms_level=ms_level,
                centroided=False,
            )

            algo = self.parameters.centroid_algo
            bl = spect.baseline()
            spect = spect.denoise(threshold=bl)
            if ms_level == 1 and algo is not None and len(spect.mz) > 0:
                spect = spect.centroid(
                    algo=algo,
                    tolerance=self.parameters.mz_tol_ms1_da,
                )

        # Create scan record even if spectrum was not cached (e.g., cachelevel excludes this ms_level)
        scan_record = {
            "scan_id": scan_id,
            "cycle": cycle,
            "ms_level": ms_level,
            "rt": rt,
            "time_fill": fill_time if spect is not None else None,
            "time_cycle": None,
            "prec_mz": row.get("precursor_mz"),
            "prec_z": row.get("precursor_z"),
            "prec_inty": None,
            "isolationWindowTargetMZ": row.get("precursor_mz"),
            "isolationWindowLowerOffset": None,
            "isolationWindowUpperOffset": None,
            "precursorIsolationWindowLowerMZ": row.get("isolation_lower_mz"),
            "precursorIsolationWindowUpperMZ": row.get("isolation_upper_mz"),
            "energy": row.get("collision_energy"),
        }
        scans.append(scan_record)

        # For MS1, store peaks for ms1_df
        if ms_level == 1 and spect is not None and len(spect.mz) > 0:
            ms1_peaks_list.append(
                pl.DataFrame(
                    {
                        "cycle": [cycle] * len(spect.mz),
                        "scan_id": [scan_id] * len(spect.mz),
                        "rt": [rt] * len(spect.mz),
                        "mz": spect.mz,
                        "inty": spect.inty,
                    },
                ),
            )

    # Create scans_df
    self.scans_df = pl.DataFrame(scans)

    # Create ms1_df
    if ms1_peaks_list:
        self.ms1_df = pl.concat(ms1_peaks_list)
    else:
        # Empty DataFrame with correct schema
        self.ms1_df = pl.DataFrame(
            schema={
                "cycle": pl.Int32,
                "scan_id": pl.Int64,
                "rt": pl.Float64,
                "mz": pl.Float64,
                "inty": pl.Float64,
            },
        )

    self.file_path = filename
    self.file_source = filename
    self.file_obj = raw_reader
    self.file_interface = "rawreader"
    self.polarity = polarity
    self.label = os.path.basename(filename)
    if self.type != "ztscan":
        self.analyze_dda()

    # Extract metadata and method from raw_reader
    metadata = raw_reader.metadata
    method = raw_reader.method
    return metadata, method


def _load_featureXML(
    self,
    filename="features.featureXML",
):
    """
    Load feature data from a FeatureXML file.

    This method reads a FeatureXML file (defaulting to "features.featureXML") using the
    OMS library's FeatureXMLFile and FeatureMap objects. The loaded feature data is stored
    in the instance variable 'features'. The method then converts the feature data into a
    DataFrame, optionally excluding peptide identification data, and cleans it using the
    '__oms_clean_df' method, saving the cleaned DataFrame into 'features_df'.

    Parameters:
        filename (str): The path to the FeatureXML file to load. Defaults to "features.featureXML".

    Returns:
        None
    """
    fh = oms.FeatureXMLFile()
    fm = oms.FeatureMap()
    fh.load(filename, fm)
    self._oms_features_map = fm


def _wiff_to_dict(
    filename=None,
):
    from alpharaw.raw_access.pysciexwifffilereader import WillFileReader

    file_reader = WillFileReader(filename)
    number_of_samples = len(file_reader.sample_names)
    metadata = []
    for si in range(number_of_samples):
        sample_reader = file_reader._wiff_file.GetSample(si)
        number_of_exps = sample_reader.MassSpectrometerSample.ExperimentCount
        for ei in range(number_of_exps):
            exp_reader = sample_reader.MassSpectrometerSample.GetMSExperiment(ei)

            exp_info = exp_reader.GetMassSpectrumInfo(ei)

            # get the details of the experiment
            exp_name = exp_reader.Details.get_ExperimentName()
            exp_type = exp_reader.Details.get_ExperimentType()

            IDA_type = exp_reader.Details.get_IDAType()
            has_MRM_Pro_Data = exp_reader.Details.get_HasMRMProData()
            has_SMRM_Data = exp_reader.Details.get_HasSMRMData()
            is_swath = exp_reader.Details.get_IsSwath()
            has_dyn_fill_time = exp_reader.Details.get_HasDynamicFillTime()
            method_fill_time = exp_reader.Details.get_MethodFillTime()
            default_resolution = exp_reader.Details.get_DefaultResolution()
            parameters = exp_reader.Details.get_Parameters()
            targeted_compound_info = exp_reader.Details.get_TargetedCompoundInfo()
            source_type = exp_reader.Details.get_SourceType()
            raw_data_type = exp_reader.Details.get_RawDataType()

            number_of_scans = exp_reader.Details.get_NumberOfScans()
            scan_group = exp_reader.Details.get_ScanGroup()
            spectrum_type = exp_reader.Details.get_SpectrumType()
            saturatrion_threshold = exp_reader.Details.get_SaturationThreshold()
            polarity = exp_reader.Details.get_Polarity()
            mass_range_info = exp_reader.Details.get_MassRangeInfo()
            start_mass = exp_reader.Details.get_StartMass()
            end_mass = exp_reader.Details.get_EndMass()

            mslevel = exp_info.MSLevel
            if mslevel > 1:
                # get the precursor information
                parent_mz = exp_info.ParentMZ
                collision_energy = exp_info.CollisionEnergy
                parent_charge_state = exp_info.ParentChargeState
            else:
                parent_mz = None
                collision_energy = None
                parent_charge_state = None

            # create a dict with the details
            exp_dict = {
                "instrument_name": sample_reader.MassSpectrometerSample.get_InstrumentName(),
                "sample_id": si,
                "experiment_id": ei,
                "experiment_name": exp_name,
                "experiment_type": exp_type,
                "IDA_type": IDA_type,
                "has_MRM_Pro_Data": has_MRM_Pro_Data,
                "has_SMRM_Data": has_SMRM_Data,
                "is_swath": is_swath,
                "has_dyn_fill_time": has_dyn_fill_time,
                "method_fill_time": method_fill_time,
                "default_resolution": default_resolution,
                "parameters": parameters,
                "targeted_compound_info": targeted_compound_info,
                "source_type": source_type,
                "raw_data_type": raw_data_type,
                "number_of_scans": number_of_scans,
                "scan_group": scan_group,
                "spectrum_type": spectrum_type,
                "saturatrion_threshold": saturatrion_threshold,
                "polarity": polarity,
                "mass_range_info": mass_range_info,
                "start_mass": start_mass,
                "end_mass": end_mass,
                "mslevel": mslevel,
                "parent_mz": parent_mz,
                "collision_energy": collision_energy,
                "parent_charge_state": parent_charge_state,
            }
            metadata.append(exp_dict)
    # convert to pandas DataFrame
    metadata = pd.DataFrame(metadata)

    return metadata


def sanitize(self):
    # iterate over all rows in self.features_df
    if self.features_df is None:
        return
    for _i, row in self.features_df.iterrows():
        # check if chrom is not None
        if row["chrom"] is not None and not isinstance(row["chrom"], Chromatogram):
            # update chrom to a Chromatogram
            new_chrom = Chromatogram(rt=np.array([]), inty=np.array([]))
            new_chrom.from_dict(row["chrom"].__dict__)
            self.features_df.at[_i, "chrom"] = new_chrom
        if row["ms2_specs"] is not None:
            if isinstance(row["ms2_specs"], list):
                for _j, ms2_specs in enumerate(row["ms2_specs"]):
                    if not isinstance(ms2_specs, Spectrum):
                        new_ms2_specs = Spectrum(mz=np.array([0]), inty=np.array([0]))
                        new_ms2_specs.from_dict(ms2_specs.__dict__)
                        self.features_df.at[_i, "ms2_specs"][_j] = new_ms2_specs


def index_raw(self, cachelevel: list[int] | None = [1], interface: str | None = None):
    """
    Reload raw data from a file based on its extension.

    This method checks whether the file at self.file_path exists and determines
    the appropriate way to load it depending on its extension:
    - If the file ends with ".wiff" or ".wiff2", it uses the SciexWiffData class for import.
      Note: .wiff2 files are automatically converted to .wiff before loading.
    - If the file ends with ".raw", it uses the ThermoRawData class for import.
    - If the file ends with ".mzml", it uses the MzMLFile loader with either
        an on-disk or in-memory MSExperiment based on the self.ondisk flag.

    It also sets the file interface and file object on the instance after successful
    import. Additionally, the number of peaks per spectrum is configured using the
    'max_points_per_spectrum' parameter from self.parameters.

    Args:
        cachelevel: List of MS levels to cache spectrum data for (e.g., [1, 2]).
                   If None, all MS levels are cached. Use [1] for MS1 only, [2] for MS2 only.
                   Default is [1] to cache only MS1 data.
        interface: Specifies which loader interface to use:
                  - None (default): Auto-detects best available loader
                    * For .wiff files: rawreader > masster > alpharaw
                    * For .raw files: rawreader > alpharaw
                    * For .mzML files: oms
                  - "rawreader": Forces rawreader (for .wiff, .raw)
                  - "masster": Forces masster's native reader (for .wiff only)
                  - "alpharaw": Forces alpharaw (for .wiff, .raw)
                  - "oms": Forces OpenMS (for .mzML)

    Raises:
            FileNotFoundError: If the file does not exist or has an unsupported extension.
    """
    # Replace .wiff2 with .wiff for compatibility
    file_to_check = self.file_source
    if file_to_check.lower().endswith(".wiff2"):
        file_to_check = file_to_check[:-1]  # Remove the trailing '2'

    # check if file_path exists and ends with .wiff or .raw
    if (os.path.exists(file_to_check) and file_to_check.lower().endswith(".wiff")) or (
        os.path.exists(self.file_source) and self.file_source.lower().endswith(".raw")
    ):
        # Update file_source to use the actual file path (without .wiff2)
        if self.file_source.lower().endswith(".wiff2"):
            self.file_source = file_to_check

        # Determine which loader to use based on interface parameter
        selected_interface = None
        is_wiff = self.file_source.lower().endswith(
            ".wiff",
        ) or file_to_check.lower().endswith(".wiff")

        if interface == "alpharaw":
            selected_interface = "alpharaw"
        elif interface == "rawreader":
            selected_interface = "rawreader"
        elif interface == "masster":
            if not is_wiff:
                raise ValueError(
                    "Interface 'masster' is only supported for WIFF files. "
                    "Use 'rawreader' or 'alpharaw' for RAW files.",
                )
            selected_interface = "masster"
        elif (
            interface is None
        ):  # Auto-detect: rawreader > masster (WIFF only) > alpharaw
            try:
                import rawreader

                selected_interface = "rawreader"
            except ImportError:
                # Try masster as fallback (only for WIFF files)
                if is_wiff:
                    try:
                        from masster.sample.sciex_v2 import SciexOndemandReader

                        selected_interface = "masster"
                    except ImportError:
                        selected_interface = "alpharaw"
                else:
                    selected_interface = "alpharaw"
        else:
            raise ValueError(
                f"Invalid interface '{interface}' for .wiff/.raw files. "
                f"Valid options: None (auto), 'rawreader', 'masster', 'alpharaw'",
            )

        if selected_interface == "rawreader":
            # Use rawreader for consistent scan numbering
            from rawreader import RawReader

            self.file_interface = "rawreader"
            raw_reader = RawReader(
                self.file_source,
                sample_id=getattr(self, "sample_idx", None),
                centroid=False,
                cachelevel=cachelevel,
                silent=True,
            )
            self.logger.info(f"Index raw data... ({self.file_interface})")
            self.file_obj = raw_reader
            # Verify scan count matches
            if len(raw_reader.scans) != len(self.scans_df):
                self.logger.warning(
                    f"Scan count mismatch: rawreader has {len(raw_reader.scans)} scans, "
                    f"but scans_df has {len(self.scans_df)} scans. This may cause 'Scan not found' errors. "
                    f"Consider reloading the sample from the original raw file.",
                )
        elif selected_interface == "masster":
            # Use masster's native SciexOndemandReader
            from masster.sample.sciex_v2 import SciexOndemandReader

            self.file_interface = "masster"
            raw_reader = SciexOndemandReader(
                self.file_source,
                sample_id=getattr(self, "sample_idx", None),
                mslevel=[1, 2],
                cachelevel=cachelevel,
                keep_k_peaks=100000,
            )
            self.logger.info(f"Index raw data... ({self.file_interface})")
            self.file_obj = raw_reader
            # Verify scan count matches
            if len(raw_reader.scans) != len(self.scans_df):
                self.logger.warning(
                    f"Scan count mismatch: masster reader has {len(raw_reader.scans)} scans, "
                    f"but scans_df has {len(self.scans_df)} scans. This may cause 'Scan not found' errors. "
                    f"Consider reloading the sample from the original raw file.",
                )
        else:  # alpharaw
            # Use alpharaw
            self.file_interface = "alpharaw"
            if self.file_source.lower().endswith(".wiff"):
                from masster.sample.sciex import SciexWiffData

                self.logger.info(f"Index raw data... ({self.file_interface})")
                raw_data_wiff = SciexWiffData(centroided=False)
                raw_data_wiff.keep_k_peaks_per_spec = (
                    self.parameters.max_points_per_spectrum
                )
                raw_data_wiff.import_raw(self.file_source)
                self.file_obj = raw_data_wiff
            elif self.file_source.lower().endswith(".raw"):
                from masster.sample.thermo import ThermoRawData

                self.logger.info(f"Index raw data... ({self.file_interface})")
                raw_data_thermo = ThermoRawData(centroided=False)
                raw_data_thermo.keep_k_peaks_per_spec = (
                    self.parameters.max_points_per_spectrum
                )
                raw_data_thermo.import_raw(self.file_source)
                self.file_obj = raw_data_thermo
    elif os.path.exists(self.file_source) and self.file_source.lower().endswith(
        ".mzml",
    ):
        self.file_interface = "oms"
        omsexp: oms.OnDiscMSExperiment | oms.MSExperiment
        if self.ondisk:
            omsexp = oms.OnDiscMSExperiment()
            self.file_obj = omsexp
        else:
            omsexp = oms.MSExperiment()
            oms.MzMLFile().load(self.file_source, omsexp)
            self.file_obj = omsexp
    elif os.path.exists(self.file_source) and self.file_source.lower().endswith(
        ".sample5",
    ):
        # this is an old save, try to see if
        if os.path.exists(self.file_source.replace(".sample5", ".wiff")):
            self.set_source(self.file_source.replace(".sample5", ".wiff"))
        elif os.path.exists(self.file_source.replace(".sample5", ".raw")):
            self.set_source(self.file_source.replace(".sample5", ".raw"))
        elif os.path.exists(self.file_source.replace(".sample5", ".mzml")):
            self.set_source(self.file_source.replace(".sample5", ".mzml"))
        else:
            raise FileNotFoundError(
                f"File {self.file_source} not found. Did the path change? Consider running source().",
            )
        self.index_raw()
    else:
        raise FileNotFoundError(
            f"File {self.file_source} not found. Did the path change? Consider running source().",
        )


def _load_ms2data(
    self,
    scans=None,
):
    # reads all ms2 data from the file object and returns a polars DataFrame

    # check if file_obj is set
    if self.file_obj is None:
        return
    # check if scan_id is set
    if scans is None:
        scans = self.scans_df["scan_id"].to_list()
    if len(scans) == 0:
        scans = self.scans_df["scan_id"].to_list()

    # check the file interface
    if self.file_interface == "oms":
        _load_ms2data(self, scans=scans)
    elif self.file_interface == "alpharaw":
        _load_ms2data_alpharaw(self, scan_uid=scans)
    elif self.file_interface == "rawreader":
        _load_ms2data_rawreader(self, scan_uid=scans)

    return


def _load_ms2data_rawreader(
    self,
    scan_uid=None,
):
    """Load MS2 data from rawreader file object."""
    ms2data = None
    scan_uid = self.scans_df["scan_id"].to_list() if scan_uid is None else scan_uid
    self.logger.info(f"Loading MS2 data for {len(scan_uid)} scans...")

    if self.file_obj is None:
        return

    raw_data = self.file_obj
    scans = raw_data.scans
    peak_df = raw_data.cache

    schema = {
        "scan_id": pl.Int64,
        "rt": pl.Float64,
        "prec_mz": pl.Float64,
        "mz": pl.Float64,
        "inty": pl.Float64,
    }

    ms2data = pl.DataFrame(
        {"scan_id": [], "rt": [], "prec_mz": [], "mz": [], "inty": []},
        schema=schema,
    )

    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    for scan_id in tqdm(
        scan_uid,
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Loading MS2",
        disable=tdqm_disable,
    ):
        # Use scan_id as positional index in rawreader scans (0-based sequential)
        if scan_id < 0 or scan_id >= len(scans):
            self.logger.warning(
                f"Scan {scan_id} not found (out of range: 0-{len(scans) - 1}).",
            )
            continue
        scan_row = scans.row(scan_id, named=True)

        if scan_row["mslevel"] == 2:
            prec_mz = scan_row["precursor_mz"]
            rt = scan_row["rt"]
            cache_start = scan_row["cache_start"]
            cache_end = scan_row["cache_end"]

            if cache_start is not None and cache_end is not None:
                peaks = peak_df.slice(
                    cache_start,
                    cache_end - cache_start,
                )

                spect = Spectrum(
                    mz=peaks["mz"].to_numpy(),
                    inty=peaks["inty"].to_numpy(),
                    ms_level=scan_row["mslevel"],
                    centroided=False,
                )
                # remove peaks with intensity <= 0
                bl = spect.baseline()
                spect = spect.denoise(threshold=bl)

                if len(spect.mz) > 0:
                    newms2data = pl.DataFrame(
                        {
                            "scan_id": scan_id,
                            "rt": rt,
                            "prec_mz": prec_mz,
                            "mz": spect.mz,
                            "inty": spect.inty,
                        },
                        schema=schema,
                    )
                    ms2data = pl.concat([ms2data, newms2data], rechunk=True)

    self.ms2data = ms2data
    return


def _load_ms2data_alpharaw(
    self,
    scan_uid=None,
):
    # reads all ms data from the file object and returns a polars DataFrame

    ms2data = None  # Placeholder for potential future use
    scan_uid = self.scans_df["scan_id"].to_list() if scan_uid is None else scan_uid
    self.logger.info(f"Loading MS2 data for {len(scan_uid)} scans...")
    # keep only scans with ms_level == 2
    if self.file_obj is None:
        return

    raw_data = self.file_obj
    scans = raw_data.spectrum_df
    # scans.rt = scans.rt * 60
    scans = scans.with_columns(pl.col("rt").round(4))

    schema = {
        "scan_id": pl.Int64,
        "rt": pl.Float64,
        "prec_mz": pl.Float64,
        "mz": pl.Float64,
        "inty": pl.Float64,
    }
    # create a polars DataFrame with explicit schema: cycle: int, rt: float, mz: float, intensity: float
    ms2data = pl.DataFrame(
        {"scan_id": [], "rt": [], "prec_mz": [], "mz": [], "inty": []},
        schema=schema,
    )
    # iterate over rows of specs
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]
    for i, s in tqdm(
        enumerate(scans.iter_rows(named=True)),
        total=len(scans),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Centroid",
        disable=tdqm_disable,
    ):
        # create a dict
        if s["ms_level"] == 2:
            prec_mz = s["precursor_mz"]
            peak_start_idx = s["peak_start_idx"]
            peak_stop_idx = s["peak_stop_idx"]
            peaks = raw_data.peak_df.slice(
                peak_start_idx,
                peak_stop_idx - peak_start_idx,
            )
            spect = Spectrum(
                mz=peaks["mz"].to_numpy(),
                inty=peaks["intensity"].to_numpy(),
                ms_level=s["ms_level"],
                centroided=False,
            )
            # remove peaks with intensity <= 0
            bl = spect.baseline()
            spect = spect.denoise(threshold=bl)

            if len(peaks) > 0:
                newms2data = pl.DataFrame(
                    {
                        "scan_id": i,
                        "rt": s["rt"],
                        "prec_mz": prec_mz,
                        "mz": spect.mz,
                        "inty": spect.inty,
                    },
                    schema=schema,
                )
                ms2data = pl.concat([ms2data, newms2data])
    self.ms2data = ms2data


def chrom_extract(
    self,
    rt_tol=6.0,
    mz_tol=0.005,
):
    """Extract MRM and EIC chromatograms from MS data file.

    Processes chrom_df to identify relevant scans in scans_df and extract chromatograms
    for MS1, MRM, and MS2 traces. Updates chrom_df in-place with scan IDs and
    chromatogram objects for downstream analysis.

    Args:
        rt_tol (float): Retention time tolerance for scan selection in seconds. Scans
            within rt_start-rt_tol to rt_end+rt_tol are included. Defaults to 6.0.
        mz_tol (float): m/z tolerance for scan selection in Daltons. Defaults to 0.005.

    Returns:
        None: Updates chrom_df in-place with scan_id and chrom columns.

    Example:
        ::

            from masster import Sample
            import polars as pl

            # Load sample and define traces to extract
            s = Sample(file="data.mzML")
            s.chrom_df = pl.DataFrame({
                "type": ["ms1", "ms2", "mrm"],
                "rt": [300.0, 450.0, 500.0],
                "rt_start": [290.0, 440.0, 490.0],
                "rt_end": [310.0, 460.0, 510.0],
                "prec_mz": [400.1234, 500.5678, 600.7890]
            })

            # Extract chromatograms with default tolerances
            s.chrom_extract()

            # Extract with custom tolerances
            s.chrom_extract(rt_tol=10.0, mz_tol=0.01)

            # Access extracted chromatogram objects
            first_chrom = s.chrom_df["chrom"][0]
            print(first_chrom.rt)  # Retention times
            print(first_chrom.inty)  # Intensities

            # Get scan IDs for first trace
            scan_ids = s.chrom_df["scan_id"][0]
            print(f"Found {len(scan_ids)} scans")

    Note:
        **Prerequisites:**

        Requires file_obj to be loaded and chrom_df to be initialized with trace
        definitions. chrom_df must contain columns: type, rt, rt_start, rt_end, prec_mz.

        **Trace Types:**

        - "ms1": MS1 survey scans (full scan)
        - "ms2": MS2 fragmentation scans (DDA or DIA)
        - "mrm": Multiple Reaction Monitoring transitions (targeted)

        **Scan Identification:**

        For each trace, identifies all scan_id values in scans_df that fall within
        the specified RT and m/z windows. Scan IDs are stored as lists in the
        scan_id column.

        **Chromatogram Objects:**

        Extracted chromatograms are stored as Chromatogram objects in the chrom column.
        Each Chromatogram contains rt (retention times) and inty (intensities) arrays.

        **Default RT Windows:**

        If rt_start or rt_end are None in chrom_df, default windows of 3 seconds
        from rt are used.

        **In-Place Updates:**

        Modifies chrom_df directly by adding/updating scan_id and chrom columns.
        No return value.

    See Also:
        - :meth:`get_eic`: Extract EIC for specific m/z and RT
        - :class:`~masster.chromatogram.Chromatogram`: Chromatogram data structure
    """
    if self.file_obj is None:
        return

    if self.chrom_df is None:
        return

    # check if mrm_df is dict, if so convert to DataFrame
    chrom_df = self.chrom_df

    chrom_df["scan_id"] = None
    chrom_df["chrom"] = None
    scan_uid = []

    # iterate over all mrms and identify the scans
    for i, trace in chrom_df.iterrows():
        if trace["type"] in ["ms1"]:
            rt = trace["rt"]
            rt_start = trace["rt_start"]
            if rt_start is None:
                rt_start = rt - 3
            rt_end = trace["rt_end"]
            if rt_end is None:
                rt_end = rt + 3
            q1 = trace["prec_mz"]  # Extracted for potential future use
            # find all rows in self.scans_df that have rt between rt_start-rt_tol and rt_end+rt_tol and mz between q1-mz_tol and q1+mz_tol
            mask = (
                (self.scans_df["rt"] >= rt_start - rt_tol)
                & (self.scans_df["rt"] <= rt_end + rt_tol)
                & (self.scans_df["ms_level"] == 1)
            )
            scans_df = self.scans_df.filter(mask)
            scan_ids = scans_df["scan_id"].to_list()
            scan_uid.extend(scan_ids)
            chrom_df.at[i, "scan_id"] = scan_ids

        elif trace["type"] in ["mrm", "ms2"]:
            rt = trace["rt"]
            rt_start = trace["rt_start"]
            if rt_start is None:
                rt_start = rt - 3
            rt_end = trace["rt_end"]
            if rt_end is None:
                rt_end = rt + 3
            q1 = trace["prec_mz"]
            # find all rows in self.scans_df that have rt between rt_start-rt_tol and rt_end+rt_tol and mz between q1-mz_tol and q1+mz_tol
            mask = (
                (self.scans_df["rt"] >= rt_start - rt_tol)
                & (self.scans_df["rt"] <= rt_end + rt_tol)
                & (self.scans_df["ms_level"] == 2)
                & (self.scans_df["prec_mz"] >= q1 - 5)
                & (self.scans_df["prec_mz"] <= q1 + 5)
            )
            scans_df = self.scans_df.filter(mask)
            # find the closes prec_mz to q1
            if scans_df.is_empty():
                continue
            # find the closest prec_mz to q1
            # sort by abs(prec_mz - q1) and take the first row
            # this is the closest precursor m/z to q1
            closest_prec_mz = scans_df.sort(abs(pl.col("prec_mz") - q1)).select(
                pl.col("prec_mz").first(),
            )
            # keep only the scans with prec_mz within mz_tol of closest_prec_mz
            scans_df = scans_df.filter(
                (pl.col("prec_mz") >= closest_prec_mz["prec_mz"][0] - 0.2)
                & (pl.col("prec_mz") <= closest_prec_mz["prec_mz"][0] + 0.2),
            )

            scan_ids = scans_df["scan_id"].to_list()
            scan_uid.extend(scan_ids)
            chrom_df.at[i, "scan_id"] = scan_ids

    # get the ms2data
    _load_ms2data(self, scans=list(set(scan_uid)) if scan_uid else None)
    tdqm_disable = self.log_level not in ["TRACE", "DEBUG", "INFO"]

    for i, trace in tqdm(
        chrom_df.iterrows(),
        total=len(chrom_df),
        desc=f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} | INFO     | {self.log_label}Extract EICs",
        disable=tdqm_disable,
    ):
        if trace["type"] in ["ms1"]:
            q1 = trace["prec_mz"]
            name = trace["name"]
            scan_id = trace["scan_id"]
            # find all ms1 data with scan_id and mz between q1-mz_tol and q1+mz_tol
            d = self.ms1_df.filter(
                (pl.col("scan_id").is_in(scan_id))
                & (pl.col("mz") >= q1 - mz_tol)
                & (pl.col("mz") <= q1 + mz_tol),
            )
            # for all unique rt values, find the maximum inty
            eic_rt = d.group_by("rt").agg(pl.col("inty").max())
            eic = Chromatogram(
                eic_rt["rt"].to_numpy(),
                inty=eic_rt["inty"].to_numpy(),
                label=f"MS1 {name} ({q1:0.3f})",
                lib_rt=trace["rt"],
            )
            chrom_df.at[i, "chrom"] = eic

        elif trace["type"] in ["mrm", "ms2"]:
            q1 = trace["prec_mz"]
            q3 = trace["prod_mz"]
            name = trace["name"]
            scan_id = trace["scan_id"]
            # find all ms2 data with scan_id and mz between q3-mz_tol and q3+mz_tol
            d = self.ms2data.filter(
                (pl.col("scan_id").is_in(scan_id))
                & (pl.col("mz") >= q3 - mz_tol)
                & (pl.col("mz") <= q3 + mz_tol),
            )
            # for all unique rt values, find the maximum inty
            eic_rt = d.group_by("rt").agg(pl.col("inty").max())
            eic = Chromatogram(
                eic_rt["rt"].to_numpy(),
                inty=eic_rt["inty"].to_numpy(),
                label=f"MRM {name} ({q1:0.3f}>{q3:0.3f})",
                lib_rt=trace["rt"],
            )
            chrom_df.at[i, "chrom"] = eic

    self.chrom_df = chrom_df


def _oms_clean_df(self, df):
    df2 = df[df["quality"] != 0]
    # change columns and order
    df = pd.DataFrame(
        columns=[
            "feature_id",
            "uid",
            "mz",
            "rt",
            "rt_start",
            "rt_end",
            "rt_delta",
            "mz_start",
            "mz_end",
            "inty",
            "quality",
            "charge",
            "iso",
            "iso_of",
            "chrom",
            "chrom_coherence",
            "chrom_prominence",
            "chrom_prominence_scaled",
            "chrom_height_scaled",
            "ms2_scans",
            "ms2_specs",
        ],
    )

    # set values of fid to 0:len(df)
    df["uid"] = df2.index.to_list()
    df["mz"] = (df2["mz"]).round(5)
    df["rt"] = (df2["RT"]).round(3)
    df["rt_start"] = (df2["RTstart"]).round(3)
    df["rt_end"] = (df2["RTend"]).round(3)
    df["rt_delta"] = (df2["RTend"] - df2["RTstart"]).round(3)
    df["mz_start"] = (df2["MZstart"]).round(5)
    df["mz_end"] = (df2["MZend"]).round(5)  # df2["MZend"]
    df["inty"] = df2["intensity"]
    df["quality"] = df2["quality"]
    df["charge"] = df2["charge"]
    df["iso"] = 0
    df["iso_of"] = None
    df["chrom"] = None
    df["chrom_coherence"] = None
    df["chrom_prominence"] = None
    df["chrom_prominence_scaled"] = None
    df["chrom_height_scaled"] = None
    df["ms2_scans"] = None
    df["ms2_specs"] = None
    df["feature_id"] = range(1, len(df) + 1)
    # df.set_index('fid', inplace=True)
    # rests index
    # df.reset_index(drop=True, inplace=True)

    return df
