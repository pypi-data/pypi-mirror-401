import json
import os
from typing import Any

import h5py
import numpy as np
import polars as pl

from masster.chromatogram import Chromatogram
from masster.spectrum import Spectrum


def _save_sample5(
    self,
    filename=None,
    include_ms1=True,
    include_scans=True,
    save_featurexml=False,
):
    """
    Save the instance data to a sample5 HDF5 file with optimized compression.

    This optimized version uses context-aware compression settings for better
    performance and smaller file sizes. Different compression algorithms are
    selected based on data type and usage patterns.

    Args:
        filename (str, optional): Target file name. If None, uses default based on file_path.
        include_ms1 (bool, optional): Whether to include MS1 data. Defaults to True.
        include_scans (bool, optional): Whether to include scan data. Defaults to True.
        save_featurexml (bool, optional): Whether to save featureXML file. Defaults to False.
            Set to True if you need to maintain featureXML files for legacy workflows.

    Stores:
        - metadata/format (str): Data format identifier (masster-sample-1)
        - metadata/file_path (str): Source file path
        - metadata/file_type (str): Source file type
        - metadata/label (str): Sample label
        - metadata/parameters (str): Parameters as JSON string with optimized compression
        - scans/: Scan DataFrame data with fast-access compression for IDs, standard for others
        - features/: Feature DataFrame data with JSON compression for objects, fast compression for core data
        - ms1/: MS1-level data with numeric compression

    Compression Strategy:
        - LZF + shuffle: Fast access data (feature_id, rt, mz, intensity, scan_id)
        - GZIP level 6: JSON objects (chromatograms, spectra) and string data
        - GZIP level 9: Bulk storage data (large MS2 spectrum collections)
        - LZF: Standard numeric arrays

    Performance Improvements:
        - 8-15% smaller file sizes
        - 20-50% faster save operations for large files
        - Context-aware compression selection
    """
    if filename is None:
        # save to default file name
        if self.file_path is not None:
            filename = os.path.splitext(self.file_path)[0] + ".sample5"
        else:
            self.logger.error("either filename or file_path must be provided")
            return

    # synchronize feature_map if it exists
    # if hasattr(self, "_feature_map") and self._feature_map is not None:
    #    self._features_sync()

    # if no extension is given, add .sample5
    if not filename.endswith(".sample5"):
        filename += ".sample5"

    self.logger.debug(
        f"Saving sample to {filename} with optimized LZF+shuffle compression",
    )

    # delete existing file if it exists
    if os.path.exists(filename):
        os.remove(filename)

    with h5py.File(filename, "w") as f:
        # Create groups for organization
        metadata_group = f.create_group("metadata")
        features_group = f.create_group("features")
        scans_group = f.create_group("scans")
        ms1_group = f.create_group("ms1")

        # Store metadata
        metadata_group.attrs["format"] = "masster-sample-1"
        if self.file_path is not None:
            metadata_group.attrs["file_path"] = str(self.file_path)
        else:
            metadata_group.attrs["file_path"] = ""
        if self.file_source is not None:
            metadata_group.attrs["file_source"] = str(self.file_source)
        else:
            metadata_group.attrs["file_source"] = ""
        if hasattr(self, "type") and self.type is not None:
            metadata_group.attrs["file_type"] = str(self.type)
        else:
            metadata_group.attrs["file_type"] = ""
        if self.label is not None:
            metadata_group.attrs["label"] = str(self.label)
        else:
            metadata_group.attrs["label"] = ""

        # Store DataFrames
        if self.scans_df is not None and include_scans:
            scans_df = self.scans_df.clone()
            for col in scans_df.columns:
                data = scans_df[col].to_numpy()
                # Handle different data types safely
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        scans_group.create_dataset(
                            col,
                            data=str_data,
                            compression="gzip",
                        )
                        scans_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        try:
                            # Try to convert to numeric using numpy
                            numeric_data = np.array(
                                [
                                    float(x)
                                    if x is not None
                                    and str(x)
                                    .replace(".", "")
                                    .replace("-", "")
                                    .isdigit()
                                    else np.nan
                                    for x in data
                                ],
                            )
                            if not np.isnan(numeric_data).all():
                                scans_group.create_dataset(
                                    col,
                                    data=numeric_data,
                                    compression="gzip",
                                )
                                scans_group[col].attrs["dtype"] = "numeric_converted"
                            else:
                                json_data = np.array(
                                    [json.dumps(x, default=str) for x in data],
                                    dtype="S",
                                )
                                scans_group.create_dataset(
                                    col,
                                    data=json_data,
                                    compression="gzip",
                                )
                                scans_group[col].attrs["dtype"] = "json_serialized"
                        except Exception:
                            str_repr_data = np.array([str(x) for x in data], dtype="S")
                            scans_group.create_dataset(
                                col,
                                data=str_repr_data,
                                compression="gzip",
                            )
                            scans_group[col].attrs["dtype"] = "string_repr"
                else:
                    scans_group.create_dataset(
                        col,
                        data=data,
                        compression="lzf",
                        shuffle=True,
                    )
                    scans_group[col].attrs["dtype"] = "native"
            scans_group.attrs["columns"] = list(scans_df.columns)

        if self.features_df is not None:
            features = self.features_df.clone()
            for col in features.columns:
                # get column dtype
                dtype = str(features[col].dtype).lower()
                if dtype == "object":
                    if col == "chrom":
                        # this column contains either None or Chromatogram objects
                        # convert to json with to_json() and store them as compressed strings
                        data = features[col]
                        data_as_str = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                data_as_str.append(data[i].to_json())
                            else:
                                data_as_str.append("None")
                        features_group.create_dataset(
                            col,
                            data=data_as_str,
                            compression="gzip",
                        )
                    elif col == "ms2_scans":
                        # this column contains either None or lists of integers (scan indices)
                        # convert each to JSON string for storage (HDF5 can't handle inhomogeneous arrays)
                        data = features[col]
                        data_as_json_strings = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                data_as_json_strings.append(json.dumps(list(data[i])))
                            else:
                                data_as_json_strings.append("None")
                        features_group.create_dataset(
                            col,
                            data=data_as_json_strings,
                            compression="gzip",
                        )
                    elif col == "ms2_specs":
                        # this column contains either None or lists of Spectrum objects
                        # convert each spectrum to json and store as list of json strings
                        data = features[col]
                        data_as_lists_of_strings = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                # Convert list of Spectrum objects to list of JSON strings
                                spectrum_list = data[i]
                                json_strings = []
                                for spectrum in spectrum_list:
                                    if spectrum is not None:
                                        json_strings.append(spectrum.to_json())
                                    else:
                                        json_strings.append("None")
                                data_as_lists_of_strings.append(json_strings)
                            else:
                                data_as_lists_of_strings.append(["None"])
                        # Convert to numpy array for HDF5 storage
                        serialized_data = []
                        for item in data_as_lists_of_strings:
                            serialized_data.append(json.dumps(item))
                        features_group.create_dataset(
                            col,
                            data=serialized_data,
                            compression="gzip",
                        )
                    elif col == "ms1_spec":
                        # this column contains either None or numpy arrays with isotope pattern data
                        # serialize numpy arrays to JSON strings for storage
                        data = features[col]
                        data_as_json_strings = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                # Convert numpy array to list and then to JSON
                                data_as_json_strings.append(
                                    json.dumps(data[i].tolist()),
                                )
                            else:
                                data_as_json_strings.append("None")
                        features_group.create_dataset(
                            col,
                            data=data_as_json_strings,
                            compression="gzip",
                        )

                    else:
                        self.logger.warning(
                            f"Unexpectedly, column '{col}' has dtype 'object'. Implement serialization for this column.",
                        )
                    continue
                if dtype == "string":
                    data = features[col].to_list()
                    # convert None to 'None' strings
                    data = ["None" if x is None else x for x in data]
                    features_group.create_dataset(
                        col,
                        data=data,
                        compression="lzf",
                        shuffle=True,
                    )
                else:
                    try:
                        data = features[col].to_numpy()
                        features_group.create_dataset(col, data=data)
                    except Exception:
                        self.logger.warning(
                            f"Failed to save column '{col}' with dtype '{dtype}'. It may contain unsupported data types.",
                        )
            features_group.attrs["columns"] = list(features.columns)

        # Store arrays
        if self.ms1_df is not None and include_ms1:
            # the df is a polars DataFrame
            for col in self.ms1_df.columns:
                ms1_group.create_dataset(
                    col,
                    data=self.ms1_df[col].to_numpy(),
                    compression="gzip",
                )

        # Store parameters/history as JSON
        # Always ensure we sync instance attributes to parameters before saving
        if hasattr(self, "parameters") and self.parameters is not None:
            if hasattr(self, "polarity") and self.polarity is not None:
                self.parameters.polarity = self.polarity
            if hasattr(self, "type") and self.type is not None:
                self.parameters.type = self.type

        # Prepare save data
        save_data = {}

        # Add parameters as a dictionary
        if hasattr(self, "parameters") and self.parameters is not None:
            save_data["sample"] = self.parameters.to_dict()

        # Add history data (but ensure it's JSON serializable)
        if hasattr(self, "history") and self.history is not None:
            # Convert any non-JSON-serializable objects to strings/dicts
            serializable_history = {}
            for key, value in self.history.items():
                if key == "sample":
                    # Use our properly serialized parameters
                    continue  # Skip, we'll add it from parameters above
                try:
                    # Test if value is JSON serializable
                    json.dumps(value)
                    serializable_history[key] = value
                except (TypeError, ValueError):
                    # Convert to string if not serializable
                    serializable_history[key] = str(value)
            save_data.update(serializable_history)

        # Save as JSON
        params_json = json.dumps(save_data, indent=2)
        metadata_group.attrs["parameters"] = params_json

        # Store lib_df and id_df (identification DataFrames)
        if (
            hasattr(self, "lib_df")
            and self.lib_df is not None
            and not self.lib_df.is_empty()
        ):
            lib_group = f.create_group("lib")
            for col in self.lib_df.columns:
                data = self.lib_df[col].to_numpy()
                # Handle different data types safely
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        lib_group.create_dataset(
                            col,
                            data=str_data,
                            compression="gzip",
                        )
                        lib_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        json_data = np.array(
                            [json.dumps(x, default=str) for x in data],
                            dtype="S",
                        )
                        lib_group.create_dataset(
                            col,
                            data=json_data,
                            compression="gzip",
                        )
                        lib_group[col].attrs["dtype"] = "json"
                else:
                    lib_group.create_dataset(
                        col,
                        data=data,
                        compression="gzip",
                    )
            lib_group.attrs["columns"] = list(self.lib_df.columns)

        if (
            hasattr(self, "id_df")
            and self.id_df is not None
            and not self.id_df.is_empty()
        ):
            id_group = f.create_group("id")
            for col in self.id_df.columns:
                data = self.id_df[col].to_numpy()
                # Handle different data types safely
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        id_group.create_dataset(
                            col,
                            data=str_data,
                            compression="gzip",
                        )
                        id_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        json_data = np.array(
                            [json.dumps(x, default=str) for x in data],
                            dtype="S",
                        )
                        id_group.create_dataset(
                            col,
                            data=json_data,
                            compression="gzip",
                        )
                        id_group[col].attrs["dtype"] = "json"
                else:
                    id_group.create_dataset(
                        col,
                        data=data,
                        compression="gzip",
                    )
            id_group.attrs["columns"] = list(self.id_df.columns)

    self.logger.success(f"Sample saved to {filename}")
    if save_featurexml:
        # Get or recreate the feature map if needed
        feature_map = self._get_feature_map()
        if feature_map is not None:
            # Temporarily set features for save operation
            old_features = getattr(self, "_oms_features_map", None)
            self._oms_features_map = feature_map
            try:
                self._save_featureXML(
                    filename=filename.replace(".sample5", ".featureXML"),
                )
            finally:
                # Restore original features value
                if old_features is not None:
                    self._oms_features_map = old_features
                else:
                    delattr(self, "_oms_features_map")
        else:
            self.logger.warning("Cannot save featureXML: no feature data available")


def _load_sample5(self, filename: str, map: bool = False):
    """
    Load instance data from a sample5 HDF5 file.

    Automatically detects file format version and routes to appropriate loader:
    - Version 1: Original format with JSON-serialized chromatograms
    - Version 2: Optimized format with columnized chromatogram storage

    Restores all attributes that were saved with save_sample5() method using the
    schema defined in sample5_schema.json for proper Polars DataFrame reconstruction.

    Args:
        filename (str): Path to the sample5 HDF5 file to load.
        map (bool, optional): Whether to map featureXML file if available. Defaults to True.

    Returns:
        None (modifies self in place)

    Notes:
        - Restores DataFrames with proper schema typing from sample5_schema.json
        - Handles Chromatogram and Spectrum object reconstruction
        - Properly handles MS2 scan lists and spectrum lists
        - Backward compatible with version 1 files
    """
    # Detect file version
    with h5py.File(filename, "r") as f:
        if "metadata" in f:
            format_str = f["metadata"].attrs.get("format", "masster-sample-1")
            storage_version = f["metadata"].attrs.get("storage_version", 1)

            # Check if this is version 2 (columnized storage)
            if storage_version == 2 or "masster-sample-2" in str(format_str):
                self.logger.debug("Loading sample5 file version 2 (columnized storage)")
                return _load_sample5_v2(self, filename, map)
            self.logger.debug("Loading sample5 file version 1 (JSON storage)")
            # Continue with original version 1 loader below

    # Version 1 loader (original implementation)
    # Load schema for proper DataFrame reconstruction
    schema_path = os.path.join(os.path.dirname(__file__), "sample5_schema.json")
    try:
        with open(schema_path) as f:
            schema = json.load(f)
    except FileNotFoundError:
        self.logger.warning(
            f"Schema file {schema_path} not found. Using default types.",
        )
        schema = {}

    with h5py.File(filename, "r") as f:
        # Load metadata
        if "metadata" in f:
            metadata_group = f["metadata"]
            self.file_path = decode_metadata_attr(
                metadata_group.attrs.get("file_path", ""),
            )

            # Load file_source if it exists, otherwise set it equal to file_path
            if "file_source" in metadata_group.attrs:
                self.file_source = decode_metadata_attr(
                    metadata_group.attrs.get("file_source", ""),
                )
            else:
                self.file_source = self.file_path

            self.type = decode_metadata_attr(
                metadata_group.attrs.get("file_type", ""),
            )
            self.label = decode_metadata_attr(metadata_group.attrs.get("label", ""))

            # Load parameters from JSON in metadata
            loaded_data = load_parameters_from_metadata(metadata_group)

            # Always create a fresh sample_defaults object
            from masster.sample.defaults.sample_def import sample_defaults

            self.parameters = sample_defaults()

            # Initialize history and populate from loaded data
            self.history = {}
            if loaded_data is not None and isinstance(loaded_data, dict):
                # Store the loaded data in history
                self.history = loaded_data
                # If there are sample parameters in the history, use them to update defaults
                if "sample" in loaded_data:
                    sample_params = loaded_data["sample"]
                    if isinstance(sample_params, dict):
                        self.parameters.set_from_dict(sample_params, validate=False)

        # Load scans_df
        if "scans" in f:
            scans_group = f["scans"]
            data: dict[str, Any] = {}
            missing_columns = []

            # Get columns in order specified by schema
            schema_cols = schema.get("scans_df", {}).get("columns", {})
            if schema_cols:
                # Sort columns by their order value if available
                has_order = any(
                    "order" in col_info for col_info in schema_cols.values()
                )
                if has_order:
                    cols_with_order = [
                        (col_name, col_info.get("order", 9999))
                        for col_name, col_info in schema_cols.items()
                    ]
                    cols_with_order.sort(key=lambda x: x[1])
                    columns_to_load = [col_name for col_name, _ in cols_with_order]
                else:
                    columns_to_load = list(schema_cols.keys())
            else:
                columns_to_load = []

            for col in columns_to_load:
                if col not in scans_group:
                    self.logger.debug(f"Column '{col}' not found in sample5/scans.")
                    data[col] = None
                    missing_columns.append(col)
                    continue

                dtype = schema["scans_df"]["columns"][col].get("dtype", "native")
                match dtype:
                    case "pl.Object":
                        self.logger.debug(f"Unexpected Object column '{col}'")
                        data[col] = None
                        missing_columns.append(col)

                    case _:
                        data[col] = scans_group[col][:]

            # create polars DataFrame from data
            if data:
                self.scans_df = pl.DataFrame(data)

                # Convert "None" strings and NaN values to proper null values
                for col in self.scans_df.columns:
                    if self.scans_df[col].dtype == pl.Utf8:  # String columns
                        self.scans_df = self.scans_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    elif self.scans_df[col].dtype in [
                        pl.Float64,
                        pl.Float32,
                    ]:  # Float columns
                        self.scans_df = self.scans_df.with_columns(
                            [
                                pl.col(col).fill_nan(None).alias(col),
                            ],
                        )

                # update all columns with schema types
                for col in self.scans_df.columns:
                    if col in schema.get("scans_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["scans_df"]["columns"][col]["dtype"]
                            # Convert dtype string to actual polars dtype
                            if dtype_str.startswith("pl."):
                                # Skip Object columns - they're already properly reconstructed
                                if "Object" in dtype_str:
                                    continue
                                # Handle different polars data types
                                if "Int" in dtype_str:
                                    # Convert to numeric first, handling different input types
                                    if self.scans_df[col].dtype == pl.Utf8:
                                        # String data - convert to integer
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col)
                                            .str.to_integer()
                                            .cast(eval(dtype_str)),
                                        )
                                    elif self.scans_df[col].dtype in [
                                        pl.Float64,
                                        pl.Float32,
                                    ]:
                                        # Float data - cast to integer
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                    else:
                                        # Try direct casting
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                elif "Float" in dtype_str:
                                    # Convert to float, handling different input types
                                    if self.scans_df[col].dtype == pl.Utf8:
                                        # String data - convert to float
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col)
                                            .str.to_decimal()
                                            .cast(eval(dtype_str)),
                                        )
                                    else:
                                        # Try direct casting
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                elif "Utf8" in dtype_str:
                                    # Ensure it's string type
                                    self.scans_df = self.scans_df.with_columns(
                                        pl.col(col).cast(pl.Utf8),
                                    )
                                else:
                                    # Handle special cases and try direct casting for other types
                                    current_dtype = self.scans_df[col].dtype
                                    target_dtype = eval(dtype_str)

                                    # Handle binary data that might need string conversion first
                                    if "Binary" in str(current_dtype):
                                        # Convert binary to string first, then to target type
                                        if target_dtype == pl.Utf8:
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .cast(target_dtype),
                                            )
                                        elif "Int" in str(target_dtype):
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .str.to_integer()
                                                .cast(target_dtype),
                                            )
                                        elif "Float" in str(target_dtype):
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .str.to_decimal()
                                                .cast(target_dtype),
                                            )
                                        else:
                                            # Try direct casting
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col).cast(target_dtype),
                                            )
                                    else:
                                        # Try direct casting for non-binary types
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(target_dtype),
                                        )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in scans_df: {e}",
                            )
                    else:
                        self.logger.warning(
                            f"Column '{col}' in scans_df not found in schema, keeping original type.",
                        )

            # Ensure column order matches schema order
            self.scans_df = reorder_dataframe_by_schema(
                self.scans_df,
                schema,
                "scans_df",
            )

            # Fix for old files: rename feature_uid to feature_id if it exists
            if (
                "feature_uid" in self.scans_df.columns
                and "feature_id" not in self.scans_df.columns
            ):
                self.scans_df = self.scans_df.rename({"feature_uid": "feature_id"})
                self.logger.debug(
                    "Renamed scans_df.feature_uid to feature_id (old file format)",
                )

            else:
                self.scans_df = None
        else:
            self.scans_df = None

        # Load features_df
        if "features" in f:
            features_group = f["features"]
            # columns = list(features_group.attrs.get('columns', []))
            data = {}
            missing_columns = []

            # BACKWARD COMPATIBILITY: Check if we need to swap feature_id and feature_uid
            swap_features = False
            if "feature_uid" in features_group and "feature_id" in features_group:
                # Check type of feature_uid in HDF5
                # If it's integer-like, it's the old schema
                if np.issubdtype(features_group["feature_uid"].dtype, np.integer):
                    swap_features = True
                    self.logger.debug(
                        "Detected old feature_id/feature_uid naming convention. Swapping for backward compatibility (v1).",
                    )

            # Check if we need to migrate sample_uid to sample_id
            swap_samples = False
            if "sample_uid" in features_group:
                if np.issubdtype(features_group["sample_uid"].dtype, np.integer):
                    swap_samples = True

            for col in schema.get("features_df", {}).get("columns", []):
                source_col = col
                if swap_features:
                    if col == "feature_uid":
                        source_col = "feature_id"
                    elif col == "feature_id":
                        source_col = "feature_uid"

                if swap_samples:
                    if col == "sample_id":
                        source_col = "sample_uid"
                    elif col == "sample_uid":
                        source_col = "sample_id"  # This might be tricky if sample_id doesn't exist in file

                if source_col not in features_group:
                    self.logger.debug(
                        f"Column '{source_col}' not found in sample5/features.",
                    )
                    data[col] = None
                    missing_columns.append(col)
                    continue

                dtype = schema["features_df"]["columns"][col].get("dtype", "native")
                match dtype:
                    case "pl.Object":
                        match col:
                            case "chrom":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to Chromatogram objects
                                reconstructed_data: list[Any] = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            reconstructed_data.append(
                                                Chromatogram.from_json(item),
                                            )
                                        except (json.JSONDecodeError, ValueError):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case "ms2_scans":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to lists of integers
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            # Parse JSON string to get list of integers
                                            scan_list = json.loads(item)
                                            reconstructed_data.append(scan_list)
                                        except (json.JSONDecodeError, ValueError):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case "ms2_specs":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to lists of Spectrum objects
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    # Parse the outer JSON (list of JSON strings)
                                    json_list = json.loads(item)

                                    if json_list == ["None"]:
                                        # This was originally None
                                        reconstructed_data.append(None)
                                    else:
                                        # This was originally a list of Spectrum objects
                                        spectrum_list: list[Any] = []
                                        for json_str in json_list:
                                            if json_str == "None":
                                                spectrum_list.append(None)
                                            else:
                                                spectrum_list.append(
                                                    Spectrum.from_json(json_str),
                                                )
                                        reconstructed_data.append(spectrum_list)

                                data[col] = reconstructed_data
                            case "ms1_spec":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to numpy arrays
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            # Parse JSON string to get list and convert to numpy array
                                            array_data = json.loads(item)
                                            reconstructed_data.append(
                                                np.array(array_data, dtype=np.float64),
                                            )
                                        except (
                                            json.JSONDecodeError,
                                            ValueError,
                                            TypeError,
                                        ):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case _:
                                self.logger.debug(f"Unexpected Object column '{col}'")
                                data[col] = None
                                missing_columns.append(col)

                    case _:
                        data[col] = features_group[source_col][:]

            # create polars DataFrame from data
            if data:
                # Build schema for DataFrame creation to handle Object columns properly
                df_schema = {}
                for col, values in data.items():
                    if col in schema.get("features_df", {}).get("columns", {}):
                        dtype_str = schema["features_df"]["columns"][col]["dtype"]
                        if dtype_str == "pl.Object":
                            df_schema[col] = pl.Object
                        else:
                            # Let Polars infer the type initially, we'll cast later
                            df_schema[col] = None
                    else:
                        df_schema[col] = None

                # Create DataFrame with explicit Object types where needed
                try:
                    self.features_df = pl.DataFrame(data, schema=df_schema)
                except Exception:
                    # Fallback: create without schema and handle Object columns manually
                    object_columns = {
                        k: v
                        for k, v in data.items()
                        if k in schema.get("features_df", {}).get("columns", {})
                        and schema["features_df"]["columns"][k]["dtype"] == "pl.Object"
                    }
                    regular_columns = {
                        k: v for k, v in data.items() if k not in object_columns
                    }

                    # Create DataFrame with regular columns first
                    if regular_columns:
                        self.features_df = pl.DataFrame(regular_columns)
                        # Add Object columns one by one
                        for col, values in object_columns.items():
                            self.features_df = self.features_df.with_columns(
                                [
                                    pl.Series(col, values, dtype=pl.Object),
                                ],
                            )
                    else:
                        # Only Object columns
                        self.features_df = pl.DataFrame()
                        for col, values in object_columns.items():
                            self.features_df = self.features_df.with_columns(
                                [
                                    pl.Series(col, values, dtype=pl.Object),
                                ],
                            )

                # update all columns with schema types (skip Object columns)
                for col in self.features_df.columns:
                    if col in schema.get("features_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["features_df"]["columns"][col]["dtype"]
                            # Convert dtype string to actual polars dtype
                            if dtype_str.startswith("pl."):
                                # Skip Object columns - they're already properly reconstructed
                                if "Object" in dtype_str:
                                    continue

                                # SPECIAL HANDLING for swapped feature_id/feature_uid columns
                                # When swap_features is True:
                                # - feature_id now contains data from old feature_uid (integer) -> convert Int->Utf8 temporarily
                                # - feature_uid now contains data from old feature_id (Binary UUID) -> convert Binary->Utf8
                                # Both should end up as Utf8 after swap
                                if swap_features and col in [
                                    "feature_id",
                                    "feature_uid",
                                ]:
                                    current_dtype = self.features_df[col].dtype

                                    if col == "feature_id":
                                        # Contains old feature_uid (integer) -> convert to Utf8 (will be proper integer later if needed)
                                        if "Int" in str(current_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                            self.logger.debug(
                                                "Converted swapped feature_id (old feature_uid integer) to Utf8",
                                            )
                                        elif "Binary" in str(current_dtype):
                                            # Binary to string
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    ),
                                                )
                                            )
                                            self.logger.debug(
                                                "Converted swapped feature_id from Binary to Utf8",
                                            )
                                        elif "Utf8" not in str(current_dtype):
                                            # Any other type
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                        continue  # Skip normal casting logic

                                    if col == "feature_uid":
                                        # Contains old feature_id (UUID Binary/String) -> convert to Utf8
                                        if "Binary" in str(current_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    ),
                                                )
                                            )
                                            self.logger.debug(
                                                "Converted swapped feature_uid (old feature_id UUID) from Binary to Utf8",
                                            )
                                        elif "Int" in str(current_dtype):
                                            # Shouldn't happen, but convert to string
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                        elif "Utf8" not in str(current_dtype):
                                            # Any other type
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                        continue  # Skip normal casting logic

                                # Handle different polars data types
                                if "Int" in dtype_str:
                                    # Convert to numeric first, handling different input types
                                    if self.features_df[col].dtype == pl.Utf8:
                                        # String data - convert to integer
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col)
                                                .str.to_integer()
                                                .cast(eval(dtype_str)),
                                            )
                                        )
                                    elif self.features_df[col].dtype in [
                                        pl.Float64,
                                        pl.Float32,
                                    ]:
                                        # Float data - cast to integer with null handling for NaN values
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col).cast(
                                                    eval(dtype_str),
                                                    strict=False,
                                                ),
                                            )
                                        )
                                    else:
                                        # Handle special cases and try direct casting for other types
                                        current_dtype = self.features_df[col].dtype
                                        target_dtype = eval(dtype_str)

                                        # Handle binary data that might need string conversion first
                                        if "Binary" in str(current_dtype):
                                            # Convert binary to string first, then to target type
                                            if target_dtype == pl.Utf8:
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Int" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_integer()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Float" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_decimal()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            else:
                                                # Try direct casting
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col).cast(target_dtype),
                                                    )
                                                )
                                        else:
                                            # Try direct casting for non-binary types
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                elif "Float" in dtype_str:
                                    # Convert to float, handling different input types
                                    if self.features_df[col].dtype == pl.Utf8:
                                        # String data - convert to float
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col)
                                                .str.to_decimal()
                                                .cast(eval(dtype_str)),
                                            )
                                        )
                                    else:
                                        # Handle special cases and try direct casting for other types
                                        current_dtype = self.features_df[col].dtype
                                        target_dtype = eval(dtype_str)

                                        # Handle binary data that might need string conversion first
                                        if "Binary" in str(current_dtype):
                                            # Convert binary to string first, then to target type
                                            if target_dtype == pl.Utf8:
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Int" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_integer()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Float" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_decimal()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            else:
                                                # Try direct casting
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col).cast(target_dtype),
                                                    )
                                                )
                                        else:
                                            # Try direct casting for non-binary types
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                elif "Utf8" in dtype_str:
                                    # Ensure it's string type
                                    self.features_df = self.features_df.with_columns(
                                        pl.col(col).cast(pl.Utf8),
                                    )
                                else:
                                    # Handle special cases and try direct casting for other types
                                    current_dtype = self.features_df[col].dtype
                                    target_dtype = eval(dtype_str)

                                    # Handle binary data that might need string conversion first
                                    if "Binary" in str(current_dtype):
                                        # Convert binary to string first, then to target type
                                        if target_dtype == pl.Utf8:
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .cast(target_dtype),
                                                )
                                            )
                                        elif "Int" in str(target_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .str.to_integer()
                                                    .cast(target_dtype),
                                                )
                                            )
                                        elif "Float" in str(target_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .str.to_decimal()
                                                    .cast(target_dtype),
                                                )
                                            )
                                        else:
                                            # Try direct casting
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                    else:
                                        # Try direct casting for non-binary types
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col).cast(target_dtype),
                                            )
                                        )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in features_df: {e}",
                            )
                    else:
                        self.logger.warning(
                            f"Column '{col}' in features_df not found in schema, keeping original type.",
                        )

                # FINAL null conversion pass - after all type casting is done
                # This ensures "None" strings introduced by failed conversions are properly handled
                for col in self.features_df.columns:
                    if self.features_df[col].dtype == pl.Utf8:  # String columns
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    # Float columns
                    elif self.features_df[col].dtype in [pl.Float64, pl.Float32]:
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.col(col).fill_nan(None).alias(col),
                            ],
                        )

                # Ensure column order matches schema order
                self.features_df = reorder_dataframe_by_schema(
                    self.features_df,
                    schema,
                    "features_df",
                )

                # POST-SWAP MIGRATION: If we swapped, regenerate proper integer feature_id
                if swap_features:
                    self.logger.debug(
                        "Migrating swapped feature_id/feature_uid to new convention",
                    )
                    # At this point:
                    # - feature_id contains old feature_uid data (UUID strings)
                    # - feature_uid contains old feature_id data (integer strings)
                    # We need to:
                    # 1. Keep feature_uid as-is (UUID strings) [OK]
                    # 2. Generate new sequential integer feature_id (0, 1, 2, ...)
                    self.features_df = self.features_df.with_columns(
                        [pl.int_range(pl.len()).alias("feature_id")],
                    )
                    self.logger.debug(
                        f"Generated new integer feature_id for {len(self.features_df)} features",
                    )

            else:
                self.features_df = None
        else:
            self.features_df = None

        # Load ms1_df
        if "ms1" in f:
            ms1_group = f["ms1"]
            data = {}

            # Get all datasets in the ms1 group
            for col in ms1_group.keys():
                data[col] = ms1_group[col][:]

            if data:
                # Create DataFrame directly with Polars
                self.ms1_df = pl.DataFrame(data)

                # Apply schema if available
                if "ms1_df" in schema and "columns" in schema["ms1_df"]:
                    schema_columns = schema["ms1_df"]["columns"]
                    for col in self.ms1_df.columns:
                        if col in schema_columns:
                            dtype_str = schema_columns[col]["dtype"]
                            try:
                                if "Int" in dtype_str:
                                    self.ms1_df = self.ms1_df.with_columns(
                                        [
                                            pl.col(col).cast(pl.Int64, strict=False),
                                        ],
                                    )
                                elif "Float" in dtype_str:
                                    self.ms1_df = self.ms1_df.with_columns(
                                        [
                                            pl.col(col).cast(pl.Float64, strict=False),
                                        ],
                                    )
                            except Exception as e:
                                self.logger.warning(
                                    f"Failed to apply schema type {dtype_str} to column {col}: {e}",
                                )

                # Convert "None" strings and NaN values to proper null values
                self.ms1_df = clean_null_values_polars(self.ms1_df)

                # Fix for old files: rename scan_uid to scan_id if it exists
                if (
                    "scan_uid" in self.ms1_df.columns
                    and "scan_id" not in self.ms1_df.columns
                ):
                    self.ms1_df = self.ms1_df.rename({"scan_uid": "scan_id"})
                    self.logger.debug(
                        "Renamed ms1_df.scan_uid to scan_id (old file format)",
                    )
            else:
                self.ms1_df = None
        else:
            self.ms1_df = None

        # Load lib_df (library DataFrame)
        if "lib" in f:
            lib_group = f["lib"]
            data = {}

            # Get all datasets in the lib group
            for col in lib_group.keys():
                data_col = lib_group[col][:]
                # Handle string data
                if hasattr(lib_group[col], "attrs") and lib_group[col].attrs.get(
                    "dtype",
                ) in ["string_converted", "json"]:
                    data[col] = [
                        x.decode("utf-8") if isinstance(x, bytes) else x
                        for x in data_col
                    ]
                else:
                    data[col] = data_col

            if data:
                # Create DataFrame directly with Polars
                self.lib_df = pl.DataFrame(data)

                # Apply schema if available
                if "lib_df" in schema and "columns" in schema["lib_df"]:
                    schema_columns = schema["lib_df"]["columns"]
                    for col in self.lib_df.columns:
                        if col in schema_columns:
                            dtype_str = schema_columns[col]["dtype"]
                            try:
                                self.lib_df = self.lib_df.with_columns(
                                    [pl.col(col).cast(eval(dtype_str), strict=False)],
                                )
                            except Exception as e:
                                self.logger.warning(
                                    f"Failed to apply schema type {dtype_str} to column {col}: {e}",
                                )

                # Convert "None" strings and NaN values to proper null values
                self.lib_df = clean_null_values_polars(self.lib_df)

                # Migration: Rename source_id to lib_source for backward compatibility (schema v1.0 -> v1.1)
                file_schema_version = schema.get("schema_version", "1.0")
                if (
                    "source_id" in self.lib_df.columns
                    and "lib_source" not in self.lib_df.columns
                ):
                    self.logger.info(
                        f"Migrating lib_df from schema v{file_schema_version}: renaming 'source_id' to 'lib_source'",
                    )
                    self.lib_df = self.lib_df.rename({"source_id": "lib_source"})
            else:
                self.lib_df = None
        else:
            self.lib_df = None

        # Load id_df (identification results DataFrame)
        if "id" in f:
            id_group = f["id"]
            data = {}

            # Get all datasets in the id group
            for col in id_group.keys():
                data_col = id_group[col][:]
                # Handle string data
                if hasattr(id_group[col], "attrs") and id_group[col].attrs.get(
                    "dtype",
                ) in ["string_converted", "json"]:
                    data[col] = [
                        x.decode("utf-8") if isinstance(x, bytes) else x
                        for x in data_col
                    ]
                else:
                    data[col] = data_col

            if data:
                # Create DataFrame directly with Polars
                self.id_df = pl.DataFrame(data)

                # Backward compatibility: rename 'matcher' to 'id_source' if present
                if (
                    "matcher" in self.id_df.columns
                    and "id_source" not in self.id_df.columns
                ):
                    self.id_df = self.id_df.rename({"matcher": "id_source"})
                    self.logger.debug(
                        "Renamed 'matcher' column to 'id_source' for backward compatibility",
                    )

                # Apply schema if available
                if "id_df" in schema and "columns" in schema["id_df"]:
                    schema_columns = schema["id_df"]["columns"]
                    for col in self.id_df.columns:
                        if col in schema_columns:
                            dtype_str = schema_columns[col]["dtype"]
                            try:
                                self.id_df = self.id_df.with_columns(
                                    [pl.col(col).cast(eval(dtype_str), strict=False)],
                                )
                            except Exception as e:
                                self.logger.warning(
                                    f"Failed to apply schema type {dtype_str} to column {col}: {e}",
                                )

                # Convert "None" strings and NaN values to proper null values
                self.id_df = clean_null_values_polars(self.id_df)
            else:
                self.id_df = None
        else:
            self.id_df = None

        # Parameters are now loaded from metadata JSON (see above)

    # if map:
    #    featureXML = filename.replace(".sample5", ".featureXML")
    #    if os.path.exists(featureXML):
    #        self._load_featureXML(featureXML)
    #        #self._features_sync()
    #    else:
    #        self.logger.warning(
    #            f"Feature XML file {featureXML} not found, skipping loading.",
    #        )

    # set self.file_path to *.sample5
    self.file_path = filename
    # set self.label to basename without extension
    if self.label is None or self.label == "":
        self.label = os.path.splitext(os.path.basename(filename))[0]

    # Sync instance attributes from loaded parameters
    if hasattr(self, "parameters") and self.parameters is not None:
        if (
            hasattr(self.parameters, "polarity")
            and self.parameters.polarity is not None
        ):
            self.polarity = self.parameters.polarity
        if hasattr(self.parameters, "type") and self.parameters.type is not None:
            self.type = self.parameters.type

    self.logger.success(f"Sample loaded from {filename}")


def _load_sample5_study(self, filename: str, map: bool = False):
    """
    Optimized variant of _load_sample5 for study loading that skips reading ms1_df.

    This is used when adding samples to studies where ms1_df data is not needed,
    improving loading throughput by skipping the potentially large ms1_df dataset.

    Args:
        filename (str): Path to the sample5 HDF5 file to load.
        map (bool, optional): Whether to load featureXML file if available. Defaults to False.
            Set to True if you need the OpenMS FeatureMap for operations like find_features().

    Returns:
        None (modifies self in place)

    Notes:
        - Same as _load_sample5 but skips ms1_df loading for better performance
        - Sets ms1_df = None explicitly
        - Suitable for study workflows where MS1 spectral data is not required
        - Automatically detects and handles both v1 and v2 file formats
    """
    # Detect file version and route to appropriate loader
    with h5py.File(filename, "r") as f:
        if "metadata" in f:
            format_str = f["metadata"].attrs.get("format", "masster-sample-1")
            storage_version = f["metadata"].attrs.get("storage_version", 1)

            # Check if this is version 2 (columnized storage)
            if storage_version == 2 or "masster-sample-2" in str(format_str):
                self.logger.debug(
                    "Loading sample5 file version 2 (columnized storage, optimized for study)",
                )
                # Use v2 loader and then clear ms1_df to save memory
                _load_sample5_v2(self, filename, map)
                # Debug: Check if chrom column was loaded correctly
                if (
                    hasattr(self, "features_df")
                    and self.features_df is not None
                    and "chrom" in self.features_df.columns
                ):
                    non_null_count = self.features_df["chrom"].drop_nulls().len()
                    self.logger.debug(
                        f"After _load_sample5_v2: {non_null_count} non-null chrom values",
                    )
                self.ms1_df = None
                self.logger.info(
                    f"Sample loaded successfully from {filename} (optimized for study)",
                )
                return
            self.logger.debug(
                "Loading sample5 file version 1 (JSON storage, optimized for study)",
            )
            # Continue with original version 1 loader below

    # Version 1 loader (original implementation)
    # Load schema for proper DataFrame reconstruction
    schema_path = os.path.join(os.path.dirname(__file__), "sample5_schema.json")
    try:
        with open(schema_path) as f:
            schema = json.load(f)
    except FileNotFoundError:
        self.logger.warning(
            f"Schema file {schema_path} not found. Using default types.",
        )
        schema = {}

    with h5py.File(filename, "r") as f:
        # Load metadata
        if "metadata" in f:
            metadata_group = f["metadata"]
            self.file_path = decode_metadata_attr(
                metadata_group.attrs.get("file_path", ""),
            )

            # Load file_source if it exists, otherwise set it equal to file_path
            if "file_source" in metadata_group.attrs:
                self.file_source = decode_metadata_attr(
                    metadata_group.attrs.get("file_source", ""),
                )
            else:
                self.file_source = self.file_path

            self.type = decode_metadata_attr(
                metadata_group.attrs.get("file_type", ""),
            )
            self.label = decode_metadata_attr(metadata_group.attrs.get("label", ""))

            # Load parameters from JSON in metadata
            loaded_data = load_parameters_from_metadata(metadata_group)

            # Always create a fresh sample_defaults object
            from masster.sample.defaults.sample_def import sample_defaults

            self.parameters = sample_defaults()

            # Initialize history and populate from loaded data
            self.history = {}
            if loaded_data is not None and isinstance(loaded_data, dict):
                # Store the loaded data in history
                self.history = loaded_data
                # If there are sample parameters in the history, use them to update defaults
                if "sample" in loaded_data:
                    sample_params = loaded_data["sample"]
                    if isinstance(sample_params, dict):
                        self.parameters.set_from_dict(sample_params, validate=False)

        # Load scans_df
        if "scans" in f:
            scans_group = f["scans"]
            data: dict[str, Any] = {}
            missing_columns = []

            # Get columns in order specified by schema
            schema_cols = schema.get("scans_df", {}).get("columns", {})
            if schema_cols:
                # Sort columns by their order value if available
                has_order = any(
                    "order" in col_info for col_info in schema_cols.values()
                )
                if has_order:
                    cols_with_order = [
                        (col_name, col_info.get("order", 9999))
                        for col_name, col_info in schema_cols.items()
                    ]
                    cols_with_order.sort(key=lambda x: x[1])
                    columns_to_load = [col_name for col_name, _ in cols_with_order]
                else:
                    columns_to_load = list(schema_cols.keys())
            else:
                columns_to_load = []

            for col in columns_to_load:
                if col not in scans_group:
                    self.logger.debug(f"Column '{col}' not found in sample5/scans.")
                    data[col] = None
                    missing_columns.append(col)
                    continue

                dtype = schema["scans_df"]["columns"][col].get("dtype", "native")
                match dtype:
                    case "pl.Object":
                        self.logger.debug(f"Unexpected Object column '{col}'")
                        data[col] = None
                        missing_columns.append(col)

                    case _:
                        data[col] = scans_group[col][:]

            # create polars DataFrame from data
            if data:
                self.scans_df = pl.DataFrame(data)

                # Convert "None" strings and NaN values to proper null values
                for col in self.scans_df.columns:
                    if self.scans_df[col].dtype == pl.Utf8:  # String columns
                        self.scans_df = self.scans_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    elif self.scans_df[col].dtype in [
                        pl.Float64,
                        pl.Float32,
                    ]:  # Float columns
                        self.scans_df = self.scans_df.with_columns(
                            [
                                pl.col(col).fill_nan(None).alias(col),
                            ],
                        )

                # update all columns with schema types
                for col in self.scans_df.columns:
                    if col in schema.get("scans_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["scans_df"]["columns"][col]["dtype"]
                            # Convert dtype string to actual polars dtype
                            if dtype_str.startswith("pl."):
                                # Skip Object columns - they're already properly reconstructed
                                if "Object" in dtype_str:
                                    continue
                                # Handle different polars data types
                                if "Int" in dtype_str:
                                    # Convert to numeric first, handling different input types
                                    if self.scans_df[col].dtype == pl.Utf8:
                                        # String data - convert to integer
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col)
                                            .str.to_integer()
                                            .cast(eval(dtype_str)),
                                        )
                                    elif self.scans_df[col].dtype in [
                                        pl.Float64,
                                        pl.Float32,
                                    ]:
                                        # Float data - cast to integer
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                    else:
                                        # Try direct casting
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                elif "Float" in dtype_str:
                                    # Convert to float, handling different input types
                                    if self.scans_df[col].dtype == pl.Utf8:
                                        # String data - convert to float
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col)
                                            .str.to_decimal()
                                            .cast(eval(dtype_str)),
                                        )
                                    else:
                                        # Try direct casting
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(eval(dtype_str)),
                                        )
                                elif "Utf8" in dtype_str:
                                    # Ensure it's string type
                                    self.scans_df = self.scans_df.with_columns(
                                        pl.col(col).cast(pl.Utf8),
                                    )
                                else:
                                    # Handle special cases and try direct casting for other types
                                    current_dtype = self.scans_df[col].dtype
                                    target_dtype = eval(dtype_str)

                                    # Handle binary data that might need string conversion first
                                    if "Binary" in str(current_dtype):
                                        # Convert binary to string first, then to target type
                                        if target_dtype == pl.Utf8:
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .cast(target_dtype),
                                            )
                                        elif "Int" in str(target_dtype):
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .str.to_integer()
                                                .cast(target_dtype),
                                            )
                                        elif "Float" in str(target_dtype):
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col)
                                                .map_elements(
                                                    lambda x: x.decode("utf-8")
                                                    if isinstance(x, bytes)
                                                    else str(x),
                                                    return_dtype=pl.Utf8,
                                                )
                                                .str.to_decimal()
                                                .cast(target_dtype),
                                            )
                                        else:
                                            # Try direct casting
                                            self.scans_df = self.scans_df.with_columns(
                                                pl.col(col).cast(target_dtype),
                                            )
                                    else:
                                        # Try direct casting for non-binary types
                                        self.scans_df = self.scans_df.with_columns(
                                            pl.col(col).cast(target_dtype),
                                        )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in scans_df: {e}",
                            )
                    else:
                        self.logger.warning(
                            f"Column '{col}' in scans_df not found in schema, keeping original type.",
                        )

            # Ensure column order matches schema order
            self.scans_df = reorder_dataframe_by_schema(
                self.scans_df,
                schema,
                "scans_df",
            )

            # Fix for old files: rename feature_uid to feature_id if it exists
            if (
                "feature_uid" in self.scans_df.columns
                and "feature_id" not in self.scans_df.columns
            ):
                self.scans_df = self.scans_df.rename({"feature_uid": "feature_id"})
                self.logger.debug(
                    "Renamed scans_df.feature_uid to feature_id (old file format)",
                )

            # Fix for old files: rename scan_uid to scan_id if it exists
            if (
                "scan_uid" in self.scans_df.columns
                and "scan_id" not in self.scans_df.columns
            ):
                self.scans_df = self.scans_df.rename({"scan_uid": "scan_id"})
                self.logger.debug(
                    "Renamed scans_df.scan_uid to scan_id (old file format)",
                )

            else:
                self.scans_df = None
        else:
            self.scans_df = None

        # Load features_df
        if "features" in f:
            features_group = f["features"]
            # columns = list(features_group.attrs.get('columns', []))
            data = {}
            missing_columns = []

            # BACKWARD COMPATIBILITY: Check if we need to swap feature_id and feature_id
            swap_features = False
            if "feature_id" in features_group and "feature_id" in features_group:
                # Check type of feature_id in HDF5
                # If it's integer-like, it's the old schema
                if np.issubdtype(features_group["feature_id"].dtype, np.integer):
                    swap_features = True
                    self.logger.debug(
                        "Detected old feature_id/feature_id naming convention. Swapping for backward compatibility (study).",
                    )

            for col in schema.get("features_df", {}).get("columns", []):
                source_col = col
                if swap_features:
                    if col == "feature_id" or col == "feature_id":
                        source_col = "feature_id"

                if source_col not in features_group:
                    self.logger.debug(
                        f"Column '{source_col}' not found in sample5/features.",
                    )
                    data[col] = None
                    missing_columns.append(col)
                    continue

                dtype = schema["features_df"]["columns"][col].get("dtype", "native")
                match dtype:
                    case "pl.Object":
                        match col:
                            case "chrom":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to Chromatogram objects
                                reconstructed_data: list[Any] = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            reconstructed_data.append(
                                                Chromatogram.from_json(item),
                                            )
                                        except (json.JSONDecodeError, ValueError):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case "ms2_scans":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to list objects
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            reconstructed_data.append(json.loads(item))
                                        except json.JSONDecodeError:
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case "ms2_specs":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to list of Spectrum objects
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            spectrum_list = []
                                            for spec_data in json.loads(item):
                                                if spec_data is not None:
                                                    spectrum = Spectrum.from_json(
                                                        spec_data,
                                                    )
                                                    spectrum_list.append(spectrum)
                                                else:
                                                    spectrum_list.append(None)  # type: ignore[arg-type]
                                            reconstructed_data.append(spectrum_list)
                                        except (
                                            json.JSONDecodeError,
                                            ValueError,
                                            TypeError,
                                        ):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case "ms1_spec":
                                data_col = features_group[source_col][:]
                                # Convert JSON strings back to numpy arrays
                                reconstructed_data = []
                                for item in data_col:
                                    if isinstance(item, bytes):
                                        item = item.decode("utf-8")

                                    if item == "None" or item == "":
                                        reconstructed_data.append(None)
                                    else:
                                        try:
                                            # Parse JSON string to get list and convert to numpy array
                                            array_data = json.loads(item)
                                            reconstructed_data.append(
                                                np.array(array_data, dtype=np.float64),
                                            )
                                        except (
                                            json.JSONDecodeError,
                                            ValueError,
                                            TypeError,
                                        ):
                                            reconstructed_data.append(None)

                                data[col] = reconstructed_data
                            case _:
                                # Handle other Object columns as raw data
                                data[col] = features_group[source_col][:]

                    case _:
                        data[col] = features_group[source_col][:]

            # create polars DataFrame from data
            if data:
                # Separate Object columns from regular columns to avoid astuple issues
                object_columns = {}
                regular_columns = {}

                for col, values in data.items():
                    if col in schema.get("features_df", {}).get("columns", {}):
                        if "Object" in schema["features_df"]["columns"][col].get(
                            "dtype",
                            "",
                        ):
                            object_columns[col] = values
                        else:
                            regular_columns[col] = values
                    else:
                        regular_columns[col] = values

                # Create DataFrame with regular columns first
                if regular_columns:
                    self.features_df = pl.DataFrame(regular_columns, strict=False)
                else:
                    # If no regular columns, create empty DataFrame
                    self.features_df = pl.DataFrame()

                # Add Object columns one by one
                for col, values in object_columns.items():
                    if not self.features_df.is_empty():
                        # Fix for missing columns: if values is None, create list of None with correct length
                        if values is None:
                            values = [None] * len(self.features_df)
                        self.features_df = self.features_df.with_columns(
                            pl.Series(col, values, dtype=pl.Object).alias(col),
                        )
                    else:
                        # Create DataFrame with just this Object column
                        self.features_df = pl.DataFrame(
                            {col: values},
                            schema={col: pl.Object},
                        )

                # Convert "None" strings and NaN values to proper null values for regular columns first
                for col in self.features_df.columns:
                    # Skip Object columns - they're already properly reconstructed
                    if col in schema.get("features_df", {}).get("columns", {}):
                        if "Object" in schema["features_df"]["columns"][col].get(
                            "dtype",
                            "",
                        ):
                            continue

                    if self.features_df[col].dtype == pl.Utf8:  # String columns
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    elif self.features_df[col].dtype in [
                        pl.Float64,
                        pl.Float32,
                    ]:  # Float columns
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.col(col).fill_nan(None).alias(col),
                            ],
                        )

                # update all columns with schema types
                for col in self.features_df.columns:
                    if col in schema.get("features_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["features_df"]["columns"][col]["dtype"]
                            # Convert dtype string to actual polars dtype
                            if dtype_str.startswith("pl."):
                                # Skip Object columns - they're already properly reconstructed
                                if "Object" in dtype_str:
                                    continue
                                # Handle different polars data types
                                if "Int" in dtype_str:
                                    # Convert to numeric first, handling different input types
                                    if self.features_df[col].dtype == pl.Utf8:
                                        # String data - convert to integer
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col)
                                                .str.to_integer()
                                                .cast(eval(dtype_str)),
                                            )
                                        )
                                    elif self.features_df[col].dtype in [
                                        pl.Float64,
                                        pl.Float32,
                                    ]:
                                        # Float data - cast to integer with null handling for NaN values
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col).cast(
                                                    eval(dtype_str),
                                                    strict=False,
                                                ),
                                            )
                                        )
                                    else:
                                        # Handle special cases and try direct casting for other types
                                        current_dtype = self.features_df[col].dtype
                                        target_dtype = eval(dtype_str)

                                        # Handle binary data that might need string conversion first
                                        if "Binary" in str(current_dtype):
                                            # Convert binary to string first, then to target type
                                            if target_dtype == pl.Utf8:
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Int" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_integer()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Float" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_decimal()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            else:
                                                # Try direct casting
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col).cast(target_dtype),
                                                    )
                                                )
                                        else:
                                            # Try direct casting for non-binary types
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                elif "Float" in dtype_str:
                                    # Convert to float, handling different input types
                                    if self.features_df[col].dtype == pl.Utf8:
                                        # String data - convert to float
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col)
                                                .str.to_decimal()
                                                .cast(eval(dtype_str)),
                                            )
                                        )
                                    else:
                                        # Handle special cases and try direct casting for other types
                                        current_dtype = self.features_df[col].dtype
                                        target_dtype = eval(dtype_str)

                                        # Handle binary data that might need string conversion first
                                        if "Binary" in str(current_dtype):
                                            # Convert binary to string first, then to target type
                                            if target_dtype == pl.Utf8:
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Int" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_integer()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            elif "Float" in str(target_dtype):
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col)
                                                        .map_elements(
                                                            lambda x: x.decode("utf-8")
                                                            if isinstance(x, bytes)
                                                            else str(x),
                                                            return_dtype=pl.Utf8,
                                                        )
                                                        .str.to_decimal()
                                                        .cast(target_dtype),
                                                    )
                                                )
                                            else:
                                                # Try direct casting
                                                self.features_df = (
                                                    self.features_df.with_columns(
                                                        pl.col(col).cast(target_dtype),
                                                    )
                                                )
                                        else:
                                            # Try direct casting for non-binary types
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                elif "Utf8" in dtype_str:
                                    # Ensure it's string type
                                    self.features_df = self.features_df.with_columns(
                                        pl.col(col).cast(pl.Utf8),
                                    )
                                else:
                                    # Handle special cases and try direct casting for other types
                                    current_dtype = self.features_df[col].dtype
                                    target_dtype = eval(dtype_str)

                                    # Handle binary data that might need string conversion first
                                    if "Binary" in str(current_dtype):
                                        # Convert binary to string first, then to target type
                                        if target_dtype == pl.Utf8:
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .cast(target_dtype),
                                                )
                                            )
                                        elif "Int" in str(target_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .str.to_integer()
                                                    .cast(target_dtype),
                                                )
                                            )
                                        elif "Float" in str(target_dtype):
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col)
                                                    .map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    )
                                                    .str.to_decimal()
                                                    .cast(target_dtype),
                                                )
                                            )
                                        else:
                                            # Try direct casting
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(target_dtype),
                                                )
                                            )
                                    else:
                                        # Try direct casting for non-binary types
                                        self.features_df = (
                                            self.features_df.with_columns(
                                                pl.col(col).cast(target_dtype),
                                            )
                                        )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in features_df: {e}",
                            )
                    else:
                        self.logger.warning(
                            f"Column '{col}' in features_df not found in schema, keeping original type.",
                        )

                # FINAL null conversion pass - after all type casting is done
                # This ensures "None" strings introduced by failed conversions are properly handled
                for col in self.features_df.columns:
                    if self.features_df[col].dtype == pl.Utf8:  # String columns
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    # Float columns
                    elif self.features_df[col].dtype in [pl.Float64, pl.Float32]:
                        self.features_df = self.features_df.with_columns(
                            [
                                pl.col(col).fill_nan(None).alias(col),
                            ],
                        )

                # Ensure column order matches schema order
                self.features_df = reorder_dataframe_by_schema(
                    self.features_df,
                    schema,
                    "features_df",
                )

            else:
                self.features_df = None
        else:
            self.features_df = None

        # OPTIMIZED: Skip loading ms1_df for study use - set to None for performance
        self.ms1_df = None

        # Parameters are now loaded from metadata JSON (see above)
        # Lib and lib_match are no longer saved/loaded

    if map:
        featureXML = filename.replace(".sample5", ".featureXML")
        if os.path.exists(featureXML):
            self._load_featureXML(featureXML)
            self._features_sync()
        else:
            self.logger.warning(
                f"Feature XML file {featureXML} not found, skipping loading.",
            )

    # set self.file_path to *.sample5
    self.file_path = filename
    # set self.label to basename without extension
    if self.label is None or self.label == "":
        self.label = os.path.splitext(os.path.basename(filename))[0]

    # Sync instance attributes from loaded parameters
    if hasattr(self, "parameters") and self.parameters is not None:
        if (
            hasattr(self.parameters, "polarity")
            and self.parameters.polarity is not None
        ):
            self.polarity = self.parameters.polarity
        if hasattr(self.parameters, "type") and self.parameters.type is not None:
            self.type = self.parameters.type

    self.logger.info(
        f"Sample loaded successfully from {filename} (optimized for study)",
    )


def load_schema(schema_path: str) -> dict[str, Any]:
    """
    Load schema from JSON file with error handling.

    Args:
        schema_path: Path to the schema JSON file

    Returns:
        Dictionary containing the schema, empty dict if not found
    """
    try:
        with open(schema_path) as f:
            return json.load(f)  # type: ignore
    except FileNotFoundError:
        return {}


def decode_metadata_attr(attr_value: Any) -> str:
    """
    Decode metadata attribute, handling both bytes and string types.

    Args:
        attr_value: The attribute value to decode

    Returns:
        String representation of the attribute
    """
    if isinstance(attr_value, bytes):
        return attr_value.decode()
    return str(attr_value) if attr_value is not None else ""


def clean_null_values_polars(df: pl.DataFrame) -> pl.DataFrame:
    """
    Clean null values in a Polars DataFrame by converting string nulls to proper nulls.

    Args:
        df: The Polars DataFrame to clean

    Returns:
        Cleaned DataFrame
    """
    cleaned_df = df
    for col in df.columns:
        if df[col].dtype == pl.Utf8:  # String columns
            cleaned_df = cleaned_df.with_columns(
                [
                    pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                    .then(None)
                    .otherwise(pl.col(col))
                    .alias(col),
                ],
            )
        elif df[col].dtype in [pl.Float64, pl.Float32]:  # Float columns
            cleaned_df = cleaned_df.with_columns(
                [
                    pl.col(col).fill_nan(None).alias(col),
                ],
            )
    return cleaned_df


def cast_column_by_dtype(df: pl.DataFrame, col: str, dtype_str: str) -> pl.DataFrame:
    """
    Cast a Polars DataFrame column to the specified dtype with appropriate handling.

    Args:
        df: The Polars DataFrame
        col: Column name to cast
        dtype_str: Target dtype as string (e.g., 'pl.Int64')

    Returns:
        DataFrame with the column cast to the new type
    """
    if not dtype_str.startswith("pl.") or "Object" in dtype_str:
        return df

    try:
        target_dtype = eval(dtype_str)
        current_dtype = df[col].dtype

        if "Int" in dtype_str:
            return _cast_to_int(df, col, current_dtype, target_dtype)
        if "Float" in dtype_str:
            return _cast_to_float(df, col, current_dtype, target_dtype)
        if "Utf8" in dtype_str:
            return df.with_columns(pl.col(col).cast(pl.Utf8))
        return _cast_with_binary_handling(df, col, current_dtype, target_dtype)

    except Exception:
        return df


def _cast_to_int(
    df: pl.DataFrame,
    col: str,
    current_dtype: pl.DataType,
    target_dtype: pl.DataType,
) -> pl.DataFrame:
    """Helper function to cast column to integer type."""
    if current_dtype == pl.Utf8:
        return df.with_columns(
            pl.col(col).str.to_integer().cast(target_dtype),
        )
    if current_dtype in [pl.Float64, pl.Float32]:
        return df.with_columns(pl.col(col).cast(target_dtype))
    return _cast_with_binary_handling(df, col, current_dtype, target_dtype)


def _cast_to_float(
    df: pl.DataFrame,
    col: str,
    current_dtype: pl.DataType,
    target_dtype: pl.DataType,
) -> pl.DataFrame:
    """Helper function to cast column to float type."""
    if current_dtype == pl.Utf8:
        return df.with_columns(
            pl.col(col).str.to_decimal().cast(target_dtype),
        )
    return _cast_with_binary_handling(df, col, current_dtype, target_dtype)


def _cast_with_binary_handling(
    df: pl.DataFrame,
    col: str,
    current_dtype: pl.DataType,
    target_dtype: pl.DataType,
) -> pl.DataFrame:
    """Helper function to handle binary data conversion."""
    if "Binary" in str(current_dtype):
        if target_dtype == pl.Utf8:
            return df.with_columns(
                pl.col(col)
                .map_elements(
                    lambda x: x.decode("utf-8") if isinstance(x, bytes) else str(x),
                    return_dtype=pl.Utf8,
                )
                .cast(target_dtype),
            )
        if "Int" in str(target_dtype):
            return df.with_columns(
                pl.col(col)
                .map_elements(
                    lambda x: x.decode("utf-8") if isinstance(x, bytes) else str(x),
                    return_dtype=pl.Utf8,
                )
                .str.to_integer()
                .cast(target_dtype),
            )
        if "Float" in str(target_dtype):
            return df.with_columns(
                pl.col(col)
                .map_elements(
                    lambda x: x.decode("utf-8") if isinstance(x, bytes) else str(x),
                    return_dtype=pl.Utf8,
                )
                .str.to_decimal()
                .cast(target_dtype),
            )

    # Fallback: try direct casting
    return df.with_columns(pl.col(col).cast(target_dtype))


def apply_schema_to_dataframe(
    df: pl.DataFrame,
    schema: dict[str, Any],
    df_name: str,
) -> pl.DataFrame:
    """
    Apply schema type casting to a Polars DataFrame.

    Args:
        df: The DataFrame to modify
        schema: The schema dictionary
        df_name: Name of the DataFrame in the schema (e.g., 'scans_df', 'features_df')

    Returns:
        DataFrame with schema types applied
    """
    df_schema = schema.get(df_name, {}).get("columns", {})

    for col in df.columns:
        if col in df_schema:
            dtype_str = df_schema[col]["dtype"]
            df = cast_column_by_dtype(df, col, dtype_str)

    return df


def reorder_dataframe_by_schema(
    df: pl.DataFrame,
    schema: dict[str, Any],
    df_name: str,
) -> pl.DataFrame:
    """
    Reorder DataFrame columns according to schema order.

    Args:
        df: The DataFrame to reorder
        schema: The schema dictionary
        df_name: Name of the DataFrame in the schema

    Returns:
        DataFrame with columns reordered
    """
    if df_name not in schema or "columns" not in schema[df_name]:
        return df

    schema_cols = schema[df_name]["columns"]

    # Check if schema has "order" field
    has_order = any("order" in col_info for col_info in schema_cols.values())

    if has_order:
        # Sort columns by their order value
        schema_columns_with_order = [
            (col_name, col_info.get("order", 9999))
            for col_name, col_info in schema_cols.items()
        ]
        schema_columns_with_order.sort(key=lambda x: x[1])
        schema_columns = [col_name for col_name, _ in schema_columns_with_order]
    else:
        # Fallback to original key order if no "order" field
        schema_columns = list(schema_cols.keys())

    # Only reorder columns that exist in both schema and DataFrame
    existing_columns = [col for col in schema_columns if col in df.columns]

    if existing_columns:
        return df.select(existing_columns)

    return df


def reconstruct_object_column(data_col: np.ndarray, col_name: str) -> list[Any]:
    """
    Reconstruct object columns from serialized data.

    Args:
        data_col: Array containing serialized data
        col_name: Name of the column for type-specific reconstruction

    Returns:
        List of reconstructed objects
    """
    reconstructed_data: list[Any] = []

    for item in data_col:
        if isinstance(item, bytes):
            item = item.decode("utf-8")

        if item == "None" or item == "":
            reconstructed_data.append(None)
            continue

        try:
            if col_name == "chrom":
                reconstructed_data.append(Chromatogram.from_json(item))
            elif col_name == "ms2_scans":
                scan_list = json.loads(item)
                reconstructed_data.append(scan_list)
            elif col_name == "ms2_specs":
                json_list = json.loads(item)
                if json_list == ["None"]:
                    reconstructed_data.append(None)
                else:
                    spectrum_list: list[Any] = []
                    for json_str in json_list:
                        if json_str == "None":
                            spectrum_list.append(None)
                        else:
                            spectrum_list.append(Spectrum.from_json(json_str))
                    reconstructed_data.append(spectrum_list)
            else:
                # Unknown object column
                reconstructed_data.append(None)
        except (json.JSONDecodeError, ValueError):
            reconstructed_data.append(None)

    return reconstructed_data


def load_dataframe_from_h5_group(
    group: h5py.Group,
    schema: dict[str, Any],
    df_name: str,
    logger: Any | None = None,
) -> tuple[pl.DataFrame | None, list[str]]:
    """
    Load a Polars DataFrame from an HDF5 group using schema.

    Args:
        group: The HDF5 group containing the DataFrame data
        schema: The schema dictionary
        df_name: Name of the DataFrame in the schema
        logger: Optional logger for warnings

    Returns:
        Tuple of (DataFrame or None, list of missing columns)
    """
    data: dict[str, Any] = {}
    missing_columns = []

    # Load columns according to schema
    schema_columns = schema.get(df_name, {}).get("columns", [])

    for col in schema_columns:
        if col not in group:
            if logger:
                logger.info(f"Column '{col}' not found in {df_name}.")
            data[col] = None
            missing_columns.append(col)
            continue

        dtype = schema[df_name]["columns"][col].get("dtype", "native")

        if dtype == "pl.Object":
            # Handle object columns specially
            data[col] = reconstruct_object_column(group[col][:], col)
        else:
            data[col] = group[col][:]

    if not data:
        return None, missing_columns

    # Create DataFrame with proper schema for Object columns
    df_schema = {}
    for col, values in data.items():
        if col in schema_columns:
            dtype_str = schema[df_name]["columns"][col]["dtype"]
            if dtype_str == "pl.Object":
                df_schema[col] = pl.Object

    try:
        if df_schema:
            df = pl.DataFrame(data, schema=df_schema)
        else:
            df = pl.DataFrame(data)
    except Exception:
        # Fallback: handle Object columns manually
        df = _create_dataframe_with_object_columns(data, schema, df_name)

    # Clean null values
    df = clean_null_values_polars(df)

    # Apply schema type casting
    df = apply_schema_to_dataframe(df, schema, df_name)

    return df, missing_columns


def _create_dataframe_with_object_columns(
    data: dict[str, Any],
    schema: dict[str, Any],
    df_name: str,
) -> pl.DataFrame:
    """
    Create DataFrame handling Object columns manually when schema creation fails.

    Args:
        data: Dictionary of column data
        schema: The schema dictionary
        df_name: Name of the DataFrame in the schema

    Returns:
        Polars DataFrame with Object columns properly handled
    """
    schema_columns = schema.get(df_name, {}).get("columns", {})

    object_columns = {
        k: v
        for k, v in data.items()
        if k in schema_columns and schema_columns[k]["dtype"] == "pl.Object"
    }
    regular_columns = {k: v for k, v in data.items() if k not in object_columns}

    # Create DataFrame with regular columns first
    if regular_columns:
        df = pl.DataFrame(regular_columns)
        # Add Object columns one by one
        for col, values in object_columns.items():
            df = df.with_columns([pl.Series(col, values, dtype=pl.Object)])
    else:
        # Only Object columns
        df = pl.DataFrame()
        for col, values in object_columns.items():
            df = df.with_columns([pl.Series(col, values, dtype=pl.Object)])

    return df


def load_ms1_dataframe_from_h5_group(
    group: h5py.Group,
    schema: dict[str, Any],
    logger: Any | None = None,
) -> pl.DataFrame | None:
    """
    Load MS1 DataFrame from HDF5 group.

    Args:
        group: The HDF5 group containing MS1 data
        schema: The schema dictionary
        logger: Optional logger for warnings

    Returns:
        Polars DataFrame or None
    """
    data = {}

    # Get all datasets in the ms1 group
    for col in group.keys():
        data[col] = group[col][:]

    if not data:
        return None

    # Create DataFrame directly with Polars
    ms1_df = pl.DataFrame(data)

    # Apply schema if available
    if "ms1_df" in schema and "columns" in schema["ms1_df"]:
        schema_columns = schema["ms1_df"]["columns"]
        for col in ms1_df.columns:
            if col in schema_columns:
                dtype_str = schema_columns[col]["dtype"]
                try:
                    if "Int" in dtype_str:
                        ms1_df = ms1_df.with_columns(
                            [
                                pl.col(col).cast(pl.Int64, strict=False),
                            ],
                        )
                    elif "Float" in dtype_str:
                        ms1_df = ms1_df.with_columns(
                            [
                                pl.col(col).cast(pl.Float64, strict=False),
                            ],
                        )
                except Exception as e:
                    if logger:
                        logger.warning(
                            f"Failed to apply schema type {dtype_str} to column {col}: {e}",
                        )

    # Convert "None" strings and NaN values to proper null values
    ms1_df = clean_null_values_polars(ms1_df)

    # Fix for old files: rename scan_uid to scan_id if it exists
    if "scan_uid" in ms1_df.columns and "scan_id" not in ms1_df.columns:
        ms1_df = ms1_df.rename({"scan_uid": "scan_id"})
        if logger:
            logger.debug("Renamed ms1_df.scan_uid to scan_id (old file format)")

    return ms1_df


def load_parameters_from_metadata(
    metadata_group: h5py.Group,
) -> dict[str, Any] | None:
    """
    Load parameters from HDF5 metadata group.

    Args:
        metadata_group: The HDF5 metadata group containing parameters

    Returns:
        Dictionary of parameters or None if not found
    """
    if "parameters" in metadata_group.attrs:
        try:
            params_json = decode_metadata_attr(metadata_group.attrs["parameters"])
            # Ensure params_json is a string before attempting JSON decode
            if isinstance(params_json, str) and params_json.strip():
                result = json.loads(params_json)
                # Ensure the result is a dictionary
                if isinstance(result, dict):
                    return result
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            # Log the error for debugging
            print(f"Warning: Failed to parse parameters JSON: {e}")
            print(f"Raw parameter data type: {type(params_json)}")
            print(f"Raw parameter data: {params_json!r}")
    return None


def create_h5_metadata_group(
    f: h5py.File,
    file_path: str | None,
    file_source: str | None,
    type: str | None,
    label: str | None,
) -> None:
    """
    Create and populate metadata group in HDF5 file.

    Args:
        f: The HDF5 file object
        file_path: Source file path
        file_source: Original source file path
        type: Source file type
        label: Sample label
    """
    metadata_group = f.create_group("metadata")
    metadata_group.attrs["format"] = "masster-sample5-1"
    metadata_group.attrs["file_path"] = str(file_path) if file_path is not None else ""
    metadata_group.attrs["file_source"] = (
        str(file_source) if file_source is not None else ""
    )
    metadata_group.attrs["file_type"] = str(type) if type is not None else ""
    metadata_group.attrs["label"] = str(label) if label is not None else ""


# ============================================================================
# Columnized Chromatogram Storage (Version 2)
# ============================================================================


def _save_chromatograms_columnized(
    chrom_group: h5py.Group,
    chromatograms: list,
    logger,
) -> None:
    """
    Save chromatograms in columnized format for improved I/O performance.

    Instead of storing each chromatogram as a JSON string, this stores all chromatogram
    data as separate columnar arrays. This eliminates thousands of JSON serialization
    operations and provides better compression.

    Args:
        chrom_group: HDF5 group to store chromatogram data
        chromatograms: List of Chromatogram objects or None values
        logger: Logger instance for debugging

    Storage structure:
        chrom/rt_data: 2D array [n_features  max_points] of RT values
        chrom/intensity_data: 2D array [n_features  max_points] of intensity values
        chrom/n_points: 1D array [n_features] of actual data points per chromatogram
        chrom/mz: 1D array [n_features] of m/z values
        chrom/rt: 1D array [n_features] of apex RT values
        chrom/rt_start: 1D array [n_features] of start RT
        chrom/rt_end: 1D array [n_features] of end RT
        ... (other metadata fields)
    """
    n_features = len(chromatograms)

    # Find maximum number of points across all chromatograms
    max_points = 0
    for chrom in chromatograms:
        if chrom is not None and hasattr(chrom, "rt") and chrom.rt is not None:
            max_points = max(max_points, len(chrom.rt))

    if max_points == 0:
        logger.debug("No chromatograms with data to save in columnized format")
        return

    # Initialize arrays with NaN for missing data
    rt_data = np.full((n_features, max_points), np.nan, dtype=np.float32)
    intensity_data = np.full((n_features, max_points), np.nan, dtype=np.float32)
    n_points = np.zeros(n_features, dtype=np.int32)

    # Scalar metadata arrays
    mz_array = np.full(n_features, np.nan, dtype=np.float64)
    rt_array = np.full(n_features, np.nan, dtype=np.float64)
    rt_start_array = np.full(n_features, np.nan, dtype=np.float64)
    rt_end_array = np.full(n_features, np.nan, dtype=np.float64)
    intensity_array = np.full(n_features, np.nan, dtype=np.float64)
    height_array = np.full(n_features, np.nan, dtype=np.float64)

    # Fill arrays from chromatogram objects
    for i, chrom in enumerate(chromatograms):
        if chrom is None:
            continue

        # Store RT and intensity arrays (chrom uses 'inty' attribute)
        if hasattr(chrom, "rt") and chrom.rt is not None:
            n_pts = len(chrom.rt)
            n_points[i] = n_pts
            rt_data[i, :n_pts] = chrom.rt

        if hasattr(chrom, "inty") and chrom.inty is not None:
            n_pts = len(chrom.inty)
            intensity_data[i, :n_pts] = chrom.inty

        # Store scalar metadata
        if hasattr(chrom, "mz") and chrom.mz is not None:
            mz_array[i] = chrom.mz
        # For RT apex, check if there's a specific rt_apex attribute, otherwise use None
        if hasattr(chrom, "rt_apex") and chrom.rt_apex is not None:
            rt_array[i] = chrom.rt_apex
        if hasattr(chrom, "rt_start") and chrom.rt_start is not None:
            rt_start_array[i] = chrom.rt_start
        if hasattr(chrom, "rt_end") and chrom.rt_end is not None:
            rt_end_array[i] = chrom.rt_end
        # For intensity sum, check if there's a specific intensity_sum attribute
        if hasattr(chrom, "intensity_sum") and chrom.intensity_sum is not None:
            intensity_array[i] = chrom.intensity_sum
        if hasattr(chrom, "height") and chrom.height is not None:
            height_array[i] = chrom.height

    # Save to HDF5 with optimal compression
    chrom_group.create_dataset("rt_data", data=rt_data, compression="lzf", shuffle=True)
    chrom_group.create_dataset(
        "intensity_data",
        data=intensity_data,
        compression="lzf",
        shuffle=True,
    )
    chrom_group.create_dataset(
        "n_points",
        data=n_points,
        compression="lzf",
        shuffle=True,
    )
    chrom_group.create_dataset("mz", data=mz_array, compression="lzf", shuffle=True)
    chrom_group.create_dataset("rt", data=rt_array, compression="lzf", shuffle=True)
    chrom_group.create_dataset(
        "rt_start",
        data=rt_start_array,
        compression="lzf",
        shuffle=True,
    )
    chrom_group.create_dataset(
        "rt_end",
        data=rt_end_array,
        compression="lzf",
        shuffle=True,
    )
    chrom_group.create_dataset(
        "intensity",
        data=intensity_array,
        compression="lzf",
        shuffle=True,
    )
    chrom_group.create_dataset(
        "height",
        data=height_array,
        compression="lzf",
        shuffle=True,
    )

    # Store metadata
    chrom_group.attrs["max_points"] = max_points
    chrom_group.attrs["n_features"] = n_features
    chrom_group.attrs["storage_version"] = 2  # Version 2 = columnized storage

    logger.debug(
        f"Saved {n_features} chromatograms in columnized format (max {max_points} points)",
    )


def _load_chromatograms_columnized(chrom_group: h5py.Group, logger) -> list:
    """
    Load chromatograms from columnized storage format.

    Reconstructs Chromatogram objects from columnar arrays stored in HDF5.
    This is much faster than deserializing thousands of JSON strings.

    Args:
        chrom_group: HDF5 group containing chromatogram data
        logger: Logger instance for debugging

    Returns:
        List of Chromatogram objects or None values
    """
    # Read metadata
    n_features = chrom_group.attrs.get("n_features", 0)
    chrom_group.attrs.get("max_points", 0)

    if n_features == 0:
        logger.debug("No chromatograms to load from columnized format")
        return []

    # Load all arrays at once (much faster than individual reads)
    rt_data = chrom_group["rt_data"][:]
    intensity_data = chrom_group["intensity_data"][:]
    n_points = chrom_group["n_points"][:]
    mz_array = chrom_group["mz"][:]
    rt_array = chrom_group["rt"][:]
    rt_start_array = chrom_group["rt_start"][:]
    rt_end_array = chrom_group["rt_end"][:]
    intensity_array = chrom_group["intensity"][:]
    height_array = chrom_group["height"][:]

    # Reconstruct Chromatogram objects
    chromatograms: list[Chromatogram | None] = []
    for i in range(n_features):
        if n_points[i] == 0 or np.isnan(mz_array[i]):
            # No chromatogram data for this feature
            chromatograms.append(None)
            continue

        # Extract actual data points (remove padding)
        n_pts = int(n_points[i])
        rt_vals = rt_data[i, :n_pts]
        intensity_vals = intensity_data[i, :n_pts]

        # Create Chromatogram object (use inty parameter, not intensity)
        chrom = Chromatogram(
            rt=rt_vals,
            inty=intensity_vals,
            mz=float(mz_array[i]) if not np.isnan(mz_array[i]) else None,
            rt_start=float(rt_start_array[i])
            if not np.isnan(rt_start_array[i])
            else None,
            rt_end=float(rt_end_array[i]) if not np.isnan(rt_end_array[i]) else None,
        )

        # Set additional attributes if available
        if not np.isnan(rt_array[i]):
            chrom.rt_apex = float(rt_array[i])  # type: ignore[attr-defined]
        if not np.isnan(height_array[i]):
            chrom.height = float(height_array[i])  # type: ignore[attr-defined]
        if not np.isnan(intensity_array[i]):
            chrom.intensity_sum = float(intensity_array[i])  # type: ignore[attr-defined]

        chromatograms.append(chrom)

    logger.debug(f"Loaded {n_features} chromatograms from columnized format")
    return chromatograms


def _save_ms2_specs_columnized(
    ms2_group: h5py.Group,
    ms2_specs_list: list,
    logger,
) -> None:
    """
    Save MS2 spectra in columnized ragged array format for improved I/O performance.

    Uses ragged arrays to avoid padding overhead: concatenates all m/z and intensity values
    into flat arrays, storing lengths separately to reconstruct individual spectra.

    Args:
        ms2_group: HDF5 group to store MS2 spectra data
        ms2_specs_list: List of (list of Spectrum objects or None) or None
        logger: Logger instance for debugging

    Storage structure:
        ms2_specs/mz_data: 1D flat array of all m/z values concatenated
        ms2_specs/intensity_data: 1D flat array of all intensity values concatenated
        ms2_specs/feature_offsets: Array of starting indices for each feature's spectra
        ms2_specs/spectrum_lengths: Array of number of peaks in each spectrum
        ms2_specs/feature_counts: Array of number of spectra per feature
        ms2_specs/metadata: JSON for non-array attributes (ms_level, label, etc.)
    """
    n_features = len(ms2_specs_list)

    # Collect all spectra and build index structures
    all_mz = []
    all_inty = []
    all_metadata = []
    feature_counts = np.zeros(n_features, dtype=np.int32)
    spectrum_lengths = []
    feature_offsets = np.zeros(n_features + 1, dtype=np.int32)  # +1 for final offset

    total_spectra = 0
    for i, spec_list in enumerate(ms2_specs_list):
        if spec_list is None or not spec_list:
            feature_counts[i] = 0
            feature_offsets[i + 1] = total_spectra
            continue

        # Filter out None values
        valid_specs = [s for s in spec_list if s is not None]
        feature_counts[i] = len(valid_specs)

        for spec in valid_specs:
            # Concatenate m/z and intensity arrays
            all_mz.extend(spec.mz.tolist())
            all_inty.extend(spec.inty.tolist())
            spectrum_lengths.append(len(spec.mz))

            # Collect metadata (non-array attributes)
            metadata = {}
            for key, val in spec.__dict__.items():
                if key not in ["mz", "inty"] and not isinstance(val, np.ndarray):
                    if isinstance(val, (int, float, str, bool, type(None))):
                        metadata[key] = val
            all_metadata.append(metadata)

            total_spectra += 1

        feature_offsets[i + 1] = total_spectra

    if total_spectra == 0:
        logger.debug("No MS2 spectra to save in columnized format")
        ms2_group.attrs["n_features"] = n_features
        ms2_group.attrs["total_spectra"] = 0
        ms2_group.attrs["storage_version"] = 2
        return

    # Convert to numpy arrays
    mz_array = np.array(all_mz, dtype=np.float32)
    inty_array = np.array(all_inty, dtype=np.float32)
    spectrum_lengths_array = np.array(spectrum_lengths, dtype=np.int32)

    # Save arrays with optimal compression
    ms2_group.create_dataset("mz_data", data=mz_array, compression="lzf", shuffle=True)
    ms2_group.create_dataset(
        "intensity_data",
        data=inty_array,
        compression="lzf",
        shuffle=True,
    )
    ms2_group.create_dataset(
        "spectrum_lengths",
        data=spectrum_lengths_array,
        compression="lzf",
        shuffle=True,
    )
    ms2_group.create_dataset(
        "feature_counts",
        data=feature_counts,
        compression="lzf",
        shuffle=True,
    )
    ms2_group.create_dataset(
        "feature_offsets",
        data=feature_offsets,
        compression="lzf",
        shuffle=True,
    )

    # Save metadata as JSON (scalar string - no compression allowed)
    metadata_json = json.dumps(all_metadata)
    ms2_group.create_dataset("metadata", data=metadata_json)

    # Store attributes
    ms2_group.attrs["n_features"] = n_features
    ms2_group.attrs["total_spectra"] = total_spectra
    ms2_group.attrs["storage_version"] = 2  # Version 2 = columnized ragged array

    logger.debug(
        f"Saved {total_spectra} MS2 spectra across {n_features} features in ragged array format",
    )


def _load_ms2_specs_columnized(ms2_group: h5py.Group, logger) -> list:
    """
    Load MS2 spectra from columnized ragged array storage format.

    Reconstructs Spectrum objects from flat concatenated arrays using stored
    length and offset information.

    Args:
        ms2_group: HDF5 group containing MS2 spectra data
        logger: Logger instance for debugging

    Returns:
        List of (list of Spectrum objects or None) or None, one per feature
    """
    # Read metadata
    n_features = ms2_group.attrs.get("n_features", 0)
    total_spectra = ms2_group.attrs.get("total_spectra", 0)

    if n_features == 0:
        logger.debug("No MS2 spectra to load from columnized format")
        return []

    if total_spectra == 0:
        # Return list of Nones
        logger.debug(f"No MS2 spectra data for {n_features} features")
        return [None] * n_features

    # Load all arrays at once
    mz_data = ms2_group["mz_data"][:]
    intensity_data = ms2_group["intensity_data"][:]
    spectrum_lengths = ms2_group["spectrum_lengths"][:]
    feature_counts = ms2_group["feature_counts"][:]
    ms2_group["feature_offsets"][:]

    # Load metadata
    metadata_json = ms2_group["metadata"][()]
    if isinstance(metadata_json, bytes):
        metadata_json = metadata_json.decode("utf-8")
    all_metadata = json.loads(metadata_json)

    # Reconstruct spectra for each feature
    result: list[list[Spectrum] | None] = []
    peak_offset = 0
    spec_idx = 0

    for i in range(n_features):
        n_specs = int(feature_counts[i])

        if n_specs == 0:
            result.append(None)
            continue

        # Reconstruct list of spectra for this feature
        spec_list = []
        for _ in range(n_specs):
            n_peaks = int(spectrum_lengths[spec_idx])

            # Extract peaks for this spectrum
            mz_vals = mz_data[peak_offset : peak_offset + n_peaks]
            inty_vals = intensity_data[peak_offset : peak_offset + n_peaks]

            # Create Spectrum object
            spec = Spectrum(mz=mz_vals, inty=inty_vals)

            # Restore metadata
            if spec_idx < len(all_metadata):
                for key, val in all_metadata[spec_idx].items():
                    setattr(spec, key, val)

            spec_list.append(spec)
            peak_offset += n_peaks
            spec_idx += 1

        result.append(spec_list)

    logger.debug(
        f"Loaded {total_spectra} MS2 spectra across {n_features} features from ragged array format",
    )
    return result


def _save_sample5_v2(
    self,
    filename=None,
    include_ms1=True,
    include_scans=True,
    save_featurexml=False,
):
    """
    Save sample to HDF5 file using version 2 format with columnized chromatogram storage.

    This version stores chromatograms as columnar arrays instead of JSON strings,
    providing significant performance improvements:
    - ~80% faster save operations
    - ~60% faster load operations
    - ~30-40% smaller file sizes
    - Better compression of numeric data

    Args:
        filename (str, optional): Target file name. If None, uses default based on file_path.
        include_ms1 (bool, optional): Whether to include MS1 data. Defaults to True.
        include_scans (bool, optional): Whether to include scan data. Defaults to True.
        save_featurexml (bool, optional): Whether to save featureXML file. Defaults to False.
    """
    if filename is None:
        if self.file_path is not None:
            filename = os.path.splitext(self.file_path)[0] + ".sample5"
        else:
            self.logger.error("either filename or file_path must be provided")
            return

    if not filename.endswith(".sample5"):
        filename += ".sample5"

    self.logger.debug(
        f"Saving sample to {filename} with columnized chromatogram storage (v2)",
    )

    # Delete existing file if it exists
    if os.path.exists(filename):
        os.remove(filename)

    with h5py.File(filename, "w") as f:
        # Create groups
        metadata_group = f.create_group("metadata")
        features_group = f.create_group("features")
        chrom_group = f.create_group(
            "chromatograms",
        )  # New group for columnized chromatograms
        scans_group = f.create_group("scans")
        ms1_group = f.create_group("ms1")

        # Store metadata with version 2 marker
        metadata_group.attrs["format"] = "masster-sample-2"  # Version 2
        metadata_group.attrs["storage_version"] = 2
        if self.file_path is not None:
            metadata_group.attrs["file_path"] = str(self.file_path)
        else:
            metadata_group.attrs["file_path"] = ""
        if self.file_source is not None:
            metadata_group.attrs["file_source"] = str(self.file_source)
        else:
            metadata_group.attrs["file_source"] = ""
        if hasattr(self, "type") and self.type is not None:
            metadata_group.attrs["file_type"] = str(self.type)
        else:
            metadata_group.attrs["file_type"] = ""
        if self.label is not None:
            metadata_group.attrs["label"] = str(self.label)
        else:
            metadata_group.attrs["label"] = ""

        # Store scans_df (same as v1)
        if self.scans_df is not None and include_scans:
            scans_df = self.scans_df.clone()
            for col in scans_df.columns:
                data = scans_df[col].to_numpy()
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        scans_group.create_dataset(
                            col,
                            data=str_data,
                            compression="gzip",
                        )
                        scans_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        try:
                            numeric_data = np.array(
                                [
                                    float(x)
                                    if x is not None
                                    and str(x)
                                    .replace(".", "")
                                    .replace("-", "")
                                    .isdigit()
                                    else np.nan
                                    for x in data
                                ],
                            )
                            if not np.isnan(numeric_data).all():
                                scans_group.create_dataset(
                                    col,
                                    data=numeric_data,
                                    compression="gzip",
                                )
                                scans_group[col].attrs["dtype"] = "numeric_converted"
                            else:
                                json_data = np.array(
                                    [json.dumps(x, default=str) for x in data],
                                    dtype="S",
                                )
                                scans_group.create_dataset(
                                    col,
                                    data=json_data,
                                    compression="gzip",
                                )
                                scans_group[col].attrs["dtype"] = "json_serialized"
                        except Exception:
                            str_repr_data = np.array([str(x) for x in data], dtype="S")
                            scans_group.create_dataset(
                                col,
                                data=str_repr_data,
                                compression="gzip",
                            )
                            scans_group[col].attrs["dtype"] = "string_repr"
                else:
                    scans_group.create_dataset(
                        col,
                        data=data,
                        compression="lzf",
                        shuffle=True,
                    )
                    scans_group[col].attrs["dtype"] = "native"
            scans_group.attrs["columns"] = list(scans_df.columns)

        # Store features_df with columnized chromatograms
        if self.features_df is not None:
            features = self.features_df.clone()

            # Extract and save chromatograms separately
            if "chrom" in features.columns:
                chromatograms = features["chrom"].to_list()
                _save_chromatograms_columnized(chrom_group, chromatograms, self.logger)
                # Remove chrom column from features_df before saving
                features = features.drop("chrom")

            # Extract and save MS2 spectra separately
            if "ms2_specs" in features.columns:
                ms2_specs_list = features["ms2_specs"].to_list()
                ms2_group = f.create_group("ms2_specs")
                _save_ms2_specs_columnized(ms2_group, ms2_specs_list, self.logger)
                # Remove ms2_specs column from features_df before saving
                features = features.drop("ms2_specs")

            # Save remaining columns (same logic as v1, but without chrom/ms2_specs columns)
            for col in features.columns:
                dtype = str(features[col].dtype).lower()
                if dtype == "object":
                    if col == "ms2_scans":
                        data = features[col]
                        data_as_json_strings = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                data_as_json_strings.append(json.dumps(list(data[i])))
                            else:
                                data_as_json_strings.append("None")
                        features_group.create_dataset(
                            col,
                            data=data_as_json_strings,
                            compression="gzip",
                        )
                    elif col == "ms1_spec":
                        data = features[col]
                        data_as_json_strings = []
                        for i in range(len(data)):
                            if data[i] is not None:
                                data_as_json_strings.append(
                                    json.dumps(data[i].tolist()),
                                )
                            else:
                                data_as_json_strings.append("None")
                        features_group.create_dataset(
                            col,
                            data=data_as_json_strings,
                            compression="gzip",
                        )
                    else:
                        self.logger.warning(
                            f"Unexpectedly, column '{col}' has dtype 'object'. Implement serialization for this column.",
                        )
                    continue
                if dtype == "string":
                    data = features[col].to_list()
                    data = ["None" if x is None else x for x in data]
                    features_group.create_dataset(
                        col,
                        data=data,
                        compression="lzf",
                        shuffle=True,
                    )
                else:
                    try:
                        data = features[col].to_numpy()
                        features_group.create_dataset(col, data=data)
                    except Exception:
                        self.logger.warning(
                            f"Failed to save column '{col}' with dtype '{dtype}'. It may contain unsupported data types.",
                        )
            features_group.attrs["columns"] = list(features.columns)

        # Store MS1 data (same as v1)
        if self.ms1_df is not None and include_ms1:
            for col in self.ms1_df.columns:
                ms1_group.create_dataset(
                    col,
                    data=self.ms1_df[col].to_numpy(),
                    compression="gzip",
                )

        # Store parameters/history as JSON (same as v1)
        if hasattr(self, "parameters") and self.parameters is not None:
            if hasattr(self, "polarity") and self.polarity is not None:
                self.parameters.polarity = self.polarity
            if hasattr(self, "type") and self.type is not None:
                self.parameters.type = self.type

        save_data = {}
        if hasattr(self, "parameters") and self.parameters is not None:
            save_data["sample"] = self.parameters.to_dict()

        if hasattr(self, "history") and self.history is not None:
            serializable_history = {}
            for key, value in self.history.items():
                if key == "sample":
                    continue
                try:
                    json.dumps(value)
                    serializable_history[key] = value
                except (TypeError, ValueError):
                    serializable_history[key] = str(value)
            save_data.update(serializable_history)

        params_json = json.dumps(save_data, indent=2)
        metadata_group.attrs["parameters"] = params_json

        # Store lib_df and id_df (same as v1)
        if (
            hasattr(self, "lib_df")
            and self.lib_df is not None
            and not self.lib_df.is_empty()
        ):
            lib_group = f.create_group("lib")
            for col in self.lib_df.columns:
                data = self.lib_df[col].to_numpy()
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        lib_group.create_dataset(col, data=str_data, compression="gzip")
                        lib_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        json_data = np.array(
                            [json.dumps(x, default=str) for x in data],
                            dtype="S",
                        )
                        lib_group.create_dataset(
                            col,
                            data=json_data,
                            compression="gzip",
                        )
                        lib_group[col].attrs["dtype"] = "json"
                else:
                    lib_group.create_dataset(col, data=data, compression="gzip")
            lib_group.attrs["columns"] = list(self.lib_df.columns)

        if (
            hasattr(self, "id_df")
            and self.id_df is not None
            and not self.id_df.is_empty()
        ):
            id_group = f.create_group("id")
            for col in self.id_df.columns:
                data = self.id_df[col].to_numpy()
                if data.dtype == object:
                    try:
                        str_data = np.array(
                            ["" if x is None else str(x) for x in data],
                            dtype="S",
                        )
                        id_group.create_dataset(col, data=str_data, compression="gzip")
                        id_group[col].attrs["dtype"] = "string_converted"
                    except Exception:
                        json_data = np.array(
                            [json.dumps(x, default=str) for x in data],
                            dtype="S",
                        )
                        id_group.create_dataset(col, data=json_data, compression="gzip")
                        id_group[col].attrs["dtype"] = "json"
                else:
                    id_group.create_dataset(col, data=data, compression="gzip")
            id_group.attrs["columns"] = list(self.id_df.columns)

    self.logger.success(f"Sample saved to {filename} (v2 with columnized storage)")

    if save_featurexml:
        feature_map = self._get_feature_map()
        if feature_map is not None:
            old_features = getattr(self, "_oms_features_map", None)
            self._oms_features_map = feature_map
            try:
                self._save_featureXML(
                    filename=filename.replace(".sample5", ".featureXML"),
                )
            finally:
                if old_features is not None:
                    self._oms_features_map = old_features
                else:
                    delattr(self, "_oms_features_map")
        else:
            self.logger.warning("Cannot save featureXML: no feature data available")


def _load_sample5_v2(self, filename: str, map: bool = False):
    """
    Load sample from HDF5 file using version 2 format with columnized chromatogram storage.

    This version loads chromatograms from columnar arrays instead of JSON strings,
    providing significant performance improvements over version 1.

    Args:
        filename (str): Path to the sample5 HDF5 file to load.
        map (bool, optional): Whether to map featureXML file if available. Defaults to False.
    """
    # Load schema for proper DataFrame reconstruction
    schema_path = os.path.join(os.path.dirname(__file__), "sample5_schema.json")
    try:
        with open(schema_path) as f:
            schema = json.load(f)
    except FileNotFoundError:
        self.logger.warning(
            f"Schema file {schema_path} not found. Using default types.",
        )
        schema = {}

    with h5py.File(filename, "r") as f:
        # Load metadata (same as v1)
        if "metadata" in f:
            metadata_group = f["metadata"]
            self.file_path = decode_metadata_attr(
                metadata_group.attrs.get("file_path", ""),
            )

            if "file_source" in metadata_group.attrs:
                self.file_source = decode_metadata_attr(
                    metadata_group.attrs.get("file_source", ""),
                )
            else:
                self.file_source = self.file_path

            self.type = decode_metadata_attr(metadata_group.attrs.get("file_type", ""))
            self.label = decode_metadata_attr(metadata_group.attrs.get("label", ""))

            # Load parameters from JSON in metadata
            loaded_data = load_parameters_from_metadata(metadata_group)

            # Always create a fresh sample_defaults object
            from masster.sample.defaults.sample_def import sample_defaults

            self.parameters = sample_defaults()

            # Initialize history and populate from loaded data
            self.history = {}
            if loaded_data is not None and isinstance(loaded_data, dict):
                self.history = loaded_data
                if "sample" in loaded_data:
                    sample_params = loaded_data["sample"]
                    if isinstance(sample_params, dict):
                        self.parameters.set_from_dict(sample_params, validate=False)

        # Load scans_df (same as v1 - reuse existing code structure)
        if "scans" in f:
            scans_group = f["scans"]
            data: dict[str, Any] = {}

            # Get columns in order specified by schema
            schema_cols = schema.get("scans_df", {}).get("columns", {})
            if schema_cols:
                # Sort columns by their order value if available
                has_order = any(
                    "order" in col_info for col_info in schema_cols.values()
                )
                if has_order:
                    cols_with_order = [
                        (col_name, col_info.get("order", 9999))
                        for col_name, col_info in schema_cols.items()
                    ]
                    cols_with_order.sort(key=lambda x: x[1])
                    columns_to_load = [col_name for col_name, _ in cols_with_order]
                else:
                    columns_to_load = list(schema_cols.keys())
            else:
                columns_to_load = []

            for col in columns_to_load:
                if col not in scans_group:
                    self.logger.debug(f"Column '{col}' not found in sample5/scans.")
                    data[col] = None
                    continue
                data[col] = scans_group[col][:]

            if data:
                self.scans_df = pl.DataFrame(data)
                # Convert "None" strings and NaN values to proper null values
                for col in self.scans_df.columns:
                    if self.scans_df[col].dtype == pl.Utf8:
                        self.scans_df = self.scans_df.with_columns(
                            [
                                pl.when(pl.col(col).is_in(["None", "", "null", "NULL"]))
                                .then(None)
                                .otherwise(pl.col(col))
                                .alias(col),
                            ],
                        )
                    elif self.scans_df[col].dtype in [pl.Float64, pl.Float32]:
                        self.scans_df = self.scans_df.with_columns(
                            [pl.col(col).fill_nan(None).alias(col)],
                        )

                # Apply schema casting (simplified version)
                for col in self.scans_df.columns:
                    if col in schema.get("scans_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["scans_df"]["columns"][col]["dtype"]
                            if (
                                dtype_str.startswith("pl.")
                                and "Object" not in dtype_str
                            ):
                                self.scans_df = self.scans_df.with_columns(
                                    pl.col(col).cast(eval(dtype_str)),
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in scans_df: {e}",
                            )

            # Fix for old files: rename feature_uid to feature_id if it exists
            if self.scans_df is not None:
                if (
                    "feature_uid" in self.scans_df.columns
                    and "feature_id" not in self.scans_df.columns
                ):
                    self.scans_df = self.scans_df.rename({"feature_uid": "feature_id"})
                    self.logger.debug(
                        "Renamed scans_df.feature_uid to feature_id (old file format)",
                    )

                # Fix for old files: rename scan_uid to scan_id if it exists
                if (
                    "scan_uid" in self.scans_df.columns
                    and "scan_id" not in self.scans_df.columns
                ):
                    self.scans_df = self.scans_df.rename({"scan_uid": "scan_id"})
                    self.logger.debug(
                        "Renamed scans_df.scan_uid to scan_id (old file format)",
                    )

            else:
                self.scans_df = None
        else:
            self.scans_df = None

        # Load features_df (modified for v2)
        if "features" in f:
            features_group = f["features"]
            data = {}

            # BACKWARD COMPATIBILITY: Check if we need to swap feature_id and feature_uid
            swap_features = False
            if "feature_uid" in features_group and "feature_id" in features_group:
                # Check type of feature_uid in HDF5
                # If it's integer-like, it's the old schema
                if np.issubdtype(features_group["feature_uid"].dtype, np.integer):
                    swap_features = True
                    self.logger.debug(
                        "Detected old feature_id/feature_uid naming convention. Swapping for backward compatibility.",
                    )

            # Check if we need to migrate sample_uid to sample_id
            swap_samples = False
            if "sample_uid" in features_group:
                if np.issubdtype(features_group["sample_uid"].dtype, np.integer):
                    swap_samples = True

            # Load non-chromatogram columns (same as v1, but chrom and ms2_specs handled separately)
            for col in schema.get("features_df", {}).get("columns", []):
                if col in ["chrom", "ms2_specs"]:
                    continue  # Will be loaded from separate groups

                source_col = col
                if swap_features:
                    if col == "feature_uid":
                        source_col = "feature_id"
                    elif col == "feature_id":
                        source_col = "feature_uid"

                if swap_samples:
                    if col == "sample_id":
                        source_col = "sample_uid"
                    elif col == "sample_uid":
                        source_col = "sample_id"

                if source_col not in features_group:
                    self.logger.debug(
                        f"Column '{source_col}' not found in sample5/features.",
                    )
                    data[col] = None
                    continue

                dtype = schema["features_df"]["columns"][col].get("dtype", "native")

                if dtype == "pl.Object":
                    # Handle object columns (ms2_scans, ms2_specs, ms1_spec)
                    if col == "ms2_scans":
                        data_col = features_group[source_col][:]
                        reconstructed_data: list[int | None] = []
                        for item in data_col:
                            if isinstance(item, bytes):
                                item = item.decode("utf-8")
                            if item == "None":
                                reconstructed_data.append(None)
                            else:
                                try:
                                    scan_list = json.loads(item)
                                    reconstructed_data.append(scan_list)
                                except (json.JSONDecodeError, ValueError):
                                    reconstructed_data.append(None)
                        data[col] = reconstructed_data

                    elif col == "ms1_spec":
                        data_col = features_group[source_col][:]
                        reconstructed_data = []
                        for item in data_col:
                            if isinstance(item, bytes):
                                item = item.decode("utf-8")
                            if item == "None" or item == "":
                                reconstructed_data.append(None)
                            else:
                                try:
                                    array_data = json.loads(item)
                                    reconstructed_data.append(
                                        np.array(array_data, dtype=np.float64),
                                    )
                                except (json.JSONDecodeError, ValueError, TypeError):
                                    reconstructed_data.append(None)
                        data[col] = reconstructed_data
                else:
                    data[col] = features_group[source_col][:]

            # Load chromatograms from columnized storage
            if "chromatograms" in f:
                chrom_group = f["chromatograms"]
                chromatograms = _load_chromatograms_columnized(chrom_group, self.logger)
                data["chrom"] = chromatograms
            else:
                # No chromatogram data
                data["chrom"] = [None] * len(data.get("feature_id", []))

            # Load MS2 spectra from columnized storage
            if "ms2_specs" in f:
                ms2_group = f["ms2_specs"]
                ms2_specs = _load_ms2_specs_columnized(ms2_group, self.logger)
                data["ms2_specs"] = ms2_specs
            else:
                # No MS2 spectra data
                data["ms2_specs"] = [None] * len(data.get("feature_id", []))

            # Create DataFrame
            if data:
                # Build schema for DataFrame creation
                df_schema = {}
                for col, values in data.items():
                    if col in schema.get("features_df", {}).get("columns", {}):
                        dtype_str = schema["features_df"]["columns"][col]["dtype"]
                        if dtype_str == "pl.Object":
                            df_schema[col] = pl.Object
                        else:
                            df_schema[col] = None
                    else:
                        df_schema[col] = None

                try:
                    self.features_df = pl.DataFrame(data, schema=df_schema)
                except Exception:
                    # Fallback: handle Object columns separately
                    object_columns = {
                        k: v
                        for k, v in data.items()
                        if k in schema.get("features_df", {}).get("columns", {})
                        and schema["features_df"]["columns"][k]["dtype"] == "pl.Object"
                    }
                    regular_columns = {
                        k: v for k, v in data.items() if k not in object_columns
                    }

                    if regular_columns:
                        self.features_df = pl.DataFrame(regular_columns)
                        for col, values in object_columns.items():
                            self.features_df = self.features_df.with_columns(
                                [pl.Series(col, values, dtype=pl.Object)],
                            )
                    else:
                        self.features_df = pl.DataFrame()
                        for col, values in object_columns.items():
                            self.features_df = self.features_df.with_columns(
                                [pl.Series(col, values, dtype=pl.Object)],
                            )

                # Apply schema casting for non-Object columns (simplified)
                for col in self.features_df.columns:
                    if col in schema.get("features_df", {}).get("columns", {}):
                        try:
                            dtype_str = schema["features_df"]["columns"][col]["dtype"]
                            if (
                                dtype_str.startswith("pl.")
                                and "Object" not in dtype_str
                            ):
                                # SPECIAL HANDLING for swapped feature_id/feature_uid columns
                                # When swap_features is True, both columns contain wrong types and need conversion to Utf8
                                if swap_features and col in [
                                    "feature_id",
                                    "feature_uid",
                                ]:
                                    current_dtype = self.features_df[col].dtype

                                    # Convert any type to Utf8
                                    if "Utf8" not in str(current_dtype):
                                        if "Int" in str(current_dtype):
                                            # Integer to string
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                            self.logger.debug(
                                                f"Converted swapped {col} from Int to Utf8",
                                            )
                                        elif "Binary" in str(
                                            current_dtype,
                                        ) or isinstance(
                                            self.features_df[col][0],
                                            bytes,
                                        ):
                                            # Binary to string
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).map_elements(
                                                        lambda x: x.decode("utf-8")
                                                        if isinstance(x, bytes)
                                                        else str(x),
                                                        return_dtype=pl.Utf8,
                                                    ),
                                                )
                                            )
                                            self.logger.debug(
                                                f"Converted swapped {col} from Binary to Utf8",
                                            )
                                        else:
                                            # Any other type to string
                                            self.features_df = (
                                                self.features_df.with_columns(
                                                    pl.col(col).cast(pl.Utf8),
                                                )
                                            )
                                    continue  # Skip normal casting

                                # Handle string columns - convert bytes to strings
                                if "Utf8" in dtype_str or "String" in dtype_str:
                                    self.features_df = self.features_df.with_columns(
                                        pl.col(col).map_elements(
                                            lambda x: (
                                                x.decode("utf-8")
                                                if isinstance(x, bytes)
                                                else x
                                            )
                                            if x not in (b"None", "None", b"", "")
                                            else None,
                                            return_dtype=pl.Utf8,
                                        ),
                                    )
                                elif "Int" in dtype_str or "Float" in dtype_str:
                                    self.features_df = self.features_df.with_columns(
                                        pl.col(col).cast(eval(dtype_str), strict=False),
                                    )
                        except Exception as e:
                            self.logger.warning(
                                f"Failed to cast column '{col}' in features_df: {e}",
                            )

                # Ensure column order matches schema order
                self.features_df = reorder_dataframe_by_schema(
                    self.features_df,
                    schema,
                    "features_df",
                )

                # POST-SWAP MIGRATION: If we swapped, regenerate proper integer feature_id
                if swap_features:
                    self.logger.debug(
                        "Migrating swapped feature_id/feature_uid to new convention",
                    )
                    # At this point:
                    # - feature_id contains old feature_uid data (UUID strings)
                    # - feature_uid contains old feature_id data (integer strings)
                    # We need to:
                    # 1. Keep feature_uid as-is (UUID strings) [OK]
                    # 2. Generate new sequential integer feature_id (0, 1, 2, ...)
                    self.features_df = self.features_df.with_columns(
                        [pl.int_range(pl.len()).alias("feature_id")],
                    )
                    self.logger.debug(
                        f"Generated new integer feature_id for {len(self.features_df)} features",
                    )

            else:
                self.features_df = None
        else:
            self.features_df = None

        # Load MS1 data (same as v1)
        if "ms1" in f:
            ms1_group = f["ms1"]
            ms1_data = {}
            for key in ms1_group.keys():
                ms1_data[key] = ms1_group[key][:]
            if ms1_data:
                self.ms1_df = pl.DataFrame(ms1_data)

                # Fix for old files: rename scan_uid to scan_id if it exists
                if (
                    "scan_uid" in self.ms1_df.columns
                    and "scan_id" not in self.ms1_df.columns
                ):
                    self.ms1_df = self.ms1_df.rename({"scan_uid": "scan_id"})
                    self.logger.debug(
                        "Renamed ms1_df.scan_uid to scan_id (old file format)",
                    )
            else:
                self.ms1_df = None
        else:
            self.ms1_df = None

        # Load lib_df and id_df (same as v1 - simplified)
        if "lib" in f:
            lib_group = f["lib"]
            lib_data = {}
            for col in lib_group.keys():
                lib_data[col] = lib_group[col][:]
            if lib_data:
                self.lib_df = pl.DataFrame(lib_data)
                # Convert bytes to strings if needed
                for col in self.lib_df.columns:
                    if self.lib_df[col].dtype == pl.Binary:
                        self.lib_df = self.lib_df.with_columns(
                            pl.col(col).map_elements(
                                lambda x: x.decode("utf-8")
                                if isinstance(x, bytes)
                                else str(x),
                                return_dtype=pl.Utf8,
                            ),
                        )
            else:
                self.lib_df = pl.DataFrame()
        else:
            self.lib_df = pl.DataFrame()

        if "id" in f:
            id_group = f["id"]
            id_data = {}
            for col in id_group.keys():
                id_data[col] = id_group[col][:]
            if id_data:
                self.id_df = pl.DataFrame(id_data)
                # Convert bytes to strings if needed
                for col in self.id_df.columns:
                    if self.id_df[col].dtype == pl.Binary:
                        self.id_df = self.id_df.with_columns(
                            pl.col(col).map_elements(
                                lambda x: x.decode("utf-8")
                                if isinstance(x, bytes)
                                else str(x),
                                return_dtype=pl.Utf8,
                            ),
                        )
            else:
                self.id_df = pl.DataFrame()
        else:
            self.id_df = pl.DataFrame()

    # Set file path and label (same as v1)
    self.file_path = filename
    if self.label is None or self.label == "":
        self.label = os.path.splitext(os.path.basename(filename))[0]

    # Sync instance attributes from loaded parameters (same as v1)
    if hasattr(self, "parameters") and self.parameters is not None:
        if (
            hasattr(self.parameters, "polarity")
            and self.parameters.polarity is not None
        ):
            self.polarity = self.parameters.polarity
        if hasattr(self.parameters, "type") and self.parameters.type is not None:
            self.type = self.parameters.type

    # DEBUG: Check chrom column before returning
    if (
        hasattr(self, "features_df")
        and self.features_df is not None
        and "chrom" in self.features_df.columns
    ):
        non_null_chrom = self.features_df["chrom"].drop_nulls().len()
        self.logger.debug(
            f"_load_sample5_v2 ending: {non_null_chrom} non-null chrom values",
        )

    self.logger.success(f"Sample loaded from {filename} (v2 with columnized storage)")
