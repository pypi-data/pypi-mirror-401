#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆªå›¾å¸ƒå±€åˆ†æå·¥å…· - ç»Ÿä¸€å®ç°
åˆ†æå·²ç”Ÿæˆçš„æˆªå›¾ï¼Œæ£€æµ‹UI bugå’Œæ˜æ˜¾ä¸åˆç†çš„åœ°æ–¹
"""

import os
import sys
import json
import base64
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from zhipuai import ZhipuAI

# Setup logger
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


class ScreenshotAnalyzer:
    """æˆªå›¾å¸ƒå±€åˆ†æå™¨"""
    
    def __init__(self, api_key: str = None, base_dir: Path = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: æ™ºè°± API Key
            base_dir: åŸºç¡€ç›®å½•ï¼ˆç”¨äºæˆªå›¾å’ŒæŠ¥å‘Šç›®å½•ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        """
        self.api_key = api_key or os.getenv("ZHIPU_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥ api_key å‚æ•°")
        
        # åˆå§‹åŒ–æ™ºè°±å®¢æˆ·ç«¯
        self.client = ZhipuAI(api_key=self.api_key)
        
        # å¼ºåˆ¶ä½¿ç”¨ glm-4.6v æ¨¡å‹è¿›è¡Œæˆªå›¾åˆ†æ
        # glm-4.6v æ˜¯è§†è§‰ç†è§£æ¨¡å‹ï¼Œæœ€é€‚åˆæˆªå›¾åˆ†æä»»åŠ¡
        self.model = "glm-4.6v"
        
        # ç›®å½•é…ç½®
        if base_dir is None:
            # å°è¯•ä»è°ƒç”¨è€…ç›®å½•æ¨æ–­ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            import inspect
            frame = inspect.currentframe()
            try:
                caller_file = frame.f_back.f_globals.get('__file__')
                if caller_file:
                    self.base_dir = Path(caller_file).parent
                else:
                    self.base_dir = Path(__file__).parent
            finally:
                del frame
        else:
            self.base_dir = Path(base_dir)
        
        self.screenshot_dir = self.base_dir / "screenshots"
        self.report_dir = self.base_dir / "analysis_reports"
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"âœ… æˆªå›¾å¸ƒå±€åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æˆªå›¾ç›®å½•: {self.screenshot_dir}")
        logger.info(f"   æŠ¥å‘Šç›®å½•: {self.report_dir}")
    
    def _encode_image(self, image_path: Path) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')
    
    def _merge_images(self, image_paths: List[Path], max_images_per_row: int = 3, 
                     max_single_width: int = 400, max_single_height: int = 800,
                     max_merged_width: int = 2000, max_merged_height: int = 3000,
                     jpeg_quality: int = 85, padding: int = 20) -> Path:
        """
        å°†å¤šå¼ å›¾ç‰‡åˆå¹¶æˆä¸€å¼ å¤§å›¾ï¼ˆç½‘æ ¼å¸ƒå±€ï¼‰ï¼Œå¹¶è¿›è¡Œå‹ç¼©ä¼˜åŒ–
        
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            max_images_per_row: æ¯è¡Œæœ€å¤šæ˜¾ç¤ºå‡ å¼ å›¾ç‰‡ï¼ˆé»˜è®¤3å¼ ï¼‰
            max_single_width: å•å¼ å›¾ç‰‡çš„æœ€å¤§å®½åº¦ï¼ˆé»˜è®¤400pxï¼Œç”¨äºå‹ç¼©ï¼‰
            max_single_height: å•å¼ å›¾ç‰‡çš„æœ€å¤§é«˜åº¦ï¼ˆé»˜è®¤800pxï¼Œç”¨äºå‹ç¼©ï¼‰
            max_merged_width: åˆå¹¶åå›¾ç‰‡çš„æœ€å¤§å®½åº¦ï¼ˆé»˜è®¤2000pxï¼‰
            max_merged_height: åˆå¹¶åå›¾ç‰‡çš„æœ€å¤§é«˜åº¦ï¼ˆé»˜è®¤3000pxï¼‰
            jpeg_quality: JPEGå‹ç¼©è´¨é‡ï¼ˆ1-100ï¼Œé»˜è®¤85ï¼Œå¹³è¡¡è´¨é‡å’Œå¤§å°ï¼‰
            padding: å›¾ç‰‡ä¹‹é—´çš„é—´è·ï¼ˆåƒç´ ï¼Œé»˜è®¤20pxï¼Œç”¨äºåˆ†éš”ç‹¬ç«‹é¡µé¢ï¼‰
        
        Returns:
            åˆå¹¶åçš„å›¾ç‰‡è·¯å¾„
        """
        try:
            from PIL import Image
            import io
        except ImportError:
            raise ImportError("è¯·å®‰è£… Pillow: pip install Pillow")
        
        if not image_paths:
            raise ValueError("å›¾ç‰‡åˆ—è¡¨ä¸ºç©º")
        
        # è¯»å–æ‰€æœ‰å›¾ç‰‡å¹¶å‹ç¼©
        images = []
        total_original_size = 0
        for img_path in image_paths:
            try:
                img = Image.open(img_path)
                total_original_size += img_path.stat().st_size
                
                # è½¬æ¢ä¸º RGB æ¨¡å¼ï¼ˆå¦‚æœæ˜¯ RGBAï¼‰
                if img.mode == 'RGBA':
                    img = img.convert('RGB')
                
                # å‹ç¼©å•å¼ å›¾ç‰‡ï¼šé™åˆ¶æœ€å¤§å°ºå¯¸
                width, height = img.size
                if width > max_single_width or height > max_single_height:
                    # ä¿æŒå®½é«˜æ¯”ï¼Œå‹ç¼©åˆ°æœ€å¤§å°ºå¯¸å†…
                    ratio = min(max_single_width / width, max_single_height / height)
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)
                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    logger.info(f"  ğŸ—œï¸  å‹ç¼©å•å¼ å›¾ç‰‡: {img_path.name} {width}x{height} â†’ {new_width}x{new_height}")
                
                images.append((img, img_path.name))
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–å›¾ç‰‡ {img_path}: {e}")
                continue
        
        if not images:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯ä»¥åˆå¹¶")
        
        # è®¡ç®—ç½‘æ ¼å¸ƒå±€
        num_images = len(images)
        num_rows = (num_images + max_images_per_row - 1) // max_images_per_row
        num_cols = min(num_images, max_images_per_row)
        
        # è·å–å•å¼ å›¾ç‰‡çš„å°ºå¯¸ï¼ˆä½¿ç”¨å‹ç¼©åçš„å°ºå¯¸ï¼‰
        max_width = max(img.width for img, _ in images)
        max_height = max(img.height for img, _ in images)
        
        # ç»Ÿä¸€å›¾ç‰‡å°ºå¯¸ï¼ˆå±…ä¸­æ”¾ç½®ï¼‰
        resized_images = []
        for img, name in images:
            # ä¿æŒå®½é«˜æ¯”ï¼Œç¼©æ”¾åˆ°ç»Ÿä¸€å°ºå¯¸
            ratio = min(max_width / img.width, max_height / img.height)
            new_width = int(img.width * ratio)
            new_height = int(img.height * ratio)
            resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # åˆ›å»ºç»Ÿä¸€å°ºå¯¸çš„ç”»å¸ƒï¼ˆå±…ä¸­æ”¾ç½®ï¼Œç™½è‰²èƒŒæ™¯ï¼‰
            canvas = Image.new('RGB', (max_width, max_height), (255, 255, 255))
            x_offset = (max_width - new_width) // 2
            y_offset = (max_height - new_height) // 2
            canvas.paste(resized, (x_offset, y_offset))
            resized_images.append((canvas, name))
        
        # è®¡ç®—åˆå¹¶åçš„ç”»å¸ƒå°ºå¯¸ï¼ˆåŒ…å«é—´è·ï¼‰
        # æ€»å®½åº¦ = åˆ—æ•° Ã— å•å¼ å®½åº¦ + (åˆ—æ•° + 1) Ã— å·¦å³é—´è·
        # æ€»é«˜åº¦ = è¡Œæ•° Ã— å•å¼ é«˜åº¦ + (è¡Œæ•° + 1) Ã— ä¸Šä¸‹é—´è·
        merged_width = max_width * num_cols + padding * (num_cols + 1)
        merged_height = max_height * num_rows + padding * (num_rows + 1)
        
        # å¦‚æœåˆå¹¶åçš„å›¾ç‰‡å¤ªå¤§ï¼Œè¿›ä¸€æ­¥å‹ç¼©ï¼ˆè€ƒè™‘é—´è·ï¼‰
        if merged_width > max_merged_width or merged_height > max_merged_height:
            # è®¡ç®—å¯ç”¨ç©ºé—´ï¼ˆå‡å»é—´è·ï¼‰
            available_width = max_merged_width - padding * (num_cols + 1)
            available_height = max_merged_height - padding * (num_rows + 1)
            
            # è®¡ç®—å‹ç¼©æ¯”ä¾‹
            ratio_width = available_width / (max_width * num_cols) if num_cols > 0 else 1
            ratio_height = available_height / (max_height * num_rows) if num_rows > 0 else 1
            ratio = min(ratio_width, ratio_height, 1.0)  # ä¸æ”¾å¤§ï¼Œåªç¼©å°
            
            if ratio < 1.0:
                max_width = int(max_width * ratio)
                max_height = int(max_height * ratio)
                merged_width = max_width * num_cols + padding * (num_cols + 1)
                merged_height = max_height * num_rows + padding * (num_rows + 1)
                logger.info(f"  ğŸ—œï¸  å‹ç¼©åˆå¹¶å°ºå¯¸: {max_width * num_cols + padding * (num_cols + 1)}x{max_height * num_rows + padding * (num_rows + 1)} â†’ {merged_width}x{merged_height}")
                
                # é‡æ–°è°ƒæ•´æ‰€æœ‰å›¾ç‰‡å°ºå¯¸
                resized_images = []
                for img, name in images:
                    ratio_single = min(max_width / img.width, max_height / img.height)
                    new_width = int(img.width * ratio_single)
                    new_height = int(img.height * ratio_single)
                    resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    canvas = Image.new('RGB', (max_width, max_height), (255, 255, 255))
                    x_offset = (max_width - new_width) // 2
                    y_offset = (max_height - new_height) // 2
                    canvas.paste(resized, (x_offset, y_offset))
                    resized_images.append((canvas, name))
        
        # åˆ›å»ºåˆå¹¶åçš„ç”»å¸ƒï¼ˆç™½è‰²èƒŒæ™¯ï¼Œé—´è·åŒºåŸŸä¹Ÿæ˜¯ç™½è‰²ï¼‰
        merged_image = Image.new('RGB', (merged_width, merged_height), (255, 255, 255))
        
        # å°†å›¾ç‰‡æŒ‰ç½‘æ ¼æ’åˆ—ï¼Œæ¯å¼ å›¾ç‰‡ä¹‹é—´ç•™å‡ºé—´è·
        for idx, (img, name) in enumerate(resized_images):
            row = idx // num_cols
            col = idx % num_cols
            # è®¡ç®—ä½ç½®ï¼šå·¦è¾¹è· + åˆ—ç´¢å¼• Ã— (å›¾ç‰‡å®½åº¦ + é—´è·) + å›¾ç‰‡å®½åº¦
            x = padding + col * (max_width + padding)
            y = padding + row * (max_height + padding)
            merged_image.paste(img, (x, y))
        
        # ä¿å­˜åˆå¹¶åçš„å›¾ç‰‡ï¼ˆä½¿ç”¨JPEGæ ¼å¼ä»¥è·å¾—æ›´å¥½çš„å‹ç¼©æ¯”ï¼‰
        temp_dir = Path("/tmp") if Path("/tmp").exists() else Path.cwd() / "temp"
        temp_dir.mkdir(exist_ok=True)
        merged_path = temp_dir / f"merged_screenshots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
        
        # ä½¿ç”¨JPEGæ ¼å¼ä¿å­˜ï¼ˆæ›´å¥½çš„å‹ç¼©æ¯”ï¼‰
        merged_image.save(merged_path, "JPEG", quality=jpeg_quality, optimize=True)
        
        # è®°å½•å‹ç¼©æ•ˆæœ
        merged_size = merged_path.stat().st_size
        compression_ratio = (merged_size / total_original_size * 100) if total_original_size > 0 else 0
        logger.info(f"  ğŸ“¦ åˆå¹¶å›¾ç‰‡å¤§å°: {merged_size / 1024:.1f}KB (åŸå§‹æ€»å¤§å°: {total_original_size / 1024:.1f}KB, å‹ç¼©ç‡: {compression_ratio:.1f}%)")
        logger.info(f"  ğŸ“ åˆå¹¶å›¾ç‰‡å°ºå¯¸: {merged_width}x{merged_height}")
        
        return merged_path
    
    def analyze_screenshots_batch(self, screenshot_paths: List[Path], batch_size: int = 6) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æå¤šå¼ æˆªå›¾ï¼ˆåˆå¹¶åç»Ÿä¸€åˆ†æï¼‰
        
        Args:
            screenshot_paths: æˆªå›¾è·¯å¾„åˆ—è¡¨
            batch_size: æ¯æ‰¹åˆå¹¶çš„å›¾ç‰‡æ•°é‡ï¼ˆé»˜è®¤6å¼ ï¼Œå³2è¡Œ3åˆ—ï¼‰
        
        Returns:
            åˆ†æç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€å¼ æˆªå›¾
        """
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(screenshot_paths), batch_size):
            batch_paths = screenshot_paths[batch_start:batch_start + batch_size]
            batch_num = batch_start // batch_size + 1
            total_batches = (len(screenshot_paths) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¸ æ‰¹é‡åˆ†æç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_paths)} å¼ æˆªå›¾)")
            
            try:
                # åˆå¹¶å›¾ç‰‡
                merged_path = self._merge_images(batch_paths, max_images_per_row=3)
                logger.info(f"  âœ… å›¾ç‰‡å·²åˆå¹¶: {merged_path.name}")
                
                # åˆ†æåˆå¹¶åçš„å›¾ç‰‡
                merged_analysis = self.analyze_screenshot_layout(merged_path, is_merged=True, image_count=len(batch_paths))
                
                # å°†åˆ†æç»“æœåˆ†é…ç»™æ¯å¼ åŸå›¾
                # å¦‚æœAIè¿”å›äº†æ¯å¼ å›¾ç‰‡çš„åˆ†æï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨åˆå¹¶åˆ†æç»“æœ
                if isinstance(merged_analysis.get('individual_analyses'), list):
                    # AIè¿”å›äº†æ¯å¼ å›¾ç‰‡çš„ç‹¬ç«‹åˆ†æ
                    for idx, (img_path, analysis) in enumerate(zip(batch_paths, merged_analysis['individual_analyses'])):
                        analysis['screenshot'] = str(img_path)
                        analysis['filename'] = img_path.name
                        analysis['batch_number'] = batch_num
                        results.append(analysis)
                else:
                    # ä½¿ç”¨åˆå¹¶åˆ†æç»“æœï¼ˆæ‰€æœ‰å›¾ç‰‡å…±äº«åˆ†æç»“æœï¼‰
                    for img_path in batch_paths:
                        result = merged_analysis.copy()
                        result['screenshot'] = str(img_path)
                        result['filename'] = img_path.name
                        result['batch_number'] = batch_num
                        result['note'] = "æ­¤åˆ†æåŸºäºåˆå¹¶å›¾ç‰‡ï¼Œå¯èƒ½ä¸å•å¼ åˆ†æç•¥æœ‰å·®å¼‚"
                        results.append(result)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    merged_path.unlink()
                except:
                    pass
                    
            except Exception as e:
                logger.error(f"  âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
                # å¦‚æœæ‰¹é‡åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°å•å¼ åˆ†æ
                logger.info(f"  ğŸ”„ å›é€€åˆ°å•å¼ åˆ†ææ¨¡å¼")
                for img_path in batch_paths:
                    try:
                        result = self.analyze_screenshot_layout(img_path)
                        results.append(result)
                    except Exception as e2:
                        logger.error(f"  âŒ å•å¼ åˆ†æä¹Ÿå¤±è´¥ {img_path.name}: {e2}")
                        results.append({
                            "success": False,
                            "error": str(e2),
                            "screenshot": str(img_path),
                            "filename": img_path.name
                        })
        
        return results
    
    def analyze_screenshot_layout(self, screenshot_path: Path, is_merged: bool = False, image_count: int = 1) -> Dict:
        """
        åˆ†æå•ä¸ªæˆªå›¾çš„å¸ƒå±€ï¼Œæ£€æµ‹ bug å’Œä¸åˆç†ä¹‹å¤„
        
        Args:
            screenshot_path: æˆªå›¾è·¯å¾„
            is_merged: æ˜¯å¦ä¸ºåˆå¹¶åçš„å›¾ç‰‡ï¼ˆé»˜è®¤Falseï¼‰
            image_count: åˆå¹¶å›¾ç‰‡ä¸­çš„å›¾ç‰‡æ•°é‡ï¼ˆä»…åœ¨is_merged=Trueæ—¶æœ‰æ•ˆï¼‰
            
        Returns:
            åˆ†æç»“æœ
        """
        print(f"\nğŸ” åˆ†ææˆªå›¾: {screenshot_path.name}")
        
        try:
            # ç¼–ç å›¾ç‰‡
            image_base64 = self._encode_image(screenshot_path)
            
            # è®¾è®¡è¯¦ç»†çš„åˆ†æä»»åŠ¡
            if is_merged and image_count > 1:
                # åˆå¹¶å›¾ç‰‡çš„åˆ†ææç¤º
                task = f"""
è¯·ä½œä¸ºä¸“ä¸šçš„UI/UXè®¾è®¡å¸ˆå’Œæµ‹è¯•å·¥ç¨‹å¸ˆï¼Œå¯¹è¿™å¼ åˆå¹¶å›¾ç‰‡ä¸­çš„ {image_count} ä¸ªç§»åŠ¨åº”ç”¨ç•Œé¢è¿›è¡Œå…¨é¢çš„å¸ƒå±€åˆ†æã€‚

**é‡è¦æç¤ºï¼š**
è¿™æ˜¯ä¸€å¼ åˆå¹¶å›¾ç‰‡ï¼ŒåŒ…å« {image_count} ä¸ªç‹¬ç«‹çš„ç•Œé¢æˆªå›¾ï¼ŒæŒ‰ç½‘æ ¼æ’åˆ—ã€‚è¯·åˆ†åˆ«åˆ†ææ¯ä¸ªç•Œé¢ï¼Œå¹¶åœ¨è¿”å›ç»“æœä¸­ä¸ºæ¯ä¸ªç•Œé¢æä¾›ç‹¬ç«‹çš„åˆ†æã€‚

**åˆ†æè¦æ±‚ï¼š**
1. æŒ‰ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹çš„é¡ºåºï¼Œä¾æ¬¡åˆ†ææ¯ä¸ªç•Œé¢
2. ä¸ºæ¯ä¸ªç•Œé¢æä¾›ç‹¬ç«‹çš„åˆ†æç»“æœ
3. å¦‚æœæŸä¸ªç•Œé¢æœ‰é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºæ˜¯ç¬¬å‡ ä¸ªç•Œé¢ï¼ˆä»1å¼€å§‹è®¡æ•°ï¼‰

è¯·ä½œä¸ºä¸“ä¸šçš„UI/UXè®¾è®¡å¸ˆå’Œæµ‹è¯•å·¥ç¨‹å¸ˆï¼Œå¯¹æ¯ä¸ªç•Œé¢è¿›è¡Œå…¨é¢çš„å¸ƒå±€åˆ†æï¼Œé‡ç‚¹æ£€æµ‹ä»¥ä¸‹é—®é¢˜ï¼š
"""
            else:
                task = """
è¯·ä½œä¸ºä¸“ä¸šçš„UI/UXè®¾è®¡å¸ˆå’Œæµ‹è¯•å·¥ç¨‹å¸ˆï¼Œå¯¹è¿™ä¸ªç§»åŠ¨åº”ç”¨ç•Œé¢è¿›è¡Œå…¨é¢çš„å¸ƒå±€åˆ†æï¼Œé‡ç‚¹æ£€æµ‹ä»¥ä¸‹é—®é¢˜ï¼š

**1. å¸ƒå±€Bugæ£€æµ‹ï¼š**
- å…ƒç´ é‡å æˆ–é®æŒ¡
- æ–‡æœ¬æˆªæ–­æˆ–æ˜¾ç¤ºä¸å®Œæ•´
- æŒ‰é’®æˆ–å›¾æ ‡é”™ä½
- é—´è·ä¸ä¸€è‡´æˆ–è¿‡å¤§/è¿‡å°
- å…ƒç´ è¶…å‡ºå±å¹•è¾¹ç•Œ
- å¸ƒå±€é”™ä¹±æˆ–å˜å½¢

**2. è§†è§‰é—®é¢˜ï¼š**
- é¢œè‰²å¯¹æ¯”åº¦ä¸è¶³ï¼ˆæ–‡å­—ä¸æ¸…æ™°ï¼‰
- å›¾ç‰‡ç¼ºå¤±æˆ–åŠ è½½å¤±è´¥
- å›¾æ ‡æˆ–å›¾ç‰‡å˜å½¢ã€æ‹‰ä¼¸
- èƒŒæ™¯è‰²ä¸å‰æ™¯è‰²å†²çª

**3. äº¤äº’é—®é¢˜ï¼š**
- æŒ‰é’®æˆ–é“¾æ¥å¤ªå°ï¼ˆä¸æ˜“ç‚¹å‡»ï¼‰
- å¯ç‚¹å‡»åŒºåŸŸä¸æ˜ç¡®
- é‡è¦æ“ä½œç¼ºå°‘è§†è§‰åé¦ˆ
- å¯¼èˆªä¸æ¸…æ™°æˆ–ç¼ºå¤±

**4. å†…å®¹é—®é¢˜ï¼š**
- æ–‡æœ¬é”™è¯¯æˆ–ä¹±ç 
- ç©ºç™½é¡µé¢æˆ–ç©ºçŠ¶æ€
- æ•°æ®æ˜¾ç¤ºå¼‚å¸¸ï¼ˆå¦‚ï¼šè´Ÿæ•°è¿›åº¦ã€è¶…è¿‡100%ï¼‰
- æç¤ºä¿¡æ¯ä¸æ˜ç¡®æˆ–ç¼ºå¤±

**5. å¯ç”¨æ€§é—®é¢˜ï¼š**
- ä¿¡æ¯å±‚çº§ä¸æ¸…æ™°
- é‡è¦åŠŸèƒ½ä¸çªå‡º
- æ“ä½œæµç¨‹ä¸åˆç†
- ç¼ºå°‘å¿…è¦çš„å¼•å¯¼

**è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š**

{"is_merged": true, "image_count": image_count, "individual_analyses": [...]} æ ¼å¼ï¼ˆåˆå¹¶å›¾ç‰‡ï¼‰ï¼š
```json
{
  "is_merged": true,
  "image_count": {image_count},
  "individual_analyses": [
    {{
      "image_index": 1,
      "overall_score": 85,
      "overall_assessment": "æ•´ä½“å¸ƒå±€è‰¯å¥½ï¼Œä½†å­˜åœ¨ä¸€äº›å°é—®é¢˜",
      "issues": [
        {{
          "severity": "high|medium|low",
          "category": "å¸ƒå±€|è§†è§‰|äº¤äº’|å†…å®¹|å¯ç”¨æ€§",
          "title": "é—®é¢˜æ ‡é¢˜",
          "description": "è¯¦ç»†æè¿°",
          "location": "é—®é¢˜ä½ç½®ï¼ˆé¡µé¢å“ªä¸ªåŒºåŸŸï¼‰",
          "suggestion": "ä¿®å¤å»ºè®®"
        }}
      ],
      "positive_points": [
        "ä¼˜ç‚¹1ï¼šå¸ƒå±€æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜",
        "ä¼˜ç‚¹2ï¼šè‰²å½©æ­é…åè°ƒ"
      ],
      "recommendations": [
        "å»ºè®®1ï¼šå¢åŠ æŒ‰é’®ç‚¹å‡»åŒºåŸŸ",
        "å»ºè®®2ï¼šä¼˜åŒ–æ–‡å­—å¤§å°"
      ]
    }},
    ...
  ]
}
```

å•å¼ å›¾ç‰‡æ ¼å¼ï¼š
```json
{{
  "overall_score": 85,
  "overall_assessment": "æ•´ä½“å¸ƒå±€è‰¯å¥½ï¼Œä½†å­˜åœ¨ä¸€äº›å°é—®é¢˜",
  "issues": [
    {{
      "severity": "high|medium|low",
      "category": "å¸ƒå±€|è§†è§‰|äº¤äº’|å†…å®¹|å¯ç”¨æ€§",
      "title": "é—®é¢˜æ ‡é¢˜",
      "description": "è¯¦ç»†æè¿°",
      "location": "é—®é¢˜ä½ç½®ï¼ˆé¡µé¢å“ªä¸ªåŒºåŸŸï¼‰",
      "suggestion": "ä¿®å¤å»ºè®®"
    }}
  ],
  "positive_points": [
    "ä¼˜ç‚¹1ï¼šå¸ƒå±€æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜",
    "ä¼˜ç‚¹2ï¼šè‰²å½©æ­é…åè°ƒ"
  ],
  "recommendations": [
    "å»ºè®®1ï¼šå¢åŠ æŒ‰é’®ç‚¹å‡»åŒºåŸŸ",
    "å»ºè®®2ï¼šä¼˜åŒ–æ–‡å­—å¤§å°"
  ]
}}
```

å¦‚æœç•Œé¢å®Œç¾æ— é—®é¢˜ï¼Œissues æ•°ç»„ä¸ºç©ºï¼Œä½†ä»éœ€ç»™å‡ºè¯„åˆ†å’Œä¼˜ç‚¹ã€‚
"""
            
            # è°ƒç”¨æ™ºè°± AI API (glm-4.6v)
            logger.info(f"  ğŸ¤– æ­£åœ¨è°ƒç”¨ AI API è¿›è¡Œåˆ†æ (æ¨¡å‹: {self.model})...")
            print("  ğŸ¤– æ­£åœ¨è°ƒç”¨ AI åˆ†æ...")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®å‚æ•°
            api_params = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}"
                                }
                            },
                            {
                                "type": "text",
                                "text": task
                            }
                        ]
                    }
                ],
            }
            
            # glm-4.6v æ¨¡å‹å‚æ•°ä¼˜åŒ–
            if self.model == "glm-4.6v":
                api_params["temperature"] = 0.2  # æ›´ä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
                api_params["max_tokens"] = 4096    # å¢åŠ æœ€å¤§tokenæ•°ä»¥å…è®¸æ›´è¯¦ç»†çš„è¾“å‡º
                # glm-4.6v ä¸éœ€è¦ stop å‚æ•°
            else:
                # å…¶ä»–æ¨¡å‹çš„å‚æ•°ï¼ˆå¦‚autoglm-phoneï¼‰
                api_params["temperature"] = 0.7
                api_params["max_tokens"] = 3000
                api_params["stop"] = ["[finish]"]  # autoglm-phone çš„ç»ˆæ­¢æ ‡è®°
            
            logger.info(f"  ğŸ“¡ è°ƒç”¨ AI API (æ¨¡å‹: {self.model}, max_tokens: {api_params.get('max_tokens', 'N/A')})...")
            response = self.client.chat.completions.create(**api_params)
            logger.info(f"  âœ… AI API è°ƒç”¨æˆåŠŸï¼Œæ”¶åˆ°å“åº”")
            
            # æå–åˆ†æç»“æœ
            analysis_text = response.choices[0].message.content
            logger.info(f"  ğŸ“ AI è¿”å›åˆ†ææ–‡æœ¬é•¿åº¦: {len(analysis_text)} å­—ç¬¦")
            
            # å°è¯•è§£æ JSON
            logger.info(f"  ğŸ” è§£æ AI è¿”å›çš„ JSON æ•°æ®...")
            result = self._parse_analysis_result(analysis_text)
            
            # Check if JSON parsing actually succeeded
            if isinstance(result, dict) and result.get('parse_error'):
                logger.warning(f"  âš ï¸  JSON è§£æå¤±è´¥: {result.get('parse_error')}")
                logger.warning(f"  âš ï¸  AI è¿”å›æ–‡æœ¬é•¿åº¦: {len(analysis_text)} å­—ç¬¦")
                logger.warning(f"  âš ï¸  AI è¿”å›æ–‡æœ¬å‰500å­—ç¬¦: {analysis_text[:500]}")
                # Return the error result directly
                result['screenshot'] = str(screenshot_path)
                result['filename'] = screenshot_path.name
                result['timestamp'] = datetime.now().isoformat()
                result['raw_analysis'] = analysis_text
                self._print_analysis_summary(result)
                return result
            else:
                logger.info(f"  âœ… JSON è§£ææˆåŠŸ")
            
            # Debug: Log parsed result structure
            result_keys = list(result.keys()) if isinstance(result, dict) else "Not a dict"
            result_success = result.get('success') if isinstance(result, dict) else None
            logger.info(f"  ğŸ” Parsed result keys: {result_keys}")
            logger.info(f"  ğŸ” Parsed result success: {result_success}")
            
            # Check if result has required fields
            if isinstance(result, dict):
                if not result.get('success', True):
                    logger.warning(f"  âš ï¸  Parsed result has success=False, error field: {result.get('error', 'NOT SET')}")
                    logger.warning(f"  âš ï¸  Full result structure: {list(result.keys())}")
                else:
                    # Check for missing fields
                    if not result.get('is_merged'):
                        # Single image format - check for required fields
                        if 'overall_score' not in result:
                            logger.warning(f"  âš ï¸  Missing 'overall_score' field in result. Available keys: {list(result.keys())}")
                            logger.warning(f"  âš ï¸  This may indicate the AI returned an unexpected format")
                            logger.warning(f"  âš ï¸  Raw analysis preview: {analysis_text[:500]}")
                        if 'overall_assessment' not in result:
                            logger.warning(f"  âš ï¸  Missing 'overall_assessment' field in result. Available keys: {list(result.keys())}")
                    else:
                        # Merged image format - check individual_analyses
                        if 'individual_analyses' not in result:
                            logger.warning(f"  âš ï¸  Merged image format but missing 'individual_analyses' field")
                        else:
                            logger.info(f"  ğŸ“Š Merged image format: {len(result.get('individual_analyses', []))} individual analyses")
            
            result['screenshot'] = str(screenshot_path)
            result['filename'] = screenshot_path.name
            result['timestamp'] = datetime.now().isoformat()
            result['raw_analysis'] = analysis_text
            
            # æ˜¾ç¤ºæ‘˜è¦
            self._print_analysis_summary(result)
            
            return result
            
        except Exception as e:
            import traceback
            import sys
            
            # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºï¼Œç¡®ä¿é”™è¯¯ä¿¡æ¯æ˜¾ç¤º
            sys.stdout.flush()
            sys.stderr.flush()
            
            error_detail = str(e) if str(e) else type(e).__name__
            print(f"\n  âŒ åˆ†æå¤±è´¥: {error_detail}", flush=True)
            print(f"  ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}", flush=True)
            print(f"  ğŸ” å®Œæ•´é”™è¯¯ä¿¡æ¯:", flush=True)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            
            return {
                'success': False,
                'error': error_detail,
                'error_type': type(e).__name__,
                'error_traceback': traceback.format_exc(),
                'screenshot': str(screenshot_path),
                'filename': screenshot_path.name,
                'timestamp': datetime.now().isoformat()
            }
    
    def _parse_analysis_result(self, text: str) -> Dict:
        """è§£æ AI è¿”å›çš„åˆ†æç»“æœ"""
        import re
        
        # å°è¯•æå– JSON éƒ¨åˆ†
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # è¿”å›åŸå§‹æ–‡æœ¬
            return {
                'success': False,
                'raw_text': text,
                'parse_error': 'Unable to parse JSON from AI response'
            }
    
    def _print_analysis_summary(self, result: Dict):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not result.get('success', True):
            error_msg = result.get('error') or result.get('parse_error') or 'Unknown error'
            print(f"  âŒ åˆ†æå¤±è´¥: {error_msg}")
            # Log additional debug info
            logger.warning(f"  âš ï¸  Analysis failed. Result keys: {list(result.keys())}")
            logger.warning(f"  âš ï¸  Error field: {result.get('error')}, Parse error: {result.get('parse_error')}")
            return
        
        # Handle merged image format (is_merged: true)
        if result.get('is_merged') and result.get('individual_analyses'):
            # For merged images, show summary of all individual analyses
            individual_analyses = result.get('individual_analyses', [])
            total_score = 0
            total_issues = 0
            valid_analyses = 0
            
            for idx, analysis in enumerate(individual_analyses, 1):
                if isinstance(analysis, dict):
                    score = analysis.get('overall_score', 0)
                    issues = analysis.get('issues', [])
                    if score > 0:  # Only count valid scores
                        total_score += score
                        valid_analyses += 1
                    total_issues += len(issues)
            
            if valid_analyses > 0:
                avg_score = total_score / valid_analyses
                score_emoji = "ğŸŸ¢" if avg_score >= 90 else "ğŸŸ¡" if avg_score >= 70 else "ğŸ”´"
                print(f"  {score_emoji} ç»¼åˆè¯„åˆ†: {avg_score:.0f}/100 (å¹³å‡ï¼Œå…± {valid_analyses} å¼ å›¾ç‰‡)")
                print(f"  âš ï¸  å‘ç° {total_issues} ä¸ªé—®é¢˜ï¼ˆå…± {len(individual_analyses)} å¼ å›¾ç‰‡ï¼‰")
            else:
                print(f"  ğŸ”´ ç»¼åˆè¯„åˆ†: 0/100 (æ— æ³•è·å–æœ‰æ•ˆè¯„åˆ†)")
                print(f"  N/A")
        else:
            # Single image format
            score = result.get('overall_score', 0)
            issues = result.get('issues', [])
            
            # Check if score is actually 0 or just missing
            if score == 0 and 'overall_score' not in result:
                # Score field is missing, not actually 0
                logger.warning(f"  âš ï¸  Missing 'overall_score' field in result. Available keys: {list(result.keys())}")
                print(f"  âš ï¸  ç»¼åˆè¯„åˆ†: 0/100 (å­—æ®µç¼ºå¤±)")
                print(f"  {result.get('overall_assessment', 'N/A')}")
            else:
                # Score is present (may be 0 or actual value)
                # è¯„åˆ†é¢œè‰²
                if score >= 90:
                    score_emoji = "ğŸŸ¢"
                elif score >= 70:
                    score_emoji = "ğŸŸ¡"
                else:
                    score_emoji = "ğŸ”´"
                
                print(f"  {score_emoji} ç»¼åˆè¯„åˆ†: {score}/100")
                assessment = result.get('overall_assessment', 'N/A')
                if assessment == 'N/A' and 'overall_assessment' not in result:
                    logger.warning(f"  âš ï¸  Missing 'overall_assessment' field in result. Available keys: {list(result.keys())}")
                print(f"  {assessment}")
            
            if issues:
                print(f"\n  âš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜ï¼š")
                for i, issue in enumerate(issues[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    severity_emoji = {
                        'high': 'ğŸ”´',
                        'medium': 'ğŸŸ¡',
                        'low': 'ğŸŸ¢'
                    }.get(issue.get('severity', 'low'), 'âšª')
                    
                    print(f"    {severity_emoji} [{issue.get('category', 'N/A')}] {issue.get('title', 'N/A')}")
                
                if len(issues) > 3:
                    print(f"    ... è¿˜æœ‰ {len(issues) - 3} ä¸ªé—®é¢˜")
            else:
                print(f"  âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    
    def analyze_directory(self, session_id: str = None) -> List[Dict]:
        """
        åˆ†ææŒ‡å®šä¼šè¯çš„æ‰€æœ‰æˆªå›¾
        
        Args:
            session_id: ä¼šè¯IDï¼ˆä¾‹å¦‚ï¼š20241226_143000ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ†ææœ€æ–°çš„
            
        Returns:
            æ‰€æœ‰åˆ†æç»“æœåˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print("  ğŸ“¸ æ‰¹é‡æˆªå›¾å¸ƒå±€åˆ†æ")
        print(f"{'='*60}\n")
        
        # æŸ¥æ‰¾æˆªå›¾æ–‡ä»¶ï¼ˆåœ¨ç‹¬ç«‹ä¼šè¯ç›®å½•ä¸­ï¼‰
        if session_id:
            # ä½¿ç”¨æŒ‡å®šä¼šè¯çš„ç›®å½•
            session_screenshot_dir = self.base_dir / "output" / session_id / "screenshots"
            
            if not session_screenshot_dir.exists():
                print(f"âŒ æœªæ‰¾åˆ°ä¼šè¯ {session_id} çš„æˆªå›¾ç›®å½•: {session_screenshot_dir}")
                return []
            
            screenshots = sorted(session_screenshot_dir.glob("*.png"))
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„ä¼šè¯ç›®å½•
            output_dir = self.base_dir / "output"
            if not output_dir.exists():
                print("âŒ æœªæ‰¾åˆ° output ç›®å½•")
                return []
            
            # è·å–æ‰€æœ‰ä¼šè¯ç›®å½•ï¼ŒæŒ‰æ—¶é—´æ’åº
            session_dirs = sorted([d for d in output_dir.iterdir() if d.is_dir()])
            if not session_dirs:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä¼šè¯ç›®å½•")
                return []
            
            # ä½¿ç”¨æœ€æ–°çš„ä¼šè¯
            latest_session_dir = session_dirs[-1]
            session_id = latest_session_dir.name
            session_screenshot_dir = latest_session_dir / "screenshots"
            
            if not session_screenshot_dir.exists():
                print(f"âŒ æœ€æ–°ä¼šè¯ {session_id} æ²¡æœ‰æˆªå›¾ç›®å½•")
                return []
            
            screenshots = sorted(session_screenshot_dir.glob("*.png"))
        
        if not screenshots:
            print(f"âŒ æœªæ‰¾åˆ°ä¼šè¯ {session_id} çš„æˆªå›¾")
            return []
        
        print(f"ğŸ“ æ‰¾åˆ° {len(screenshots)} ä¸ªæˆªå›¾æ–‡ä»¶")
        print(f"ğŸ“… ä¼šè¯ID: {session_id}")
        print(f"ğŸ“‚ ç›®å½•: {session_screenshot_dir}\n")
        
        # åˆ†ææ‰€æœ‰æˆªå›¾
        results = []
        for i, screenshot in enumerate(screenshots, 1):
            print(f"[{i}/{len(screenshots)}]", end=" ")
            result = self.analyze_screenshot_layout(screenshot)
            results.append(result)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self._generate_summary_report(results, session_id)
        
        return results
    
    def _generate_summary_report(self, results: List[Dict], session_id: str):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("  ğŸ“Š åˆ†ææ±‡æ€»æŠ¥å‘Š")
        print(f"{'='*60}\n")
        
        # ç»Ÿè®¡
        total = len(results)
        successful = sum(1 for r in results if r.get('success', True))
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        all_issues = []
        scores = []
        
        for result in results:
            if result.get('overall_score'):
                scores.append(result['overall_score'])
            
            for issue in result.get('issues', []):
                all_issues.append({
                    **issue,
                    'screenshot': result.get('filename', 'Unknown')
                })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        high_issues = [i for i in all_issues if i.get('severity') == 'high']
        medium_issues = [i for i in all_issues if i.get('severity') == 'medium']
        low_issues = [i for i in all_issues if i.get('severity') == 'low']
        
        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡ï¼š")
        print(f"  - åˆ†ææˆªå›¾æ•°: {total}")
        print(f"  - æˆåŠŸåˆ†æ: {successful}")
        print(f"  - å¹³å‡è¯„åˆ†: {sum(scores)/len(scores):.1f}/100" if scores else "  - å¹³å‡è¯„åˆ†: N/A")
        print(f"  - å‘ç°é—®é¢˜æ€»æ•°: {len(all_issues)}")
        print(f"    ğŸ”´ ä¸¥é‡: {len(high_issues)}")
        print(f"    ğŸŸ¡ ä¸­ç­‰: {len(medium_issues)}")
        print(f"    ğŸŸ¢ è½»å¾®: {len(low_issues)}")
        
        # æ˜¾ç¤ºä¸¥é‡é—®é¢˜
        if high_issues:
            print(f"\nğŸ”´ ä¸¥é‡é—®é¢˜åˆ—è¡¨ï¼š")
            for i, issue in enumerate(high_issues[:10], 1):
                print(f"  {i}. [{issue.get('category', 'N/A')}] {issue.get('title', 'N/A')}")
                print(f"     ä½ç½®: {issue.get('location', 'N/A')}")
                print(f"     æˆªå›¾: {issue.get('screenshot', 'N/A')}")
                print()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.report_dir / f"analysis_report_{session_id}.json"
        report_data = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_screenshots': total,
                'successful_analysis': successful,
                'average_score': sum(scores)/len(scores) if scores else 0,
                'total_issues': len(all_issues),
                'high_severity': len(high_issues),
                'medium_severity': len(medium_issues),
                'low_severity': len(low_issues)
            },
            'high_priority_issues': high_issues[:20],  # å‰20ä¸ªä¸¥é‡é—®é¢˜
            'all_results': results
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file.name}")
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_file = self._generate_markdown_report(report_data, session_id)
        print(f"ğŸ“„ Markdown æŠ¥å‘Š: {md_file.name}")
        
        print(f"\n{'='*60}")
    
    def _generate_markdown_report(self, report_data: Dict, session_id: str) -> Path:
        """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
        md_file = self.report_dir / f"analysis_report_{session_id}.md"
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# æˆªå›¾å¸ƒå±€åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ä¼šè¯ID**: {session_id}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ±‡æ€»ç»Ÿè®¡
            summary = report_data['summary']
            f.write(f"## ğŸ“Š æ±‡æ€»ç»Ÿè®¡\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| åˆ†ææˆªå›¾æ•° | {summary['total_screenshots']} |\n")
            f.write(f"| æˆåŠŸåˆ†æ | {summary['successful_analysis']} |\n")
            f.write(f"| å¹³å‡è¯„åˆ† | {summary['average_score']:.1f}/100 |\n")
            f.write(f"| å‘ç°é—®é¢˜æ€»æ•° | {summary['total_issues']} |\n")
            f.write(f"| ğŸ”´ ä¸¥é‡é—®é¢˜ | {summary['high_severity']} |\n")
            f.write(f"| ğŸŸ¡ ä¸­ç­‰é—®é¢˜ | {summary['medium_severity']} |\n")
            f.write(f"| ğŸŸ¢ è½»å¾®é—®é¢˜ | {summary['low_severity']} |\n")
            f.write(f"\n")
            
            # ä¸¥é‡é—®é¢˜è¯¦æƒ…
            high_issues = report_data['high_priority_issues']
            if high_issues:
                f.write(f"## ğŸ”´ ä¸¥é‡é—®é¢˜è¯¦æƒ…\n\n")
                for i, issue in enumerate(high_issues, 1):
                    f.write(f"### {i}. {issue.get('title', 'N/A')}\n\n")
                    f.write(f"- **ç±»åˆ«**: {issue.get('category', 'N/A')}\n")
                    f.write(f"- **ä½ç½®**: {issue.get('location', 'N/A')}\n")
                    f.write(f"- **æˆªå›¾**: {issue.get('screenshot', 'N/A')}\n")
                    f.write(f"- **æè¿°**: {issue.get('description', 'N/A')}\n")
                    f.write(f"- **å»ºè®®**: {issue.get('suggestion', 'N/A')}\n")
                    f.write(f"\n")
            
            # æ¯ä¸ªæˆªå›¾çš„è¯¦ç»†åˆ†æ
            f.write(f"## ğŸ“¸ è¯¦ç»†åˆ†æç»“æœ\n\n")
            for result in report_data['all_results']:
                if not result.get('success', True):
                    continue
                
                f.write(f"### {result.get('filename', 'N/A')}\n\n")
                f.write(f"**è¯„åˆ†**: {result.get('overall_score', 'N/A')}/100\n\n")
                f.write(f"**è¯„ä¼°**: {result.get('overall_assessment', 'N/A')}\n\n")
                
                issues = result.get('issues', [])
                if issues:
                    f.write(f"**å‘ç°çš„é—®é¢˜** ({len(issues)}ä¸ª):\n\n")
                    for issue in issues:
                        severity_emoji = {
                            'high': 'ğŸ”´',
                            'medium': 'ğŸŸ¡',
                            'low': 'ğŸŸ¢'
                        }.get(issue.get('severity', 'low'), 'âšª')
                        f.write(f"- {severity_emoji} **[{issue.get('category', 'N/A')}]** {issue.get('title', 'N/A')}\n")
                        f.write(f"  - æè¿°: {issue.get('description', 'N/A')}\n")
                        f.write(f"  - ä½ç½®: {issue.get('location', 'N/A')}\n")
                        f.write(f"  - å»ºè®®: {issue.get('suggestion', 'N/A')}\n")
                
                positive_points = result.get('positive_points', [])
                if positive_points:
                    f.write(f"\n**ä¼˜ç‚¹**:\n\n")
                    for point in positive_points:
                        f.write(f"- âœ… {point}\n")
                
                f.write(f"\n---\n\n")
        
        return md_file

