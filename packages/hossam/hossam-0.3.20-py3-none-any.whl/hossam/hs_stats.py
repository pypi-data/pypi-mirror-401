# -*- coding: utf-8 -*-
from __future__ import annotations

# -------------------------------------------------------------
import numpy as np
from typing import Tuple
from itertools import combinations
from pandas import DataFrame, Series, concat
from pandas.api.types import is_bool_dtype, is_numeric_dtype

from sklearn.metrics import (
    confusion_matrix,
    roc_auc_score,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score,
)

from scipy.stats import (
    shapiro,
    normaltest,
    bartlett,
    levene,
    ttest_1samp,
    ttest_ind as scipy_ttest_ind,
    ttest_rel,
    wilcoxon,
    pearsonr,
    spearmanr,
)

import statsmodels.api as sm
from statsmodels.stats.diagnostic import linear_reset, het_breuschpagan, het_white
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.stattools import durbin_watson

from pingouin import anova, pairwise_tukey, welch_anova, pairwise_gameshowell

# ===================================================================
# ê²°ì¸¡ì¹˜ ë¶„ì„ (Missing Values Analysis)
# ===================================================================
def missing_values(data: DataFrame, *fields: str):
    """ë°ì´í„°í”„ë ˆì„ì˜ ê²°ì¸¡ì¹˜ ì •ë³´ë¥¼ ì»¬ëŸ¼ ë‹¨ìœ„ë¡œ ë°˜í™˜í•œë‹¤.

    ê° ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ìˆ˜ì™€ ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ì»¬ëŸ¼ì„ ì²˜ë¦¬.

    Returns:
        DataFrame: ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
            ì¸ë±ìŠ¤ëŠ” FIELD(ì»¬ëŸ¼ëª…)ì´ë©°, ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨:

            - missing_count (int): ê²°ì¸¡ì¹˜ì˜ ìˆ˜
            - missing_rate (float): ì „ì²´ í–‰ì—ì„œ ê²°ì¸¡ì¹˜ì˜ ë¹„ìœ¨(%)

    Examples:
        ì „ì²´ ì»¬ëŸ¼ì— ëŒ€í•œ ê²°ì¸¡ì¹˜ í™•ì¸:

        >>> from hossam import missing_values
        >>> import pandas as pd
        >>> df = pd.DataFrame({'x': [1, 2, None, 4], 'y': [10, None, None, 40]})
        >>> result = missing_values(df)
        >>> print(result)

        íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„:

        >>> result = missing_values(df, 'x', 'y')
        >>> print(result)
    """
    if not fields:
        fields = data.columns

    result = []
    for f in fields:
        missing_count = data[f].isna().sum()
        missing_rate = (missing_count / len(data)) * 100

        iq = {
            "field": f,
            "missing_count": missing_count,
            "missing_rate": missing_rate
        }

        result.append(iq)

    return DataFrame(result).set_index("field")


# ===================================================================
# ì´ìƒì¹˜ ë¶„ì„ (Outlier Analysis)
# ===================================================================
def outlier_table(data: DataFrame, *fields: str):
    """ë°ì´í„°í”„ë ˆì„ì˜ ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì´ìƒì¹˜ ê²½ê³„ê°’, ì™œë„ë¥¼ êµ¬í•œë‹¤.

    Tukeyì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê° ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•œ ì‚¬ë¶„ìœ„ìˆ˜(Q1, Q2, Q3)ì™€
    ì´ìƒì¹˜ íŒë‹¨ì„ ìœ„í•œ í•˜í•œ(DOWN)ê³¼ ìƒí•œ(UP) ê²½ê³„ê°’ì„ ê³„ì‚°í•œë‹¤.
    í•¨ìˆ˜ í˜¸ì¶œ ì „ ìƒìê·¸ë¦¼ì„ í†µí•´ ì´ìƒì¹˜ê°€ í™•ì¸ëœ í•„ë“œì— ëŒ€í•´ì„œë§Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ì„ ì²˜ë¦¬.

    Returns:
        DataFrame: ê° í•„ë“œë³„ ì‚¬ë¶„ìœ„ìˆ˜ ë° ì´ìƒì¹˜ ê²½ê³„ê°’ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
            ì¸ë±ìŠ¤ëŠ” FIELD(ì»¬ëŸ¼ëª…)ì´ë©°, ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨:

            - q1 (float): ì œ1ì‚¬ë¶„ìœ„ìˆ˜ (25th percentile)
            - q2 (float): ì œ2ì‚¬ë¶„ìœ„ìˆ˜ (ì¤‘ì•™ê°’, 50th percentile)
            - q3 (float): ì œ3ì‚¬ë¶„ìœ„ìˆ˜ (75th percentile)
            - iqr (float): ì‚¬ë¶„ìœ„ ë²”ìœ„ (q3 - q1)
            - up (float): ì´ìƒì¹˜ ìƒí•œ ê²½ê³„ê°’ (q3 + 1.5 * iqr)
            - down (float): ì´ìƒì¹˜ í•˜í•œ ê²½ê³„ê°’ (q1 - 1.5 * iqr)
            - min (float): ìµœì†Œê°’
            - max (float): ìµœëŒ€ê°’
            - skew (float): ì™œë„
            - outlier_count (int): ì´ìƒì¹˜ ê°œìˆ˜
            - outlier_rate (float): ì´ìƒì¹˜ ë¹„ìœ¨(%)

    Examples:
        ì „ì²´ ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•œ ì´ìƒì¹˜ ê²½ê³„ í™•ì¸:

        >>> from hossam import outlier_table
        >>> import pandas as pd
        >>> df = pd.DataFrame({'x': [1, 2, 3, 100], 'y': [10, 20, 30, 40]})
        >>> result = outlier_table(df)
        >>> print(result)

        íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„:

        >>> result = outlier_table(df, 'x', 'y')
        >>> print(result[['Q1', 'Q3', 'UP', 'DOWN']])

    Notes:
        - DOWN ë¯¸ë§Œì´ê±°ë‚˜ UP ì´ˆê³¼ì¸ ê°’ì€ ì´ìƒì¹˜(outlier)ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
        - ìˆ«ìí˜•ì´ ì•„ë‹Œ ì»¬ëŸ¼ì€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤.
        - Tukeyì˜ 1.5 * IQR ê·œì¹™ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (ìƒìê·¸ë¦¼ì˜ í‘œì¤€ ë°©ë²•).
    """
    if not fields:
        fields = data.columns

    result = []
    for f in fields:
        # ìˆ«ì íƒ€ì…ì´ ì•„ë‹ˆë¼ë©´ ê±´ë„ˆëœ€
        if data[f].dtypes not in [
            "int",
            "int32",
            "int64",
            "float",
            "float32",
            "float64",
        ]:
            continue

        # ì‚¬ë¶„ìœ„ìˆ˜
        q1 = data[f].quantile(q=0.25)
        q2 = data[f].quantile(q=0.5)
        q3 = data[f].quantile(q=0.75)
        min_value = data[f].min()
        max_value = data[f].max()

        # ì´ìƒì¹˜ ê²½ê³„ (Tukey's fences)
        iqr = q3 - q1
        down = q1 - 1.5 * iqr
        up = q3 + 1.5 * iqr

        # ì™œë„
        skew = data[f].skew()

        # ì´ìƒì¹˜ ê°œìˆ˜ ë° ë¹„ìœ¨
        outlier_count = ((data[f] < down) | (data[f] > up)).sum()
        outlier_rate = (outlier_count / len(data)) * 100

        iq = {
            "field": f,
            "q1": q1,
            "q2": q2,
            "q3": q3,
            "iqr": iqr,
            "up": up,
            "down": down,
            "min": min_value,
            "max": max_value,
            "skew": skew,
            "outlier_count": outlier_count,
            "outlier_rate": outlier_rate
        }

        result.append(iq)

    return DataFrame(result).set_index("field")


# ===================================================================
# ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ (Categorical Variable Analysis)
# ===================================================================
def category_table(data: DataFrame, *fields: str):
    """ë°ì´í„°í”„ë ˆì„ì˜ ëª…ëª©í˜•(ë²”ì£¼í˜•) ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ìˆ í†µê³„ë¥¼ ë°˜í™˜í•œë‹¤.

    ê° ëª…ëª©í˜• ì»¬ëŸ¼ì˜ ë²”ì£¼ê°’ë³„ ë¹ˆë„ìˆ˜ì™€ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ëª…ëª©í˜• ì»¬ëŸ¼ì„ ì²˜ë¦¬.

    Returns:
        DataFrame: ê° ì»¬ëŸ¼ë³„ ë²”ì£¼ê°’ì˜ ë¹ˆë„ì™€ ë¹„ìœ¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
            ì¸ë±ìŠ¤ëŠ” FIELD(ì»¬ëŸ¼ëª…)ì™€ CATEGORY(ë²”ì£¼ê°’)ì´ë©°, ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨:

            - count (int): í•´ë‹¹ ë²”ì£¼ê°’ì˜ ë¹ˆë„ìˆ˜
            - rate (float): ì „ì²´ í–‰ì—ì„œ í•´ë‹¹ ë²”ì£¼ê°’ì˜ ë¹„ìœ¨(%)

    Examples:
        ì „ì²´ ëª…ëª©í˜• ì»¬ëŸ¼ì— ëŒ€í•œ ê¸°ìˆ í†µê³„:

        >>> from hossam import category_table
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     'color': ['red', 'blue', 'red', 'green', 'blue', 'red'],
        ...     'size': ['S', 'M', 'L', 'M', 'S', 'M'],
        ...     'price': [100, 200, 150, 300, 120, 180]
        ... })
        >>> result = category_table(df)
        >>> print(result)

        íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„:

        >>> result = category_table(df, 'color', 'size')
        >>> print(result)

    Notes:
        - ìˆ«ìí˜• ì»¬ëŸ¼ì€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤.
        - ê° ë²”ì£¼ê°’ë³„ë¡œ ë³„ë„ì˜ í–‰ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
        - NaN ê°’ë„ í•˜ë‚˜ì˜ ë²”ì£¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.
    """
    if not fields:
        # ëª…ëª©í˜•(ë²”ì£¼í˜•) ì»¬ëŸ¼ ì„ íƒ: object, category, bool íƒ€ì…
        fields = data.select_dtypes(include=['object', 'category', 'bool']).columns

    result = []
    for f in fields:
        # ìˆ«ìí˜• ì»¬ëŸ¼ì€ ê±´ë„ˆëœ€
        if data[f].dtypes in [
            "int",
            "int32",
            "int64",
            "float",
            "float32",
            "float64",
        ]:
            continue

        # ê° ë²”ì£¼ê°’ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚° (NaN í¬í•¨)
        value_counts = data[f].value_counts(dropna=False)

        for category, count in value_counts.items():
            rate = (count / len(data)) * 100

            iq = {
                "field": f,
                "category": category,
                "count": count,
                "rate": rate
            }

            result.append(iq)

    return DataFrame(result).set_index(["field", "category"])


# ===================================================================
# ë²”ì£¼í˜• ë³€ìˆ˜ ìš”ì•½ (Categorical Variable Summary)
# ===================================================================
def category_describe(data: DataFrame, *fields: str):
    """ë°ì´í„°í”„ë ˆì„ì˜ ëª…ëª©í˜•(ë²”ì£¼í˜•) ë³€ìˆ˜ì— ëŒ€í•œ ë¶„í¬ í¸í–¥ì„ ìš”ì•½í•œë‹¤.

    ê° ëª…ëª©í˜• ì»¬ëŸ¼ì˜ ìµœë‹¤ ë²”ì£¼ì™€ ìµœì†Œ ë²”ì£¼ì˜ ì •ë³´ë¥¼ ìš”ì•½í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ëª…ëª©í˜• ì»¬ëŸ¼ì„ ì²˜ë¦¬.

    Returns:
        DataFrame: ê° ì»¬ëŸ¼ë³„ ìµœë‹¤/ìµœì†Œ ë²”ì£¼ ì •ë³´ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
            ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨:

            - ë³€ìˆ˜ (str): ì»¬ëŸ¼ëª…
            - ìµœë‹¤_ë²”ì£¼: ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë²”ì£¼ê°’
            - ìµœë‹¤_ë¹„ìœ¨(%) (float): ìµœë‹¤ ë²”ì£¼ì˜ ë¹„ìœ¨
            - ìµœì†Œ_ë²”ì£¼: ê°€ì¥ ë¹ˆë„ê°€ ë‚®ì€ ë²”ì£¼ê°’
            - ìµœì†Œ_ë¹„ìœ¨(%) (float): ìµœì†Œ ë²”ì£¼ì˜ ë¹„ìœ¨

    Examples:
        ì „ì²´ ëª…ëª©í˜• ì»¬ëŸ¼ì— ëŒ€í•œ ë¶„í¬ í¸í–¥ ìš”ì•½:

        >>> from hossam import category_describe
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     'cut': ['Ideal', 'Premium', 'Good', 'Ideal', 'Premium'],
        ...     'color': ['E', 'F', 'G', 'E', 'F'],
        ...     'price': [100, 200, 150, 300, 120]
        ... })
        >>> result = category_describe(df)
        >>> print(result)

        íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„:

        >>> result = category_describe(df, 'cut', 'color')
        >>> print(result)

    Notes:
        - ìˆ«ìí˜• ì»¬ëŸ¼ì€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤.
        - NaN ê°’ë„ í•˜ë‚˜ì˜ ë²”ì£¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.
    """
    if not fields:
        # ëª…ëª©í˜•(ë²”ì£¼í˜•) ì»¬ëŸ¼ ì„ íƒ: object, category, bool íƒ€ì…
        fields = data.select_dtypes(include=['object', 'category', 'bool']).columns

    result = []
    for f in fields:
        # ìˆ«ìí˜• ì»¬ëŸ¼ì€ ê±´ë„ˆëœ€
        if data[f].dtypes in [
            "int",
            "int32",
            "int64",
            "float",
            "float32",
            "float64",
        ]:
            continue

        # ê° ë²”ì£¼ê°’ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚° (NaN í¬í•¨)
        value_counts = data[f].value_counts(dropna=False)

        if len(value_counts) == 0:
            continue

        # ìµœë‹¤ ë²”ì£¼ (ì²« ë²ˆì§¸)
        max_category = value_counts.index[0]
        max_count = value_counts.iloc[0]
        max_rate = (max_count / len(data)) * 100

        # ìµœì†Œ ë²”ì£¼ (ë§ˆì§€ë§‰)
        min_category = value_counts.index[-1]
        min_count = value_counts.iloc[-1]
        min_rate = (min_count / len(data)) * 100

        iq = {
            "ë³€ìˆ˜": f,
            "ìµœë‹¤_ë²”ì£¼": max_category,
            "ìµœë‹¤_ë¹„ìœ¨(%)": round(max_rate, 2),
            "ìµœì†Œ_ë²”ì£¼": min_category,
            "ìµœì†Œ_ë¹„ìœ¨(%)": round(min_rate, 2)
        }

        result.append(iq)

    return DataFrame(result)

# -------------------------------------------------------------------
# Backward-compatibility alias for categorical summary
# ê¸°ì¡´ í•¨ìˆ˜ëª…(category_summary)ì„ ê³„ì† ì§€ì›í•©ë‹ˆë‹¤.
def category_summary(data: DataFrame, *fields: str):
    """Deprecated alias for category_describe.

    ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ category_describeë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return category_describe(data, *fields)

# ===================================================================
# ì •ê·œì„± ê²€ì • (Normal Test)
# ===================================================================
def normal_test(data: DataFrame, columns: list | str | None = None, method: str = "n") -> DataFrame:
    """ì§€ì •ëœ ì»¬ëŸ¼(ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)ì— ëŒ€í•´ ì •ê·œì„± ê²€ì •ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    ì •ê·œì„± ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ "ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤"ì´ë¯€ë¡œ, p-value > 0.05ì¼ ë•Œ
    ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì§€ ì•Šìœ¼ë©° ì •ê·œì„±ì„ ì¶©ì¡±í•œë‹¤ê³  í•´ì„í•œë‹¤.

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        columns (list | str | None, optional): ê²€ì • ëŒ€ìƒ ì»¬ëŸ¼ëª….
            - None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰.
            - ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸: ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ê²€ì • ìˆ˜í–‰.
            - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´: "A, B, C" í˜•ì‹ìœ¼ë¡œ ì»¬ëŸ¼ëª… ì§€ì • ê°€ëŠ¥.
            ê¸°ë³¸ê°’ì€ None.
        method (str, optional): ì •ê·œì„± ê²€ì • ë°©ë²•.
            - "n": D'Agostino and Pearson's Omnibus test (í‘œë³¸ í¬ê¸° 20 ì´ìƒ ê¶Œì¥)
            - "s": Shapiro-Wilk test (í‘œë³¸ í¬ê¸° 5000 ì´í•˜ ê¶Œì¥)
            ê¸°ë³¸ê°’ì€ "n".

    Returns:
        DataFrame: ê° ì»¬ëŸ¼ì˜ ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - method (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - column (str): ì»¬ëŸ¼ëª…
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - is_normal (bool): ì •ê·œì„± ì¶©ì¡± ì—¬ë¶€ (p-value > 0.05)

    Raises:
        ValueError: ë©”ì„œë“œê°€ "n" ë˜ëŠ” "s"ê°€ ì•„ë‹ ê²½ìš°.

    Examples:
        >>> from hossam.analysis import normal_test
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x': np.random.normal(0, 1, 100),
        ...     'y': np.random.exponential(2, 100)
        ... })
        >>> # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê²€ì •
        >>> result = normal_test(df, method='n')
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¦¬ìŠ¤íŠ¸)
        >>> result = normal_test(df, columns=['x'], method='n')
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¬¸ìì—´)
        >>> result = normal_test(df, columns='x, y', method='n')
    """
    if method not in ["n", "s"]:
        raise ValueError(f"methodëŠ” 'n' ë˜ëŠ” 's'ì—¬ì•¼ í•©ë‹ˆë‹¤. ì…ë ¥ê°’: {method}")

    # columnsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(columns, str):
        columns = [col.strip() for col in columns.split(',')]

    # ì»¬ëŸ¼ ì„ íƒ: ì§€ì •ëœ ì»¬ëŸ¼ ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
    if columns is None or len(columns) == 0:
        # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (bool ì œì™¸)
        numeric_df = data.select_dtypes(include=[np.number])
        target_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]
    else:
        # ì§€ì •ëœ ì»¬ëŸ¼ ì‚¬ìš©
        target_cols = columns

    results = []

    for c in target_cols:
        # NaN ê°’ ì œê±° (í†µê³„ ê²€ì • ìˆ˜í–‰)
        col_data = data[c].dropna()

        if len(col_data) == 0:
            results.append({
                "method": method,
                "column": c,
                "statistic": np.nan,
                "p-value": np.nan,
                "is_normal": False
            })
            continue

        try:
            if method == "n":
                method_name = "normaltest"
                s, p = normaltest(col_data)
            else:
                method_name = "shapiro"
                s, p = shapiro(col_data)

            results.append({
                "method": method_name,
                "column": c,
                "statistic": s,
                "p-value": p,
                "is_normal": p > 0.05
            })
        except Exception as e:
            # ê²€ì • ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ê¸°ë¡
            results.append({
                "method": method,
                "column": c,
                "statistic": np.nan,
                "p-value": np.nan,
                "is_normal": False
            })

    result_df = DataFrame(results)
    return result_df


# ===================================================================
# ë“±ë¶„ì‚°ì„± ê²€ì •
# ===================================================================
def equal_var_test(data: DataFrame, columns: list | str | None = None, normal_dist: bool | None = None) -> DataFrame:
    """ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì˜ ë¶„ì‚°ì´ ê°™ì€ì§€ ê²€ì •í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    ë“±ë¶„ì‚°ì„± ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ "ëª¨ë“  ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ê°™ë‹¤"ì´ë¯€ë¡œ, p-value > 0.05ì¼ ë•Œ
    ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì§€ ì•Šìœ¼ë©° ë“±ë¶„ì‚°ì„±ì„ ì¶©ì¡±í•œë‹¤ê³  í•´ì„í•œë‹¤.

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        columns (list | str | None, optional): ê²€ì • ëŒ€ìƒ ì»¬ëŸ¼ëª….
            - None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰.
            - ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸: ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ê²€ì • ìˆ˜í–‰.
            - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´: "A, B, C" í˜•ì‹ìœ¼ë¡œ ì»¬ëŸ¼ëª… ì§€ì • ê°€ëŠ¥.
            ê¸°ë³¸ê°’ì€ None.
        normal_dist (bool | None, optional): ë“±ë¶„ì‚°ì„± ê²€ì • ë°©ë²•.
            - True: Bartlett ê²€ì • (ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ, ëª¨ë“  í‘œë³¸ì´ ê°™ì€ í¬ê¸°ì¼ ë•Œ ê¶Œì¥)
            - False: Levene ê²€ì • (ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì„ ë•Œ ë” ê°•ê±´í•¨)
            - None: normal_test()ë¥¼ ì´ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ê·œì„±ì„ íŒë³„ í›„ ì ì ˆí•œ ê²€ì • ë°©ë²• ì„ íƒ.
              ëª¨ë“  ì»¬ëŸ¼ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë©´ Bartlett, í•˜ë‚˜ë¼ë„ ë”°ë¥´ì§€ ì•Šìœ¼ë©´ Levene ì‚¬ìš©.
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - method (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²• (Bartlett ë˜ëŠ” Levene)
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - is_equal_var (bool): ë“±ë¶„ì‚°ì„± ì¶©ì¡± ì—¬ë¶€ (p-value > 0.05)
            - n_columns (int): ê²€ì •ì— ì‚¬ìš©ëœ ì»¬ëŸ¼ ìˆ˜
            - columns (str): ê²€ì •ì— í¬í•¨ëœ ì»¬ëŸ¼ëª… (ì‰¼í‘œë¡œ êµ¬ë¶„)
            - normality_checked (bool): normal_distê°€ Noneì´ì—ˆëŠ”ì§€ ì—¬ë¶€ (ìë™ íŒë³„ ì‚¬ìš© ì—¬ë¶€)

    Raises:
        ValueError: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ 2ê°œ ë¯¸ë§Œì¼ ê²½ìš° (ê²€ì •ì— ìµœì†Œ 2ê°œ í•„ìš”).

    Examples:
        >>> from hossam.analysis import equal_var_test
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x': np.random.normal(0, 1, 100),
        ...     'y': np.random.normal(0, 1, 100),
        ...     'z': np.random.normal(0, 2, 100)
        ... })
        >>> # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ íŒë³„
        >>> result = equal_var_test(df)
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¦¬ìŠ¤íŠ¸)
        >>> result = equal_var_test(df, columns=['x', 'y'])
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¬¸ìì—´)
        >>> result = equal_var_test(df, columns='x, y')
        >>> # ëª…ì‹œì  ì§€ì •
        >>> result = equal_var_test(df, normal_dist=True)
    """
    # columnsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(columns, str):
        columns = [col.strip() for col in columns.split(',')]

    # ì»¬ëŸ¼ ì„ íƒ: ì§€ì •ëœ ì»¬ëŸ¼ ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
    if columns is None or len(columns) == 0:
        # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (bool ì œì™¸)
        numeric_df = data.select_dtypes(include=[np.number])
        numeric_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]
    else:
        # ì§€ì •ëœ ì»¬ëŸ¼ ì‚¬ìš©
        numeric_cols = columns

    if len(numeric_cols) < 2:
        raise ValueError(f"ë“±ë¶„ì‚°ì„± ê²€ì •ì—ëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(numeric_cols)}")

    # ê° ì»¬ëŸ¼ë³„ë¡œ NaNì„ ì œê±°í•˜ì—¬ í•„ë“œ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    fields = []
    for col in numeric_cols:
        col_data = data[col].dropna()
        if len(col_data) > 0:
            fields.append(col_data)

    if len(fields) < 2:
        raise ValueError("NaNì„ ì œê±°í•œ í›„ ìµœì†Œ 2ê°œì˜ ìœ íš¨í•œ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # normal_distê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ì •ê·œì„± íŒë³„
    normality_checked = False
    if normal_dist is None:
        normality_checked = True
        normality_result = normal_test(data[numeric_cols], method="n")
        # ëª¨ë“  ì»¬ëŸ¼ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸
        all_normal = normality_result["is_normal"].all()
        normal_dist = all_normal

    try:
        if normal_dist:
            method_name = "Bartlett"
            s, p = bartlett(*fields)
        else:
            method_name = "Levene"
            s, p = levene(*fields)

        result_df = DataFrame([{
            "method": method_name,
            "statistic": s,
            "p-value": p,
            "is_equal_var": p > 0.05,
            "n_columns": len(fields),
            "columns": ", ".join(numeric_cols[:len(fields)]),
            "normality_checked": normality_checked
        }])

        return result_df

    except Exception as e:
        # ê²€ì • ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ê¸°ë¡
        method_name = "Bartlett" if normal_dist else "Levene"
        result_df = DataFrame([{
            "method": method_name,
            "statistic": np.nan,
            "p-value": np.nan,
            "is_equal_var": False,
            "n_columns": len(fields),
            "columns": ", ".join(numeric_cols[:len(fields)]),
            "normality_checked": normality_checked
        }])
        return result_df


# ===================================================================
# ì¼í‘œë³¸ Tê²€ì •
# ===================================================================
def ttest_1samp(data, mean_value: float = 0.0) -> DataFrame:
    """ì—°ì†í˜• ë°ì´í„°ì— ëŒ€í•´ ì¼í‘œë³¸ t-ê²€ì •ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    ì¼í‘œë³¸ t-ê²€ì •ì€ í‘œë³¸ í‰ê· ì´ íŠ¹ì • ê°’(mean_value)ê³¼ ê°™ì€ì§€ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): ëª¨ì§‘ë‹¨ í‰ê·  = mean_value
    ëŒ€ë¦½ê°€ì„¤(H1): alternativeì— ë”°ë¼ ë‹¬ë¼ì§ (!=, <, >)

    Args:
        data (array-like): ê²€ì • ëŒ€ìƒ ì—°ì†í˜• ë°ì´í„° (ë¦¬ìŠ¤íŠ¸, Series, ndarray ë“±).
        mean_value (float, optional): ê·€ë¬´ê°€ì„¤ì˜ ê¸°ì¤€ê°’(ë¹„êµ ëŒ€ìƒ í‰ê· ê°’).
            ê¸°ë³¸ê°’ì€ 0.0.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥ (two-sided, less, greater)
            - statistic (float): t-í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€ (p-value > 0.05)
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€ (p-value <= 0.05)
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„ ë¬¸ìì—´

    Examples:
        >>> from hossam.hs_stats import ttest_1samp
        >>> import pandas as pd
        >>> import numpy as np
        >>> # ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì •
        >>> data = [5.1, 4.9, 5.3, 5.0, 4.8]
        >>> result = ttest_1samp(data, mean_value=5.0)
        >>> # Series ë°ì´í„°ë¡œ ê²€ì •
        >>> s = pd.Series(np.random.normal(5, 1, 100))
        >>> result = ttest_1samp(s, mean_value=5)
    """
    # ë°ì´í„°ë¥¼ Seriesë¡œ ë³€í™˜í•˜ê³  ì´ë¦„ ê°ì§€
    if isinstance(data, Series):
        col_data = data.dropna()
    else:
        col_data = Series(data).dropna()

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []

    # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶„ì‚°ì´ 0ì¸ ê²½ìš°
    if len(col_data) == 0 or col_data.std(ddof=1) == 0:
        for a in alternative:
            result.append({
                "alternative": a,
                "statistic": np.nan,
                "p-value": np.nan,
                "H0": False,
                "H1": False,
                "interpretation": f"ê²€ì • ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ë¶„ì‚°=0)"
            })
    else:
        for a in alternative:
            try:
                s, p = ttest_1samp(col_data, mean_value, alternative=a)

                itp = None

                if a == "two-sided":
                    itp = "Î¼ {0} {1}".format("==" if p > 0.05 else "!=", mean_value)
                elif a == "less":
                    itp = "Î¼ {0} {1}".format(">=" if p > 0.05 else "<", mean_value)
                else:
                    itp = "Î¼ {0} {1}".format("<=" if p > 0.05 else ">", mean_value)

                result.append({
                    "alternative": a,
                    "statistic": round(s, 3),
                    "p-value": round(p, 4),
                    "H0": p > 0.05,
                    "H1": p <= 0.05,
                    "interpretation": itp,
                })
            except Exception as e:
                result.append({
                    "alternative": a,
                    "statistic": np.nan,
                    "p-value": np.nan,
                    "H0": False,
                    "H1": False,
                    "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}"
                })

    rdf = DataFrame(result)
    rdf.set_index(["field", "alternative"], inplace=True)

    return rdf


# ===================================================================
# ë…ë¦½í‘œë³¸ t-ê²€ì • ë˜ëŠ” Welch's t-test
# ===================================================================
def ttest_ind(x, y, equal_var: bool | None = None) -> DataFrame:
    """ë‘ ë…ë¦½ ì§‘ë‹¨ì˜ í‰ê·  ì°¨ì´ë¥¼ ê²€ì •í•œë‹¤ (ë…ë¦½í‘œë³¸ t-ê²€ì • ë˜ëŠ” Welch's t-test).

    ë…ë¦½í‘œë³¸ t-ê²€ì •ì€ ë‘ ë…ë¦½ëœ ì§‘ë‹¨ì˜ í‰ê· ì´ ê°™ì€ì§€ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): Î¼1 = Î¼2 (ë‘ ì§‘ë‹¨ì˜ í‰ê· ì´ ê°™ë‹¤)

    Args:
        x (array-like): ì²« ë²ˆì§¸ ì§‘ë‹¨ì˜ ì—°ì†í˜• ë°ì´í„° (ë¦¬ìŠ¤íŠ¸, Series, ndarray ë“±).
        y (array-like): ë‘ ë²ˆì§¸ ì§‘ë‹¨ì˜ ì—°ì†í˜• ë°ì´í„° (ë¦¬ìŠ¤íŠ¸, Series, ndarray ë“±).
        equal_var (bool | None, optional): ë“±ë¶„ì‚°ì„± ê°€ì • ì—¬ë¶€.
            - True: ë…ë¦½í‘œë³¸ t-ê²€ì • (ë“±ë¶„ì‚° ê°€ì •)
            - False: Welch's t-test (ë“±ë¶„ì‚° ê°€ì •í•˜ì§€ ì•ŠìŒ, ë” ê°•ê±´í•¨)
            - None: equal_var_test()ë¡œ ìë™ íŒë³„
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - test (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥
            - statistic (float): t-í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„

    Examples:
        >>> from hossam.hs_stats import ttest_ind
        >>> import pandas as pd
        >>> import numpy as np
        >>> # ë¦¬ìŠ¤íŠ¸ë¡œ ê²€ì •
        >>> group1 = [5.1, 4.9, 5.3, 5.0, 4.8]
        >>> group2 = [5.5, 5.7, 5.4, 5.6, 5.8]
        >>> result = ttest_ind(group1, group2)
        >>> # Seriesë¡œ ê²€ì •
        >>> s1 = pd.Series(np.random.normal(5, 1, 100))
        >>> s2 = pd.Series(np.random.normal(5.5, 1, 100))
        >>> result = ttest_ind(s1, s2, equal_var=False)
    """
    # ë°ì´í„°ë¥¼ Seriesë¡œ ë³€í™˜
    if isinstance(x, Series):
        x_data = x.dropna()
    else:
        x_data = Series(x).dropna()

    if isinstance(y, Series):
        y_data = y.dropna()
    else:
        y_data = Series(y).dropna()

    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    if len(x_data) < 2 or len(y_data) < 2:
        raise ValueError(f"ê° ì§‘ë‹¨ì— ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. x: {len(x_data)}, y: {len(y_data)}")

    # equal_varê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ë“±ë¶„ì‚°ì„± íŒë³„
    var_checked = False
    if equal_var is None:
        var_checked = True
        # ë‘ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë“±ë¶„ì‚°ì„± ê²€ì •
        temp_df = DataFrame({'x': x_data, 'y': y_data})
        var_result = equal_var_test(temp_df)
        equal_var = var_result["is_equal_var"].iloc[0]

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []
    fmt: str = "Î¼(x) {0} Î¼(y)"

    for a in alternative:
        try:
            s, p = scipy_ttest_ind(x_data, y_data, equal_var=equal_var, alternative=a)
            n = "t-test_ind" if equal_var else "Welch's t-test"

            # ê²€ì • ê²°ê³¼ í•´ì„
            itp = None

            if a == "two-sided":
                itp = fmt.format("==" if p > 0.05 else "!=")
            elif a == "less":
                itp = fmt.format(">=" if p > 0.05 else "<")
            else:
                itp = fmt.format("<=" if p > 0.05 else ">")

            result.append({
                "test": n,
                "alternative": a,
                "statistic": round(s, 3),
                "p-value": round(p, 4),
                "H0": p > 0.05,
                "H1": p <= 0.05,
                "interpretation": itp,
                "equal_var_checked": var_checked
            })
        except Exception as e:
            result.append({
                "test": "t-test_ind" if equal_var else "Welch's t-test",
                "alternative": a,
                "statistic": np.nan,
                "p-value": np.nan,
                "H0": False,
                "H1": False,
                "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}",
                "equal_var_checked": var_checked
            })

    rdf = DataFrame(result)
    rdf.set_index(["test", "alternative"], inplace=True)
    return rdf


# ===================================================================
# ëŒ€ì‘í‘œë³¸ t-ê²€ì • ë˜ëŠ” Wilcoxon test
# ===================================================================
def ttest_rel(x, y, parametric: bool | None = None) -> DataFrame:
    """ëŒ€ì‘í‘œë³¸ t-ê²€ì • ë˜ëŠ” Wilcoxon signed-rank testë¥¼ ìˆ˜í–‰í•œë‹¤.

    ëŒ€ì‘í‘œë³¸ t-ê²€ì •ì€ ë™ì¼ ê°œì²´ì—ì„œ ì¸¡ì •ëœ ë‘ ì‹œì ì˜ í‰ê·  ì°¨ì´ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): ë‘ ì‹œì ì˜ í‰ê·  ì°¨ì´ê°€ 0ì´ë‹¤.

    Args:
        x (array-like): ì²« ë²ˆì§¸ ì¸¡ì •ê°’ì˜ ì—°ì†í˜• ë°ì´í„° (ë¦¬ìŠ¤íŠ¸, Series, ndarray ë“±).
        y (array-like): ë‘ ë²ˆì§¸ ì¸¡ì •ê°’ì˜ ì—°ì†í˜• ë°ì´í„° (ë¦¬ìŠ¤íŠ¸, Series, ndarray ë“±).
        parametric (bool | None, optional): ì •ê·œì„± ê°€ì • ì—¬ë¶€.
            - True: ëŒ€ì‘í‘œë³¸ t-ê²€ì • (ì°¨ì´ì˜ ì •ê·œë¶„í¬ ê°€ì •)
            - False: Wilcoxon signed-rank test (ë¹„ëª¨ìˆ˜ ê²€ì •, ë” ê°•ê±´í•¨)
            - None: ì°¨ì´ì˜ ì •ê·œì„±ì„ ìë™ìœ¼ë¡œ ê²€ì •í•˜ì—¬ íŒë³„
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - test (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„

    Examples:
        >>> from hossam.hs_stats import ttest_rel
        >>> import pandas as pd
        >>> import numpy as np
        >>> # ë¦¬ìŠ¤íŠ¸ë¡œ ê²€ì •
        >>> before = [5.1, 4.9, 5.3, 5.0, 4.8]
        >>> after = [5.5, 5.2, 5.7, 5.3, 5.1]
        >>> result = ttest_rel(before, after)
        >>> # Seriesë¡œ ê²€ì •
        >>> s1 = pd.Series(np.random.normal(5, 1, 100))
        >>> s2 = pd.Series(np.random.normal(5.3, 1, 100))
        >>> result = ttest_rel(s1, s2, parametric=False)
    """
    # ë°ì´í„°ë¥¼ Seriesë¡œ ë³€í™˜
    if isinstance(x, Series):
        x_data = x.dropna()
    else:
        x_data = Series(x).dropna()

    if isinstance(y, Series):
        y_data = y.dropna()
    else:
        y_data = Series(y).dropna()

    # ëŒ€ì‘í‘œë³¸ì´ë¯€ë¡œ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•¨
    if len(x_data) != len(y_data):
        raise ValueError(f"ëŒ€ì‘í‘œë³¸ì€ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•©ë‹ˆë‹¤. x: {len(x_data)}, y: {len(y_data)}")

    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    if len(x_data) < 2:
        raise ValueError(f"ìµœì†Œ 2ê°œ ì´ìƒì˜ ëŒ€ì‘ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(x_data)}")

    # parametricì´ Noneì´ë©´ ì°¨ì´ì˜ ì •ê·œì„±ì„ ìë™ìœ¼ë¡œ ê²€ì •
    var_checked = False
    if parametric is None:
        var_checked = True
        # ëŒ€ì‘í‘œë³¸ì˜ ì°¨ì´ ê³„ì‚° ë° ì •ê·œì„± ê²€ì •
        diff = x_data - y_data
        try:
            _, p_normal = shapiro(diff)  # í‘œë³¸ í¬ê¸° 5000 ì´í•˜ì¼ ë•Œ ê¶Œì¥
            parametric = p_normal > 0.05  # p > 0.05ë©´ ì •ê·œë¶„í¬ ë”°ë¦„
        except Exception:
            # shapiro ì‹¤íŒ¨ ì‹œ normaltest ì‚¬ìš©
            try:
                _, p_normal = normaltest(diff)
                parametric = p_normal > 0.05
            except Exception:
                # ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹„ëª¨ìˆ˜ ê²€ì • ì‚¬ìš©
                parametric = False

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []
    fmt: str = "Î¼(x) {0} Î¼(y)"

    for a in alternative:
        try:
            if parametric:
                s, p = ttest_rel(x_data, y_data, alternative=a)
                n = "t-test_paired"
            else:
                # Wilcoxon signed-rank test (ëŒ€ì‘í‘œë³¸ìš© ë¹„ëª¨ìˆ˜ ê²€ì •)
                s, p = wilcoxon(x_data, y_data, alternative=a)
                n = "Wilcoxon signed-rank"

            itp = None

            if a == "two-sided":
                itp = fmt.format("==" if p > 0.05 else "!=")
            elif a == "less":
                itp = fmt.format(">=" if p > 0.05 else "<")
            else:
                itp = fmt.format("<=" if p > 0.05 else ">")

            result.append({
                "test": n,
                "alternative": a,
                "statistic": round(s, 3) if not np.isnan(s) else s,
                "p-value": round(p, 4) if not np.isnan(p) else p,
                "H0": p > 0.05,
                "H1": p <= 0.05,
                "interpretation": itp,
                "normality_checked": var_checked
            })
        except Exception as e:
            result.append({
                "test": "t-test_paired" if parametric else "Wilcoxon signed-rank",
                "alternative": a,
                "statistic": np.nan,
                "p-value": np.nan,
                "H0": False,
                "H1": False,
                "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}",
                "normality_checked": var_checked
            })

    rdf = DataFrame(result)
    rdf.set_index(["test", "alternative"], inplace=True)

    return rdf


# ===================================================================
# ë…ë¦½ë³€ìˆ˜ê°„ ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
# ===================================================================
def vif_filter(
    data: DataFrame,
    yname: str = None,
    ignore: list | None = None,
    threshold: float = 10.0,
    verbose: bool = False,
) -> DataFrame:
    """ë…ë¦½ë³€ìˆ˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„±ì„ ê²€ì‚¬í•˜ì—¬ VIFê°€ threshold ì´ìƒì¸ ë³€ìˆ˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•œë‹¤.

    Args:
        data (DataFrame): ë°ì´í„°í”„ë ˆì„
        yname (str, optional): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª…. Defaults to None.
        ignore (list | None, optional): ì œì™¸í•  ì»¬ëŸ¼ ëª©ë¡. Defaults to None.
        threshold (float, optional): VIF ì„ê³„ê°’. Defaults to 10.0.
        verbose (bool, optional): Trueì¼ ê²½ìš° ê° ë‹¨ê³„ì˜ VIFë¥¼ ì¶œë ¥í•œë‹¤. Defaults to False.

    Returns:
        DataFrame: VIFê°€ threshold ì´í•˜ì¸ ë³€ìˆ˜ë§Œ ë‚¨ì€ ë°ì´í„°í”„ë ˆì„ (ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ ìœ ì§€)

    Examples:
        ê¸°ë³¸ ì‚¬ìš© ì˜ˆ:

        >>> from hossam.analysis import vif_filter
        >>> filtered = hs_vif_filter(df, yname="target", ignore=["id"], threshold=10.0)
        >>> filtered.head()
    """

    df = data.copy()

    # y ë¶„ë¦¬ (ìˆë‹¤ë©´)
    y = None
    if yname and yname in df.columns:
        y = df[yname]
        df = df.drop(columns=[yname])

    # ì œì™¸í•  ëª©ë¡ ì •ë¦¬
    ignore = ignore or []
    ignore_cols_present = [c for c in ignore if c in df.columns]

    # VIF ëŒ€ìƒ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (boolì€ ì—°ì†í˜•ì´ ì•„ë‹ˆë¯€ë¡œ ì œì™¸)
    numeric_df = df.select_dtypes(include=[np.number])
    numeric_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]

    # VIF ëŒ€ìƒ X êµ¬ì„± (ìˆ˜ì¹˜í˜•ì—ì„œ ì œì™¸ ëª©ë¡ ì œê±°)
    X = df[numeric_cols]
    if ignore_cols_present:
        X = X.drop(columns=ignore_cols_present, errors="ignore")

    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if X.shape[1] == 0:
        result = data.copy()
        return result

    def _compute_vifs(X_: DataFrame) -> dict:
        # NA ì œê±° í›„ ìƒìˆ˜í•­ ì¶”ê°€
        X_clean = X_.dropna()
        if X_clean.shape[0] == 0:
            # ë°ì´í„°ê°€ ëª¨ë‘ NAì¸ ê²½ìš° VIF ê³„ì‚° ë¶ˆê°€: NaN ë°˜í™˜
            return {col: np.nan for col in X_.columns}
        if X_clean.shape[1] == 1:
            # ë‹¨ì¼ ì˜ˆì¸¡ë³€ìˆ˜ì˜ ê²½ìš° ë‹¤ë¥¸ ì„¤ëª…ë³€ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ VIFëŠ” 1ë¡œ ê°„ì£¼
            return {col: 1.0 for col in X_clean.columns}
        exog = sm.add_constant(X_clean, prepend=True)
        vifs = {}
        for i, col in enumerate(X_clean.columns, start=0):
            # exogì˜ ì²« ì—´ì€ ìƒìˆ˜í•­ì´ë¯€ë¡œ ë³€ìˆ˜ ì¸ë±ìŠ¤ëŠ” +1
            try:
                vifs[col] = float(variance_inflation_factor(exog.values, i + 1))
            except Exception:
                # ê³„ì‚° ì‹¤íŒ¨ ì‹œ ë¬´í•œëŒ€ë¡œ ì²˜ë¦¬í•˜ì—¬ ìš°ì„  ì œê±° ëŒ€ìƒìœ¼ë¡œ
                vifs[col] = float("inf")
        return vifs

    # ë°˜ë³µ ì œê±° ë£¨í”„
    while True:
        if X.shape[1] == 0:
            break
        vifs = _compute_vifs(X)
        if verbose:
            print(vifs)
        # ëª¨ë“  ë³€ìˆ˜ê°€ ì„ê³„ê°’ ì´í•˜ì´ë©´ ì¢…ë£Œ
        max_key = max(vifs, key=lambda k: (vifs[k] if not np.isnan(vifs[k]) else -np.inf))
        max_vif = vifs[max_key]
        if np.isnan(max_vif) or max_vif <= threshold:
            break
        # ê°€ì¥ í° VIF ë³€ìˆ˜ ì œê±°
        X = X.drop(columns=[max_key])

    # ì¶œë ¥ ì˜µì…˜ì´ Falseì¼ ê²½ìš° ìµœì¢… ê°’ë§Œ ì¶œë ¥
    if not verbose:
        final_vifs = _compute_vifs(X) if X.shape[1] > 0 else {}
        print(final_vifs)

    # ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ ìœ ì§€í•˜ë©° ì œê±°ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì œì™¸
    kept_numeric_cols = list(X.columns)
    removed_numeric_cols = [c for c in numeric_cols if c not in kept_numeric_cols]
    result = data.drop(columns=removed_numeric_cols, errors="ignore")

    return result

# -------------------------------------------------------------------
# Backward-compatibility alias for describe (typo support)
# ì˜¤íƒ€(discribe)ë¡œ ì‚¬ìš©ëœ ê²½ìš°ë¥¼ ì§€ì›í•˜ì—¬ í˜¼ë€ì„ ì¤„ì…ë‹ˆë‹¤.
def discribe(data: DataFrame, *fields: str, columns: list = None):
    """Deprecated alias for describe.

    ë‚´ë¶€ì ìœ¼ë¡œ describeë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return describe(data, *fields, columns=columns)


# ===================================================================
# x, y ë°ì´í„°ì— ëŒ€í•œ ì¶”ì„¸ì„ ì„ êµ¬í•œë‹¤.
# ===================================================================
def trend(x: any, y: any, degree: int = 1, value_count: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """x, y ë°ì´í„°ì— ëŒ€í•œ ì¶”ì„¸ì„ ì„ êµ¬í•œë‹¤.

    Args:
        x (_type_): ì‚°ì ë„ ê·¸ë˜í”„ì— ëŒ€í•œ x ë°ì´í„°
        y (_type_): ì‚°ì ë„ ê·¸ë˜í”„ì— ëŒ€í•œ y ë°ì´í„°
        degree (int, optional): ì¶”ì„¸ì„  ë°©ì •ì‹ì˜ ì°¨ìˆ˜. Defaults to 1.
        value_count (int, optional): x ë°ì´í„°ì˜ ë²”ìœ„ ì•ˆì—ì„œ ê°„ê²© ìˆ˜. Defaults to 100.

    Returns:
        tuple: (v_trend, t_trend)

    Examples:
        2ì°¨ ë‹¤í•­ íšŒê·€ ì¶”ì„¸ì„ :

        >>> from hossam.analysis import trend
        >>> vx, vy = hs_trend(x, y, degree=2, value_count=200)
        >>> len(vx), len(vy)
        (200, 200)
    """
    # [ a, b, c ] ==> ax^2 + bx + c
    x_arr = np.asarray(x)
    y_arr = np.asarray(y)

    if x_arr.ndim == 0 or y_arr.ndim == 0:
        raise ValueError("x, yëŠ” 1ì°¨ì› ì´ìƒì˜ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    coeff = np.polyfit(x_arr, y_arr, degree)

    minx = np.min(x_arr)
    maxx = np.max(x_arr)
    v_trend = np.linspace(minx, maxx, value_count)

    # np.polyval ì‚¬ìš©ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì¶”ì„¸ì„  ê³„ì‚°
    t_trend = np.polyval(coeff, v_trend)

    return (v_trend, t_trend)


# ===================================================================
# ì„ í˜•íšŒê·€ ìš”ì•½ ë¦¬í¬íŠ¸
# ===================================================================
def ols_report(fit, data, full=False, alpha=0.05):
    """ì„ í˜•íšŒê·€ ì í•© ê²°ê³¼ë¥¼ ìš”ì•½ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    Args:
        fit: statsmodels OLS ë“± ì„ í˜•íšŒê·€ ê²°ê³¼ ê°ì²´ (`fit.summary()`ë¥¼ ì§€ì›í•´ì•¼ í•¨).
        data: ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ DataFrame.
        full: Trueì´ë©´ 6ê°œ ê°’ ë°˜í™˜, Falseì´ë©´ íšŒê·€ê³„ìˆ˜ í…Œì´ë¸”(rdf)ë§Œ ë°˜í™˜. ê¸°ë³¸ê°’ True.
        alpha: ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        tuple: full=Trueì¼ ë•Œ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•œë‹¤.
            - ì„±ëŠ¥ ì§€í‘œ í‘œ (`pdf`, DataFrame): R, RÂ², Adj. RÂ², F, p-value, Durbin-Watson.
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame): ë³€ìˆ˜ë³„ B, í‘œì¤€ì˜¤ì°¨, Beta, t, p-value, significant, ê³µì°¨, VIF.
            - ì í•©ë„ ìš”ì•½ (`result_report`, str): R, RÂ², F, p-value, Durbin-Watson ë“± í•µì‹¬ ì§€í‘œ ë¬¸ìì—´.
            - ëª¨í˜• ë³´ê³  ë¬¸ì¥ (`model_report`, str): F-ê²€ì • ìœ ì˜ì„±ì— ê¸°ë°˜í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - ë³€ìˆ˜ë³„ ë³´ê³  ë¦¬ìŠ¤íŠ¸ (`variable_reports`, list[str]): ê° ì˜ˆì¸¡ë³€ìˆ˜ì— ëŒ€í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - íšŒê·€ì‹ ë¬¸ìì—´ (`equation_text`, str): ìƒìˆ˜í•­ê³¼ ê³„ìˆ˜ë¥¼ í¬í•¨í•œ íšŒê·€ì‹ í‘œí˜„.

        full=Falseì¼ ë•Œ:
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame)

    Examples:
        >>> import statsmodels.api as sm
        >>> y = data['target']
        >>> X = sm.add_constant(data[['x1', 'x2']])
        >>> fit = sm.OLS(y, X).fit()
        >>> # ì „ì²´ ë¦¬í¬íŠ¸
        >>> pdf, rdf, result_report, model_report, variable_reports, eq = ols_report(fit, data)
        >>> # ê°„ë‹¨í•œ ë²„ì „ (íšŒê·€ê³„ìˆ˜ í…Œì´ë¸”ë§Œ)
        >>> rdf = ols_report(fit, data, full=False)
    """

    # summary2() ê²°ê³¼ì—ì„œ ì‹¤ì œ íšŒê·€ê³„ìˆ˜ DataFrame ì¶”ì¶œ
    summary_obj = fit.summary2()
    tbl = summary_obj.tables[1]  # íšŒê·€ê³„ìˆ˜ í…Œì´ë¸”ì€ tables[1]ì— ìœ„ì¹˜

    # ì¢…ì†ë³€ìˆ˜ ì´ë¦„
    yname = fit.model.endog_names

    # ë…ë¦½ë³€ìˆ˜ ì´ë¦„(ìƒìˆ˜í•­ ì œì™¸)
    xnames = [n for n in fit.model.exog_names if n != "const"]

    # ë…ë¦½ë³€ìˆ˜ ë¶€ë¶„ ë°ì´í„° (VIF ê³„ì‚°ìš©)
    indi_df = data.filter(xnames)

    # ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ë¥¼ ëˆ„ì 
    variables = []

    # VIF ê³„ì‚° (ìƒìˆ˜í•­ í¬í•¨ ì„¤ê³„í–‰ë ¬ ì‚¬ìš©)
    vif_dict = {}
    indi_df_const = sm.add_constant(indi_df, has_constant="add")
    for i, col in enumerate(indi_df.columns, start=1):  # ìƒìˆ˜í•­ì´ 0ì´ë¯€ë¡œ 1ë¶€í„° ì‹œì‘
        try:
            with np.errstate(divide='ignore', invalid='ignore'):
                vif_value = variance_inflation_factor(indi_df_const.values, i)
                # infë‚˜ ë§¤ìš° í° ê°’ ì²˜ë¦¬
                if np.isinf(vif_value) or vif_value > 1e10:
                    vif_dict[col] = np.inf
                else:
                    vif_dict[col] = vif_value
        except:
            vif_dict[col] = np.inf

    for idx, row in tbl.iterrows():
        name = idx
        if name not in xnames:
            continue

        b = float(row['Coef.'])
        se = float(row['Std.Err.'])
        t = float(row['t'])
        p = float(row['P>|t|'])

        # í‘œì¤€í™” íšŒê·€ê³„ìˆ˜(Î²) ê³„ì‚°
        beta = b * (data[name].std(ddof=1) / data[yname].std(ddof=1))

        # VIF ê°’
        vif = vif_dict.get(name, np.nan)

        # ìœ ì˜í™•ë¥ ê³¼ ë³„í‘œ í‘œì‹œ
        stars = "***" if p < 0.001 else "**" if p < 0.01 else "*" if p < 0.05 else ""

        # í•œ ë³€ìˆ˜ì— ëŒ€í•œ ë³´ê³  ì •ë³´ ì¶”ê°€
        variables.append(
            {
                "ì¢…ì†ë³€ìˆ˜": yname,  # ì¢…ì†ë³€ìˆ˜ ì´ë¦„
                "ë…ë¦½ë³€ìˆ˜": name,  # ë…ë¦½ë³€ìˆ˜ ì´ë¦„
                "B": f"{b:.6f}",  # ë¹„í‘œì¤€í™” íšŒê·€ê³„ìˆ˜(B)
                "í‘œì¤€ì˜¤ì°¨": f"{se:.6f}",  # ê³„ìˆ˜ í‘œì¤€ì˜¤ì°¨
                "Beta": beta,  # í‘œì¤€í™” íšŒê·€ê³„ìˆ˜(Î²)
                "t": f"{t:.3f}{stars}",  # t-í†µê³„ëŸ‰(+ë³„í‘œ)
                "p-value": p,  # ê³„ìˆ˜ ìœ ì˜í™•ë¥ 
                "significant": p <= alpha,  # ìœ ì˜ì„± ì—¬ë¶€ (boolean)
                "ê³µì°¨": 1 / vif,  # ê³µì°¨(Tolerance = 1/VIF)
                "vif": vif,  # ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜
            }
        )

    rdf = DataFrame(variables)

    # summary í‘œì—ì„œ ì í•©ë„ ì •ë³´ë¥¼ key-valueë¡œ ì¶”ì¶œ
    result_dict = {}
    summary_main = fit.summary()
    for i in [0, 2]:
        for item in summary_main.tables[i].data:
            n = len(item)
            for i in range(0, n, 2):
                key = item[i].strip()[:-1]
                value = item[i + 1].strip()
                if not key or not value:
                    continue
                result_dict[key] = value

    # ì í•©ë„ ë³´ê³  ë¬¸ìì—´ êµ¬ì„±
    result_report = f"ğ‘…({result_dict['R-squared']}), ğ‘…^2({result_dict['Adj. R-squared']}), ğ¹({result_dict['F-statistic']}), ìœ ì˜í™•ë¥ ({result_dict['Prob (F-statistic)']}), Durbin-Watson({result_dict['Durbin-Watson']})"

    # ëª¨í˜• ë³´ê³  ë¬¸ì¥ êµ¬ì„±
    tpl = "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %s, p %s 0.05)."
    model_report = tpl % (
        rdf["ì¢…ì†ë³€ìˆ˜"][0],
        ",".join(list(rdf["ë…ë¦½ë³€ìˆ˜"])),
        (
            "ìœ ì˜í•˜ë‹¤"
            if float(result_dict["Prob (F-statistic)"]) <= 0.05
            else "ìœ ì˜í•˜ì§€ ì•Šë‹¤"
        ),
        result_dict["Df Model"],
        result_dict["Df Residuals"],
        result_dict["F-statistic"],
        "<=" if float(result_dict["Prob (F-statistic)"]) <= 0.05 else ">",
    )

    # ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    variable_reports = []
    s = "%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %s(p %s 0.05)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

    for i in rdf.index:
        row = rdf.iloc[i]
        variable_reports.append(
            s
            % (
                row["ë…ë¦½ë³€ìˆ˜"],
                row["B"],
                "<=" if float(row["p-value"]) < 0.05 else ">",
                row["ì¢…ì†ë³€ìˆ˜"],
                "ìœ ì˜ë¯¸í•œ" if float(row["p-value"]) < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šì€",
            )
        )

    # -----------------------------
    # íšŒê·€ì‹ ìë™ ì¶œë ¥
    # -----------------------------
    intercept = fit.params["const"]
    terms = []

    for name in xnames:
        coef = fit.params[name]
        sign = "+" if coef >= 0 else "-"
        terms.append(f" {sign} {abs(coef):.3f}Â·{name}")

    equation_text = f"{yname} = {intercept:.3f}" + "".join(terms)

    # ì„±ëŠ¥ ì§€í‘œ í‘œ ìƒì„± (pdf)
    pdf = DataFrame(
        {
            "R": [float(result_dict.get('R-squared', np.nan))],
            "RÂ²": [float(result_dict.get('Adj. R-squared', np.nan))],
            "F": [float(result_dict.get('F-statistic', np.nan))],
            "p-value": [float(result_dict.get('Prob (F-statistic)', np.nan))],
            "Durbin-Watson": [float(result_dict.get('Durbin-Watson', np.nan))],
        }
    )

    if full:
        return pdf, rdf, result_report, model_report, variable_reports, equation_text
    else:
        return pdf


# ===================================================================
# ì„ í˜•íšŒê·€
# ===================================================================
def ols(df: DataFrame, yname: str, report=False):
    """ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì í•© ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    OLS(Ordinary Least Squares) ì„ í˜•íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œë‹¤.
    í•„ìš”ì‹œ ìƒì„¸í•œ í†µê³„ ë³´ê³ ì„œë¥¼ í•¨ê»˜ ì œê³µí•œë‹¤.

    Args:
        df (DataFrame): ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        yname (str): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª….
        report: ë¦¬í¬íŠ¸ ëª¨ë“œ ì„¤ì •. ë‹¤ìŒ ê°’ ì¤‘ í•˜ë‚˜:
            - False (ê¸°ë³¸ê°’): ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©. fit ê°ì²´ë§Œ ë°˜í™˜.
            - 1 ë˜ëŠ” 'summary': ìš”ì•½ ë¦¬í¬íŠ¸ ë°˜í™˜ (full=False).
            - 2 ë˜ëŠ” 'full': í’€ ë¦¬í¬íŠ¸ ë°˜í™˜ (full=True).
            - True: í’€ ë¦¬í¬íŠ¸ ë°˜í™˜ (2ì™€ ë™ì¼).

    Returns:
        statsmodels.regression.linear_model.RegressionResultsWrapper: report=Falseì¼ ë•Œ.
            ì„ í˜•íšŒê·€ ì í•© ê²°ê³¼ ê°ì²´. fit.summary()ë¡œ ìƒì„¸ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥.

        tuple (6ê°œ): report=1 ë˜ëŠ” 'summary'ì¼ ë•Œ.
            (fit, rdf, result_report, model_report, variable_reports, equation_text) í˜•íƒœë¡œ (pdf ì œì™¸).

        tuple (7ê°œ): report=2, 'full' ë˜ëŠ” Trueì¼ ë•Œ.
            (fit, pdf, rdf, result_report, model_report, variable_reports, equation_text) í˜•íƒœë¡œ:
            - fit: ì„ í˜•íšŒê·€ ì í•© ê²°ê³¼ ê°ì²´
            - pdf: ì„±ëŠ¥ ì§€í‘œ í‘œ (DataFrame): R, RÂ², F, p-value, Durbin-Watson
            - rdf: íšŒê·€ê³„ìˆ˜ í‘œ (DataFrame)
            - result_report: ì í•©ë„ ìš”ì•½ (str)
            - model_report: ëª¨í˜• ë³´ê³  ë¬¸ì¥ (str)
            - variable_reports: ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (list[str])
            - equation_text: íšŒê·€ì‹ ë¬¸ìì—´ (str)

    Examples:
        >>> from hossam.analysis import linear
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'target': np.random.normal(100, 10, 100),
        ...     'x1': np.random.normal(0, 1, 100),
        ...     'x2': np.random.normal(0, 1, 100)
        ... })
        >>> # ì í•© ê²°ê³¼ë§Œ ë°˜í™˜
        >>> fit = hs_ols(df, 'target')
        >>> print(fit.summary())

        >>> # ìš”ì•½ ë¦¬í¬íŠ¸ ë°˜í™˜
        >>> fit, result, features = hs_ols(df, 'target', report=1)

        >>> # í’€ ë¦¬í¬íŠ¸ ë°˜í™˜
        >>> fit, pdf, rdf, result_report, model_report, var_reports, eq = hs_ols(df, 'target', report=2)
    """
    x = df.drop(yname, axis=1)
    y = df[yname]

    X_const = sm.add_constant(x)
    linear_model = sm.OLS(y, X_const)
    linear_fit = linear_model.fit()

    # report íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ì²˜ë¦¬
    if not report or report is False:
        # ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©
        return linear_fit
    elif report == 1 or report == 'summary':
        # ìš”ì•½ ë¦¬í¬íŠ¸ (full=False)
        pdf, rdf, result_report, model_report, variable_reports, equation_text = ols_report(linear_fit, df, full=True, alpha=0.05)
        return linear_fit, pdf, rdf
    elif report == 2 or report == 'full' or report is True:
        # í’€ ë¦¬í¬íŠ¸ (full=True)
        pdf, rdf, result_report, model_report, variable_reports, equation_text = ols_report(linear_fit, df, full=True, alpha=0.05)
        return linear_fit, pdf, rdf, result_report, model_report, variable_reports, equation_text
    else:
        # ê¸°ë³¸ê°’: ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©
        return linear_fit


# ===================================================================
# ë¡œì§€ìŠ¤í‹± íšŒê·€ ìš”ì•½ ë¦¬í¬íŠ¸
# ===================================================================
def logit_report(fit, data, threshold=0.5, full=False, alpha=0.05):
    """ë¡œì§€ìŠ¤í‹± íšŒê·€ ì í•© ê²°ê³¼ë¥¼ ìƒì„¸ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    Args:
        fit: statsmodels Logit ê²°ê³¼ ê°ì²´ (`fit.summary()`ì™€ ì˜ˆì¸¡ í™•ë¥ ì„ ì§€ì›í•´ì•¼ í•¨).
        data: ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ DataFrame.
        threshold: ì˜ˆì¸¡ í™•ë¥ ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜í•  ì„ê³„ê°’. ê¸°ë³¸ê°’ 0.5.
        full: Trueì´ë©´ 6ê°œ ê°’ ë°˜í™˜, Falseì´ë©´ ì£¼ìš” 2ê°œ(cdf, rdf)ë§Œ ë°˜í™˜. ê¸°ë³¸ê°’ False.
        alpha: ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        tuple: full=Trueì¼ ë•Œ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•œë‹¤.
            - ì„±ëŠ¥ ì§€í‘œ í‘œ (`cdf`, DataFrame): McFadden Pseudo RÂ², Accuracy, Precision, Recall, FPR, TNR, AUC, F1.
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame): B, í‘œì¤€ì˜¤ì°¨, z, p-value, significant, OR, 95% CI, VIF ë“±.
            - ì í•©ë„ ë° ì˜ˆì¸¡ ì„±ëŠ¥ ìš”ì•½ (`result_report`, str): Pseudo RÂ², LLR Ï‡Â², p-value, Accuracy, AUC.
            - ëª¨í˜• ë³´ê³  ë¬¸ì¥ (`model_report`, str): LLR p-valueì— ê¸°ë°˜í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - ë³€ìˆ˜ë³„ ë³´ê³  ë¦¬ìŠ¤íŠ¸ (`variable_reports`, list[str]): ê° ì˜ˆì¸¡ë³€ìˆ˜ì˜ ì˜¤ì¦ˆë¹„ í•´ì„ ë¬¸ì¥.
            - í˜¼ë™í–‰ë ¬ (`cm`, ndarray): ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œê°’ì˜ í˜¼ë™í–‰ë ¬ [[TN, FP], [FN, TP]].

        full=Falseì¼ ë•Œ:
            - ì„±ëŠ¥ ì§€í‘œ í‘œ (`cdf`, DataFrame)
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame)

    Examples:
        >>> import statsmodels.api as sm
        >>> y = data['target']
        >>> X = sm.add_constant(data[['x1', 'x2']])
        >>> fit = sm.Logit(y, X).fit(disp=0)
        >>> # ì „ì²´ ë¦¬í¬íŠ¸
        >>> cdf, rdf, result_report, model_report, variable_reports, cm = hs_logit_report(fit, data)
        >>> # ê°„ë‹¨í•œ ë²„ì „ (ì£¼ìš” í…Œì´ë¸”ë§Œ)
        >>> cdf, rdf = hs_logit_report(fit, data, full=False)
    """

    # -----------------------------
    # ì„±ëŠ¥í‰ê°€ì§€í‘œ
    # -----------------------------
    yname = fit.model.endog_names
    y_true = data[yname]
    y_pred = fit.predict(fit.model.exog)
    y_pred_fix = (y_pred >= threshold).astype(int)

    # í˜¼ë™í–‰ë ¬
    cm = confusion_matrix(y_true, y_pred_fix)
    tn, fp, fn, tp = cm.ravel()

    acc = accuracy_score(y_true, y_pred_fix)  # ì •í™•ë„
    pre = precision_score(y_true, y_pred_fix)  # ì •ë°€ë„
    tpr = recall_score(y_true, y_pred_fix)  # ì¬í˜„ìœ¨
    fpr = fp / (fp + tn)  # ìœ„ì–‘ì„±ìœ¨
    tnr = 1 - fpr  # íŠ¹ì´ì„±
    f1 = f1_score(y_true, y_pred_fix)  # f1-score
    ras = roc_auc_score(y_true, y_pred)  # auc score

    cdf = DataFrame(
        {
            "ì„¤ëª…ë ¥(P-Rsqe)": [fit.prsquared],
            "ì •í™•ë„(Accuracy)": [acc],
            "ì •ë°€ë„(Precision)": [pre],
            "ì¬í˜„ìœ¨(Recall,TPR)": [tpr],
            "ìœ„ì–‘ì„±ìœ¨(Fallout,FPR)": [fpr],
            "íŠ¹ì´ì„±(Specif city,TNR)": [tnr],
            "RAS(auc score)": [ras],
            "F1": [f1],
        }
    )

    # -----------------------------
    # íšŒê·€ê³„ìˆ˜ í‘œ êµ¬ì„± (OR ì¤‘ì‹¬)
    # -----------------------------
    tbl = fit.summary2().tables[1]

    # ë…ë¦½ë³€ìˆ˜ ì´ë¦„(ìƒìˆ˜í•­ ì œì™¸)
    xnames = [n for n in fit.model.exog_names if n != "const"]

    # ë…ë¦½ë³€ìˆ˜
    x = data[xnames]

    variables = []

    # VIF ê³„ì‚° (ìƒìˆ˜í•­ í¬í•¨ ì„¤ê³„í–‰ë ¬ ì‚¬ìš©)
    vif_dict = {}
    x_const = sm.add_constant(x, has_constant="add")
    for i, col in enumerate(x.columns, start=1):  # ìƒìˆ˜í•­ì´ 0ì´ë¯€ë¡œ 1ë¶€í„° ì‹œì‘
        vif_dict[col] = variance_inflation_factor(x_const.values, i)

    for idx, row in tbl.iterrows():
        name = idx
        if name not in xnames:
            continue

        beta = float(row['Coef.'])
        se = float(row['Std.Err.'])
        z = float(row['z'])
        p = float(row['P>|z|'])

        or_val = np.exp(beta)
        ci_low = np.exp(beta - 1.96 * se)
        ci_high = np.exp(beta + 1.96 * se)

        stars = "***" if p < 0.001 else "**" if p < 0.01 else "*" if p < 0.05 else ""

        variables.append(
            {
                "ì¢…ì†ë³€ìˆ˜": yname,
                "ë…ë¦½ë³€ìˆ˜": name,
                "B(Î²)": beta,
                "í‘œì¤€ì˜¤ì°¨": se,
                "z": f"{z:.3f}{stars}",
                "p-value": p,
                "significant": p <= alpha,
                "OR": or_val,
                "CI_lower": ci_low,
                "CI_upper": ci_high,
                "VIF": vif_dict.get(name, np.nan),
            }
        )

    rdf = DataFrame(variables)

    # ---------------------------------
    # ëª¨ë¸ ì í•©ë„ + ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ
    # ---------------------------------
    auc = roc_auc_score(y_true, y_pred)

    result_report = (
        f"Pseudo RÂ²(McFadden) = {fit.prsquared:.3f}, "
        f"LLR Ï‡Â²({int(fit.df_model)}) = {fit.llr:.3f}, "
        f"p-value = {fit.llr_pvalue:.4f}, "
        f"Accuracy = {acc:.3f}, "
        f"AUC = {auc:.3f}"
    )

    # -----------------------------
    # ëª¨í˜• ë³´ê³  ë¬¸ì¥
    # -----------------------------
    tpl = (
        "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, "
        "ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(Ï‡Â²(%s) = %.3f, p %s 0.05)í•˜ì˜€ë‹¤."
    )

    model_report = tpl % (
        yname,
        ", ".join(xnames),
        "ìœ ì˜" if fit.llr_pvalue <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ",
        int(fit.df_model),
        fit.llr,
        "<=" if fit.llr_pvalue <= 0.05 else ">",
    )

    # -----------------------------
    # ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥
    # -----------------------------
    variable_reports = []

    s = (
        "%sì˜ ì˜¤ì¦ˆë¹„ëŠ” %.3f(p %s 0.05)ë¡œ, "
        "%s ë°œìƒ oddsì— %s ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."
    )

    for _, row in rdf.iterrows():
        variable_reports.append(
            s
            % (
                row["ë…ë¦½ë³€ìˆ˜"],
                row["OR"],
                "<=" if row["p-value"] < 0.05 else ">",
                row["ì¢…ì†ë³€ìˆ˜"],
                "ìœ ì˜ë¯¸í•œ" if row["p-value"] < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šì€",
            )
        )

    if full:
        return cdf, rdf, result_report, model_report, variable_reports, cm
    else:
        return cdf, rdf


# ===================================================================
# ë¡œì§€ìŠ¤í‹± íšŒê·€
# ===================================================================
def logit(df: DataFrame, yname: str, report=False):
    """ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì í•© ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    ì¢…ì†ë³€ìˆ˜ê°€ ì´í•­(binary) í˜•íƒœì¼ ë•Œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œë‹¤.
    í•„ìš”ì‹œ ìƒì„¸í•œ í†µê³„ ë³´ê³ ì„œë¥¼ í•¨ê»˜ ì œê³µí•œë‹¤.

    Args:
        df (DataFrame): ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        yname (str): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª…. ì´í•­ ë³€ìˆ˜ì—¬ì•¼ í•œë‹¤.
        report: ë¦¬í¬íŠ¸ ëª¨ë“œ ì„¤ì •. ë‹¤ìŒ ê°’ ì¤‘ í•˜ë‚˜:
            - False (ê¸°ë³¸ê°’): ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©. fit ê°ì²´ë§Œ ë°˜í™˜.
            - 1 ë˜ëŠ” 'summary': ìš”ì•½ ë¦¬í¬íŠ¸ ë°˜í™˜ (full=False).
            - 2 ë˜ëŠ” 'full': í’€ ë¦¬í¬íŠ¸ ë°˜í™˜ (full=True).
            - True: í’€ ë¦¬í¬íŠ¸ ë°˜í™˜ (2ì™€ ë™ì¼).

    Returns:
        statsmodels.genmod.generalized_linear_model.BinomialResults: report=Falseì¼ ë•Œ.
            ë¡œì§€ìŠ¤í‹± íšŒê·€ ì í•© ê²°ê³¼ ê°ì²´. fit.summary()ë¡œ ìƒì„¸ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥.

        tuple (4ê°œ): report=1 ë˜ëŠ” 'summary'ì¼ ë•Œ.
            (fit, rdf, result_report, variable_reports) í˜•íƒœë¡œ (cdf ì œì™¸).

        tuple (6ê°œ): report=2, 'full' ë˜ëŠ” Trueì¼ ë•Œ.
            (fit, cdf, rdf, result_report, model_report, variable_reports) í˜•íƒœë¡œ:
            - fit: ë¡œì§€ìŠ¤í‹± íšŒê·€ ì í•© ê²°ê³¼ ê°ì²´
            - cdf: ì„±ëŠ¥ ì§€í‘œ í‘œ (DataFrame)
            - rdf: íšŒê·€ê³„ìˆ˜ í‘œ (DataFrame)
            - result_report: ì í•©ë„ ë° ì˜ˆì¸¡ ì„±ëŠ¥ ìš”ì•½ (str)
            - model_report: ëª¨í˜• ë³´ê³  ë¬¸ì¥ (str)
            - variable_reports: ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (list[str])

    Examples:
        >>> from hossam.analysis import logit
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'target': np.random.binomial(1, 0.5, 100),
        ...     'x1': np.random.normal(0, 1, 100),
        ...     'x2': np.random.normal(0, 1, 100)
        ... })
        >>> # ì í•© ê²°ê³¼ë§Œ ë°˜í™˜
        >>> fit = hs_logit(df, 'target')
        >>> print(fit.summary())

        >>> # ìš”ì•½ ë¦¬í¬íŠ¸ ë°˜í™˜
        >>> fit, rdf, result_report, var_reports = hs_logit(df, 'target', report=1)

        >>> # í’€ ë¦¬í¬íŠ¸ ë°˜í™˜
        >>> fit, cdf, rdf, result_report, model_report, var_reports = hs_logit(df, 'target', report=2)
    """
    x = df.drop(yname, axis=1)
    y = df[yname]

    X_const = sm.add_constant(x)
    logit_model = sm.Logit(y, X_const)
    logit_fit = logit_model.fit(disp=False)

    # report íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ì²˜ë¦¬
    if not report or report is False:
        # ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©
        return logit_fit
    elif report == 1 or report == 'summary':
        # ìš”ì•½ ë¦¬í¬íŠ¸ (full=False)
        cdf, rdf = logit_report(logit_fit, df, threshold=0.5, full=False, alpha=0.05)
        # ìš”ì•½ì—ì„œëŠ” result_reportì™€ variable_reportsë§Œ í¬í•¨
        # ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ resultì™€ variable_reportsë§Œ ìƒì„±
        return logit_fit, rdf
    elif report == 2 or report == 'full' or report is True:
        # í’€ ë¦¬í¬íŠ¸ (full=True)
        cdf, rdf, result_report, model_report, variable_reports, cm = logit_report(logit_fit, df, threshold=0.5, full=True, alpha=0.05)
        return logit_fit, cdf, rdf, result_report, model_report, variable_reports
    else:
        # ê¸°ë³¸ê°’: ë¦¬í¬íŠ¸ ë¯¸ì‚¬ìš©
        return logit_fit


# ===================================================================
# ì„ í˜•ì„± ê²€ì • (Linearity Test)
# ===================================================================
def ols_linearity_test(fit, power: int = 2, alpha: float = 0.05) -> DataFrame:
    """íšŒê·€ëª¨í˜•ì˜ ì„ í˜•ì„±ì„ Ramsey RESET ê²€ì •ìœ¼ë¡œ í‰ê°€í•œë‹¤.

    ì í•©ëœ íšŒê·€ëª¨í˜•ì— ëŒ€í•´ Ramsey RESET(Regression Specification Error Test) ê²€ì •ì„ ìˆ˜í–‰í•˜ì—¬
    ëª¨í˜•ì˜ ì„ í˜•ì„± ê°€ì •ì´ íƒ€ë‹¹í•œì§€ë¥¼ ê²€ì •í•œë‹¤. ê·€ë¬´ê°€ì„¤ì€ 'ëª¨í˜•ì´ ì„ í˜•ì´ë‹¤'ì´ë‹¤.

    Args:
        fit: íšŒê·€ ëª¨í˜• ê°ì²´ (statsmodelsì˜ RegressionResultsWrapper).
             OLS ë˜ëŠ” WLS ëª¨í˜•ì´ì–´ì•¼ í•œë‹¤.
        power (int, optional): RESET ê²€ì •ì— ì‚¬ìš©í•  ê±°ë“­ì œê³± ìˆ˜. ê¸°ë³¸ê°’ 2.
                               power=2ì¼ ë•Œ ì˜ˆì¸¡ê°’ì˜ ì œê³±í•­ì´ ì¶”ê°€ë¨.
                               powerê°€ í´ìˆ˜ë¡ ë” ë†’ì€ ì°¨ìˆ˜ì˜ ë¹„ì„ í˜•ì„±ì„ ê°ì§€.
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        DataFrame: ì„ í˜•ì„± ê²€ì • ê²°ê³¼ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
                   - ê²€ì •í†µê³„ëŸ‰: F-statistic
                   - p-value: ê²€ì •ì˜ pê°’
                   - ìœ ì˜ì„±: alpha ê¸°ì¤€ ê²°ê³¼ (bool)
                   - í•´ì„: ì„ í˜•ì„± íŒì • (ë¬¸ìì—´)

    Examples:
        >>> import statsmodels.api as sm
        >>> X = sm.add_constant(df[['x1', 'x2']])
        >>> y = df['y']
        >>> fit = sm.OLS(y, X).fit()
        >>> result = linearity_test(fit)
        >>> print(result)

    Notes:
        - p-value > alpha: ì„ í˜•ì„± ê°€ì •ì„ ë§Œì¡± (ê·€ë¬´ê°€ì„¤ ì±„íƒ)
        - p-value <= alpha: ì„ í˜•ì„± ê°€ì • ìœ„ë°˜ ê°€ëŠ¥ (ê·€ë¬´ê°€ì„¤ ê¸°ê°)
    """
    import re

    # Ramsey RESET ê²€ì • ìˆ˜í–‰
    reset_result = linear_reset(fit, power=power)

    # ContrastResults ê°ì²´ì—ì„œ ê²°ê³¼ ì¶”ì¶œ
    test_stat = None
    p_value = None

    try:
        # ë¬¸ìì—´ í‘œí˜„ì—ì„œ ìˆ«ì ì¶”ì¶œ ì‹œë„
        result_str = str(reset_result)

        # ì •ê·œì‹ìœ¼ë¡œ ìˆ«ìê°’ë“¤ ì¶”ì¶œ (F-statisticê³¼ p-value)
        numbers = re.findall(r'\d+\.?\d*[eE]?-?\d*', result_str)

        if len(numbers) >= 2:
            # ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ëŠ” F-statistic, ë§ˆì§€ë§‰ì€ p-value
            test_stat = float(numbers[0])
            p_value = float(numbers[-1])
    except (ValueError, IndexError, AttributeError):
        pass

    # ì •ê·œì‹ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ì†ì„± ì ‘ê·¼
    if test_stat is None or p_value is None:
        attr_pairs = [
            ('statistic', 'pvalue'),
            ('test_stat', 'pvalue'),
            ('fvalue', 'pvalue'),
        ]

        for attr_stat, attr_pval in attr_pairs:
            if hasattr(reset_result, attr_stat) and hasattr(reset_result, attr_pval):
                try:
                    test_stat = float(getattr(reset_result, attr_stat))
                    p_value = float(getattr(reset_result, attr_pval))
                    break
                except (ValueError, TypeError):
                    continue

    # ì—¬ì „íˆ ê°’ì„ ëª» ì°¾ìœ¼ë©´ ì—ëŸ¬
    if test_stat is None or p_value is None:
        raise ValueError(f"linear_reset ê²°ê³¼ë¥¼ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°˜í™˜ê°’: {reset_result}")

    # ìœ ì˜ì„± íŒì •
    significant = p_value <= alpha

    # í•´ì„ ë¬¸êµ¬
    if significant:
        interpretation = f"ì„ í˜•ì„± ê°€ì • ìœ„ë°˜ (p={p_value:.4f} <= {alpha})"
    else:
        interpretation = f"ì„ í˜•ì„± ê°€ì • ë§Œì¡± (p={p_value:.4f} > {alpha})"

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜
    result_df = DataFrame({
        "ê²€ì •": ["Ramsey RESET"],
        "ê²€ì •í†µê³„ëŸ‰ (F)": [f"{test_stat:.4f}"],
        "p-value": [f"{p_value:.4f}"],
        "ìœ ì˜ìˆ˜ì¤€": [alpha],
        "ì„ í˜•ì„±_ìœ„ë°˜": [significant],  # True: ì„ í˜•ì„± ìœ„ë°˜, False: ì„ í˜•ì„± ë§Œì¡±
        "í•´ì„": [interpretation]
    })

    return result_df


# ===================================================================
# ì •ê·œì„± ê²€ì • (Normality Test)
# ===================================================================
def ols_normality_test(fit, alpha: float = 0.05) -> DataFrame:
    """íšŒê·€ëª¨í˜• ì”ì°¨ì˜ ì •ê·œì„±ì„ ê²€ì •í•œë‹¤.

    íšŒê·€ëª¨í˜•ì˜ ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ Shapiro-Wilk ê²€ì •ê³¼ Jarque-Bera ê²€ì •ìœ¼ë¡œ í‰ê°€í•œë‹¤.
    ì •ê·œì„± ê°€ì •ì€ íšŒê·€ë¶„ì„ì˜ ì¶”ë¡ (ì‹ ë¢°êµ¬ê°„, ê°€ì„¤ê²€ì •)ì´ íƒ€ë‹¹í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê°€ì •ì´ë‹¤.

    Args:
        fit: íšŒê·€ ëª¨í˜• ê°ì²´ (statsmodelsì˜ RegressionResultsWrapper).
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        DataFrame: ì •ê·œì„± ê²€ì • ê²°ê³¼ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
                   - ê²€ì •ëª…: ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
                   - ê²€ì •í†µê³„ëŸ‰: ê²€ì • í†µê³„ëŸ‰ ê°’
                   - p-value: ê²€ì •ì˜ pê°’
                   - ìœ ì˜ìˆ˜ì¤€: ì„¤ì •ëœ ìœ ì˜ìˆ˜ì¤€
                   - ì •ê·œì„±_ìœ„ë°˜: alpha ê¸°ì¤€ ê²°ê³¼ (bool)
                   - í•´ì„: ì •ê·œì„± íŒì • (ë¬¸ìì—´)

    Examples:
        >>> import statsmodels.api as sm
        >>> X = sm.add_constant(df[['x1', 'x2']])
        >>> y = df['y']
        >>> fit = sm.OLS(y, X).fit()
        >>> result = normality_test(fit)
        >>> print(result)

    Notes:
        - Shapiro-Wilk: ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ì„ ë•Œ (< 5000) ê°•ë ¥í•œ ê²€ì •
        - Jarque-Bera: ì™œë„(skewness)ì™€ ì²¨ë„(kurtosis) ê¸°ë°˜ ê²€ì •
        - p-value > alpha: ì •ê·œì„± ê°€ì • ë§Œì¡± (ê·€ë¬´ê°€ì„¤ ì±„íƒ)
        - p-value <= alpha: ì •ê·œì„± ê°€ì • ìœ„ë°˜ (ê·€ë¬´ê°€ì„¤ ê¸°ê°)
    """
    from scipy.stats import jarque_bera

    # fit ê°ì²´ì—ì„œ ì”ì°¨ ì¶”ì¶œ
    residuals = fit.resid
    n = len(residuals)

    results = []

    # 1. Shapiro-Wilk ê²€ì • (ìƒ˜í”Œ í¬ê¸° < 5000ì¼ ë•Œ ê¶Œì¥)
    if n < 5000:
        try:
            stat_sw, p_sw = shapiro(residuals)
            significant_sw = p_sw <= alpha

            if significant_sw:
                interpretation_sw = f"ì •ê·œì„± ìœ„ë°˜ (p={p_sw:.4f} <= {alpha})"
            else:
                interpretation_sw = f"ì •ê·œì„± ë§Œì¡± (p={p_sw:.4f} > {alpha})"

            results.append({
                "ê²€ì •": "Shapiro-Wilk",
                "ê²€ì •í†µê³„ëŸ‰": f"{stat_sw:.4f}",
                "p-value": f"{p_sw:.4f}",
                "ìœ ì˜ìˆ˜ì¤€": alpha,
                "ì •ê·œì„±_ìœ„ë°˜": significant_sw,
                "í•´ì„": interpretation_sw
            })
        except Exception as e:
            pass

    # 2. Jarque-Bera ê²€ì • (í•­ìƒ ìˆ˜í–‰)
    try:
        stat_jb, p_jb = jarque_bera(residuals)
        significant_jb = p_jb <= alpha

        if significant_jb:
            interpretation_jb = f"ì •ê·œì„± ìœ„ë°˜ (p={p_jb:.4f} <= {alpha})"
        else:
            interpretation_jb = f"ì •ê·œì„± ë§Œì¡± (p={p_jb:.4f} > {alpha})"

        results.append({
            "ê²€ì •": "Jarque-Bera",
            "ê²€ì •í†µê³„ëŸ‰": f"{stat_jb:.4f}",
            "p-value": f"{p_jb:.4f}",
            "ìœ ì˜ìˆ˜ì¤€": alpha,
            "ì •ê·œì„±_ìœ„ë°˜": significant_jb,
            "í•´ì„": interpretation_jb
        })
    except Exception as e:
        pass

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜
    if not results:
        raise ValueError("ì •ê·œì„± ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    result_df = DataFrame(results)
    return result_df


# ===================================================================
# ë“±ë¶„ì‚°ì„± ê²€ì • (Homoscedasticity Test)
# ===================================================================
def ols_variance_test(fit, alpha: float = 0.05) -> DataFrame:
    """íšŒê·€ëª¨í˜•ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ ê²€ì •í•œë‹¤.

    ì”ì°¨ì˜ ë¶„ì‚°ì´ ì˜ˆì¸¡ê°’ì˜ ìˆ˜ì¤€ì— ê´€ê³„ì—†ì´ ì¼ì •í•œì§€ Breusch-Pagan ê²€ì •ê³¼ White ê²€ì •ìœ¼ë¡œ í‰ê°€í•œë‹¤.
    ë“±ë¶„ì‚°ì„± ê°€ì •ì€ íšŒê·€ë¶„ì„ì˜ ì¶”ë¡ (í‘œì¤€ì˜¤ì°¨, ê²€ì •í†µê³„ëŸ‰)ì´ ì •í™•í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê°€ì •ì´ë‹¤.

    Args:
        fit: íšŒê·€ ëª¨í˜• ê°ì²´ (statsmodelsì˜ RegressionResultsWrapper).
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        DataFrame: ë“±ë¶„ì‚°ì„± ê²€ì • ê²°ê³¼ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
                   - ê²€ì •ëª…: ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
                   - ê²€ì •í†µê³„ëŸ‰: ê²€ì • í†µê³„ëŸ‰ ê°’ (LM ë˜ëŠ” F)
                   - p-value: ê²€ì •ì˜ pê°’
                   - ìœ ì˜ìˆ˜ì¤€: ì„¤ì •ëœ ìœ ì˜ìˆ˜ì¤€
                   - ë“±ë¶„ì‚°ì„±_ìœ„ë°˜: alpha ê¸°ì¤€ ê²°ê³¼ (bool)
                   - í•´ì„: ë“±ë¶„ì‚°ì„± íŒì • (ë¬¸ìì—´)

    Examples:
        >>> import statsmodels.api as sm
        >>> X = sm.add_constant(df[['x1', 'x2']])
        >>> y = df['y']
        >>> fit = sm.OLS(y, X).fit()
        >>> result = homoscedasticity_test(fit)
        >>> print(result)

    Notes:
        - Breusch-Pagan: ì”ì°¨ ì œê³±ê³¼ ë…ë¦½ë³€ìˆ˜ì˜ ì„ í˜•ê´€ê³„ ê²€ì •
        - White: ì”ì°¨ ì œê³±ê³¼ ë…ë¦½ë³€ìˆ˜ ë° ê·¸ ì œê³±, êµì°¨í•­ì˜ ê´€ê³„ ê²€ì •
        - p-value > alpha: ë“±ë¶„ì‚°ì„± ê°€ì • ë§Œì¡± (ê·€ë¬´ê°€ì„¤ ì±„íƒ)
        - p-value <= alpha: ì´ë¶„ì‚°ì„± ì¡´ì¬ (ê·€ë¬´ê°€ì„¤ ê¸°ê°)
    """

    # fit ê°ì²´ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
    exog = fit.model.exog  # ì„¤ëª…ë³€ìˆ˜ (ìƒìˆ˜í•­ í¬í•¨)
    resid = fit.resid      # ì”ì°¨

    results = []

    # 1. Breusch-Pagan ê²€ì •
    try:
        lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(resid, exog)
        significant_bp = lm_pvalue <= alpha

        if significant_bp:
            interpretation_bp = f"ë“±ë¶„ì‚°ì„± ìœ„ë°˜ (p={lm_pvalue:.4f} <= {alpha})"
        else:
            interpretation_bp = f"ë“±ë¶„ì‚°ì„± ë§Œì¡± (p={lm_pvalue:.4f} > {alpha})"

        results.append({
            "ê²€ì •": "Breusch-Pagan",
            "ê²€ì •í†µê³„ëŸ‰ (LM)": f"{lm:.4f}",
            "p-value": f"{lm_pvalue:.4f}",
            "ìœ ì˜ìˆ˜ì¤€": alpha,
            "ë“±ë¶„ì‚°ì„±_ìœ„ë°˜": significant_bp,
            "í•´ì„": interpretation_bp
        })
    except Exception as e:
        pass

    # 2. White ê²€ì •
    try:
        lm, lm_pvalue, fvalue, f_pvalue = het_white(resid, exog)
        significant_white = lm_pvalue <= alpha

        if significant_white:
            interpretation_white = f"ë“±ë¶„ì‚°ì„± ìœ„ë°˜ (p={lm_pvalue:.4f} <= {alpha})"
        else:
            interpretation_white = f"ë“±ë¶„ì‚°ì„± ë§Œì¡± (p={lm_pvalue:.4f} > {alpha})"

        results.append({
            "ê²€ì •": "White",
            "ê²€ì •í†µê³„ëŸ‰ (LM)": f"{lm:.4f}",
            "p-value": f"{lm_pvalue:.4f}",
            "ìœ ì˜ìˆ˜ì¤€": alpha,
            "ë“±ë¶„ì‚°ì„±_ìœ„ë°˜": significant_white,
            "í•´ì„": interpretation_white
        })
    except Exception as e:
        pass

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜
    if not results:
        raise ValueError("ë“±ë¶„ì‚°ì„± ê²€ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    result_df = DataFrame(results)
    return result_df


# ===================================================================
# ë…ë¦½ì„± ê²€ì • (Independence Test - Durbin-Watson)
# ===================================================================
def ols_independence_test(fit, alpha: float = 0.05) -> DataFrame:
    """íšŒê·€ëª¨í˜•ì˜ ë…ë¦½ì„± ê°€ì •(ìê¸°ìƒê´€ ì—†ìŒ)ì„ ê²€ì •í•œë‹¤.

    Durbin-Watson ê²€ì •ì„ ì‚¬ìš©í•˜ì—¬ ì”ì°¨ì˜ 1ì°¨ ìê¸°ìƒê´€ ì—¬ë¶€ë¥¼ ê²€ì •í•œë‹¤.
    ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.

    Args:
        fit: statsmodels íšŒê·€ë¶„ì„ ê²°ê³¼ ê°ì²´ (RegressionResultsWrapper).
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ì€ 0.05.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ ë°ì´í„°í”„ë ˆì„.
            - ê²€ì •: ê²€ì • ë°©ë²•ëª…
            - ê²€ì •í†µê³„ëŸ‰(DW): Durbin-Watson í†µê³„ëŸ‰ (0~4 ë²”ìœ„, 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìê¸°ìƒê´€ ì—†ìŒ)
            - ë…ë¦½ì„±_ìœ„ë°˜: ìê¸°ìƒê´€ ì˜ì‹¬ ì—¬ë¶€ (True/False)
            - í•´ì„: ê²€ì • ê²°ê³¼ í•´ì„

    Examples:
        >>> import pandas as pd
        >>> import statsmodels.api as sm
        >>> from hossam.hs_stats import ols_independence_test
        >>>
        >>> # ì˜ˆì œ ë°ì´í„°
        >>> df = pd.DataFrame({
        ...     'x': range(100),
        ...     'y': [i + np.random.randn() for i in range(100)]
        ... })
        >>> X = sm.add_constant(df['x'])
        >>> model = sm.OLS(df['y'], X)
        >>> fit = model.fit()
        >>>
        >>> # ë…ë¦½ì„± ê²€ì •
        >>> result = ols_independence_test(fit)
        >>> print(result)

    Notes:
        - Durbin-Watson í†µê³„ëŸ‰ í•´ì„:
          * 2ì— ê°€ê¹Œìš°ë©´: ìê¸°ìƒê´€ ì—†ìŒ (ë…ë¦½ì„± ë§Œì¡±)
          * 0ì— ê°€ê¹Œìš°ë©´: ì–‘ì˜ ìê¸°ìƒê´€ (ë…ë¦½ì„± ìœ„ë°˜)
          * 4ì— ê°€ê¹Œìš°ë©´: ìŒì˜ ìê¸°ìƒê´€ (ë…ë¦½ì„± ìœ„ë°˜)
        - ì¼ë°˜ì ìœ¼ë¡œ 1.5~2.5 ë²”ìœ„ë¥¼ ìê¸°ìƒê´€ ì—†ìŒìœ¼ë¡œ íŒë‹¨
        - ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ê´€ì¸¡ì¹˜ì— ìˆœì„œê°€ ìˆëŠ” ê²½ìš° ì¤‘ìš”í•œ ê²€ì •
    """
    from pandas import DataFrame

    # Durbin-Watson í†µê³„ëŸ‰ ê³„ì‚°
    dw_stat = durbin_watson(fit.resid)

    # ìê¸°ìƒê´€ íŒë‹¨ (1.5 < DW < 2.5 ë²”ìœ„ë¥¼ ë…ë¦½ì„± ë§Œì¡±ìœ¼ë¡œ íŒë‹¨)
    is_autocorrelated = dw_stat < 1.5 or dw_stat > 2.5

    # í•´ì„ ë©”ì‹œì§€ ìƒì„±
    if dw_stat < 1.5:
        interpretation = f"DW={dw_stat:.4f} < 1.5 (ì–‘ì˜ ìê¸°ìƒê´€)"
    elif dw_stat > 2.5:
        interpretation = f"DW={dw_stat:.4f} > 2.5 (ìŒì˜ ìê¸°ìƒê´€)"
    else:
        interpretation = f"DW={dw_stat:.4f} (ë…ë¦½ì„± ê°€ì • ë§Œì¡±)"

    # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    result_df = DataFrame(
        {
            "ê²€ì •": ["Durbin-Watson"],
            "ê²€ì •í†µê³„ëŸ‰(DW)": [dw_stat],
            "ë…ë¦½ì„±_ìœ„ë°˜": [is_autocorrelated],
            "í•´ì„": [interpretation],
        }
    )

    return result_df


# ===================================================================
# ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ
# ===================================================================
def corr(data: DataFrame, *fields: str) -> tuple[DataFrame, DataFrame]:
    """ë°ì´í„°í”„ë ˆì„ì˜ ì—°ì†í˜• ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µê³¼ ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ë¥¼ ë°˜í™˜í•œë‹¤.

    ì •ê·œì„± ê²€ì •ì„ í†µí•´ í”¼ì–´ìŠ¨ ë˜ëŠ” ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ë¥¼ ìë™ ì„ íƒí•˜ì—¬ ê³„ì‚°í•œë‹¤.
    ì„ íƒëœ ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ë¥¼ ë³„ë„ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ êµì°¨í‘œ(í–‰ë ¬) í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ì„ ì‚¬ìš©.

    Returns:
        tuple[DataFrame, DataFrame]: ìƒê´€ê³„ìˆ˜ í–‰ë ¬ê³¼ ì‚¬ìš©ëœ ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ ì •ë³´ë¥¼ í¬í•¨í•œ ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„.

            - ì²« ë²ˆì§¸ DataFrame: ìƒê´€ê³„ìˆ˜ í–‰ë ¬ (ê° ë³€ìˆ˜ ìŒì˜ ìƒê´€ê³„ìˆ˜ ê°’)
            - ë‘ ë²ˆì§¸ DataFrame: ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ (êµì°¨í‘œ í˜•íƒœ)
                - í–‰ê³¼ ì—´: ë³€ìˆ˜ëª…
                - ì…€ì˜ ê°’: ê° ë³€ìˆ˜ ìŒì— ì‚¬ìš©ëœ ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ ('Pearson' ë˜ëŠ” 'Spearman')

    Examples:
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x1': np.random.normal(0, 1, 100),
        ...     'x2': np.random.normal(0, 1, 100),
        ...     'x3': np.random.normal(0, 1, 100),
        ... })
        >>> # ëª¨ë“  ì—°ì†í˜• ë³€ìˆ˜ì— ëŒ€í•´ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        >>> corr_matrix, corr_types = corr(df)
        >>> print(corr_matrix)
        >>>     x1   x2   x3
        >>> x1 1.00 0.12 -0.05
        >>> x2 0.12 1.00  0.08
        >>> x3 -0.05 0.08 1.00
        >>> print(corr_types)
        >>>       x1       x2       x3
        >>> x1  Pearson Pearson Pearson
        >>> x2  Pearson Pearson Pearson
        >>> x3  Pearson Pearson Pearson

        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„
        >>> corr_matrix, corr_info = corr(df, 'x1', 'x2')
        >>> print(corr_matrix)
    """
    # ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼ ê²°ì •
    if fields:
        # ì§€ì •ëœ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        numeric_cols = list(fields)
    else:
        # ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ ì„ íƒ
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

    # ë¶„ì„ ë°ì´í„° ì¶”ì¶œ
    analysis_data = data[numeric_cols].copy()

    # ìƒ˜í”Œ í¬ê¸°ì— ë”°ë¼ ìë™ìœ¼ë¡œ shapiro ë˜ëŠ” normaltest ì„ íƒ
    test_method = 's' if len(analysis_data) <= 5000 else 'n'
    normality_results = normal_test(analysis_data, columns=numeric_cols, method=test_method)

    # ì •ê·œì„± ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    normality_info = dict(zip(normality_results['column'], normality_results['is_normal']))

    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°: ëª¨ë“  ë³€ìˆ˜ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë©´ Pearson, í•˜ë‚˜ë¼ë„ ì•„ë‹ˆë©´ Spearman ì‚¬ìš©
    all_normal = all(normality_info.values())
    if all_normal:
        # Pearson ìƒê´€ê³„ìˆ˜
        corr_matrix = analysis_data.corr(method='pearson')
        selected_corr_type = 'Pearson'
    else:
        # Spearman ìƒê´€ê³„ìˆ˜
        corr_matrix = analysis_data.corr(method='spearman')
        selected_corr_type = 'Spearman'

    # ìƒê´€ê³„ìˆ˜ ì •ë³´ ë°ì´í„°í”„ë ˆì„ ìƒì„± (êµì°¨í‘œ í˜•íƒœ - ìƒê´€í–‰ë ¬ê³¼ ë™ì¼í•œ êµ¬ì¡°)
    corr_info_df = DataFrame(
        selected_corr_type,
        index=numeric_cols,
        columns=numeric_cols
    )

    return corr_matrix, corr_info_df


# ===================================================================
# ìŒë³„ ìƒê´€ë¶„ì„ (ì„ í˜•ì„±/ì´ìƒì¹˜ ì ê²€ í›„ Pearson/Spearman ìë™ ì„ íƒ)
# ===================================================================
def corr_pairwise(
    data: DataFrame,
    fields: list[str] | None = None,
    alpha: float = 0.05,
    z_thresh: float = 3.0,
    min_n: int = 8,
    linearity_power: tuple[int, ...] = (2,),
    p_adjust: str = "none",
) -> tuple[DataFrame, DataFrame]:
    """ê° ë³€ìˆ˜ ìŒì— ëŒ€í•´ ì„ í˜•ì„±Â·ì´ìƒì¹˜ ì—¬ë¶€ë¥¼ ì ê²€í•œ ë’¤ Pearson/Spearmanì„ ìë™ ì„ íƒí•´ ìƒê´€ì„ ìš”ì•½í•œë‹¤.

    ì ˆì°¨:
    1) z-score ê¸°ì¤€(|z|>z_thresh)ìœ¼ë¡œ ê° ë³€ìˆ˜ì˜ ì´ìƒì¹˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ íŒŒì•…
    2) ë‹¨ìˆœíšŒê·€ y~xì— ëŒ€í•´ Ramsey RESET(linearity_power)ë¡œ ì„ í˜•ì„± ê²€ì • (ëª¨ë“  p>alpha â†’ ì„ í˜•ì„± ì¶©ì¡±)
    3) ì„ í˜•ì„± ì¶©ì¡±ì´ê³  ì–‘ìª½ ë³€ìˆ˜ì—ì„œ |z|>z_thresh ì´ìƒì¹˜ê°€ ì—†ìœ¼ë©´ Pearson, ê·¸ ì™¸ì—” Spearman ì„ íƒ
    4) ìƒê´€ê³„ìˆ˜/ìœ ì˜í™•ë¥ , ìœ ì˜ì„± ì—¬ë¶€, ê°•ë„(strong/medium/weak/no correlation) ê¸°ë¡
    5) ì„ íƒì ìœ¼ë¡œ ë‹¤ì¤‘ë¹„êµ ë³´ì •(p_adjust="fdr_bh" ë“±) ì ìš©í•˜ì—¬ pval_adjì™€ significant_adj ì¶”ê°€

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        fields (list[str]|None): ë¶„ì„í•  ìˆ«ìí˜• ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ ì‚¬ìš©. ê¸°ë³¸ê°’ None.
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ 0.05.
        z_thresh (float, optional): ì´ìƒì¹˜ íŒë‹¨ ì„ê³„ê°’(|z| ê¸°ì¤€). ê¸°ë³¸ 3.0.
        min_n (int, optional): ìŒë³„ ìµœì†Œ í‘œë³¸ í¬ê¸°. ë¯¸ë§Œì´ë©´ ê³„ì‚° ìƒëµ. ê¸°ë³¸ 8.
        linearity_power (tuple[int,...], optional): RESET ê²€ì •ì—ì„œ í¬í•¨í•  ì°¨ìˆ˜ ì§‘í•©. ê¸°ë³¸ (2,).
        p_adjust (str, optional): ë‹¤ì¤‘ë¹„êµ ë³´ì • ë°©ë²•. "none" ë˜ëŠ” statsmodels.multipletests ì§€ì›ê°’ ì¤‘ í•˜ë‚˜(e.g., "fdr_bh"). ê¸°ë³¸ "none".

    Returns:
        tuple[DataFrame, DataFrame]: ë‘ ê°œì˜ ë°ì´í„°í”„ë ˆì„ì„ ë°˜í™˜.
            [0] result_df: ê° ë³€ìˆ˜ìŒë³„ ê²°ê³¼ í…Œì´ë¸”. ì»¬ëŸ¼:
                var_a, var_b, n, linearity(bool), outlier_flag(bool), chosen('pearson'|'spearman'),
                corr, pval, significant(bool), strength(str), (ë³´ì • ì‚¬ìš© ì‹œ) pval_adj, significant_adj
            [1] corr_matrix: ìƒê´€ê³„ìˆ˜ í–‰ë ¬ (í–‰ê³¼ ì—´ì— ë³€ìˆ˜ëª…, ê°’ì— ìƒê´€ê³„ìˆ˜)

    Examples:
        >>> from hossam.hs_stats import corr_pairwise
        >>> import pandas as pd
        >>> df = pd.DataFrame({'x1': [1,2,3,4,5], 'x2': [2,4,5,4,6], 'x3': [10,20,25,24,30]})
        >>> # ì „ì²´ ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•´ ìƒê´€ë¶„ì„
        >>> result_df, corr_matrix = corr_pairwise(df)
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„
        >>> result_df, corr_matrix = corr_pairwise(df, fields=['x1', 'x2'])
    """

    # 0) ì»¬ëŸ¼ ì„ ì • (ìˆ«ìí˜•ë§Œ)
    if fields is None:
        # Noneì´ë©´ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ ì‚¬ìš©
        cols = data.select_dtypes(include=[np.number]).columns.tolist()
    else:
        # fields ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ì— ìˆëŠ” ê²ƒë§Œ ì„ íƒí•˜ë˜, ìˆ«ìí˜•ë§Œ í•„í„°ë§
        cols = [c for c in fields if c in data.columns and is_numeric_dtype(data[c])]

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ì´ 2ê°œ ë¯¸ë§Œì´ë©´ ìƒê´€ë¶„ì„ ë¶ˆê°€ëŠ¥
    if len(cols) < 2:
        empty_df = DataFrame(columns=["var_a", "var_b", "n", "linearity", "outlier_flag", "chosen", "corr", "pval", "significant", "strength"])
        return empty_df, DataFrame()

    # z-score ê¸°ë°˜ ì´ìƒì¹˜ ìœ ë¬´ ê³„ì‚°
    z_outlier_flags = {}
    for c in cols:
        col = data[c].dropna()
        if col.std(ddof=1) == 0:
            z_outlier_flags[c] = False
            continue
        z = (col - col.mean()) / col.std(ddof=1)
        z_outlier_flags[c] = (z.abs() > z_thresh).any()

    rows = []

    for a, b in combinations(cols, 2):
        # ê³µí†µ ê´€ì¸¡ì¹˜ ì‚¬ìš©
        pair_df = data[[a, b]].dropna()
        if len(pair_df) < max(3, min_n):
            # í‘œë³¸ì´ ë„ˆë¬´ ì ìœ¼ë©´ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
            rows.append(
                {
                    "var_a": a,
                    "var_b": b,
                    "n": len(pair_df),
                    "linearity": False,
                    "outlier_flag": True,
                    "chosen": None,
                    "corr": np.nan,
                    "pval": np.nan,
                    "significant": False,
                    "strength": "no correlation",
                }
            )
            continue

        x = pair_df[a]
        y = pair_df[b]

        # ìƒìˆ˜ì—´/ë¶„ì‚° 0 ì²´í¬ â†’ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ë¶ˆê°€
        if x.nunique(dropna=True) <= 1 or y.nunique(dropna=True) <= 1:
            rows.append(
                {
                    "var_a": a,
                    "var_b": b,
                    "n": len(pair_df),
                    "linearity": False,
                    "outlier_flag": True,
                    "chosen": None,
                    "corr": np.nan,
                    "pval": np.nan,
                    "significant": False,
                    "strength": "no correlation",
                }
            )
            continue

        # 1) ì„ í˜•ì„±: Ramsey RESET (ì§€ì • ì°¨ìˆ˜ ì „ë¶€ p>alpha ì—¬ì•¼ í†µê³¼)
        linearity_ok = False
        try:
            X_const = sm.add_constant(x)
            model = sm.OLS(y, X_const).fit()
            pvals = []
            for pwr in linearity_power:
                reset = linear_reset(model, power=pwr, use_f=True)
                pvals.append(reset.pvalue)
            # ëª¨ë“  ì°¨ìˆ˜ì—ì„œ ìœ ì˜í•˜ì§€ ì•Šì„ ë•Œ ì„ í˜•ì„± ì¶©ì¡±ìœ¼ë¡œ ê°„ì£¼
            if len(pvals) > 0:
                linearity_ok = all([pv > alpha for pv in pvals])
        except Exception:
            linearity_ok = False

        # 2) ì´ìƒì¹˜ í”Œë˜ê·¸ (ë‘ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ë¼ë„ z-outlier ìˆìœ¼ë©´ True)
        outlier_flag = bool(z_outlier_flags.get(a, False) or z_outlier_flags.get(b, False))

        # 3) ìƒê´€ ê³„ì‚°: ì„ í˜•Â·ë¬´ì´ìƒì¹˜ë©´ Pearson, ì•„ë‹ˆë©´ Spearman
        try:
            if linearity_ok and not outlier_flag:
                chosen = "pearson"
                corr_val, pval = pearsonr(x, y)
            else:
                chosen = "spearman"
                corr_val, pval = spearmanr(x, y)
        except Exception:
            chosen = None
            corr_val, pval = np.nan, np.nan

        # 4) ìœ ì˜ì„±, ê°•ë„
        significant = False if np.isnan(pval) else pval <= alpha
        abs_r = abs(corr_val) if not np.isnan(corr_val) else 0
        if abs_r > 0.7:
            strength = "strong"
        elif abs_r > 0.3:
            strength = "medium"
        elif abs_r > 0:
            strength = "weak"
        else:
            strength = "no correlation"

        rows.append(
            {
                "var_a": a,
                "var_b": b,
                "n": len(pair_df),
                "linearity": linearity_ok,
                "outlier_flag": outlier_flag,
                "chosen": chosen,
                "corr": corr_val,
                "pval": pval,
                "significant": significant,
                "strength": strength,
            }
        )

    result_df = DataFrame(rows)

    # 5) ë‹¤ì¤‘ë¹„êµ ë³´ì • (ì„ íƒ)
    if p_adjust.lower() != "none" and not result_df.empty:
        # ìœ íš¨í•œ pë§Œ ë³´ì •
        mask = result_df["pval"].notna()
        if mask.any():
            _, p_adj, _, _ = multipletests(result_df.loc[mask, "pval"], alpha=alpha, method=p_adjust)
            result_df.loc[mask, "pval_adj"] = p_adj
            result_df["significant_adj"] = result_df["pval_adj"] <= alpha

    # 6) ìƒê´€í–‰ë ¬ ìƒì„± (result_df ê¸°ë°˜)
    # ëª¨ë“  ë³€ìˆ˜ë¥¼ í–‰ê³¼ ì—´ë¡œ í•˜ëŠ” ëŒ€ì¹­ í–‰ë ¬ ìƒì„±
    corr_matrix = DataFrame(np.nan, index=cols, columns=cols)
    # ëŒ€ê°ì„ : 1 (ìê¸°ìƒê´€)
    for c in cols:
        corr_matrix.loc[c, c] = 1.0
    # ìŒë³„ ìƒê´€ê³„ìˆ˜ ì±„ìš°ê¸° (ëŒ€ì¹­ì„± ìœ ì§€)
    if not result_df.empty:
        for _, row in result_df.iterrows():
            a, b, corr_val = row["var_a"], row["var_b"], row["corr"]
            corr_matrix.loc[a, b] = corr_val
            corr_matrix.loc[b, a] = corr_val  # ëŒ€ì¹­ì„±

    return result_df, corr_matrix


# ===================================================================
# ì¼ì› ë¶„ì‚°ë¶„ì„ (One-way ANOVA)
# ===================================================================
def oneway_anova(data: DataFrame, dv: str, between: str, alpha: float = 0.05) -> tuple[DataFrame, str, DataFrame | None, str]:
    """ì¼ì›ë¶„ì‚°ë¶„ì„(One-way ANOVA)ì„ ì¼ê´„ ì²˜ë¦¬í•œë‹¤.

    ì •ê·œì„± ë° ë“±ë¶„ì‚°ì„± ê²€ì •ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•œ í›„,
    ê·¸ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ANOVA ë°©ì‹ì„ ì„ íƒí•˜ì—¬ ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.
    ANOVA ê²°ê³¼ê°€ ìœ ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬í›„ê²€ì •ì„ ì‹¤ì‹œí•œë‹¤.

    ë¶„ì„ íë¦„:
    1. ì •ê·œì„± ê²€ì • (ê° ê·¸ë£¹ë³„ë¡œ normaltest ìˆ˜í–‰)
    2. ë“±ë¶„ì‚°ì„± ê²€ì • (ì •ê·œì„± ë§Œì¡± ì‹œ Bartlett, ë¶ˆë§Œì¡± ì‹œ Levene)
    3. ANOVA ìˆ˜í–‰ (ë“±ë¶„ì‚° ë§Œì¡± ì‹œ parametric ANOVA, ë¶ˆë§Œì¡± ì‹œ Welch's ANOVA)
    4. ANOVA p-value â‰¤ alpha ì¼ ë•Œ ì‚¬í›„ê²€ì • (ë“±ë¶„ì‚° ë§Œì¡± ì‹œ Tukey HSD, ë¶ˆë§Œì¡± ì‹œ Games-Howell)

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„. ì¢…ì†ë³€ìˆ˜ì™€ ê·¸ë£¹ ë³€ìˆ˜ë¥¼ í¬í•¨í•´ì•¼ í•¨.
        dv (str): ì¢…ì†ë³€ìˆ˜(Dependent Variable) ì»¬ëŸ¼ëª….
        between (str): ê·¸ë£¹ êµ¬ë¶„ ë³€ìˆ˜ ì»¬ëŸ¼ëª….
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ê°’ 0.05.

    Returns:
        tuple:
            - anova_df (DataFrame): ANOVA ë˜ëŠ” Welch ê²°ê³¼ í…Œì´ë¸”(Source, ddof1, ddof2, F, p-unc, np2 ë“± í¬í•¨).
            - anova_report (str): ì •ê·œì„±/ë“±ë¶„ì‚° ì—¬ë¶€ì™€ F, pê°’, íš¨ê³¼í¬ê¸°ë¥¼ ìš”ì•½í•œ ë³´ê³  ë¬¸ì¥.
            - posthoc_df (DataFrame|None): ì‚¬í›„ê²€ì • ê²°ê³¼(Tukey HSD ë˜ëŠ” Games-Howell). ANOVAê°€ ìœ ì˜í•  ë•Œë§Œ ìƒì„±.
            - posthoc_report (str): ì‚¬í›„ê²€ì • ìœ ë¬´ì™€ ìœ ì˜í•œ ìŒ ì •ë³´ë¥¼ ìš”ì•½í•œ ë³´ê³  ë¬¸ì¥.

    Examples:
        >>> from hossam import oneway_anova
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     'score': [5.1, 4.9, 5.3, 5.0, 4.8, 5.5, 5.2, 5.7, 5.3, 5.1],
        ...     'group': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']
        ... })
        >>> anova_df, anova_report, posthoc_df, posthoc_report = oneway_anova(df, dv='score', between='group')
        >>> print(anova_report)
        >>> if posthoc_df is not None:
        ...     print(posthoc_report)
        ...     print(posthoc_df.head())

    Raises:
        ValueError: dv ë˜ëŠ” between ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ì„ ê²½ìš°.
    """
    # ì»¬ëŸ¼ ìœ íš¨ì„± ê²€ì‚¬
    if dv not in data.columns:
        raise ValueError(f"'{dv}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
    if between not in data.columns:
        raise ValueError(f"'{between}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")

    df_filtered = data[[dv, between]].dropna()

    # ============================================
    # 1. ì •ê·œì„± ê²€ì • (ê° ê·¸ë£¹ë³„ë¡œ ìˆ˜í–‰)
    # ============================================
    group_names = sorted(df_filtered[between].unique())
    normality_satisfied = True

    for group in group_names:
        group_values = df_filtered[df_filtered[between] == group][dv].dropna()
        if len(group_values) > 0:
            s, p = normaltest(group_values)
            if p <= alpha:
                normality_satisfied = False
                break

    # ============================================
    # 2. ë“±ë¶„ì‚°ì„± ê²€ì • (ê·¸ë£¹ë³„ë¡œ ìˆ˜í–‰)
    # ============================================
    # ê° ê·¸ë£¹ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
    group_data_dict = {}
    for group in group_names:
        group_data_dict[group] = df_filtered[df_filtered[between] == group][dv].dropna().values

    # ë“±ë¶„ì‚° ê²€ì • ìˆ˜í–‰
    if len(group_names) > 1:
        if normality_satisfied:
            # ì •ê·œì„±ì„ ë§Œì¡±í•˜ë©´ Bartlett ê²€ì •
            s, p = bartlett(*group_data_dict.values())
        else:
            # ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ Levene ê²€ì •
            s, p = levene(*group_data_dict.values())
        equal_var_satisfied = p > alpha
    else:
        # ê·¸ë£¹ì´ 1ê°œì¸ ê²½ìš° ë“±ë¶„ì‚°ì„± ê²€ì • ë¶ˆê°€ëŠ¥
        equal_var_satisfied = True

    # ============================================
    # 3. ANOVA ìˆ˜í–‰
    # ============================================
    if equal_var_satisfied:
        # ë“±ë¶„ì‚°ì„ ë§Œì¡±í•  ë•Œ ì¼ë°˜ì ì¸ ANOVA ì‚¬ìš©
        anova_method = "ANOVA"
        anova_df = anova(data=df_filtered, dv=dv, between=between)
    else:
        # ë“±ë¶„ì‚°ì„ ë§Œì¡±í•˜ì§€ ì•Šì„ ë•Œ Welch's ANOVA ì‚¬ìš©
        anova_method = "Welch"
        anova_df = welch_anova(data=df_filtered, dv=dv, between=between)

    # ANOVA ê²°ê³¼ì— ë©”íƒ€ì •ë³´ ì¶”ê°€
    anova_df.insert(1, 'normality', normality_satisfied)
    anova_df.insert(2, 'equal_var', equal_var_satisfied)
    anova_df.insert(3, 'method', anova_method)

    # ìœ ì˜ì„± ì—¬ë¶€ ì»¬ëŸ¼ ì¶”ê°€
    if 'p-unc' in anova_df.columns:
        anova_df['significant'] = anova_df['p-unc'] <= alpha

    # ANOVA ê²°ê³¼ê°€ ìœ ì˜í•œì§€ í™•ì¸
    p_unc = float(anova_df.loc[0, 'p-unc'])
    anova_significant = p_unc <= alpha

    # ANOVA ë³´ê³  ë¬¸ì¥ ìƒì„±
    def _safe_get(col: str, default: float = np.nan) -> float:
        try:
            return float(anova_df.loc[0, col]) if col in anova_df.columns else default
        except Exception:
            return default

    df1 = _safe_get('ddof1')
    df2 = _safe_get('ddof2')
    fval = _safe_get('F')
    eta2 = _safe_get('np2')

    anova_sig_text = "ê·¸ë£¹ë³„ í‰ê· ì´ ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤." if anova_significant else "ê·¸ë£¹ë³„ í‰ê·  ì°¨ì´ì— ëŒ€í•œ ê·¼ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    assumption_text = f"ì •ê·œì„±ì€ {'ëŒ€ì²´ë¡œ ë§Œì¡±' if normality_satisfied else 'ì¶©ì¡±ë˜ì§€ ì•Šì•˜ê³ '}, ë“±ë¶„ì‚°ì„±ì€ {'ì¶©ì¡±' if equal_var_satisfied else 'ì¶©ì¡±ë˜ì§€ ì•Šì•˜ë‹¤'}ê³  íŒë‹¨ë©ë‹ˆë‹¤."

    anova_report = (
        f"{between}ë³„ë¡œ {dv} í‰ê· ì„ ë¹„êµí•œ {anova_method} ê²°ê³¼: F({df1:.3f}, {df2:.3f}) = {fval:.3f}, p = {p_unc:.4f}. "
        f"í•´ì„: {anova_sig_text} {assumption_text}"
    )

    if not np.isnan(eta2):
        anova_report += f" íš¨ê³¼ í¬ê¸°(Î·Â²p) â‰ˆ {eta2:.3f}, ê°’ì´ í´ìˆ˜ë¡ ê·¸ë£¹ ì°¨ì´ê°€ ëšœë ·í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤."

    # ============================================
    # 4. ì‚¬í›„ê²€ì • (ANOVA ìœ ì˜í•  ë•Œë§Œ)
    # ============================================
    posthoc_df = None
    posthoc_method = 'None'
    posthoc_report = "ANOVA ê²°ê³¼ê°€ ìœ ì˜í•˜ì§€ ì•Šì•„ ì‚¬í›„ê²€ì •ì„ ì§„í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    if anova_significant:
        if equal_var_satisfied:
            # ë“±ë¶„ì‚°ì„ ë§Œì¡±í•˜ë©´ Tukey HSD ì‚¬ìš©
            posthoc_method = "Tukey HSD"
            posthoc_df = pairwise_tukey(data=df_filtered, dv=dv, between=between)
        else:
            # ë“±ë¶„ì‚°ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ Games-Howell ì‚¬ìš©
            posthoc_method = "Games-Howell"
            posthoc_df = pairwise_gameshowell(df_filtered, dv=dv, between=between)

        # ì‚¬í›„ê²€ì • ê²°ê³¼ì— ë©”íƒ€ì •ë³´ ì¶”ê°€
        # posthoc_df.insert(0, 'normality', normality_satisfied)
        # posthoc_df.insert(1, 'equal_var', equal_var_satisfied)
        posthoc_df.insert(0, 'method', posthoc_method)

        # p-value ì»¬ëŸ¼ íƒìƒ‰
        p_cols = [c for c in ["p-tukey", "pval", "p-adjust", "p_adj", "p-corr", "p", "p-unc", "pvalue", "p_value"] if c in posthoc_df.columns]
        p_col = p_cols[0] if p_cols else None

        if p_col:
            # ìœ ì˜ì„± ì—¬ë¶€ ì»¬ëŸ¼ ì¶”ê°€
            posthoc_df['significant'] = posthoc_df[p_col] <= alpha

            sig_pairs_df = posthoc_df[posthoc_df[p_col] <= alpha]
            sig_count = len(sig_pairs_df)
            total_count = len(posthoc_df)
            pair_samples = []
            if not sig_pairs_df.empty and {'A', 'B'}.issubset(sig_pairs_df.columns):
                pair_samples = [f"{row['A']} vs {row['B']}" for _, row in sig_pairs_df.head(3).iterrows()]

            if sig_count > 0:
                posthoc_report = (
                    f"{posthoc_method} ì‚¬í›„ê²€ì •ì—ì„œ {sig_count}/{total_count}ìŒì´ ì˜ë¯¸ ìˆëŠ” ì°¨ì´ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤ (alpha={alpha})."
                )
                if pair_samples:
                    posthoc_report += " ì˜ˆ: " + ", ".join(pair_samples) + " ë“±."
            else:
                posthoc_report = f"{posthoc_method} ì‚¬í›„ê²€ì •ì—ì„œ ì¶”ê°€ë¡œ ìœ ì˜í•œ ìŒì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:
            posthoc_report = f"{posthoc_method} ê²°ê³¼ëŠ” ìƒì„±í–ˆì§€ë§Œ p-value ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ìœ ì˜ì„±ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ============================================
    # 5. ê²°ê³¼ ë°˜í™˜
    # ============================================
    return anova_df, anova_report, posthoc_df, posthoc_report


# ===================================================================
# ì´ì› ë¶„ì‚°ë¶„ì„ (Two-way ANOVA: ë‘ ë²”ì£¼í˜• ë…ë¦½ë³€ìˆ˜)
# ===================================================================
def twoway_anova(
    data: DataFrame,
    dv: str,
    factor_a: str,
    factor_b: str,
    alpha: float = 0.05,
) -> tuple[DataFrame, str, DataFrame | None, str]:
    """ë‘ ë²”ì£¼í˜• ìš”ì¸ì— ëŒ€í•œ ì´ì›ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  í•´ì„ìš© ë³´ê³ ë¬¸ì„ ë°˜í™˜í•œë‹¤.

    ë¶„ì„ íë¦„:
    1) ê° ì…€(ìš”ì¸ ì¡°í•©)ë³„ ì •ê·œì„± ê²€ì •
    2) ì „ì²´ ì…€ì„ ëŒ€ìƒìœ¼ë¡œ ë“±ë¶„ì‚°ì„± ê²€ì • (ì •ê·œì„± ì¶©ì¡± ì‹œ Bartlett, ë¶ˆì¶©ì¡± ì‹œ Levene)
    3) ë‘ ìš”ì¸ ë° êµí˜¸ì‘ìš©ì„ í¬í•¨í•œ 2ì› ANOVA ìˆ˜í–‰
    4) ìœ ì˜í•œ ìš”ì¸ì— ëŒ€í•´ Tukey HSD ì‚¬í›„ê²€ì •(ìš”ì¸ë³„) ì‹¤í–‰

    Args:
        data (DataFrame): ì¢…ì†ë³€ìˆ˜ì™€ ë‘ ê°œì˜ ë²”ì£¼í˜• ìš”ì¸ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        dv (str): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª….
        factor_a (str): ì²« ë²ˆì§¸ ìš”ì¸ ì»¬ëŸ¼ëª….
        factor_b (str): ë‘ ë²ˆì§¸ ìš”ì¸ ì»¬ëŸ¼ëª….
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ 0.05.

    Returns:
        tuple:
            - anova_df (DataFrame): 2ì› ANOVA ê²°ê³¼(ê° ìš”ì¸ê³¼ ìƒí˜¸ì‘ìš©ì˜ F, p, Î·Â²p í¬í•¨).
            - anova_report (str): ë‘ ìš”ì¸ ë° ìƒí˜¸ì‘ìš©ì˜ ìœ ì˜ì„±/ê°€ì • ì¶©ì¡± ì—¬ë¶€ë¥¼ ìš”ì•½í•œ ë¬¸ì¥.
            - posthoc_df (DataFrame|None): ìœ ì˜í•œ ìš”ì¸ì— ëŒ€í•œ Tukey ì‚¬í›„ê²€ì • ê²°ê³¼(ìš”ì¸ëª…, A, B, p í¬í•¨). ì—†ìœ¼ë©´ None.
            - posthoc_report (str): ì‚¬í›„ê²€ì • ìœ ë¬´ ë° ìœ ì˜ ìŒ ìš”ì•½ ë¬¸ì¥.

    Raises:
        ValueError: ì…ë ¥ ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ì„ ë•Œ.
    """
    # ì»¬ëŸ¼ ìœ íš¨ì„± ê²€ì‚¬
    for col in [dv, factor_a, factor_b]:
        if col not in data.columns:
            raise ValueError(f"'{col}' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")

    df_filtered = data[[dv, factor_a, factor_b]].dropna()

    # 1) ì…€ë³„ ì •ê·œì„± ê²€ì •
    normality_satisfied = True
    for (a, b), subset in df_filtered.groupby([factor_a, factor_b], observed=False):
        vals = subset[dv].dropna()
        if len(vals) > 0:
            _, p = normaltest(vals)
            if p <= alpha:
                normality_satisfied = False
                break

    # 2) ë“±ë¶„ì‚°ì„± ê²€ì • (ì…€ ë‹¨ìœ„)
    cell_values = [g[dv].dropna().values for _, g in df_filtered.groupby([factor_a, factor_b], observed=False)]
    if len(cell_values) > 1:
        if normality_satisfied:
            _, p_var = bartlett(*cell_values)
        else:
            _, p_var = levene(*cell_values)
        equal_var_satisfied = p_var > alpha
    else:
        equal_var_satisfied = True

    # 3) 2ì› ANOVA ìˆ˜í–‰ (pingouin anova with between factors)
    anova_df = anova(data=df_filtered, dv=dv, between=[factor_a, factor_b], effsize="np2")
    anova_df.insert(0, "normality", normality_satisfied)
    anova_df.insert(1, "equal_var", equal_var_satisfied)
    if 'p-unc' in anova_df.columns:
        anova_df['significant'] = anova_df['p-unc'] <= alpha

    # ë³´ê³ ë¬¸ ìƒì„±
    def _safe(row, col, default=np.nan):
        try:
            return float(row[col])
        except Exception:
            return default

    # ìš”ì¸ë³„ ë¬¸ì¥
    reports = []
    sig_flags = {}
    for _, row in anova_df.iterrows():
        term = row.get("Source", "")
        fval = _safe(row, "F")
        pval = _safe(row, "p-unc")
        eta2 = _safe(row, "np2")
        sig = pval <= alpha
        sig_flags[term] = sig
        if term.lower() == "residual":
            continue
        effect_name = term.replace("*", "ì™€ ")
        msg = f"{effect_name}: F={fval:.3f}, p={pval:.4f}. í•´ì„: "
        msg += "ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤." if sig else "ìœ ì˜í•œ ì°¨ì´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        if not np.isnan(eta2):
            msg += f" íš¨ê³¼ í¬ê¸°(Î·Â²p)â‰ˆ{eta2:.3f}."
        reports.append(msg)

    assumption_text = f"ì •ê·œì„±ì€ {'ëŒ€ì²´ë¡œ ë§Œì¡±' if normality_satisfied else 'ì¶©ì¡±ë˜ì§€ ì•ŠìŒ'}, ë“±ë¶„ì‚°ì„±ì€ {'ì¶©ì¡±' if equal_var_satisfied else 'ì¶©ì¡±ë˜ì§€ ì•ŠìŒ'}ìœ¼ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤."
    anova_report = " ".join(reports) + " " + assumption_text

    # 4) ì‚¬í›„ê²€ì •: ìœ ì˜í•œ ìš”ì¸(êµí˜¸ì‘ìš© ì œì™¸) ëŒ€ìƒ, ìˆ˜ì¤€ì´ 2 ì´ˆê³¼ì¼ ë•Œë§Œ ì‹¤í–‰
    posthoc_df_list = []
    interaction_name = f"{factor_a}*{factor_b}".lower()
    interaction_name_spaced = f"{factor_a} * {factor_b}".lower()

    for factor, sig in sig_flags.items():
        if factor is None:
            continue
        factor_lower = str(factor).lower()

        # êµí˜¸ì‘ìš©(residual í¬í•¨) í˜¹ì€ ë¹„ìœ ì˜ í•­ì€ ê±´ë„ˆë›´ë‹¤
        if factor_lower in ["residual", interaction_name, interaction_name_spaced] or not sig:
            continue

        # ì‹¤ì œ ì»¬ëŸ¼ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›´ë‹¤ (ex: "A * B" ê°™ì€ êµí˜¸ì‘ìš© ì´ë¦„)
        if factor not in df_filtered.columns:
            continue

        levels = df_filtered[factor].unique()
        if len(levels) <= 2:
            continue
        tukey_df = pairwise_tukey(data=df_filtered, dv=dv, between=factor)
        tukey_df.insert(0, "factor", factor)
        posthoc_df_list.append(tukey_df)

    posthoc_df = None
    posthoc_report = "ì‚¬í›„ê²€ì •ì´ í•„ìš”í•˜ì§€ ì•Šê±°ë‚˜ ìœ ì˜í•œ ìš”ì¸ì´ ì—†ìŠµë‹ˆë‹¤."
    if posthoc_df_list:
        posthoc_df = concat(posthoc_df_list, ignore_index=True)
        p_cols = [c for c in ["p-tukey", "pval", "p-adjust", "p_adj", "p-corr", "p", "p-unc", "pvalue", "p_value"] if c in posthoc_df.columns]
        p_col = p_cols[0] if p_cols else None
        if p_col:
            posthoc_df['significant'] = posthoc_df[p_col] <= alpha
            sig_df = posthoc_df[posthoc_df[p_col] <= alpha]
            sig_count = len(sig_df)
            total_count = len(posthoc_df)
            examples = []
            if not sig_df.empty and {"A", "B"}.issubset(sig_df.columns):
                examples = [f"{row['A']} vs {row['B']}" for _, row in sig_df.head(3).iterrows()]
            if sig_count > 0:
                posthoc_report = f"ì‚¬í›„ê²€ì •(Tukey)ì—ì„œ {sig_count}/{total_count}ìŒì´ ì˜ë¯¸ ìˆëŠ” ì°¨ì´ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤."
                if examples:
                    posthoc_report += " ì˜ˆ: " + ", ".join(examples) + " ë“±."
            else:
                posthoc_report = "ì‚¬í›„ê²€ì • ê²°ê³¼ ì¶”ê°€ë¡œ ìœ ì˜í•œ ìŒì€ ì—†ì—ˆìŠµë‹ˆë‹¤."
        else:
            posthoc_report = "ì‚¬í›„ê²€ì • ê²°ê³¼ë¥¼ ìƒì„±í–ˆìœ¼ë‚˜ p-value ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return anova_df, anova_report, posthoc_df, posthoc_report


# ===================================================================
# ëª¨ë¸ ì˜ˆì¸¡ (Model Prediction)
# ===================================================================
def predict(fit, data: DataFrame | Series) -> DataFrame | Series | float:
    """íšŒê·€ ë˜ëŠ” ë¡œì§€ìŠ¤í‹± ëª¨í˜•ì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•œë‹¤.

    statsmodelsì˜ RegressionResultsWrapper(ì„ í˜•íšŒê·€) ë˜ëŠ”
    BinaryResultsWrapper(ë¡œì§€ìŠ¤í‹± íšŒê·€) ê°ì²´ë¥¼ ë°›ì•„ ë°ì´í„°ì— ëŒ€í•œ
    ì˜ˆì¸¡ê°’ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•œë‹¤.

    ëª¨í˜• í•™ìŠµ ì‹œ ìƒìˆ˜í•­ì´ í¬í•¨ë˜ì—ˆë‹¤ë©´, ì˜ˆì¸¡ ë°ì´í„°ì—ë„ ìë™ìœ¼ë¡œ
    ìƒìˆ˜í•­ì„ ì¶”ê°€í•˜ì—¬ ì°¨ì›ì„ ë§ì¶˜ë‹¤.

    ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš° ì˜ˆì¸¡ í™•ë¥ ê³¼ í•¨ê»˜ ë¶„ë¥˜ í•´ì„ì„ í¬í•¨í•œë‹¤.

    Args:
        fit: í•™ìŠµëœ íšŒê·€ ëª¨í˜• ê°ì²´.
             - statsmodels.regression.linear_model.RegressionResultsWrapper (ì„ í˜•íšŒê·€)
             - statsmodels.discrete.discrete_model.BinaryResultsWrapper (ë¡œì§€ìŠ¤í‹± íšŒê·€)
        data (DataFrame|Series): ì˜ˆì¸¡ì— ì‚¬ìš©í•  ì„¤ëª…ë³€ìˆ˜.
                                 - DataFrame: ì—¬ëŸ¬ ê°œì˜ ê´€ì¸¡ì¹˜
                                 - Series: ë‹¨ì¼ ê´€ì¸¡ì¹˜ ë˜ëŠ” ë³€ìˆ˜ í•˜ë‚˜
                                 ì›ë³¸ ëª¨í˜• í•™ìŠµ ì‹œ ì‚¬ìš©í•œ íŠ¹ì„±ê³¼ ë™ì¼í•´ì•¼ í•¨.
                                 (ìƒìˆ˜í•­ ì œì™¸, ìë™ìœ¼ë¡œ ì¶”ê°€ë¨)

    Returns:
        DataFrame|Series|float: ì˜ˆì¸¡ê°’.
                          - DataFrame ì…ë ¥:
                            - ì„ í˜•íšŒê·€: ì˜ˆì¸¡ê°’ ì»¬ëŸ¼ì„ í¬í•¨í•œ DataFrame
                            - ë¡œì§€ìŠ¤í‹±: í™•ë¥ , ë¶„ë¥˜, í•´ì„ ì»¬ëŸ¼ì„ í¬í•¨í•œ DataFrame
                          - Series ì…ë ¥: ë‹¨ì¼ ì˜ˆì¸¡ê°’ (float)

    Raises:
        ValueError: fit ê°ì²´ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” íƒ€ì…ì¸ ê²½ìš°.
        Exception: ë°ì´í„°ì™€ ëª¨í˜•ì˜ íŠ¹ì„± ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ predict ì‹¤íŒ¨.

    Examples:
        >>> import statsmodels.api as sm
        >>> # ì„ í˜•íšŒê·€ (ìƒìˆ˜í•­ ìë™ ì¶”ê°€)
        >>> X = sm.add_constant(df[['x1', 'x2']])
        >>> y = df['y']
        >>> fit_ols = sm.OLS(y, X).fit()
        >>> pred = predict(fit_ols, df_new[['x1', 'x2']])  # DataFrame ë°˜í™˜

        >>> # ë¡œì§€ìŠ¤í‹± íšŒê·€ (ìƒìˆ˜í•­ ìë™ ì¶”ê°€)
        >>> fit_logit = sm.Logit(y_binary, X).fit()
        >>> pred_prob = predict(fit_logit, df_new[['x1', 'x2']])  # DataFrame ë°˜í™˜ (í•´ì„ í¬í•¨)
    """
    from statsmodels.regression.linear_model import RegressionResultsWrapper
    from statsmodels.discrete.discrete_model import BinaryResults

    # fit ê°ì²´ì˜ íƒ€ì… í™•ì¸
    fit_type = type(fit).__name__

    # RegressionResultsWrapperì¸ì§€ BinaryResultsì¸ì§€ í™•ì¸
    is_linear = isinstance(fit, RegressionResultsWrapper)
    is_logit = isinstance(fit, BinaryResults) or 'BinaryResult' in fit_type

    if not (is_linear or is_logit):
        raise ValueError(
            f"ì§€ì›ë˜ì§€ ì•ŠëŠ” fit ê°ì²´ íƒ€ì…ì…ë‹ˆë‹¤: {fit_type}\n"
            "RegressionResultsWrapper ë˜ëŠ” BinaryResultsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        )

    # Seriesë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    if isinstance(data, Series):
        data_to_predict = data.to_frame().T
        is_series = True
    else:
        data_to_predict = data.copy()
        is_series = False

    try:
        # ëª¨í˜•ì˜ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì™€ ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„± ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ìƒìˆ˜í•­ í•„ìš” ì—¬ë¶€ íŒë‹¨
        n_params = len(fit.params)
        n_features = data_to_predict.shape[1]

        # ìƒìˆ˜í•­ì´ í•„ìš”í•œ ê²½ìš° ìë™ìœ¼ë¡œ ì¶”ê°€
        if n_params == n_features + 1:
            data_to_predict = sm.add_constant(data_to_predict, has_constant='skip')
        elif n_params != n_features:
            raise ValueError(
                f"íŠ¹ì„± ìˆ˜ ë¶ˆì¼ì¹˜: ëª¨í˜•ì€ {n_params}ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ëŒ€í•˜ì§€ë§Œ, "
                f"ì…ë ¥ ë°ì´í„°ëŠ” {n_features}ê°œì˜ íŠ¹ì„±ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. "
                f"(ìƒìˆ˜í•­ ìë™ ê°ì§€ í›„ì—ë„ ë¶ˆì¼ì¹˜)"
            )

        # ì˜ˆì¸¡ê°’ ìƒì„±
        predictions = fit.predict(data_to_predict)

        # Series ì…ë ¥ì¸ ê²½ìš° ë‹¨ì¼ ê°’ ë°˜í™˜
        if is_series:
            return float(predictions.iloc[0])

        # DataFrame ì…ë ¥ì¸ ê²½ìš°
        if isinstance(data, DataFrame):
            result_df = DataFrame({'ì˜ˆì¸¡ê°’': predictions}, index=data.index)

            # ë¡œì§€ìŠ¤í‹± íšŒê·€ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ í¬í•¨
            if is_logit:
                # í™•ë¥  í™•ì¸
                result_df['í™•ë¥ (%)'] = (predictions * 100).round(2)
                # ë¶„ë¥˜ (0.5 ê¸°ì¤€)
                result_df['ë¶„ë¥˜'] = (predictions >= 0.5).astype(int)
                # í•´ì„ ì¶”ê°€
                result_df['í•´ì„'] = result_df['ë¶„ë¥˜'].apply(
                    lambda x: 'ì–‘ì„±(1)' if x == 1 else 'ìŒì„±(0)'
                )
                # ì‹ ë¢°ë„ í‰ê°€
                result_df['ì‹ ë¢°ë„'] = result_df['í™•ë¥ (%)'].apply(
                    lambda x: f"{abs(x - 50):.1f}% í™•ì‹¤"
                )

            return result_df

        return predictions

    except Exception as e:
        raise Exception(
            f"ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
            f"ëª¨í˜• í•™ìŠµ ì‹œ ì‚¬ìš©í•œ íŠ¹ì„±ê³¼ ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„±ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
            f"ì›ë³¸ ì˜¤ë¥˜: {str(e)}"
        )


# ===================================================================
# í™•ì¥ëœ ê¸°ìˆ í†µê³„ëŸ‰ (Extended Descriptive Statistics)
# ===================================================================
def describe(data: DataFrame, *fields: str, columns: list = None):
    """ë°ì´í„°í”„ë ˆì„ì˜ ì—°ì†í˜• ë³€ìˆ˜ì— ëŒ€í•œ í™•ì¥ëœ ê¸°ìˆ í†µê³„ëŸ‰ì„ ë°˜í™˜í•œë‹¤.

    ê° ì—°ì†í˜•(ìˆ«ìí˜•) ì»¬ëŸ¼ì˜ ê¸°ìˆ í†µê³„ëŸ‰(describe)ì„ êµ¬í•˜ê³ , ì´ì— ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„(IQR),
    ì´ìƒì¹˜ ê²½ê³„ê°’(UP, DOWN), ì™œë„(skew), ì´ìƒì¹˜ ê°œìˆ˜ ë° ë¹„ìœ¨, ë¶„í¬ íŠ¹ì„±, ë¡œê·¸ë³€í™˜ í•„ìš”ì„±ì„
    ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        *fields (str): ë¶„ì„í•  ì»¬ëŸ¼ëª… ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ì„ ì²˜ë¦¬.
        columns (list, optional): ë°˜í™˜í•  í†µê³„ëŸ‰ ì»¬ëŸ¼ ëª©ë¡. Noneì´ë©´ ëª¨ë“  í†µê³„ëŸ‰ ë°˜í™˜.

    Returns:
        DataFrame: ê° í•„ë“œë³„ í™•ì¥ëœ ê¸°ìˆ í†µê³„ëŸ‰ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
            í–‰ì€ ë‹¤ìŒê³¼ ê°™ì€ í†µê³„ëŸ‰ì„ í¬í•¨:

            - count (float): ë¹„ê²°ì¸¡ì¹˜ì˜ ìˆ˜
            - mean (float): í‰ê· ê°’
            - std (float): í‘œì¤€í¸ì°¨
            - min (float): ìµœì†Œê°’
            - 25% (float): ì œ1ì‚¬ë¶„ìœ„ìˆ˜ (Q1)
            - 50% (float): ì œ2ì‚¬ë¶„ìœ„ìˆ˜ (ì¤‘ì•™ê°’)
            - 75% (float): ì œ3ì‚¬ë¶„ìœ„ìˆ˜ (Q3)
            - max (float): ìµœëŒ€ê°’
            - iqr (float): ì‚¬ë¶„ìœ„ ë²”ìœ„ (Q3 - Q1)
            - up (float): ì´ìƒì¹˜ ìƒí•œ ê²½ê³„ê°’ (Q3 + 1.5 * IQR)
            - down (float): ì´ìƒì¹˜ í•˜í•œ ê²½ê³„ê°’ (Q1 - 1.5 * IQR)
            - skew (float): ì™œë„
            - outlier_count (int): ì´ìƒì¹˜ ê°œìˆ˜
            - outlier_rate (float): ì´ìƒì¹˜ ë¹„ìœ¨(%)
            - dist (str): ë¶„í¬ íŠ¹ì„± ("ê·¹ë‹¨ ìš°ì¸¡ ê¼¬ë¦¬", "ê±°ì˜ ëŒ€ì¹­" ë“±)
            - log_need (str): ë¡œê·¸ë³€í™˜ í•„ìš”ì„± ("ë†’ìŒ", "ì¤‘ê°„", "ë‚®ìŒ")

    Examples:
        ì „ì²´ ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•œ í™•ì¥ëœ ê¸°ìˆ í†µê³„:

        >>> from hossam import summary
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     'x': [1, 2, 3, 4, 5, 100],
        ...     'y': [10, 20, 30, 40, 50, 60],
        ...     'z': ['a', 'b', 'c', 'd', 'e', 'f']
        ... })
        >>> result = summary(df)
        >>> print(result)

        íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¶„ì„:

        >>> result = summary(df, 'x', 'y')
        >>> print(result)

    Notes:
        - ìˆ«ìí˜•ì´ ì•„ë‹Œ ì»¬ëŸ¼ì€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤.
        - ê²°ê³¼ëŠ” í•„ë“œ(ì»¬ëŸ¼)ê°€ í–‰ìœ¼ë¡œ, í†µê³„ëŸ‰ì´ ì—´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
        - Tukeyì˜ 1.5 * IQR ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íŒì •í•©ë‹ˆë‹¤.
        - ë¶„í¬ íŠ¹ì„±ì€ ì™œë„ ê°’ìœ¼ë¡œ íŒì •í•©ë‹ˆë‹¤.
        - ë¡œê·¸ë³€í™˜ í•„ìš”ì„±ì€ ì™œë„ì˜ ì ˆëŒ“ê°’ í¬ê¸°ë¡œ íŒì •í•©ë‹ˆë‹¤.
    """
    if not fields:
        fields = data.select_dtypes(include=['int', 'int32', 'int64', 'float', 'float32', 'float64']).columns

    # ê¸°ìˆ í†µê³„ëŸ‰ êµ¬í•˜ê¸°
    desc = data[list(fields)].describe().T

    # ì¶”ê°€ í†µê³„ëŸ‰ ê³„ì‚°
    additional_stats = []
    for f in fields:
        # ìˆ«ì íƒ€ì…ì´ ì•„ë‹ˆë¼ë©´ ê±´ë„ˆëœ€
        if data[f].dtype not in [
            'int',
            'int32',
            'int64',
            'float',
            'float32',
            'float64',
            'int64',
            'float64',
            'float32'
        ]:
            continue

        # ì‚¬ë¶„ìœ„ìˆ˜
        q1 = data[f].quantile(q=0.25)
        q3 = data[f].quantile(q=0.75)

        # ì´ìƒì¹˜ ê²½ê³„ (Tukey's fences)
        iqr = q3 - q1
        down = q1 - 1.5 * iqr
        up = q3 + 1.5 * iqr

        # ì™œë„
        skew = data[f].skew()

        # ì´ìƒì¹˜ ê°œìˆ˜ ë° ë¹„ìœ¨
        outlier_count = ((data[f] < down) | (data[f] > up)).sum()
        outlier_rate = (outlier_count / len(data)) * 100

        # ë¶„í¬ íŠ¹ì„± íŒì • (ì™œë„ ê¸°ì¤€)
        abs_skew = abs(skew)
        if abs_skew < 0.5:
            dist = "ê±°ì˜ ëŒ€ì¹­"
        elif abs_skew < 1.0:
            if skew > 0:
                dist = "ì•½í•œ ìš°ì¸¡ ê¼¬ë¦¬"
            else:
                dist = "ì•½í•œ ì¢Œì¸¡ ê¼¬ë¦¬"
        elif abs_skew < 2.0:
            if skew > 0:
                dist = "ì¤‘ê°„ ìš°ì¸¡ ê¼¬ë¦¬"
            else:
                dist = "ì¤‘ê°„ ì¢Œì¸¡ ê¼¬ë¦¬"
        else:
            if skew > 0:
                dist = "ê·¹ë‹¨ ìš°ì¸¡ ê¼¬ë¦¬"
            else:
                dist = "ê·¹ë‹¨ ì¢Œì¸¡ ê¼¬ë¦¬"

        # ë¡œê·¸ë³€í™˜ í•„ìš”ì„± íŒì •
        if abs_skew < 0.5:
            log_need = "ë‚®ìŒ"
        elif abs_skew < 1.0:
            log_need = "ì¤‘ê°„"
        else:
            log_need = "ë†’ìŒ"

        additional_stats.append({
            'field': f,
            'iqr': iqr,
            'up': up,
            'down': down,
            'outlier_count': outlier_count,
            'outlier_rate': outlier_rate,
            'skew': skew,
            'dist': dist,
            'log_need': log_need
        })

    additional_df = DataFrame(additional_stats).set_index('field')

    # ê²°ê³¼ ë³‘í•©
    result = concat([desc, additional_df], axis=1)

    # columns íŒŒë¼ë¯¸í„°ê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì»¬ëŸ¼ë§Œ í•„í„°ë§
    if columns is not None:
        result = result[columns]

    return result


# ===================================================================
# ìƒê´€ê³„ìˆ˜ ë° íš¨ê³¼í¬ê¸° ë¶„ì„ (Correlation & Effect Size)
# ===================================================================
def corr_effect_size(data: DataFrame, dv: str, *fields: str, alpha: float = 0.05) -> DataFrame:
    """ì¢…ì†ë³€ìˆ˜ì™€ì˜ í¸ìƒê´€ê³„ìˆ˜ ë° íš¨ê³¼í¬ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.

    ê° ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ë˜, ì •ê·œì„±ê³¼ ì„ í˜•ì„±ì„ ê²€ì‚¬í•˜ì—¬
    Pearson ë˜ëŠ” Spearman ìƒê´€ê³„ìˆ˜ë¥¼ ì ì ˆíˆ ì„ íƒí•œë‹¤.
    Cohen's d (íš¨ê³¼í¬ê¸°)ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒê´€ ê°•ë„ë¥¼ ì •ëŸ‰í™”í•œë‹¤.

    Args:
        data (DataFrame): ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
        dv (str): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ ì´ë¦„.
        *fields (str): ë…ë¦½ë³€ìˆ˜ ì»¬ëŸ¼ ì´ë¦„ë“¤. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ dv ì œì™¸ ëª¨ë‘ ì‚¬ìš©.
        alpha (float, optional): ìœ ì˜ìˆ˜ì¤€. ê¸°ë³¸ 0.05.

    Returns:
        DataFrame: ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„:
            - Variable (str): ë…ë¦½ë³€ìˆ˜ ì´ë¦„
            - Correlation (float): ìƒê´€ê³„ìˆ˜ (Pearson ë˜ëŠ” Spearman)
            - Corr_Type (str): ì„ íƒëœ ìƒê´€ê³„ìˆ˜ ì¢…ë¥˜ ('Pearson' ë˜ëŠ” 'Spearman')
            - P-value (float): ìƒê´€ê³„ìˆ˜ì˜ ìœ ì˜í™•ë¥ 
            - Cohens_d (float): í‘œì¤€í™”ëœ íš¨ê³¼í¬ê¸°
            - Effect_Size (str): íš¨ê³¼í¬ê¸° ë¶„ë¥˜ ('Large', 'Medium', 'Small', 'Negligible')

    Examples:
        >>> from hossam import hs_stats
        >>> import pandas as pd
        >>> df = pd.DataFrame({'age': [20, 30, 40, 50],
        ...                     'bmi': [22, 25, 28, 30],
        ...                     'charges': [1000, 2000, 3000, 4000]})
        >>> result = hs_stats.corr_effect_size(df, 'charges', 'age', 'bmi')
        >>> print(result)
    """

    # fieldsê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ dv ì œì™¸ ëª¨ë‘ ì‚¬ìš©
    if not fields:
        fields = [col for col in data.columns
                 if is_numeric_dtype(data[col]) and col != dv]

    # dvê°€ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸
    if not is_numeric_dtype(data[dv]):
        raise ValueError(f"Dependent variable '{dv}' must be numeric type")

    results = []

    for var in fields:
        if not is_numeric_dtype(data[var]):
            continue

        # ê²°ì¸¡ì¹˜ ì œê±°
        valid_idx = data[[var, dv]].notna().all(axis=1)
        x = data.loc[valid_idx, var].values
        y = data.loc[valid_idx, dv].values

        if len(x) < 3:
            continue

        # ì •ê·œì„± ê²€ì‚¬ (Shapiro-Wilk: n <= 5000 ê¶Œì¥, ê·¸ ì™¸ D'Agostino)
        method_x = 's' if len(x) <= 5000 else 'n'
        method_y = 's' if len(y) <= 5000 else 'n'

        normal_x_result = normal_test(data[[var]], columns=[var], method=method_x)
        normal_y_result = normal_test(data[[dv]], columns=[dv], method=method_y)

        # ì •ê·œì„± íŒì • (p > alphaë©´ ì •ê·œë¶„í¬ ê°€ì •)
        normal_x = normal_x_result.loc[var, 'p-val'] > alpha if var in normal_x_result.index else False
        normal_y = normal_y_result.loc[dv, 'p-val'] > alpha if dv in normal_y_result.index else False

        # Pearson (ëª¨ë‘ ì •ê·œ) vs Spearman (í•˜ë‚˜ë¼ë„ ë¹„ì •ê·œ)
        if normal_x and normal_y:
            r, p = pearsonr(x, y)
            corr_type = 'Pearson'
        else:
            r, p = spearmanr(x, y)
            corr_type = 'Spearman'

        # Cohen's d ê³„ì‚° (ìƒê´€ê³„ìˆ˜ì—ì„œ íš¨ê³¼í¬ê¸°ë¡œ ë³€í™˜)
        # d = 2*r / sqrt(1-r^2)
        if r**2 < 1:
            d = (2 * r) / np.sqrt(1 - r**2)
        else:
            d = 0

        # íš¨ê³¼í¬ê¸° ë¶„ë¥˜ (Cohen's d ê¸°ì¤€)
        # Small: 0.2 < |d| <= 0.5
        # Medium: 0.5 < |d| <= 0.8
        # Large: |d| > 0.8
        abs_d = abs(d)
        if abs_d > 0.8:
            effect_size = 'Large'
        elif abs_d > 0.5:
            effect_size = 'Medium'
        elif abs_d > 0.2:
            effect_size = 'Small'
        else:
            effect_size = 'Negligible'

        results.append({
            'Variable': var,
            'Correlation': r,
            'Corr_Type': corr_type,
            'P-value': p,
            'Cohens_d': d,
            'Effect_Size': effect_size
        })

    result_df = DataFrame(results)

    # ìƒê´€ê³„ìˆ˜ë¡œ ì •ë ¬ (ì ˆëŒ“ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    if len(result_df) > 0:
        result_df = result_df.sort_values('Correlation', key=lambda x: x.abs(), ascending=False).reset_index(drop=True)

    return result_df
