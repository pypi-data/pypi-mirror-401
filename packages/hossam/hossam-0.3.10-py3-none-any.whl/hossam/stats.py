# -*- coding: utf-8 -*-
# -------------------------------------------------------------
import numpy as np
from typing import Tuple
from pandas import DataFrame
from pandas.api.types import is_bool_dtype

from sklearn.metrics import (
    confusion_matrix,
    roc_auc_score,
    accuracy_score,
    recall_score,
    precision_score,
    f1_score,
)

from scipy.stats import (
    shapiro,
    normaltest,
    bartlett,
    levene,
    ttest_1samp,
    ttest_ind,
    ttest_rel,
    wilcoxon
)

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# -------------------------------------------------------------

def hs_normal_test(data: DataFrame, columns: list | str | None = None, method: str = "n") -> DataFrame:
    """ì§€ì •ëœ ì»¬ëŸ¼(ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)ì— ëŒ€í•´ ì •ê·œì„± ê²€ì •ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    ì •ê·œì„± ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ "ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤"ì´ë¯€ë¡œ, p-value > 0.05ì¼ ë•Œ
    ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì§€ ì•Šìœ¼ë©° ì •ê·œì„±ì„ ì¶©ì¡±í•œë‹¤ê³  í•´ì„í•œë‹¤.

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        columns (list | str | None, optional): ê²€ì • ëŒ€ìƒ ì»¬ëŸ¼ëª….
            - None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰.
            - ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸: ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ê²€ì • ìˆ˜í–‰.
            - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´: "A, B, C" í˜•ì‹ìœ¼ë¡œ ì»¬ëŸ¼ëª… ì§€ì • ê°€ëŠ¥.
            ê¸°ë³¸ê°’ì€ None.
        method (str, optional): ì •ê·œì„± ê²€ì • ë°©ë²•.
            - "n": D'Agostino and Pearson's Omnibus test (í‘œë³¸ í¬ê¸° 20 ì´ìƒ ê¶Œì¥)
            - "s": Shapiro-Wilk test (í‘œë³¸ í¬ê¸° 5000 ì´í•˜ ê¶Œì¥)
            ê¸°ë³¸ê°’ì€ "n".

    Returns:
        DataFrame: ê° ì»¬ëŸ¼ì˜ ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - method (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - column (str): ì»¬ëŸ¼ëª…
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - is_normal (bool): ì •ê·œì„± ì¶©ì¡± ì—¬ë¶€ (p-value > 0.05)

    Raises:
        ValueError: ë©”ì„œë“œê°€ "n" ë˜ëŠ” "s"ê°€ ì•„ë‹ ê²½ìš°.

    Examples:
        >>> from hossam.analysis import hs_normal_test
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x': np.random.normal(0, 1, 100),
        ...     'y': np.random.exponential(2, 100)
        ... })
        >>> # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê²€ì •
        >>> result = hs_normal_test(df, method='n')
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¦¬ìŠ¤íŠ¸)
        >>> result = hs_normal_test(df, columns=['x'], method='n')
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¬¸ìì—´)
        >>> result = hs_normal_test(df, columns='x, y', method='n')
    """
    if method not in ["n", "s"]:
        raise ValueError(f"methodëŠ” 'n' ë˜ëŠ” 's'ì—¬ì•¼ í•©ë‹ˆë‹¤. ì…ë ¥ê°’: {method}")

    # columnsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(columns, str):
        columns = [col.strip() for col in columns.split(',')]

    # ì»¬ëŸ¼ ì„ íƒ: ì§€ì •ëœ ì»¬ëŸ¼ ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
    if columns is None or len(columns) == 0:
        # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (bool ì œì™¸)
        numeric_df = data.select_dtypes(include=[np.number])
        target_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]
    else:
        # ì§€ì •ëœ ì»¬ëŸ¼ ì‚¬ìš©
        target_cols = columns

    results = []

    for c in target_cols:
        # NaN ê°’ ì œê±° (í†µê³„ ê²€ì • ìˆ˜í–‰)
        col_data = data[c].dropna()

        if len(col_data) == 0:
            results.append({
                "method": method,
                "column": c,
                "statistic": np.nan,
                "p-value": np.nan,
                "is_normal": False
            })
            continue

        try:
            if method == "n":
                method_name = "normaltest"
                s, p = normaltest(col_data)
            else:  # method == "s"
                method_name = "shapiro"
                s, p = shapiro(col_data)

            results.append({
                "method": method_name,
                "column": c,
                "statistic": s,
                "p-value": p,
                "is_normal": p > 0.05
            })
        except Exception as e:
            # ê²€ì • ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ê¸°ë¡
            results.append({
                "method": method,
                "column": c,
                "statistic": np.nan,
                "p-value": np.nan,
                "is_normal": False
            })

    result_df = DataFrame(results)
    return result_df


# -------------------------------------------------------------

def hs_equal_var_test(data: DataFrame, columns: list | str | None = None, normal_dist: bool | None = None) -> DataFrame:
    """ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì˜ ë¶„ì‚°ì´ ê°™ì€ì§€ ê²€ì •í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

    ë“±ë¶„ì‚°ì„± ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ "ëª¨ë“  ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ê°™ë‹¤"ì´ë¯€ë¡œ, p-value > 0.05ì¼ ë•Œ
    ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì§€ ì•Šìœ¼ë©° ë“±ë¶„ì‚°ì„±ì„ ì¶©ì¡±í•œë‹¤ê³  í•´ì„í•œë‹¤.

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        columns (list | str | None, optional): ê²€ì • ëŒ€ìƒ ì»¬ëŸ¼ëª….
            - None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰.
            - ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸: ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ê²€ì • ìˆ˜í–‰.
            - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´: "A, B, C" í˜•ì‹ìœ¼ë¡œ ì»¬ëŸ¼ëª… ì§€ì • ê°€ëŠ¥.
            ê¸°ë³¸ê°’ì€ None.
        normal_dist (bool | None, optional): ë“±ë¶„ì‚°ì„± ê²€ì • ë°©ë²•.
            - True: Bartlett ê²€ì • (ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ, ëª¨ë“  í‘œë³¸ì´ ê°™ì€ í¬ê¸°ì¼ ë•Œ ê¶Œì¥)
            - False: Levene ê²€ì • (ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì„ ë•Œ ë” ê°•ê±´í•¨)
            - None: hs_normal_test()ë¥¼ ì´ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ê·œì„±ì„ íŒë³„ í›„ ì ì ˆí•œ ê²€ì • ë°©ë²• ì„ íƒ.
              ëª¨ë“  ì»¬ëŸ¼ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë©´ Bartlett, í•˜ë‚˜ë¼ë„ ë”°ë¥´ì§€ ì•Šìœ¼ë©´ Levene ì‚¬ìš©.
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - method (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²• (Bartlett ë˜ëŠ” Levene)
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - is_equal_var (bool): ë“±ë¶„ì‚°ì„± ì¶©ì¡± ì—¬ë¶€ (p-value > 0.05)
            - n_columns (int): ê²€ì •ì— ì‚¬ìš©ëœ ì»¬ëŸ¼ ìˆ˜
            - columns (str): ê²€ì •ì— í¬í•¨ëœ ì»¬ëŸ¼ëª… (ì‰¼í‘œë¡œ êµ¬ë¶„)
            - normality_checked (bool): normal_distê°€ Noneì´ì—ˆëŠ”ì§€ ì—¬ë¶€ (ìë™ íŒë³„ ì‚¬ìš© ì—¬ë¶€)

    Raises:
        ValueError: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ 2ê°œ ë¯¸ë§Œì¼ ê²½ìš° (ê²€ì •ì— ìµœì†Œ 2ê°œ í•„ìš”).

    Examples:
        >>> from hossam.analysis import hs_equal_var_test
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x': np.random.normal(0, 1, 100),
        ...     'y': np.random.normal(0, 1, 100),
        ...     'z': np.random.normal(0, 2, 100)
        ... })
        >>> # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ íŒë³„
        >>> result = hs_equal_var_test(df)
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¦¬ìŠ¤íŠ¸)
        >>> result = hs_equal_var_test(df, columns=['x', 'y'])
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¬¸ìì—´)
        >>> result = hs_equal_var_test(df, columns='x, y')
        >>> # ëª…ì‹œì  ì§€ì •
        >>> result = hs_equal_var_test(df, normal_dist=True)
    """
    # columnsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(columns, str):
        columns = [col.strip() for col in columns.split(',')]

    # ì»¬ëŸ¼ ì„ íƒ: ì§€ì •ëœ ì»¬ëŸ¼ ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
    if columns is None or len(columns) == 0:
        # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (bool ì œì™¸)
        numeric_df = data.select_dtypes(include=[np.number])
        numeric_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]
    else:
        # ì§€ì •ëœ ì»¬ëŸ¼ ì‚¬ìš©
        numeric_cols = columns

    if len(numeric_cols) < 2:
        raise ValueError(f"ë“±ë¶„ì‚°ì„± ê²€ì •ì—ëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(numeric_cols)}")

    # ê° ì»¬ëŸ¼ë³„ë¡œ NaNì„ ì œê±°í•˜ì—¬ í•„ë“œ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    fields = []
    for col in numeric_cols:
        col_data = data[col].dropna()
        if len(col_data) > 0:
            fields.append(col_data)

    if len(fields) < 2:
        raise ValueError("NaNì„ ì œê±°í•œ í›„ ìµœì†Œ 2ê°œì˜ ìœ íš¨í•œ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # normal_distê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ì •ê·œì„± íŒë³„
    normality_checked = False
    if normal_dist is None:
        normality_checked = True
        normality_result = hs_normal_test(data[numeric_cols], method="n")
        # ëª¨ë“  ì»¬ëŸ¼ì´ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸
        all_normal = normality_result["is_normal"].all()
        normal_dist = all_normal

    try:
        if normal_dist:
            method_name = "Bartlett"
            s, p = bartlett(*fields)
        else:
            method_name = "Levene"
            s, p = levene(*fields)

        result_df = DataFrame([{
            "method": method_name,
            "statistic": s,
            "p-value": p,
            "is_equal_var": p > 0.05,
            "n_columns": len(fields),
            "columns": ", ".join(numeric_cols[:len(fields)]),
            "normality_checked": normality_checked
        }])

        return result_df

    except Exception as e:
        # ê²€ì • ì‹¤íŒ¨ ì‹œ NaNìœ¼ë¡œ ê¸°ë¡
        method_name = "Bartlett" if normal_dist else "Levene"
        result_df = DataFrame([{
            "method": method_name,
            "statistic": np.nan,
            "p-value": np.nan,
            "is_equal_var": False,
            "n_columns": len(fields),
            "columns": ", ".join(numeric_cols[:len(fields)]),
            "normality_checked": normality_checked
        }])
        return result_df


# -------------------------------------------------------------

def hs_ttest_1samp(data: DataFrame, columns: list | str | None = None, mean_value: float = 0.0) -> DataFrame:
    """ì§€ì •ëœ ì»¬ëŸ¼(ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)ì— ëŒ€í•´ ì¼í‘œë³¸ t-ê²€ì •ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    ì¼í‘œë³¸ t-ê²€ì •ì€ í‘œë³¸ í‰ê· ì´ íŠ¹ì • ê°’(mean_value)ê³¼ ê°™ì€ì§€ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): ëª¨ì§‘ë‹¨ í‰ê·  = mean_value
    ëŒ€ë¦½ê°€ì„¤(H1): alternativeì— ë”°ë¼ ë‹¬ë¼ì§ (!=, <, >)

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        columns (list | str | None, optional): ê²€ì • ëŒ€ìƒ ì»¬ëŸ¼ëª….
            - None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸: ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ê²€ì • ìˆ˜í–‰.
            - ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸: ì§€ì •ëœ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ê²€ì • ìˆ˜í–‰.
            - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´: "A, B, C" í˜•ì‹ìœ¼ë¡œ ì»¬ëŸ¼ëª… ì§€ì • ê°€ëŠ¥.
            ê¸°ë³¸ê°’ì€ None.
        mean_value (float, optional): ê·€ë¬´ê°€ì„¤ì˜ ê¸°ì¤€ê°’(ë¹„êµ ëŒ€ìƒ í‰ê· ê°’).
            ê¸°ë³¸ê°’ì€ 0.0.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - field (str): ì»¬ëŸ¼ëª…
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥ (two-sided, less, greater)
            - statistic (float): t-í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€ (p-value > 0.05)
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€ (p-value <= 0.05)
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„ ë¬¸ìì—´

    Examples:
        >>> from hossam.analysis import hs_ttest_1samp
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'x': np.random.normal(5, 1, 100),
        ...     'y': np.random.normal(0, 1, 100)
        ... })
        >>> # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ í‰ê· ì´ 0ì¸ì§€ ê²€ì •
        >>> result = hs_ttest_1samp(df, mean_value=0)
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¦¬ìŠ¤íŠ¸)
        >>> result = hs_ttest_1samp(df, columns=['x'], mean_value=5)
        >>> # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê²€ì • (ë¬¸ìì—´)
        >>> result = hs_ttest_1samp(df, columns='x, y', mean_value=5)
    """
    # columnsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(columns, str):
        columns = [col.strip() for col in columns.split(',')]

    # ì»¬ëŸ¼ ì„ íƒ: ì§€ì •ëœ ì»¬ëŸ¼ ë˜ëŠ” ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
    if columns is None or len(columns) == 0:
        # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (bool ì œì™¸)
        numeric_df = data.select_dtypes(include=[np.number])
        target_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]
    else:
        # ì§€ì •ëœ ì»¬ëŸ¼ ì‚¬ìš©
        target_cols = columns

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []

    for c in target_cols:
        # NaN ê°’ ì œê±°
        col_data = data[c].dropna()

        # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶„ì‚°ì´ 0ì¸ ê²½ìš° ê±´ë„ˆëœ€
        if len(col_data) == 0 or col_data.std(ddof=1) == 0:
            for a in alternative:
                result.append({
                    "field": c,
                    "alternative": a,
                    "statistic": np.nan,
                    "p-value": np.nan,
                    "H0": False,
                    "H1": False,
                    "interpretation": f"ê²€ì • ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ë¶„ì‚°=0)"
                })
            continue

        for a in alternative:
            try:
                s, p = ttest_1samp(col_data, mean_value, alternative=a)

                itp = None

                if a == "two-sided":
                    itp = "Î¼ {0} {1}".format("==" if p > 0.05 else "!=", mean_value)
                elif a == "less":
                    itp = "Î¼ {0} {1}".format(">=" if p > 0.05 else "<", mean_value)
                else:
                    itp = "Î¼ {0} {1}".format("<=" if p > 0.05 else ">", mean_value)

                result.append({
                    "field": c,
                    "alternative": a,
                    "statistic": round(s, 3),
                    "p-value": round(p, 3),
                    "H0": p > 0.05,
                    "H1": p <= 0.05,
                    "interpretation": itp,
                })
            except Exception as e:
                result.append({
                    "field": c,
                    "alternative": a,
                    "statistic": np.nan,
                    "p-value": np.nan,
                    "H0": False,
                    "H1": False,
                    "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}"
                })

    rdf = DataFrame(result)
    rdf.set_index(["field", "alternative"], inplace=True)

    return rdf


# -------------------------------------------------------------

def hs_ttest_ind(
    data: DataFrame, xname: str, yname: str, equal_var: bool | None = None
) -> DataFrame:
    """ë‘ ë…ë¦½ ì§‘ë‹¨ì˜ í‰ê·  ì°¨ì´ë¥¼ ê²€ì •í•œë‹¤ (ë…ë¦½í‘œë³¸ t-ê²€ì • ë˜ëŠ” Welch's t-test).

    ë…ë¦½í‘œë³¸ t-ê²€ì •ì€ ë‘ ë…ë¦½ëœ ì§‘ë‹¨ì˜ í‰ê· ì´ ê°™ì€ì§€ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): Î¼1 = Î¼2 (ë‘ ì§‘ë‹¨ì˜ í‰ê· ì´ ê°™ë‹¤)

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        xname (str): ì²« ë²ˆì§¸ ì§‘ë‹¨ì˜ ì»¬ëŸ¼ëª….
        yname (str): ë‘ ë²ˆì§¸ ì§‘ë‹¨ì˜ ì»¬ëŸ¼ëª….
        equal_var (bool | None, optional): ë“±ë¶„ì‚°ì„± ê°€ì • ì—¬ë¶€.
            - True: ë…ë¦½í‘œë³¸ t-ê²€ì • (ë“±ë¶„ì‚° ê°€ì •)
            - False: Welch's t-test (ë“±ë¶„ì‚° ê°€ì •í•˜ì§€ ì•ŠìŒ, ë” ê°•ê±´í•¨)
            - None: hs_equal_var_test()ë¡œ ìë™ íŒë³„
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - test (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥
            - statistic (float): t-í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„

    Examples:
        >>> from hossam.analysis import hs_ttest_ind
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'group1': np.random.normal(5, 1, 100),
        ...     'group2': np.random.normal(5.5, 1, 100)
        ... })
        >>> # ìë™ ë“±ë¶„ì‚°ì„± íŒë³„
        >>> result = hs_ttest_ind(df, 'group1', 'group2')
        >>> # ëª…ì‹œì  ì§€ì •
        >>> result = hs_ttest_ind(df, 'group1', 'group2', equal_var=False)
    """
    # NaN ì œê±°
    x_data = data[xname].dropna()
    y_data = data[yname].dropna()

    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    if len(x_data) < 2 or len(y_data) < 2:
        raise ValueError(f"ê° ì§‘ë‹¨ì— ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. {xname}: {len(x_data)}, {yname}: {len(y_data)}")

    # equal_varê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ë“±ë¶„ì‚°ì„± íŒë³„
    var_checked = False
    if equal_var is None:
        var_checked = True
        var_result = hs_equal_var_test(data[[xname, yname]])
        equal_var = var_result["is_equal_var"].iloc[0]

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []
    fmt: str = "Î¼({f0}) {0} Î¼({f1})"

    for a in alternative:
        try:
            s, p = ttest_ind(x_data, y_data, equal_var=equal_var, alternative=a)
            n = "t-test_ind" if equal_var else "Welch's t-test"

            # ê²€ì • ê²°ê³¼ í•´ì„
            itp = None

            if a == "two-sided":
                itp = fmt.format("==" if p > 0.05 else "!=", f0=xname, f1=yname)
            elif a == "less":
                itp = fmt.format(">=" if p > 0.05 else "<", f0=xname, f1=yname)
            else:
                itp = fmt.format("<=" if p > 0.05 else ">", f0=xname, f1=yname)

            result.append({
                "test": n,
                "alternative": a,
                "statistic": round(s, 3),
                "p-value": round(p, 3),
                "H0": p > 0.05,
                "H1": p <= 0.05,
                "interpretation": itp,
                "equal_var_checked": var_checked
            })
        except Exception as e:
            result.append({
                "test": "t-test_ind" if equal_var else "Welch's t-test",
                "alternative": a,
                "statistic": np.nan,
                "p-value": np.nan,
                "H0": False,
                "H1": False,
                "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}",
                "equal_var_checked": var_checked
            })

    rdf = DataFrame(result)
    rdf.set_index(["test", "alternative"], inplace=True)
    return rdf


# -------------------------------------------------------------

def hs_ttest_rel(
    data: DataFrame, xname: str, yname: str, equal_var: bool | None = None
) -> DataFrame:
    """ëŒ€ì‘í‘œë³¸ t-ê²€ì • ë˜ëŠ” Wilcoxon signed-rank testë¥¼ ìˆ˜í–‰í•œë‹¤.

    ëŒ€ì‘í‘œë³¸ t-ê²€ì •ì€ ë™ì¼ ê°œì²´ì—ì„œ ì¸¡ì •ëœ ë‘ ì‹œì ì˜ í‰ê·  ì°¨ì´ë¥¼ ê²€ì •í•œë‹¤.
    ê·€ë¬´ê°€ì„¤(H0): ë‘ ì‹œì ì˜ í‰ê·  ì°¨ì´ê°€ 0ì´ë‹¤.

    Args:
        data (DataFrame): ê²€ì • ëŒ€ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.
        xname (str): ì²« ë²ˆì§¸ ì¸¡ì •ê°’ì˜ ì»¬ëŸ¼ëª….
        yname (str): ë‘ ë²ˆì§¸ ì¸¡ì •ê°’ì˜ ì»¬ëŸ¼ëª….
        equal_var (bool | None, optional): ì •ê·œì„±/ë“±ë¶„ì‚°ì„± ê°€ì • ì—¬ë¶€.
            - True: ëŒ€ì‘í‘œë³¸ t-ê²€ì • (ì •ê·œë¶„í¬ ê°€ì •)
            - False: Wilcoxon signed-rank test (ë¹„ëª¨ìˆ˜ ê²€ì •, ë” ê°•ê±´í•¨)
            - None: hs_equal_var_test()ë¡œ ìë™ íŒë³„
            ê¸°ë³¸ê°’ì€ None.

    Returns:
        DataFrame: ê²€ì • ê²°ê³¼ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„. ë‹¤ìŒ ì»¬ëŸ¼ í¬í•¨:
            - test (str): ì‚¬ìš©ëœ ê²€ì • ë°©ë²•
            - alternative (str): ëŒ€ë¦½ê°€ì„¤ ë°©í–¥
            - statistic (float): ê²€ì • í†µê³„ëŸ‰
            - p-value (float): ìœ ì˜í™•ë¥ 
            - H0 (bool): ê·€ë¬´ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - H1 (bool): ëŒ€ë¦½ê°€ì„¤ ì±„íƒ ì—¬ë¶€
            - interpretation (str): ê²€ì • ê²°ê³¼ í•´ì„

    Examples:
        >>> from hossam.analysis import hs_ttest_rel
        >>> import pandas as pd
        >>> import numpy as np
        >>> df = pd.DataFrame({
        ...     'before': np.random.normal(5, 1, 100),
        ...     'after': np.random.normal(5.3, 1, 100)
        ... })
        >>> # ìë™ ì •ê·œì„± íŒë³„
        >>> result = hs_ttest_rel(df, 'before', 'after')
        >>> # ëª…ì‹œì ìœ¼ë¡œ ë¹„ëª¨ìˆ˜ ê²€ì •
        >>> result = hs_ttest_rel(df, 'before', 'after', equal_var=False)
    """
    # NaN ì œê±° (ëŒ€ì‘í‘œë³¸ì´ë¯€ë¡œ í–‰ ë‹¨ìœ„ë¡œ ì œê±°)
    valid_idx = data[[xname, yname]].dropna().index
    x_data = data.loc[valid_idx, xname]
    y_data = data.loc[valid_idx, yname]

    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    if len(x_data) < 2:
        raise ValueError(f"ìµœì†Œ 2ê°œ ì´ìƒì˜ ëŒ€ì‘ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(x_data)}")

    # equal_varê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ë“±ë¶„ì‚°ì„± íŒë³„
    var_checked = False
    if equal_var is None:
        var_checked = True
        var_result = hs_equal_var_test(data[[xname, yname]])
        equal_var = var_result["is_equal_var"].iloc[0]

    alternative: list = ["two-sided", "less", "greater"]
    result: list = []
    fmt: str = "Î¼({f0}) {0} Î¼({f1})"

    for a in alternative:
        try:
            if equal_var:
                s, p = ttest_rel(x_data, y_data, alternative=a)
                n = "t-test_paired"
            else:
                # Wilcoxon signed-rank test (ëŒ€ì‘í‘œë³¸ìš© ë¹„ëª¨ìˆ˜ ê²€ì •)
                s, p = wilcoxon(x_data, y_data, alternative=a)
                n = "Wilcoxon signed-rank"

            itp = None

            if a == "two-sided":
                itp = fmt.format("==" if p > 0.05 else "!=", f0=xname, f1=yname)
            elif a == "less":
                itp = fmt.format(">=" if p > 0.05 else "<", f0=xname, f1=yname)
            else:
                itp = fmt.format("<=" if p > 0.05 else ">", f0=xname, f1=yname)

            result.append({
                "test": n,
                "alternative": a,
                "statistic": round(s, 3) if not np.isnan(s) else s,
                "p-value": round(p, 3) if not np.isnan(p) else p,
                "H0": p > 0.05,
                "H1": p <= 0.05,
                "interpretation": itp,
                "equal_var_checked": var_checked
            })
        except Exception as e:
            result.append({
                "test": "t-test_paired" if equal_var else "Wilcoxon signed-rank",
                "alternative": a,
                "statistic": np.nan,
                "p-value": np.nan,
                "H0": False,
                "H1": False,
                "interpretation": f"ê²€ì • ì‹¤íŒ¨: {str(e)}",
                "equal_var_checked": var_checked
            })

    rdf = DataFrame(result)
    rdf.set_index(["test", "alternative"], inplace=True)

    return rdf


# -------------------------------------------------------------
def hs_vif_filter(
    data: DataFrame,
    yname: str = None,
    ignore: list | None = None,
    threshold: float = 10.0,
    verbose: bool = False,
) -> DataFrame:
    """ë…ë¦½ë³€ìˆ˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„±ì„ ê²€ì‚¬í•˜ì—¬ VIFê°€ threshold ì´ìƒì¸ ë³€ìˆ˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•œë‹¤.

    Args:
        data (DataFrame): ë°ì´í„°í”„ë ˆì„
        yname (str, optional): ì¢…ì†ë³€ìˆ˜ ì»¬ëŸ¼ëª…. Defaults to None.
        ignore (list | None, optional): ì œì™¸í•  ì»¬ëŸ¼ ëª©ë¡. Defaults to None.
        threshold (float, optional): VIF ì„ê³„ê°’. Defaults to 10.0.
        verbose (bool, optional): Trueì¼ ê²½ìš° ê° ë‹¨ê³„ì˜ VIFë¥¼ ì¶œë ¥í•œë‹¤. Defaults to False.

    Returns:
        DataFrame: VIFê°€ threshold ì´í•˜ì¸ ë³€ìˆ˜ë§Œ ë‚¨ì€ ë°ì´í„°í”„ë ˆì„ (ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ ìœ ì§€)

    Examples:
        ê¸°ë³¸ ì‚¬ìš© ì˜ˆ:

        >>> from hossam.analysis import hs_vif_filter
        >>> filtered = hs_vif_filter(df, yname="target", ignore=["id"], threshold=10.0)
        >>> filtered.head()
    """

    df = data.copy()

    # y ë¶„ë¦¬ (ìˆë‹¤ë©´)
    y = None
    if yname and yname in df.columns:
        y = df[yname]
        df = df.drop(columns=[yname])

    # ì œì™¸í•  ëª©ë¡ ì •ë¦¬
    ignore = ignore or []
    ignore_cols_present = [c for c in ignore if c in df.columns]

    # VIF ëŒ€ìƒ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ íƒ (boolì€ ì—°ì†í˜•ì´ ì•„ë‹ˆë¯€ë¡œ ì œì™¸)
    numeric_df = df.select_dtypes(include=[np.number])
    numeric_cols = [c for c in numeric_df.columns if not is_bool_dtype(numeric_df[c])]

    # VIF ëŒ€ìƒ X êµ¬ì„± (ìˆ˜ì¹˜í˜•ì—ì„œ ì œì™¸ ëª©ë¡ ì œê±°)
    X = df[numeric_cols]
    if ignore_cols_present:
        X = X.drop(columns=ignore_cols_present, errors="ignore")

    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if X.shape[1] == 0:
        result = data.copy()
        return result

    def _compute_vifs(X_: DataFrame) -> dict:
        # NA ì œê±° í›„ ìƒìˆ˜í•­ ì¶”ê°€
        X_clean = X_.dropna()
        if X_clean.shape[0] == 0:
            # ë°ì´í„°ê°€ ëª¨ë‘ NAì¸ ê²½ìš° VIF ê³„ì‚° ë¶ˆê°€: NaN ë°˜í™˜
            return {col: np.nan for col in X_.columns}
        if X_clean.shape[1] == 1:
            # ë‹¨ì¼ ì˜ˆì¸¡ë³€ìˆ˜ì˜ ê²½ìš° ë‹¤ë¥¸ ì„¤ëª…ë³€ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ VIFëŠ” 1ë¡œ ê°„ì£¼
            return {col: 1.0 for col in X_clean.columns}
        exog = sm.add_constant(X_clean, prepend=True)
        vifs = {}
        for i, col in enumerate(X_clean.columns, start=0):
            # exogì˜ ì²« ì—´ì€ ìƒìˆ˜í•­ì´ë¯€ë¡œ ë³€ìˆ˜ ì¸ë±ìŠ¤ëŠ” +1
            try:
                vifs[col] = float(variance_inflation_factor(exog.values, i + 1))
            except Exception:
                # ê³„ì‚° ì‹¤íŒ¨ ì‹œ ë¬´í•œëŒ€ë¡œ ì²˜ë¦¬í•˜ì—¬ ìš°ì„  ì œê±° ëŒ€ìƒìœ¼ë¡œ
                vifs[col] = float("inf")
        return vifs

    # ë°˜ë³µ ì œê±° ë£¨í”„
    while True:
        if X.shape[1] == 0:
            break
        vifs = _compute_vifs(X)
        if verbose:
            print(vifs)
        # ëª¨ë“  ë³€ìˆ˜ê°€ ì„ê³„ê°’ ì´í•˜ì´ë©´ ì¢…ë£Œ
        max_key = max(vifs, key=lambda k: (vifs[k] if not np.isnan(vifs[k]) else -np.inf))
        max_vif = vifs[max_key]
        if np.isnan(max_vif) or max_vif <= threshold:
            break
        # ê°€ì¥ í° VIF ë³€ìˆ˜ ì œê±°
        X = X.drop(columns=[max_key])

    # ì¶œë ¥ ì˜µì…˜ì´ Falseì¼ ê²½ìš° ìµœì¢… ê°’ë§Œ ì¶œë ¥
    if not verbose:
        final_vifs = _compute_vifs(X) if X.shape[1] > 0 else {}
        print(final_vifs)

    # ì›ë³¸ ì»¬ëŸ¼ ìˆœì„œ ìœ ì§€í•˜ë©° ì œê±°ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì œì™¸
    kept_numeric_cols = list(X.columns)
    removed_numeric_cols = [c for c in numeric_cols if c not in kept_numeric_cols]
    result = data.drop(columns=removed_numeric_cols, errors="ignore")

    return result


# -------------------------------------------------------------
def hs_trend(x: any, y: any, degree: int = 1, value_count: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """x, y ë°ì´í„°ì— ëŒ€í•œ ì¶”ì„¸ì„ ì„ êµ¬í•œë‹¤.

    Args:
        x (_type_): ì‚°ì ë„ ê·¸ë˜í”„ì— ëŒ€í•œ x ë°ì´í„°
        y (_type_): ì‚°ì ë„ ê·¸ë˜í”„ì— ëŒ€í•œ y ë°ì´í„°
        degree (int, optional): ì¶”ì„¸ì„  ë°©ì •ì‹ì˜ ì°¨ìˆ˜. Defaults to 1.
        value_count (int, optional): x ë°ì´í„°ì˜ ë²”ìœ„ ì•ˆì—ì„œ ê°„ê²© ìˆ˜. Defaults to 100.

    Returns:
        tuple: (v_trend, t_trend)

    Examples:
        2ì°¨ ë‹¤í•­ íšŒê·€ ì¶”ì„¸ì„ :

        >>> from hossam.analysis import hs_trend
        >>> vx, vy = hs_trend(x, y, degree=2, value_count=200)
        >>> len(vx), len(vy)
        (200, 200)
    """
    # [ a, b, c ] ==> ax^2 + bx + c
    x_arr = np.asarray(x)
    y_arr = np.asarray(y)

    if x_arr.ndim == 0 or y_arr.ndim == 0:
        raise ValueError("x, yëŠ” 1ì°¨ì› ì´ìƒì˜ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    coeff = np.polyfit(x_arr, y_arr, degree)

    minx = np.min(x_arr)
    maxx = np.max(x_arr)
    v_trend = np.linspace(minx, maxx, value_count)

    # np.polyval ì‚¬ìš©ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì¶”ì„¸ì„  ê³„ì‚°
    t_trend = np.polyval(coeff, v_trend)

    return (v_trend, t_trend)


# -------------------------------------------------------------
def hs_linear_report(fit, data):
    """ì„ í˜•íšŒê·€ ì í•© ê²°ê³¼ë¥¼ ìš”ì•½ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    Args:
        fit: statsmodels OLS ë“± ì„ í˜•íšŒê·€ ê²°ê³¼ ê°ì²´ (`fit.summary()`ë¥¼ ì§€ì›í•´ì•¼ í•¨).
        data: ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ DataFrame.

    Returns:
        tuple: ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•œë‹¤.
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame): ë³€ìˆ˜ë³„ B, í‘œì¤€ì˜¤ì°¨, Beta, t, p-value, ê³µì°¨, VIF.
            - ì í•©ë„ ìš”ì•½ (`result_report`, str): R, RÂ², F, p-value, Durbin-Watson ë“± í•µì‹¬ ì§€í‘œ ë¬¸ìì—´.
            - ëª¨í˜• ë³´ê³  ë¬¸ì¥ (`model_report`, str): F-ê²€ì • ìœ ì˜ì„±ì— ê¸°ë°˜í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - ë³€ìˆ˜ë³„ ë³´ê³  ë¦¬ìŠ¤íŠ¸ (`variable_reports`, list[str]): ê° ì˜ˆì¸¡ë³€ìˆ˜ì— ëŒ€í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - íšŒê·€ì‹ ë¬¸ìì—´ (`equation_text`, str): ìƒìˆ˜í•­ê³¼ ê³„ìˆ˜ë¥¼ í¬í•¨í•œ íšŒê·€ì‹ í‘œí˜„.

    Examples:
        >>> import statsmodels.api as sm
        >>> y = data['target']
        >>> X = sm.add_constant(data[['x1', 'x2']])
        >>> fit = sm.OLS(y, X).fit()
        >>> rdf, result_report, model_report, variable_reports, eq = hs_linear_report(fit, data)
        >>> print(eq)
    """

    tbl = fit.summary()

    # ì¢…ì†ë³€ìˆ˜ ì´ë¦„
    yname = fit.model.endog_names

    # ë…ë¦½ë³€ìˆ˜ ì´ë¦„(ìƒìˆ˜í•­ ì œì™¸)
    xnames = [n for n in fit.model.exog_names if n != "const"]

    # ë…ë¦½ë³€ìˆ˜ ë¶€ë¶„ ë°ì´í„° (VIF ê³„ì‚°ìš©)
    indi_df = data.filter(xnames)

    # ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ë¥¼ ëˆ„ì 
    variables = []
    for i, v in enumerate(tbl.tables[1].data):
        # í•œ í–‰ì˜ ë³€ìˆ˜ëª… ì¶”ì¶œ í›„ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
        name = v[0].strip()
        if name not in xnames:
            continue

        # VIF ê³„ì‚°: ìƒìˆ˜í•­ì„ í¬í•¨í•œ ì„¤ê³„í–‰ë ¬ì—ì„œ ëŒ€ìƒ ë³€ìˆ˜ì˜ ì—´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©
        indi_df_const = sm.add_constant(indi_df, has_constant="add")
        j = list(indi_df_const.columns).index(name)
        vif = variance_inflation_factor(indi_df_const.values, j)

        # ìœ ì˜í™•ë¥ ê³¼ ë³„í‘œ í‘œì‹œ í•¨ìˆ˜
        p = float(v[4].strip())
        stars = lambda p: (
            "***" if p < 0.001 else "**" if p < 0.01 else "*" if p < 0.05 else ""
        )

        # í•œ ë³€ìˆ˜ì— ëŒ€í•œ ë³´ê³  ì •ë³´ ì¶”ê°€
        variables.append(
            {
                "ì¢…ì†ë³€ìˆ˜": yname,  # ì¢…ì†ë³€ìˆ˜ ì´ë¦„
                "ë…ë¦½ë³€ìˆ˜": name,  # ë…ë¦½ë³€ìˆ˜ ì´ë¦„
                "B": v[1].strip(),  # ë¹„í‘œì¤€í™” íšŒê·€ê³„ìˆ˜(B)
                "í‘œì¤€ì˜¤ì°¨": v[2].strip(),  # ê³„ìˆ˜ í‘œì¤€ì˜¤ì°¨
                "Beta": float(fit.params[name])
                * (
                    data[name].std(ddof=1) / data[yname].std(ddof=1)
                ),  # í‘œì¤€í™” íšŒê·€ê³„ìˆ˜(Î²)
                "t": "%s%s" % (v[3].strip(), stars(p)),  # t-í†µê³„ëŸ‰(+ë³„í‘œ)
                "p-value": p,  # ê³„ìˆ˜ ìœ ì˜í™•ë¥ 
                "ê³µì°¨": 1 / vif,  # ê³µì°¨(Tolerance = 1/VIF)
                "vif": vif,  # ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜
            }
        )

    rdf = DataFrame(variables)

    # summary í‘œì—ì„œ ì í•©ë„ ì •ë³´ë¥¼ key-valueë¡œ ì¶”ì¶œ
    result_dict = {}
    for i in [0, 2]:
        for item in tbl.tables[i].data:
            n = len(item)
            for i in range(0, n, 2):
                key = item[i].strip()[:-1]
                value = item[i + 1].strip()
                if not key or not value:
                    continue
                result_dict[key] = value

    # ì í•©ë„ ë³´ê³  ë¬¸ìì—´ êµ¬ì„±
    result_report = f"ğ‘…({result_dict['R-squared']}), ğ‘…^2({result_dict['Adj. R-squared']}), ğ¹({result_dict['F-statistic']}), ìœ ì˜í™•ë¥ ({result_dict['Prob (F-statistic)']}), Durbin-Watson({result_dict['Durbin-Watson']})"

    # ëª¨í˜• ë³´ê³  ë¬¸ì¥ êµ¬ì„±
    tpl = "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %s, p %s 0.05)."
    model_report = tpl % (
        rdf["ì¢…ì†ë³€ìˆ˜"][0],
        ",".join(list(rdf["ë…ë¦½ë³€ìˆ˜"])),
        (
            "ìœ ì˜í•˜ë‹¤"
            if float(result_dict["Prob (F-statistic)"]) <= 0.05
            else "ìœ ì˜í•˜ì§€ ì•Šë‹¤"
        ),
        result_dict["Df Model"],
        result_dict["Df Residuals"],
        result_dict["F-statistic"],
        "<=" if float(result_dict["Prob (F-statistic)"]) <= 0.05 else ">",
    )

    # ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    variable_reports = []
    s = "%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %s(p %s 0.05)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."

    for i in rdf.index:
        row = rdf.iloc[i]
        variable_reports.append(
            s
            % (
                row["ë…ë¦½ë³€ìˆ˜"],
                row["B"],
                "<=" if float(row["p-value"]) < 0.05 else ">",
                row["ì¢…ì†ë³€ìˆ˜"],
                "ìœ ì˜ë¯¸í•œ" if float(row["p-value"]) < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šì€",
            )
        )

    # -----------------------------
    # íšŒê·€ì‹ ìë™ ì¶œë ¥
    # -----------------------------
    intercept = fit.params["const"]
    terms = []

    for name in xnames:
        coef = fit.params[name]
        sign = "+" if coef >= 0 else "-"
        terms.append(f" {sign} {abs(coef):.3f}Â·{name}")

    equation_text = f"{yname} = {intercept:.3f}" + "".join(terms)

    return rdf, result_report, model_report, variable_reports, equation_text


# -------------------------------------------------------------
def hs_logit_report(fit, data, threshold=0.5):
    """ë¡œì§€ìŠ¤í‹± íšŒê·€ ì í•© ê²°ê³¼ë¥¼ ìƒì„¸ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    Args:
        fit: statsmodels Logit ê²°ê³¼ ê°ì²´ (`fit.summary()`ì™€ ì˜ˆì¸¡ í™•ë¥ ì„ ì§€ì›í•´ì•¼ í•¨).
        data: ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ëª¨ë‘ í¬í•¨í•œ DataFrame.
        threshold: ì˜ˆì¸¡ í™•ë¥ ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜í•  ì„ê³„ê°’. ê¸°ë³¸ê°’ 0.5.

    Returns:
        tuple: ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•œë‹¤.
            - ì„±ëŠ¥ ì§€í‘œ í‘œ (`cdf`, DataFrame): McFadden Pseudo RÂ², Accuracy, Precision, Recall, FPR, TNR, AUC, F1.
            - íšŒê·€ê³„ìˆ˜ í‘œ (`rdf`, DataFrame): B, í‘œì¤€ì˜¤ì°¨, z, p-value, OR, 95% CI, VIF ë“±.
            - ì í•©ë„ ë° ì˜ˆì¸¡ ì„±ëŠ¥ ìš”ì•½ (`result_report`, str): Pseudo RÂ², LLR Ï‡Â², p-value, Accuracy, AUC.
            - ëª¨í˜• ë³´ê³  ë¬¸ì¥ (`model_report`, str): LLR p-valueì— ê¸°ë°˜í•œ ì„œìˆ í˜• ë¬¸ì¥.
            - ë³€ìˆ˜ë³„ ë³´ê³  ë¦¬ìŠ¤íŠ¸ (`variable_reports`, list[str]): ê° ì˜ˆì¸¡ë³€ìˆ˜ì˜ ì˜¤ì¦ˆë¹„ í•´ì„ ë¬¸ì¥.

    Examples:
        >>> import statsmodels.api as sm
        >>> y = data['target']
        >>> X = sm.add_constant(data[['x1', 'x2']])
        >>> fit = sm.Logit(y, X).fit(disp=0)
        >>> cdf, rdf, result_report, model_report, variable_reports = hs_logit_report(fit, data, threshold=0.5)
        >>> print(variable_reports[0])
    """

    # -----------------------------
    # ì„±ëŠ¥í‰ê°€ì§€í‘œ
    # -----------------------------
    yname = fit.model.endog_names
    y_true = data[yname]
    y_pred = fit.predict(fit.model.exog)
    y_pred_fix = (y_pred >= threshold).astype(int)

    # í˜¼ë™í–‰ë ¬
    cm = confusion_matrix(y_true, y_pred_fix)
    tn, fp, fn, tp = cm.ravel()

    acc = accuracy_score(y_true, y_pred_fix)  # ì •í™•ë„
    pre = precision_score(y_true, y_pred_fix)  # ì •ë°€ë„
    tpr = recall_score(y_true, y_pred_fix)  # ì¬í˜„ìœ¨
    fpr = fp / (fp + tn)  # ìœ„ì–‘ì„±ìœ¨
    tnr = 1 - fpr  # íŠ¹ì´ì„±
    f1 = f1_score(y_true, y_pred_fix)  # f1-score
    ras = roc_auc_score(y_true, y_pred)  # auc score

    cdf = DataFrame(
        {
            "ì„¤ëª…ë ¥(P-Rsqe)": [fit.prsquared],
            "ì •í™•ë„(Accuracy)": [acc],
            "ì •ë°€ë„(Precision)": [pre],
            "ì¬í˜„ìœ¨(Recall,TPR)": [tpr],
            "ìœ„ì–‘ì„±ìœ¨(Fallout,FPR)": [fpr],
            "íŠ¹ì´ì„±(Specif city,TNR)": [tnr],
            "RAS(auc score)": [ras],
            "F1": [f1],
        }
    )

    # -----------------------------
    # íšŒê·€ê³„ìˆ˜ í‘œ êµ¬ì„± (OR ì¤‘ì‹¬)
    # -----------------------------
    tbl = fit.summary()

    # ë…ë¦½ë³€ìˆ˜ ì´ë¦„(ìƒìˆ˜í•­ ì œì™¸)
    xnames = [n for n in fit.model.exog_names if n != "const"]

    # ë…ë¦½ë³€ìˆ˜
    x = data[xnames]

    variables = []

    # VIF ê³„ì‚° (ìƒìˆ˜í•­ í¬í•¨ ì„¤ê³„í–‰ë ¬ ì‚¬ìš©)
    vif_dict = {}
    x_const = sm.add_constant(x, has_constant="add")
    for col in x.columns:
        col_idx = list(x_const.columns).index(col)
        vif_dict[col] = variance_inflation_factor(x_const.values, col_idx)

    for v in tbl.tables[1].data:
        name = v[0].strip()
        if name not in xnames:
            continue

        beta = float(v[1])
        se = float(v[2])
        z = float(v[3])
        p = float(v[4])

        or_val = np.exp(beta)
        ci_low = np.exp(beta - 1.96 * se)
        ci_high = np.exp(beta + 1.96 * se)

        stars = "***" if p < 0.001 else "**" if p < 0.01 else "*" if p < 0.05 else ""

        variables.append(
            {
                "ì¢…ì†ë³€ìˆ˜": yname,
                "ë…ë¦½ë³€ìˆ˜": name,
                "B(Î²)": beta,
                "í‘œì¤€ì˜¤ì°¨": se,
                "z": f"{z:.3f}{stars}",
                "p-value": p,
                "OR": or_val,
                "CI_lower": ci_low,
                "CI_upper": ci_high,
                "VIF": vif_dict.get(name, np.nan),
            }
        )

    rdf = DataFrame(variables)

    # ---------------------------------
    # ëª¨ë¸ ì í•©ë„ + ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ
    # ---------------------------------
    auc = roc_auc_score(y_true, y_pred)

    result_report = (
        f"Pseudo RÂ²(McFadden) = {fit.prsquared:.3f}, "
        f"LLR Ï‡Â²({int(fit.df_model)}) = {fit.llr:.3f}, "
        f"p-value = {fit.llr_pvalue:.4f}, "
        f"Accuracy = {acc:.3f}, "
        f"AUC = {auc:.3f}"
    )

    # -----------------------------
    # ëª¨í˜• ë³´ê³  ë¬¸ì¥
    # -----------------------------
    tpl = (
        "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, "
        "ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(Ï‡Â²(%s) = %.3f, p %s 0.05)í•˜ì˜€ë‹¤."
    )

    model_report = tpl % (
        yname,
        ", ".join(xnames),
        "ìœ ì˜" if fit.llr_pvalue <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ",
        int(fit.df_model),
        fit.llr,
        "<=" if fit.llr_pvalue <= 0.05 else ">",
    )

    # -----------------------------
    # ë³€ìˆ˜ë³„ ë³´ê³  ë¬¸ì¥
    # -----------------------------
    variable_reports = []

    s = (
        "%sì˜ ì˜¤ì¦ˆë¹„ëŠ” %.3f(p %s 0.05)ë¡œ, "
        "%s ë°œìƒ oddsì— %s ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤."
    )

    for _, row in rdf.iterrows():
        variable_reports.append(
            s
            % (
                row["ë…ë¦½ë³€ìˆ˜"],
                row["OR"],
                "<=" if row["p-value"] < 0.05 else ">",
                row["ì¢…ì†ë³€ìˆ˜"],
                "ìœ ì˜ë¯¸í•œ" if row["p-value"] < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šì€",
            )
        )

    return cdf, rdf, result_report, model_report, variable_reports