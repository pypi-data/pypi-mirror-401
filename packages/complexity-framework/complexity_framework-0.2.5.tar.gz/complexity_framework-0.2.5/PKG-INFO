Metadata-Version: 2.4
Name: complexity-framework
Version: 0.2.5
Summary: Modular Python framework for building LLMs with INL Dynamics stability
Author-email: Complexity-ML <contact@complexity-ml.org>
License: CC-BY-NC-4.0
Project-URL: Homepage, https://github.com/Complexity-ML/complexity-framework
Project-URL: Documentation, https://github.com/Complexity-ML/complexity-framework/tree/main/docs
Project-URL: Repository, https://github.com/Complexity-ML/complexity-framework
Project-URL: Issues, https://github.com/Complexity-ML/complexity-framework/issues
Keywords: llm,transformer,deep-learning,pytorch,attention,moe,mixture-of-experts,inl-dynamics,flash-attention,mamba,rwkv
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: einops>=0.6.0
Provides-Extra: cuda
Requires-Dist: triton>=2.0.0; extra == "cuda"
Requires-Dist: flash-attn>=2.0.0; extra == "cuda"
Provides-Extra: all
Requires-Dist: triton>=2.0.0; extra == "all"
Requires-Dist: flash-attn>=2.0.0; extra == "all"
Requires-Dist: transformers>=4.30.0; extra == "all"
Requires-Dist: tokenizers>=0.13.0; extra == "all"
Requires-Dist: datasets>=2.0.0; extra == "all"
Requires-Dist: wandb>=0.15.0; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Dynamic: license-file

# Complexity Framework

**Modular Python framework for building LLMs with INL Dynamics stability.**

```bash
pip install complexity-framework
```

## Quick Start

```python
from complexity.api import (
    # Building blocks
    Attention, MLP, RMSNorm, RoPE, INLDynamics,
    # Optimizations
    CUDA, Efficient,
    # Linear architectures O(N)
    Architecture, Mamba, RWKV,
)

# Flash Attention
attn = CUDA.flash(hidden_size=4096, num_heads=32)

# INL Dynamics (training stability)
dynamics = INLDynamics(hidden_size=768, beta_max=2.0)
h, velocity = dynamics(hidden_states, velocity)

# Small budget model
model = Efficient.tiny_llm(vocab_size=32000)  # ~125M params
```

## Features

| Module | Description |
|--------|-------------|
| **Core (O(NÂ²))** | Attention (GQA/MHA/MQA), MLP (SwiGLU/GeGLU/MoE), Position (RoPE/YaRN/ALiBi) |
| **INL Dynamics** | Velocity tracking for training stability |
| **CUDA/Triton** | Flash Attention, Sliding Window, Sparse, Linear |
| **Efficient** | Quantization, Mixed Precision, Small Models |
| **Linear (O(N))** | Mamba, RWKV, RetNet |
| **Multimodal** | Vision, Audio, Fusion |

## INL Dynamics

Velocity tracking to prevent explosion after 400k+ steps:

```python
# CRITICAL: beta in [0, 2], NOT [0, inf)!
dynamics = INLDynamics(
    hidden_size=768,
    beta_max=2.0,       # Clamp beta for stability
    velocity_max=10.0,  # Limit velocity
)
```

## Mixture of Experts (MoE)

```python
from complexity.api import MLP

# Token-Routed MoE (our innovation - deterministic, no aux_loss)
moe = MLP.moe(hidden_size=4096, num_experts=4)
output = moe(hidden_states, token_ids)  # No aux_loss!

# Sparse MoE (standard - learned routing)
moe = MLP.sparse_moe(hidden_size=4096, num_experts=8, top_k=2)
output, aux_loss = moe(hidden_states)
```

## Small Budget Training

```python
from complexity.api import Efficient

# Pre-configured models
model = Efficient.nano_llm(vocab_size=32000)   # ~10M params
model = Efficient.tiny_llm(vocab_size=32000)   # ~125M params
model = Efficient.small_llm(vocab_size=32000)  # ~350M params

# Memory optimizations
Efficient.enable_checkpointing(model)
```

## Linear Architectures (O(N))

```python
from complexity.api import Architecture

model = Architecture.mamba(hidden_size=768, num_layers=12)
model = Architecture.rwkv(hidden_size=768, num_layers=12)
model = Architecture.retnet(hidden_size=768, num_layers=12)
```

## Documentation

Full documentation with training curves and examples:
**[GitHub](https://github.com/Complexity-ML/complexity-framework)**

## License

CC BY-NC 4.0 (Creative Commons Attribution-NonCommercial 4.0)
