Metadata-Version: 2.4
Name: easyroutine
Version: 1.1.4
Summary: 
License-File: LICENSE
Author: Francesc0rtu
Author-email: francescortu@live.it
Requires-Python: >=3.10,<3.13
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: accelerate (>=1.1.1,<2.0.0)
Requires-Dist: einops (>=0.8.0,<0.9.0)
Requires-Dist: litellm (>=1.74.0,<2.0.0)
Requires-Dist: numpy (>=1.24,<2)
Requires-Dist: pandas (>=2.3.0,<3.0.0)
Requires-Dist: pydantic (>=2.10.6,<3.0.0)
Requires-Dist: pytest (>=8.3.3,<9.0.0)
Requires-Dist: python-dotenv (>=1.0.1,<2.0.0)
Requires-Dist: pyyaml (>=6.0.2,<7.0.0)
Requires-Dist: rich (>=13.9.4,<14.0.0)
Requires-Dist: sentencepiece (>=0.2.0,<0.3.0)
Requires-Dist: torch (>=2.4.0,<3.0.0)
Requires-Dist: torchvision (>=0.22,<0.23)
Requires-Dist: transformers (==4.51.1)
Requires-Dist: vllm (==0.9)
Description-Content-Type: text/markdown

# EasyRoutine

This is just a simple collection of routines that I use frequently. I have found that I often need to do the same things over and over again, so I have created this repository to store them. I hope you find them useful.


## Installation


## Interpretability
The interpretability module contains wrapper of huggingface LLM/VLM that help to perform interpretability tasks on the model. Currently, it supports:
- Extract activations of any component of the model
- Perform ablation study on the model during inference
- Perform activation patching on the model during inference

### Simple Tutorial
```python
# First we need to import the HookedModel and the config classes
from easyroutine.interpretability import HookedModel,  ExtractionConfig

hooked_model = HookedModel.from_pretrained(
    model_name="mistral-community/pixtral-12b", # the model name
    device_map = "auto"
)

# Now let's define a simple dataset
dataset = [
    "This is a test",
    "This is another test"
]

tokenizer = hooked_model.get_tokenizer()

dataset = tokenizer(dataset, padding=True, truncation=True, return_tensors="pt") 

cache = hooked_model.extract_cache(
    dataset,
    target_token_positions = ["last"],
    extraction_config = ExtractionConfig(
        extract_resid_out = True
    )
)

````



### Development
For publish the package push a commit with the flag:
  - `[patch]`: x.x.7 -> x.x.8
  - `[minor]`: x.7.x -> x.8.0
  - `[major]`: 2.x.x -> 3.0.0

Example commit: `fix multiple bus [patch]`

-

