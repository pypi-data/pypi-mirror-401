# SPDX-License-Identifier: Apache-2.0

# Functions are adapted from
# https://github.com/vllm-project/vllm/blob/v0.8.3/vllm/v1/sample/ops/penalties.py
#
# Copyright 2025 Huawei Technologies Co., Ltd.
# Copyright 2025 The vLLM team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Optional

import torch
from mindspore import mint


def apply_top_k_top_p_ms(logits, k, p):
    """
    Apply top-k and top-p masks to the logits for high performance.
    which is reference from 'apply_top_k_top_p_tpu' in vllm.
    """
    if k is not None:
        return apply_top_k_opt(logits, k, p)

    if p is not None:
        probs = logits.softmax(dim=-1)
        probs_sort, _ = mint.sort(probs, dim=-1, descending=False)
        cumprob = mint.cumsum(probs_sort, dim=-1)
        top_p_mask = cumprob <= 1 - p.unsqueeze(dim=1)
        top_p_mask[:, -1] = False  # at least one

        top_p_count = top_p_mask.sum(dim=-1).unsqueeze(1)
        top_p_cutoff = probs_sort.gather(-1, top_p_count)
        elements_to_discard = probs < top_p_cutoff
        logits.masked_fill_(elements_to_discard, -float("inf"))

    return logits


def random_sample(
    probs: torch.Tensor,
    generators: dict[int, torch.Generator],
) -> torch.Tensor:
    """Randomly sample from the probabilities.

    We use this function instead of torch.multinomial because torch.multinomial
    causes CPU-GPU synchronization.
    """
    q = torch.empty_like(probs)
    # NOTE(woosuk): To batch-process the requests without their own seeds,
    # which is the common case, we first assume that every request does
    # not have its own seed. Then, we overwrite the values for the requests
    # that have their own seeds.
    if len(generators) != probs.shape[0]:
        q.exponential_()
    if generators:
        # TODO(woosuk): This can be slow because we handle each request
        # one by one. Optimize this.
        for i, generator in generators.items():
            q[i].exponential_(generator=generator)
    # if use probs.div_(q) instead of probs = probs.div(q), it will cause
    # a error when running.
    probs = probs.div(q)
    return probs.argmax(dim=-1).view(-1)


def topk_topp_sampler_forward_native(
    self,
    logits: torch.Tensor,
    generators: dict[int, torch.Generator],
    k: Optional[torch.Tensor],
    p: Optional[torch.Tensor],
) -> torch.Tensor:
    logits = apply_top_k_top_p_ms(logits, k, p)
    logits_to_return = None
    if self.logprobs_mode == "processed_logits":
        logits_to_return = logits
    elif self.logprobs_mode == "processed_logprobs":
        logits_to_return = logits.log_softmax(dim=-1, dtype=torch.float32)
    probs = logits.softmax(dim=-1, dtype=torch.float32)
    return random_sample(probs, generators), logits_to_return


def apply_top_k_top_p(
    logits: torch.Tensor,
    k: Optional[torch.Tensor],
    p: Optional[torch.Tensor],
) -> torch.Tensor:
    """Apply top-k and top-p masks to the logits.

    If a top-p is used, this function will sort the logits tensor,
    which can be slow for large batches.

    The logits tensor may be updated in-place.
    """
    if p is None:
        if k is None:
            return logits

        # Avoid sorting vocab for top-k only case.
        return apply_top_k_only(logits, k)

    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)

    if k is not None:
        # Apply top-k.
        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B
        # Get all the top_k values.
        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))
        top_k_mask = logits_sort < top_k_mask
        logits_sort.masked_fill_(top_k_mask, -float("inf"))

    if p is not None:
        # Apply top-p.
        probs_sort = logits_sort.softmax(dim=-1)
        probs_sum = torch.cumsum(probs_sort, dim=-1)
        top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)
        # at least one
        top_p_mask[:, -1] = False
        logits_sort.masked_fill_(top_p_mask, -float("inf"))

    # Re-sort the probabilities.
    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)
    return logits


def apply_top_k_only(
    logits: torch.Tensor,
    k: torch.Tensor,
) -> torch.Tensor:
    """
    Apply top-k mask to the logits.

    This implementation doesn't involve sorting the entire vocab.

    The logits tensor may be updated in-place.
    """
    no_top_k_mask = k == logits.shape[1]
    # Set non-top-k rows to 1 so that we can gather.
    k = k.masked_fill(no_top_k_mask, 1)
    max_top_k = k.max()
    # topk.values tensor has shape [batch_size, max_top_k].
    # Convert top k to 0-based index in range [0, max_top_k).
    k_index = k.sub_(1).unsqueeze(1).expand(logits.shape[0], 1)

    # tensor.item() will cause GPU-CPU Sync, so place as later as possible.
    # can be deleted after logits.topk() support tensor-type input.
    int_max_top_k = max_top_k.item()

    top_k_mask = logits.topk(int_max_top_k, dim=1)[0].gather(1, k_index.long())
    # Handle non-topk rows.
    top_k_mask.masked_fill_(no_top_k_mask.unsqueeze(1), -float("inf"))
    logits.masked_fill_(logits < top_k_mask, -float("inf"))
    return logits


def apply_top_k_opt(logits, k, p):
    '''
    Apply top-k and top-p masks to the logits.
    
    This optimized version performs top-p calculations *only* on the 
    top-k elements to get better performance.
    '''
    no_top_k_mask = (k == logits.shape[1])
    k_for_topk = k.masked_fill(no_top_k_mask, 1)
    max_top_k = k_for_topk.max()
    int_max_top_k = max_top_k.item()
    top_k_logits, top_k_indices = logits.topk(int_max_top_k, dim=-1)
    indices_to_keep = mint.arange(int_max_top_k).unsqueeze(0)
    k_mask = indices_to_keep < k.unsqueeze(1)
    k_mask = k_mask | no_top_k_mask.unsqueeze(1)
    top_k_logits.masked_fill_(~k_mask, -float("inf"))

    if p is not None:
        probs_k = top_k_logits.softmax(dim=-1)
        cumprob_k = mint.cumsum(probs_k, dim=-1)
        shifted_cumprob_k = cumprob_k - probs_k
        top_p_mask = shifted_cumprob_k > p.unsqueeze(dim=1)
        top_k_logits.masked_fill_(top_p_mask, -float("inf"))

    final_logits = mint.full_like(logits, -float("inf"))
    final_logits.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)
    final_logits = mint.where(no_top_k_mask.unsqueeze(1), logits, final_logits)
    return final_logits
