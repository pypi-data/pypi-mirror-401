{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089aa3e-98a5-443f-9f16-b8c862bea225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training started!\n",
      "Step: 5000, Elapsed Time: 17.63s\n",
      "Step: 10000, Elapsed Time: 34.40s\n",
      "Step: 15000, Elapsed Time: 50.92s\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2e+03    |\n",
      "|    ep_rew_mean      | -0.66    |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 294      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000252 |\n",
      "|    n_updates        | 496      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | -0.66    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 294      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Step: 20000, Elapsed Time: 67.77s\n",
      "Step: 25000, Elapsed Time: 84.50s\n",
      "Step: 30000, Elapsed Time: 101.13s\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2e+03    |\n",
      "|    ep_rew_mean      | -0.572   |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 296      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 996      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | -0.572   |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 296      |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Step: 35000, Elapsed Time: 117.81s\n",
      "Step: 40000, Elapsed Time: 134.51s\n",
      "Step: 45000, Elapsed Time: 151.26s\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2e+03    |\n",
      "|    ep_rew_mean      | -0.541   |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 297      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00011  |\n",
      "|    n_updates        | 1496     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | -0.541   |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 297      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Step: 50000, Elapsed Time: 167.93s\n",
      "Step: 55000, Elapsed Time: 184.53s\n",
      "Step: 60000, Elapsed Time: 201.14s\n"
     ]
    }
   ],
   "source": [
    "#sb3中的并行环境包装器\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from gym_env_wrapper import create_env #包括 HyFlexEnv以及Reward奖励设计等\n",
    "\n",
    "\n",
    "# gym保存文件路径\n",
    "gym_env_save_path = \"/home/chaofan/Documents/pyhyflex/hhrl/results/\"\n",
    "# 载入的配置文件路径\n",
    "gym_env_config_path = '/home/chaofan/Documents/pyhyflex/hhrl/configs/fir_discrete.ini'\n",
    "# 固定种子参数\n",
    "gym_env_seed = 7  # 固定种子为 7\n",
    "savefig_dpi = 200\n",
    "\n",
    "\n",
    "# 使用的CPU数量\n",
    "Num_CPU = 8\n",
    "# 解决问题的类型\n",
    "Problem_Type = 'BP'\n",
    "# 解决问题的类型下的实例\n",
    "Instance_ID = 0\n",
    "# 每个episode中的步数限制\n",
    "Iteration_Limit = 2000\n",
    "# 总步长\n",
    "TotalTimestep = 1000000\n",
    "\n",
    "\n",
    "\n",
    "# 输出timestep的时间和最后的绘图\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ProgressCallback, self).__init__(verbose)\n",
    "        self.start_time = None\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.start_time = time.time()\n",
    "        print(\"Training started!\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        if len(self.locals['infos']) > 0:\n",
    "            for info in self.locals['infos']:\n",
    "                if 'episode' in info.keys():\n",
    "                    self.episode_rewards.append(info['episode']['r'])\n",
    "                    \n",
    "        if self.num_timesteps % 5000 == 0:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            print(f\"Step: {self.num_timesteps}, Elapsed Time: {elapsed_time:.2f}s\")\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        print(f\"Training ended! Total time: {elapsed_time:.2f}s\")\n",
    "        \n",
    "        # Plot rewards only if there are any\n",
    "        if self.episode_rewards:\n",
    "            plt.plot(np.arange(len(self.episode_rewards)), self.episode_rewards)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.title('Reward Trend')\n",
    "            # 保存图表在当前目录下\n",
    "            plt.savefig('reward_trend_50k.png', dpi=savefig_dpi)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No episode rewards recorded.\")\n",
    "\n",
    "\n",
    "\n",
    "#使用Monitor以及再次包装环境，以进行并行化运行 (参数必须内置函数内部)\n",
    "def make_env(problem, instance_id, run_id, iteration_limit, overwrite):\n",
    "    # 保存文件路径\n",
    "    _save_path = gym_env_save_path\n",
    "    # 载入的配置文件路径\n",
    "    _config_path = gym_env_config_path\n",
    "    # 固定种子参数\n",
    "    _seed = gym_env_seed \n",
    "    \n",
    "    def _init():\n",
    "        env = create_env(problem, \n",
    "                         instance_id, \n",
    "                         _seed, \n",
    "                         run_id, \n",
    "                         iteration_limit, \n",
    "                         _config_path, \n",
    "                         _save_path, \n",
    "                         overwrite)\n",
    "        \n",
    "        env = Monitor(env)  # 使用Monitor包装环境\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 创建并行环境\n",
    "    env = SubprocVecEnv([make_env(Problem_Type, Instance_ID, _i, Iteration_Limit, True) for _i in range(Num_CPU)])\n",
    "    \n",
    "    # 创建并训练模型\n",
    "    model = DQN('MlpPolicy', env, verbose=1)\n",
    "    \n",
    "    # 创建进度回调\n",
    "    progress_callback = ProgressCallback(verbose=1)\n",
    "    \n",
    "    # 传递回调函数给learn方法\n",
    "    model.learn(total_timesteps=TotalTimestep, callback=progress_callback)\n",
    "    \n",
    "    # 保存模型\n",
    "    model.save(\"dqn_hyflex\")\n",
    "    \n",
    "    # 关闭环境\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20967de-9b08-4b35-b8a6-5447a645bddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhyflex_ipykernel",
   "language": "python",
   "name": "pyhyflex_ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
