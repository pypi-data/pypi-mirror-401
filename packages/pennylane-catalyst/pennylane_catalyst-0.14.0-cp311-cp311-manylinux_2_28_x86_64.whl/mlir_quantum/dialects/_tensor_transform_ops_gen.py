
# Autogenerated by mlir-tblgen; don't manually edit.

from ._ods_common import _cext as _ods_cext
from ._ods_common import (
    equally_sized_accessor as _ods_equally_sized_accessor,
    get_default_loc_context as _ods_get_default_loc_context,
    get_op_results_or_values as _get_op_results_or_values,
    segmented_accessor as _ods_segmented_accessor,
)
_ods_ir = _ods_cext.ir
_ods_cext.globals.register_traceback_file_exclusion(__file__)

import builtins
from typing import Sequence as _Sequence, Union as _Union, Optional as _Optional


from ._transform_ops_gen import _Dialect

@_ods_cext.register_operation(_Dialect)
class ApplyBubbleUpExtractSlicePatternsOp(_ods_ir.OpView):
  r"""
  Indicates that producers of tensor.extract_slice should swap and operate on 
  the result of the slice.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.bubble_up_extract_slice"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_bubble_up_extract_slice(*, loc=None, ip=None) -> ApplyBubbleUpExtractSlicePatternsOp:
  return ApplyBubbleUpExtractSlicePatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyDecomposeTensorConcatPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that tensor.concat ops should be decomposed into a chain of
  tensor.insert_slice operations inserting into a materialized destination.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.decompose_concat"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_decompose_concat(*, loc=None, ip=None) -> ApplyDecomposeTensorConcatPatternsOp:
  return ApplyDecomposeTensorConcatPatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyDropRedundantInsertSliceRankExpansionPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that redundant tensor.insert_slice rank reductions should be
  dropped. E.g., cases where a tensor.extract_slice rank reduction immediately
  follows an inverse tensor.insert_slice rank expansion.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.drop_redundant_insert_slice_rank_expansion"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_drop_redundant_insert_slice_rank_expansion(*, loc=None, ip=None) -> ApplyDropRedundantInsertSliceRankExpansionPatternsOp:
  return ApplyDropRedundantInsertSliceRankExpansionPatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyFoldTensorEmptyPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that tensor.extract_slice and reassociative reshapes should be
  folded into tensor.empty.
  
  If `fold_single_use_only` is set to "true", only tensor.empty that have a
  single use are folded.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.fold_tensor_empty"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, fold_single_use_only=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    if fold_single_use_only is not None: attributes["fold_single_use_only"] = (fold_single_use_only if (
        isinstance(fold_single_use_only, _ods_ir.Attribute) or
        not _ods_ir.AttrBuilder.contains('BoolAttr')) else
          _ods_ir.AttrBuilder.get('BoolAttr')(fold_single_use_only, context=_ods_context))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def fold_single_use_only(self) -> _ods_ir.BoolAttr:
    return self.operation.attributes["fold_single_use_only"]

  @fold_single_use_only.setter
  def fold_single_use_only(self, value: _ods_ir.BoolAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["fold_single_use_only"] = value

def apply_patterns_tensor_fold_tensor_empty(*, fold_single_use_only=None, loc=None, ip=None) -> ApplyFoldTensorEmptyPatternsOp:
  return ApplyFoldTensorEmptyPatternsOp(fold_single_use_only=fold_single_use_only, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyFoldTensorSubsetOpsIntoVectorTransfersPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that tensor.extract_slice -> vector.transfer_read and
  vector.transfer_write -> tensor.insert_slice op chains should be folded into
  vector tranfer read and write ops
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.fold_tensor_subset_ops_into_vector_transfers"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_fold_tensor_subset_ops_into_vector_transfers(*, loc=None, ip=None) -> ApplyFoldTensorSubsetOpsIntoVectorTransfersPatternsOp:
  return ApplyFoldTensorSubsetOpsIntoVectorTransfersPatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyFoldTensorSubsetOpsPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that tensor.empty should be folded with tensor.extract_slice,
  tensor.expand_shape and tensor.collapse_shape.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.fold_tensor_subset_ops"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_fold_tensor_subset_ops(*, loc=None, ip=None) -> ApplyFoldTensorSubsetOpsPatternsOp:
  return ApplyFoldTensorSubsetOpsPatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyMergeConsecutiveInsertExtractSlicePatternsOp(_ods_ir.OpView):
  r"""
  Indicates that consecutive tensor.extract_slice/tensor.insert_slice ops
  should be merged into a single op. These patterns are not canonicalizations
  because the bufferization is sensitive to IR structure.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.merge_consecutive_insert_extract_slice"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_merge_consecutive_insert_extract_slice(*, loc=None, ip=None) -> ApplyMergeConsecutiveInsertExtractSlicePatternsOp:
  return ApplyMergeConsecutiveInsertExtractSlicePatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyReassociativeReshapeFoldingPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that reassociative reshapes (tensor.collapse_shape /
  tensor.expand_shape) should be folded with inverse rank expansions / rank
  reductions (via tensor.insert_slice / tensor.extract_slice).
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.reassociative_reshape_folding"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

def apply_patterns_tensor_reassociative_reshape_folding(*, loc=None, ip=None) -> ApplyReassociativeReshapeFoldingPatternsOp:
  return ApplyReassociativeReshapeFoldingPatternsOp(loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class ApplyRewriteTensorOpsAsConstantPatternsOp(_ods_ir.OpView):
  r"""
  Indicates that tensor ops (such as tensor.generate) should be replaced with
  constants (arith.constant) when possible.
  """

  OPERATION_NAME = "transform.apply_patterns.tensor.rewrite_as_constant"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, aggressive=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    if bool(aggressive): attributes["aggressive"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def aggressive(self) -> bool:
    return "aggressive" in self.operation.attributes

  @aggressive.setter
  def aggressive(self, value):
    if bool(value):
      self.operation.attributes["aggressive"] = _ods_ir.UnitAttr.get()
    elif "aggressive" in self.operation.attributes:
      del self.operation.attributes["aggressive"]

  @aggressive.deleter
  def aggressive(self):
    del self.operation.attributes["aggressive"]

def apply_patterns_tensor_rewrite_as_constant(*, aggressive=None, loc=None, ip=None) -> ApplyRewriteTensorOpsAsConstantPatternsOp:
  return ApplyRewriteTensorOpsAsConstantPatternsOp(aggressive=aggressive, loc=loc, ip=ip)

@_ods_cext.register_operation(_Dialect)
class MakeLoopIndependentOp(_ods_ir.OpView):
  r"""
  Rewrite the targeted ops such that their index-typed operands no longer
  depend on any loop induction variable of the `num_loop` enclosing `scf.for`
  loops. I.e., compute an upper bound that is independent of any such loop IV
  for every tensor dimension. The transformed op could then be hoisted from
  the `num_loop` enclosing loops. To preserve the original semantics, place a
  `tensor.extract_slice` inside the loop.
  
  Currently supported operations are:
  - tensor.empty: Replaced with a new tensor.empty with upper bound sizes,
    followed by a tensor.extract_slice.
  - tensor.pad: Replaced by an upper bound padding, followed by a
    tensor.extract_slice.
  
  #### Return modes
  
  This operation fails if at least one induction variable could not be
  eliminated. In case the targeted op is already independent of induction
  variables, this transform succeeds and returns the unmodified target op.
  
  Otherwise, the returned handle points to a subset of the produced ops:
  - tensor.empty: The returned handle points to the tensor.extract_slice op.
  - tensor.pad: The returned handle points to the tensor.extract_slice op.
  
  This transform op consumes the target handle and produces a result handle.
  """

  OPERATION_NAME = "transform.tensor.make_loop_independent"

  _ODS_REGIONS = (0, True)

  def __init__(self, transformed, target, num_loops, *, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    operands.append(target)
    _ods_context = _ods_get_default_loc_context(loc)
    attributes["num_loops"] = (num_loops if (
    isinstance(num_loops, _ods_ir.Attribute) or
    not _ods_ir.AttrBuilder.contains('I64Attr')) else
      _ods_ir.AttrBuilder.get('I64Attr')(num_loops, context=_ods_context))
    results = []
    results.append(transformed)
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def target(self) -> _ods_ir.Value:
    return self.operation.operands[0]

  @builtins.property
  def num_loops(self) -> _ods_ir.IntegerAttr:
    return self.operation.attributes["num_loops"]

  @num_loops.setter
  def num_loops(self, value: _ods_ir.IntegerAttr):
    if value is None:
      raise ValueError("'None' not allowed as value for mandatory attributes")
    self.operation.attributes["num_loops"] = value

  @builtins.property
  def transformed(self) -> _ods_ir.OpResult:
    return self.operation.results[0]

def tensor_make_loop_independent(transformed, target, num_loops, *, loc=None, ip=None) -> _ods_ir.OpResult:
  return MakeLoopIndependentOp(transformed=transformed, target=target, num_loops=num_loops, loc=loc, ip=ip).result

@_ods_cext.register_operation(_Dialect)
class TypeConversionCastShapeDynamicDimsOp(_ods_ir.OpView):
  r"""
  Populates a type converter with conversion materialization functions that
  cast a tensor value between two cast-compatible tensors. See `tensor.cast`
  for more information on cast compatibility between tensors.
  
  If `ignore_dynamic_info` is not set, this will set an additional constraint
  that source materializations do not cast dynamic dimensions to static ones.
  """

  OPERATION_NAME = "transform.type_conversion.tensor.cast_shape_dynamic_dims"

  _ODS_REGIONS = (0, True)

  def __init__(self, *, ignore_dynamic_info=None, loc=None, ip=None):
    operands = []
    attributes = {}
    regions = None
    _ods_context = _ods_get_default_loc_context(loc)
    if bool(ignore_dynamic_info): attributes["ignore_dynamic_info"] = _ods_ir.UnitAttr.get(
      _ods_get_default_loc_context(loc))
    results = []
    _ods_successors = None
    super().__init__(self.OPERATION_NAME, self._ODS_REGIONS, self._ODS_OPERAND_SEGMENTS, self._ODS_RESULT_SEGMENTS, attributes=attributes, results=results, operands=operands, successors=_ods_successors, regions=regions, loc=loc, ip=ip)

  @builtins.property
  def ignore_dynamic_info(self) -> bool:
    return "ignore_dynamic_info" in self.operation.attributes

  @ignore_dynamic_info.setter
  def ignore_dynamic_info(self, value):
    if bool(value):
      self.operation.attributes["ignore_dynamic_info"] = _ods_ir.UnitAttr.get()
    elif "ignore_dynamic_info" in self.operation.attributes:
      del self.operation.attributes["ignore_dynamic_info"]

  @ignore_dynamic_info.deleter
  def ignore_dynamic_info(self):
    del self.operation.attributes["ignore_dynamic_info"]

def type_conversion_tensor_cast_shape_dynamic_dims(*, ignore_dynamic_info=None, loc=None, ip=None) -> TypeConversionCastShapeDynamicDimsOp:
  return TypeConversionCastShapeDynamicDimsOp(ignore_dynamic_info=ignore_dynamic_info, loc=loc, ip=ip)
