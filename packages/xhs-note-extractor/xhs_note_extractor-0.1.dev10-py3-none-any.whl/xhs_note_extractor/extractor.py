"""
å°çº¢ä¹¦ç¬”è®°æå–å™¨æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†ä»å°çº¢ä¹¦URLä¸­æå–ç¬”è®°ä¿¡æ¯çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- URLè§£æå’Œè½¬æ¢
- è®¾å¤‡è¿æ¥å’Œé¡µé¢è·³è½¬
- ç¬”è®°å†…å®¹æå–ï¼ˆæ­£æ–‡ã€å›¾ç‰‡ã€ç‚¹èµæ•°ç­‰ï¼‰
- ç»“æ„åŒ–æ•°æ®è¿”å›

ä½œè€…: JoyCode Agent
ç‰ˆæœ¬: 1.0.0
"""

import uiautomator2 as u2
import time
import re
import requests
import logging
from typing import Dict, List, Optional, Union
from urllib.parse import urlparse, parse_qs
import xml.etree.ElementTree as ET
from .date_desc_utils import parse_time_to_timestamp_ms
from .number_utils import parse_count_to_int

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class XHSNoteExtractor:
    """
    å°çº¢ä¹¦ç¬”è®°æå–å™¨ç±»
    
    æä¾›äº†ä»å°çº¢ä¹¦URLä¸­æå–ç¬”è®°ä¿¡æ¯çš„å®Œæ•´åŠŸèƒ½ï¼Œ
    åŒ…æ‹¬URLè§£æã€è®¾å¤‡è¿æ¥ã€é¡µé¢è·³è½¬å’Œç¬”è®°å†…å®¹æå–ã€‚
    """
    
    def __init__(self, device_serial: Optional[str] = None, enable_time_logging: bool = True):
        """
        åˆå§‹åŒ–å°çº¢ä¹¦ç¬”è®°æå–å™¨
        
        Args:
            device_serial (str, optional): è®¾å¤‡åºåˆ—å·ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è¿æ¥å¯ç”¨è®¾å¤‡
            enable_time_logging (bool, optional): æ˜¯å¦å¯ç”¨è€—æ—¶æ‰“å°ï¼Œé»˜è®¤ä¸ºTrue
            
        Raises:
            RuntimeError: å½“æ²¡æœ‰å¯ç”¨è®¾å¤‡æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        self.device = None
        self.device_serial = device_serial
        self.enable_time_logging = enable_time_logging
        self.available_devices = []
        self.current_device_index = 0
        
        # è·å–å¯ç”¨è®¾å¤‡åˆ—è¡¨
        self._get_available_devices()
        
        # å°è¯•è¿æ¥è®¾å¤‡
        if not self.connect_device():
            # ä¸ç«‹å³æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸åç»­é‡è¯•
            logger.warning("åˆå§‹åŒ–æ—¶æœªæ‰¾åˆ°å¯ç”¨è®¾å¤‡ï¼Œå°†åœ¨æå–æ—¶å°è¯•é‡è¯•")
    
    def _time_method(self, method_name, start_time):
        """
        è®°å½•æ–¹æ³•æ‰§è¡Œæ—¶é—´
        
        Args:
            method_name (str): æ–¹æ³•åç§°
            start_time (float): å¼€å§‹æ—¶é—´
        """
        if self.enable_time_logging:
            elapsed_time = time.time() - start_time
            if elapsed_time < 1:
                logger.info(f"â±ï¸  [{method_name}] è€—æ—¶: {elapsed_time*1000:.0f}ms")
            else:
                logger.info(f"â±ï¸  [{method_name}] è€—æ—¶: {elapsed_time:.2f}s")
    
    def _get_available_devices(self) -> List[str]:
        """
        è·å–æ‰€æœ‰å¯ç”¨è®¾å¤‡åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨è®¾å¤‡åºåˆ—å·åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨adbè·å–è®¾å¤‡åˆ—è¡¨
            import subprocess
            result = subprocess.run(['adb', 'devices'], capture_output=True, text=True)
            devices = []
            for line in result.stdout.split('\n')[1:]:  # è·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
                if '\tdevice' in line:
                    device_serial = line.split('\t')[0]
                    devices.append(device_serial)
            self.available_devices = devices
            logger.info(f"å‘ç° {len(devices)} ä¸ªå¯ç”¨è®¾å¤‡: {devices}")
            return devices
        except Exception as e:
            logger.error(f"è·å–è®¾å¤‡åˆ—è¡¨å¤±è´¥: {e}")
            self.available_devices = []
            return []
    
    def connect_device(self, device_serial: Optional[str] = None) -> bool:
        """
        è¿æ¥è®¾å¤‡
        
        Args:
            device_serial (str, optional): æŒ‡å®šè®¾å¤‡åºåˆ—å·ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.device_serial
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè¿æ¥è®¾å¤‡
        """
        start_time = time.time()
        
        # å¦‚æœæŒ‡å®šäº†è®¾å¤‡åºåˆ—å·ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è®¾å¤‡
        target_device = device_serial or self.device_serial
        
        try:
            if target_device:
                self.device = u2.connect(target_device)
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè®¾å¤‡ï¼Œå°è¯•è¿æ¥ç¬¬ä¸€ä¸ªå¯ç”¨è®¾å¤‡
                if hasattr(self, 'available_devices') and self.available_devices:
                    self.device = u2.connect(self.available_devices[0])
                    self.current_device_index = 0
                else:
                    self.device = u2.connect()
            logger.info(f"âœ“ å·²è¿æ¥è®¾å¤‡: {self.device.serial}")
            self._time_method("connect_device", start_time)
            return True
        except Exception as e:
            logger.error(f"âœ— è®¾å¤‡è¿æ¥å¤±è´¥: {e}")
            self._time_method("connect_device", start_time)
            return False
    
    def switch_to_next_device(self) -> bool:
        """
        åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨è®¾å¤‡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡
        """
        if not hasattr(self, 'available_devices') or not self.available_devices or len(self.available_devices) <= 1:
            logger.warning("æ²¡æœ‰æ›´å¤šå¯ç”¨è®¾å¤‡å¯ä»¥åˆ‡æ¢")
            return False
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡
        self.current_device_index = (self.current_device_index + 1) % len(self.available_devices)
        next_device_serial = self.available_devices[self.current_device_index]
        
        logger.info(f"å°è¯•åˆ‡æ¢åˆ°è®¾å¤‡: {next_device_serial}")
        return self.connect_device(next_device_serial)
    def is_device_connected(self) -> bool:
        """
        æ£€æŸ¥è®¾å¤‡æ˜¯å¦ä»ç„¶è¿æ¥
        
        Returns:
            bool: è®¾å¤‡æ˜¯å¦è¿æ¥
        """
        if not self.device:
            return False
        try:
            # é€šè¿‡è·å–è®¾å¤‡ä¿¡æ¯æ¥éªŒè¯è¿æ¥
            self.device.info
            return True
        except:
            return False
    
    def restart_xhs_app(self) -> bool:
        """
        é‡å¯å°çº¢ä¹¦APP
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡å¯APP
        """
        start_time = time.time()
        try:
            # å°çº¢ä¹¦APPçš„åŒ…å
            xhs_package_name = "com.xingin.xhs"
            
            # å…ˆå°è¯•åœæ­¢APP
            logger.info("æ­£åœ¨åœæ­¢å°çº¢ä¹¦APP...")
            self.device.app_stop(xhs_package_name)
            time.sleep(1)
            
            # ç„¶åå¯åŠ¨APP
            logger.info("æ­£åœ¨å¯åŠ¨å°çº¢ä¹¦APP...")
            self.device.app_start(xhs_package_name)
            
            # ç­‰å¾…APPå¯åŠ¨å®Œæˆ
            logger.info("ç­‰å¾…APPå¯åŠ¨å®Œæˆ...")
            time.sleep(3)  # ç»™APPè¶³å¤Ÿçš„å¯åŠ¨æ—¶é—´
            
            logger.info("âœ“ å°çº¢ä¹¦APPé‡å¯æˆåŠŸ")
            if self.enable_time_logging:
                elapsed_time = time.time() - start_time
                logger.info(f"[restart_xhs_app] è€—æ—¶: {elapsed_time:.3f}ç§’")
            return True
        except Exception as e:
            logger.error(f"âœ— é‡å¯å°çº¢ä¹¦APPå¤±è´¥: {e}")
            if self.enable_time_logging:
                elapsed_time = time.time() - start_time
                logger.info(f"[restart_xhs_app] è€—æ—¶: {elapsed_time:.3f}ç§’")
            return False

    @staticmethod
    def parse_xhs_url(url: str) -> Dict[str, str]:
        """
        è§£æå°çº¢ä¹¦URLï¼Œæå–note_idå’Œxsec_token
        
        Args:
            url (str): å°çº¢ä¹¦URLï¼Œæ”¯æŒæ ‡å‡†æ ¼å¼æˆ–xhsdiscoveråè®®æ ¼å¼
            
        Returns:
            Dict[str, str]: åŒ…å«note_idå’Œxsec_tokençš„å­—å…¸
            
        Raises:
            ValueError: å½“URLæ ¼å¼ä¸æ­£ç¡®æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        start_time = time.time()
        # å¤„ç†xhsdiscoveråè®®æ ¼å¼
        if url.startswith("xhsdiscover://"):
            # æå–note_id
            note_id_match = re.search(r'item/([^?]+)', url)
            if not note_id_match:
                raise ValueError("æ— æ³•ä»xhsdiscover URLä¸­æå–note_id")
            
            note_id = note_id_match.group(1)
            
            # å°è¯•ä»open_urlå‚æ•°ä¸­æå–åŸå§‹URL
            open_url_match = re.search(r'open_url=([^&]+)', url)
            xsec_token = ""
            if open_url_match:
                open_url = open_url_match.group(1)
                # è§£ç URL
                import urllib.parse
                decoded_url = urllib.parse.unquote(open_url)
                # ä»åŸå§‹URLä¸­æå–xsec_token
                token_match = re.search(r'xsec_token=([^&]+)', decoded_url)
                if token_match:
                    xsec_token = token_match.group(1)
            
            return {
                "note_id": note_id,
                "xsec_token": xsec_token,
                "original_url": url
            }
        
        # å¤„ç†æ ‡å‡†URLæ ¼å¼
        elif "xiaohongshu.com" in url:
            parsed_url = urlparse(url)
            path_parts = parsed_url.path.strip('/').split('/')
            
            # æŸ¥æ‰¾exploreéƒ¨åˆ†å’Œnote_id
            if 'explore' in path_parts:
                explore_index = path_parts.index('explore')
                if explore_index + 1 < len(path_parts):
                    note_id = path_parts[explore_index + 1]
                else:
                    raise ValueError("URLä¸­ç¼ºå°‘note_id")
            # å…¼å®¹ /discovery/item/ æ ¼å¼
            elif 'discovery' in path_parts and 'item' in path_parts:
                item_index = path_parts.index('item')
                if item_index + 1 < len(path_parts):
                    note_id = path_parts[item_index + 1]
                else:
                    raise ValueError("URLä¸­ç¼ºå°‘note_id")
            else:
                raise ValueError("URLæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘/explore/æˆ–/discovery/item/è·¯å¾„")
            
            # æå–æŸ¥è¯¢å‚æ•°ä¸­çš„xsec_token
            query_params = parse_qs(parsed_url.query)
            xsec_token = query_params.get('xsec_token', [''])[0]
            
            elapsed_time = time.time() - start_time
            logger.info(f"[parse_xhs_url] è€—æ—¶: {elapsed_time:.3f}ç§’")
            return {
                "note_id": note_id,
                "xsec_token": xsec_token,
                "original_url": url
            }
        
        else:
            elapsed_time = time.time() - start_time
            logger.info(f"[parse_xhs_url] è€—æ—¶: {elapsed_time:.3f}ç§’")
            raise ValueError("ä¸æ”¯æŒçš„URLæ ¼å¼")
    
    @staticmethod
    def validate_url(url: str) -> bool:
        """
        éªŒè¯URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„å°çº¢ä¹¦URL
        
        Args:
            url (str): è¦éªŒè¯çš„URL
            
        Returns:
            bool: URLæ˜¯å¦æœ‰æ•ˆ
        """
        try:
            XHSNoteExtractor.parse_xhs_url(url)
            return True
        except ValueError:
            return False
    
    @staticmethod
    def convert_to_xhsdiscover_format(note_id: str, xsec_token: str = "") -> str:
        """
        å°†note_idå’Œxsec_tokenè½¬æ¢ä¸ºxhsdiscoveråè®®æ ¼å¼
        
        Args:
            note_id (str): ç¬”è®°ID
            xsec_token (str): xsec_tokenå‚æ•°
            
        Returns:
            str: xhsdiscoveråè®®æ ¼å¼çš„URL
        """
        start_time = time.time()
        result = ""
        if xsec_token:
            original_url = f"http://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source=pc_feed"
            encoded_url = requests.utils.quote(original_url)
            result = f"xhsdiscover://item/{note_id}?open_url={encoded_url}"
        else:
            result = f"xhsdiscover://item/{note_id}"
        
        elapsed_time = time.time() - start_time
        logger.info(f"[convert_to_xhsdiscover_format] è€—æ—¶: {elapsed_time:.3f}ç§’")
        return result
    
    def extract_note_data(self, url: Optional[str] = None, note_id: Optional[str] = None,
                         xsec_token: Optional[str] = None) -> Optional[Dict[str, Union[str, List[str]]]]:
        """
        ä»å°çº¢ä¹¦ç¬”è®°ä¸­æå–æ•°æ®ï¼Œæ”¯æŒè®¾å¤‡é‡è¯•æœºåˆ¶
        
        Args:
            url (str, optional): å°çº¢ä¹¦URLï¼Œå¦‚æœæä¾›åˆ™ä¼šè§£æå…¶ä¸­çš„note_idå’Œxsec_token
            note_id (str, optional): ç¬”è®°IDï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨
            xsec_token (str, optional): xsec_tokenå‚æ•°
            
        Returns:
            Optional[Dict[str, Union[str, List[str]]]]: åŒ…å«ç¬”è®°æ•°æ®çš„å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰æˆåŠŸåˆ™è¿”å›None
            
        Raises:
            Exception: å½“æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        start_time = time.time()
        # å¦‚æœæä¾›äº†URLï¼Œåˆ™å…ˆè§£æå®ƒï¼ˆéªŒè¯URLæœ‰æ•ˆæ€§ï¼‰
        if url:
            parsed_data = self.parse_xhs_url(url)
            note_id = parsed_data["note_id"]
            xsec_token = parsed_data["xsec_token"]
        
        max_retries = len(self.available_devices) if hasattr(self, 'available_devices') and self.available_devices else 1
        attempted_devices = []
        
        for attempt in range(max_retries):
            logger.info(f"å°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡æå–ç¬”è®°: {note_id}")
            
            # æ£€æŸ¥è®¾å¤‡æ˜¯å¦è¿æ¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•è¿æ¥
            if self.device is None:
                if not self.connect_device():
                    logger.warning("è®¾å¤‡è¿æ¥å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªè®¾å¤‡")
                    if hasattr(self, 'available_devices') and self.available_devices and attempt < len(self.available_devices) - 1:
                        self.switch_to_next_device()
                    continue
            
            # æ„å»ºè·³è½¬URL
            jump_url = self.convert_to_xhsdiscover_format(note_id, xsec_token)
            
            logger.info(f"æ­£åœ¨å°è¯•è·³è½¬è‡³ç¬”è®°: {note_id} (è®¾å¤‡: {self.device.serial if self.device else 'æœªçŸ¥'})")
            
            try:
                # # åœ¨è·³è½¬é“¾æ¥å‰é‡å¯APP
                # logger.info(f"ğŸ”„ å‡†å¤‡è·³è½¬è‡³ç¬”è®° {note_id}ï¼Œæ­£åœ¨é‡å¯APP...")
                # self.restart_xhs_app()
                
                # å‘èµ·è·³è½¬
                self.device.open_url(jump_url)
                logger.info("âœ“ å·²å‘é€è·³è½¬æŒ‡ä»¤ï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
                
                # ä½¿ç”¨ç°æœ‰çš„xhs_utilsåŠŸèƒ½æå–æ•°æ®
                data = self._get_detail_data()
                
                # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜éœ€è¦ç™»å½•ï¼Œå°è¯•ä¸‹ä¸€ä¸ªè®¾å¤‡
                if data is None:
                    logger.warning(f"å½“å‰è®¾å¤‡éœ€è¦ç™»å½•ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡")
                    attempted_devices.append(self.device.serial if self.device else "æœªçŸ¥è®¾å¤‡")
                    
                    # å¦‚æœæ²¡æœ‰æ›´å¤šè®¾å¤‡å¯ç”¨ï¼Œè¿”å›None
                    if not self.switch_to_next_device():
                        logger.error("æ²¡æœ‰æ›´å¤šå¯ç”¨è®¾å¤‡ï¼Œæå–å¤±è´¥")
                        self._time_method("extract_note_data", start_time)
                        return {
                            
                        }
                    
                    continue
                
                logger.info(f"âœ“ æˆåŠŸæå–ç¬”è®°æ•°æ®ï¼Œç‚¹èµæ•°: {data['likes']}, å›¾ç‰‡æ•°: {len(data['image_urls'])}")
                self._time_method("extract_note_data", start_time)
                return data
                
            except Exception as e:
                logger.error(f"âœ— æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
                attempted_devices.append(self.device.serial if self.device else "æœªçŸ¥è®¾å¤‡")
                
                # å¦‚æœè¿˜æœ‰è®¾å¤‡å¯ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                if attempt < max_retries - 1 and self.switch_to_next_device():
                    continue
                else:
                    logger.error("æ‰€æœ‰è®¾å¤‡å°è¯•å®Œæ¯•ï¼Œæå–å¤±è´¥")
                    self._time_method("extract_note_data", start_time)
        logger.error(f"æ‰€æœ‰è®¾å¤‡å°è¯•å®Œæ¯•ï¼Œæå–å¤±è´¥ã€‚å°è¯•è¿‡çš„è®¾å¤‡: {attempted_devices}")
        self._time_method("extract_note_data", start_time)
        return {}
    
    def _get_detail_data(self) -> Dict[str, Union[str, List[str]]]:
        """
        ä»å½“å‰å·²ç»æ‰“å¼€çš„å°çº¢ä¹¦è¯¦æƒ…é¡µæå–å®Œæ•´æ­£æ–‡ã€å›¾ç‰‡å’Œç‚¹èµæ•°ã€‚
        ä¼˜åŒ–ç‰ˆæœ¬: ä½¿ç”¨ dump_hierarchy æ›¿ä»£éå†ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚
        
        Returns:
            Dict[str, Union[str, List[str]]]: åŒ…å«ç¬”è®°æ•°æ®çš„å­—å…¸
        """
        start_time = time.time()
        logger.info("ğŸ” è¿›å…¥æ·±åº¦æå–æ¨¡å¼ (XMLä¼˜åŒ–ç‰ˆ)...")
        
        # 1. éªŒè¯æ˜¯å¦è¿›å…¥è¯¦æƒ…é¡µ & å±•å¼€å…¨æ–‡
        detail_loaded = False
        detail_keywords = ["è¯´ç‚¹ä»€ä¹ˆ", "å†™è¯„è®º", "å†™ç‚¹ä»€ä¹ˆ", "æ”¶è—", "ç‚¹èµ", "è¯„è®º", "åˆ†äº«", "å‘å¼¹å¹•"]
        login_keywords = ["å…¶ä»–ç™»å½•æ–¹å¼", "æˆ‘å·²é˜…è¯»å¹¶åŒæ„", "è´¦å·ä¸¢å¤±äº†", "å¾®ä¿¡ç™»å½•"]
        # å°è¯•ç‚¹å‡»å±•å¼€ (é¢„å…ˆåŠ¨ä½œ)
        try:
            # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æœ‰å±•å¼€æŒ‰é’®
            for btn_text in ["å±•å¼€", "æŸ¥çœ‹å…¨éƒ¨", "å…¨æ–‡"]:
                if self.device(text=btn_text).exists:
                    self.device(text=btn_text).click()
                    break
        except: pass

        # ç­‰å¾…åŠ è½½å®Œæ•´
        login_need = False
        for i in range(5):
            if any(self.device(textContains=kw).exists or self.device(descriptionContains=kw).exists for kw in login_keywords):
                login_need = True
                break
            time.sleep(0.5)
        print(f"login_need: {login_need}")
        if login_need:
            logger.error("âœ— éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹è¯¦æƒ…é¡µå†…å®¹ï¼Œæå–ç»ˆæ­¢")
            return None
        # ç­‰å¾…åŠ è½½å®Œæ•´
        for i in range(5):
            if any(self.device(textContains=kw).exists or self.device(descriptionContains=kw).exists for kw in detail_keywords):
                detail_loaded = True
                break
            if i == 2:
                # å¯èƒ½æ˜¯è§†é¢‘ï¼Œç‚¹å‡»å±å¹•ä¸­å¿ƒå°è¯•æ¿€æ´» UI
                self.device.click(540, 900)
            time.sleep(0.5)
        
        if not detail_loaded:
            logger.warning("âš  è­¦å‘Š:è¯¦æƒ…é¡µç‰¹å¾æœªå‘ç°,æå–å¯èƒ½ä¸å®Œæ•´")

        # å‘ä¸‹æ»šåŠ¨ç›´åˆ°çœ‹åˆ°è¯„è®ºåŒºæ ‡å¿—
        try:
            logger.info("ğŸ“œ å‘ä¸‹æ»šåŠ¨ä»¥æ˜¾ç¤ºå‘å¸ƒæ—¶é—´...")
            max_scrolls = 20  # æœ€å¤šæ»šåŠ¨5æ¬¡
            comment_section_found = False
            
            for scroll_attempt in range(max_scrolls):
                # æ£€æŸ¥æ˜¯å¦å·²ç»çœ‹åˆ°è¯„è®ºåŒºæ ‡å¿—
                xml_check = self.device.dump_hierarchy()
                if re.search(r'å…±\s*\d+\s*æ¡è¯„è®º', xml_check) and re.search(r'è¯´ç‚¹ä»€ä¹ˆ', xml_check):
                    logger.info(f"âœ“ æ‰¾åˆ°è¯„è®ºåŒºæ ‡å¿—,åœæ­¢æ»šåŠ¨ (æ»šåŠ¨{scroll_attempt}æ¬¡)")
                    comment_section_found = True
                    break
                
                # ç»§ç»­æ»šåŠ¨
                self.device.swipe(540, 1500, 540, 1000, 0.3)
                time.sleep(0.3)
            
            if not comment_section_found:
                logger.warning(f"âš  æ»šåŠ¨{max_scrolls}æ¬¡åä»æœªæ‰¾åˆ°è¯„è®ºåŒºæ ‡å¿—")
                
        except Exception as e:
            logger.warning(f"æ»šåŠ¨å¤±è´¥: {e}")

        # 2. è·å– UIå±‚çº§ (æ ¸å¿ƒä¼˜åŒ–)
        xml_dump_start = time.time()
        xml_content = self.device.dump_hierarchy()
        self._time_method("dump_hierarchy", xml_dump_start)
        
        # 3. è§£æ XML
        root = ET.fromstring(xml_content)
        
        content = ""
        likes = 0
        collects = 0
        comments = 0
        author_name = "Unknown"
        image_urls = []
        
        # æ”¶é›†æ‰€æœ‰ TextView èŠ‚ç‚¹ä¿¡æ¯
        text_nodes = []
        
        def parse_nodes(node):
            # if node.attrib.get('class') == 'android.widget.TextView': # ä¸å†é™åˆ¶ class
            text = node.attrib.get('text', '')
            if not text:
                text = node.attrib.get('content-desc', '')
                
            bounds_str = node.attrib.get('bounds', '[0,0][0,0]')
            # è§£æ bounds: [x1,y1][x2,y2]
            try:
                coords = bounds_str.replace('][', ',').replace('[', '').replace(']', '').split(',')
                x1, y1, x2, y2 = map(int, coords)
                if text:
                    text_nodes.append({
                        'text': text,
                        'l': x1, 't': y1, 'r': x2, 'b': y2,
                        'cx': (x1 + x2) / 2, 'cy': (y1 + y2) / 2
                    })
            except: pass
            for child in node:
                parse_nodes(child)
                
        parse_nodes(root)
        
        # 4. åˆ†æèŠ‚ç‚¹æ•°æ®
        
        # A. ä½œè€…æå– (å¯»æ‰¾ "å…³æ³¨" é™„è¿‘çš„æ–‡æœ¬)
        # ç­–ç•¥: æ‰¾åˆ°åŒ…å« "å…³æ³¨" çš„èŠ‚ç‚¹ï¼Œå–å…¶å·¦ä¾§æœ€è¿‘çš„èŠ‚ç‚¹
        follow_node = None
        for n in text_nodes:
            if n['text'] in ["å…³æ³¨", "å·²å…³æ³¨"]:
                follow_node = n
                break
        
        if follow_node:
            best_dist = 9999
            for n in text_nodes:
                if n == follow_node: continue
                if n['text'] in ["å…³æ³¨", "å·²å…³æ³¨"] or len(n['text']) > 30: continue
                
                # å‚ç›´æ¥è¿‘
                if abs(n['cy'] - follow_node['cy']) < 100:
                    # åœ¨å·¦ä¾§
                    if n['r'] <= follow_node['l'] + 50:
                        dist = follow_node['l'] - n['r']
                        if dist < best_dist:
                            best_dist = dist
                            author_name = n['text']
            logger.info(f"âœ“ è¯†åˆ«åˆ°ä½œè€…: {author_name}")

        # A.5 æ—¥æœŸæå–
        publish_time = 0
        date_desc = ""
        
        # ç¡®å®šæœç´¢èŒƒå›´
        # é¡¶éƒ¨è¾¹ç•Œ: ä½œè€…ä¿¡æ¯ä¸‹æ–¹ / çŠ¶æ€æ ä¸‹æ–¹
        min_y = 150 # é»˜è®¤è·³è¿‡çŠ¶æ€æ 
        if follow_node:
            min_y = max(min_y, follow_node['b'])
            
        # åº•éƒ¨è¾¹ç•Œ: è¯„è®ºåŒº / åº•éƒ¨äº’åŠ¨æ 
        limit_y = 2500 # é»˜è®¤ç»™ä¸ªå¤§å€¼
        
        # å¯»æ‰¾åº•éƒ¨ç‰¹å¾èŠ‚ç‚¹
        for n in text_nodes:
            # è¯„è®ºåŒºå¤´éƒ¨ "å…± 100 æ¡è¯„è®º"
            if re.match(r"^å…±\s*\d+\s*æ¡è¯„è®º$", n['text']):
                limit_y = min(limit_y, n['t'])
            # åº•éƒ¨è¾“å…¥æ¡† / äº’åŠ¨æ æ–‡å­—
            if n['text'] in ["è¯´ç‚¹ä»€ä¹ˆ", "å†™è¯„è®º", "å†™ç‚¹ä»€ä¹ˆ", "è¿™é‡Œæ˜¯è¯„è®ºåŒº"]:
                limit_y = min(limit_y, n['t'])
                
        # ç­›é€‰å€™é€‰èŠ‚ç‚¹
        candidate_nodes = [n for n in text_nodes if n != follow_node]
        if author_name != "Unknown":
             candidate_nodes = [n for n in candidate_nodes if n['text'] != author_name]
        
        # ç©ºé—´è¿‡æ»¤: ä½œè€…ä¸‹æ–¹ AND è¯„è®ºåŒºä¸Šæ–¹
        candidate_nodes = [n for n in candidate_nodes if n['t'] > min_y and n['b'] < limit_y]
        
        # æ’åºï¼šä»ä¸Šåˆ°ä¸‹
        candidate_nodes.sort(key=lambda x: x['t'])
        
        for n in candidate_nodes:
            text = n['text'].strip()
            if len(text) < 2 or len(text) > 50: continue 
            
            # æ’é™¤æ˜æ˜¾çš„äº’åŠ¨æ•°æ®
            if text in ["ç‚¹èµ", "æ”¶è—", "è¯„è®º", "å…³æ³¨", "åˆ†äº«", "å›å¤"]: continue
            
            try:
                # å°è¯•è§£æ
                ts = parse_time_to_timestamp_ms(text)
                # æš‚å­˜ä¸ºæœ€ä½³å€™é€‰ (æ—¥æœŸé€šå¸¸åœ¨æ­£æ–‡æœ€åï¼Œä¿ç•™æœ€åä¸€ä¸ªåˆæ³•çš„)
                publish_time = ts
                date_desc = text
            except ValueError:
                continue
        
        if date_desc:
            logger.info(f"âœ“ è¯†åˆ«åˆ°å‘å¸ƒæ—¶é—´: {date_desc} -> {publish_time}")

        # B. äº’åŠ¨æ•°æ®æå– (åº•éƒ¨åŒºåŸŸ)
        # ä½¿ç”¨ limit_y ä½œä¸ºåˆ†å‰²çº¿å¤§æ¦‚ç‡æ›´å‡†ç¡®
        bottom_nodes = [n for n in text_nodes if n['t'] >= limit_y - 300] # äº’åŠ¨æ é€šå¸¸åœ¨ limit_y ä¸Šæ–¹ä¸€ç‚¹ç‚¹ æˆ–è€… å°±åœ¨ mask åŒºåŸŸ
        bottom_nodes.sort(key=lambda x: x['l']) # ä»å·¦åˆ°å³
        
        for n in bottom_nodes:
            txt = n['text']
            # ä¿ç•™æ•°å­—ã€å°æ•°ç‚¹ã€w/W å’Œ "ä¸‡" å­—
            num_txt = ''.join(c for c in txt if c.isdigit() or c in ['.', 'w', 'W', 'ä¸‡'])
            if not num_txt: continue
            
            cx = n['cx']
            if 500 < cx < 750:
                likes = parse_count_to_int(num_txt)
            elif 750 < cx < 900:
                collects = parse_count_to_int(num_txt)
            elif cx >= 900:
                comments = parse_count_to_int(num_txt)

        # C. æ­£æ–‡æå–
        # è¿‡æ»¤æ‰éæ­£æ–‡å†…å®¹
        content_lines = []
        exclude_keywords = ['æ”¶è—', 'ç‚¹èµ', 'è¯„è®º', 'åˆ†äº«', 'å‘å¸ƒäº', 'è¯´ç‚¹ä»€ä¹ˆ', 'æ¡è¯„è®º', 'å…³æ³¨', author_name]
        if date_desc:
            exclude_keywords.append(date_desc)
        
        # æŒ‰ç…§å‚ç›´ä½ç½®æ’åº (ä½¿ç”¨ min_y å’Œ limit_y çº¦æŸ)
        content_nodes = [n for n in text_nodes if min_y < n['t'] < limit_y]
        content_nodes.sort(key=lambda x: x['t'])
        
        for n in content_nodes:
            t = n['text']
            if len(t) < 2: continue
            if any(k in t for k in exclude_keywords): continue
            
            # ç®€å•çš„å»é‡ç­–ç•¥
            if content_lines and t in content_lines[-1]: continue
            content_lines.append(t)
            
        content = "\n".join(content_lines)
        logger.info(f"æå–æ­£æ–‡: {content}")
        # 5. å›¾ç‰‡æå– (ä¿æŒåŸæœ‰é€»è¾‘ä½†ä¼˜åŒ–ç­‰å¾…)
        try:
             # è¿™é‡Œè¿˜æ˜¯éœ€è¦äº¤äº’ï¼Œæ— æ³•çº¯é XML
            share_btn = self.device(description="åˆ†äº«")
            if share_btn.exists:
                share_btn.click()
                # æ˜¾å¼ç­‰å¾… "å¤åˆ¶é“¾æ¥"
                copy_link = self.device(text="å¤åˆ¶é“¾æ¥")
                if copy_link.wait(timeout=2.0):
                    copy_link.click()
                    # ç­‰å¾…å‰ªè´´æ¿æ›´æ–°? ç¨å¾®ç¼“ä¸€ä¸‹
                    time.sleep(0.5)
                    share_link = self.device.clipboard
                    if "http" in str(share_link):
                        image_urls = self._fetch_web_images(share_link)
                else:
                    logger.warning("æœªæ‰¾åˆ°å¤åˆ¶é“¾æ¥æŒ‰é’®")
                    self.device.press("back")
        except Exception as e:
            logger.warning(f"âš  å›¾ç‰‡æå–å¼‚å¸¸: {e}")

        self._time_method("_get_detail_data", start_time)
        return {
            "content": content,
            "image_urls": image_urls,
            "likes": likes,
            "collects": collects,
            "comments": comments,
            "comments": comments,
            "author_name": author_name,
            "publish_time": publish_time,
            "date_desc": date_desc
        }
    
    def _fetch_web_images(self, url: str) -> List[str]:
        """
        ä»åˆ†äº«é“¾æ¥ä¸­è§£æå›¾ç‰‡åœ°å€
        
        Args:
            url (str): åˆ†äº«é“¾æ¥URL
            
        Returns:
            List[str]: å›¾ç‰‡URLåˆ—è¡¨
        """
        start_time = time.time()
        try:
            headers = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1"}
            res = requests.get(url, headers=headers, timeout=10)
            html = res.text
            img_patterns = [
                r'property="og:image" content="(https://[^"]+)"',
                r'"url":"(https://sns-img-[^"]+)"',
                r'"url":"(https://sns-img-qc\.xhscdn\.com/[^"]+)"'
            ]
            found = []
            for pattern in img_patterns:
                matches = re.findall(pattern, html)
                for m in matches:
                    clean_url = m.replace('\\u002F', '/')
                    if clean_url not in found: found.append(clean_url)
            self._time_method("_fetch_web_images", start_time)
            return found
        except:
            self._time_method("_fetch_web_images", start_time)
            return []
    
    def save_note_data(self, data: Dict[str, Union[str, List[str]]], 
                      filename: str = "last_extracted_note.txt", 
                      note_url: str = "") -> None:
        """
        ä¿å­˜ç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            data (Dict[str, Union[str, List[str]]]): ç¬”è®°æ•°æ®
            filename (str): ä¿å­˜æ–‡ä»¶å
            note_url (str): ç¬”è®°URL
        """
        start_time = time.time()
        try:
            with open(filename, "w", encoding="utf-8") as f:
                f.write("=" * 50 + "\n")
                f.write("ã€å°çº¢ä¹¦ç¬”è®°æå–ç»“æœã€‘\n")
                f.write("=" * 50 + "\n")
                if note_url:
                    f.write(f"ç¬”è®°URL: {note_url}\n")
                    f.write("=" * 50 + "\n")
                f.write(f"ä½œè€…: {data.get('author_name', 'Unknown')}\n")
                f.write(f"ç‚¹èµæ•°: {data.get('likes', '0')}\n")
                f.write(f"æ”¶è—æ•°: {data.get('collects', '0')}\n")
                f.write(f"è¯„è®ºæ•°: {data.get('comments', '0')}\n")
                f.write(f"è¯„è®ºæ•°: {data.get('comments', '0')}\n")
                f.write(f"å›¾ç‰‡æ•°: {len(data.get('image_urls', []))}\n")
                f.write(f"å‘å¸ƒæ—¶é—´: {data.get('date_desc', '')} ({data.get('publish_time', 0)})\n")
                f.write("=" * 50 + "\n")
                f.write("ã€æ­£æ–‡å†…å®¹ã€‘\n")
                f.write(data['content'])
                f.write("\n" + "=" * 50 + "\n")
                if data['image_urls']:
                    f.write("ã€å›¾ç‰‡URLã€‘\n")
                    for i, url in enumerate(data['image_urls'], 1):
                        f.write(f"{i}. {url}\n")
                    f.write("=" * 50 + "\n")
            
            logger.info(f"âœ“ ç¬”è®°æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            self._time_method("save_note_data", start_time)
        except Exception as e:
            logger.error(f"âœ— ä¿å­˜ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            self._time_method("save_note_data", start_time)
            raise


def extract_note_from_url(url: str, device_serial: Optional[str] = None, enable_time_logging: bool = True) -> Optional[Dict[str, Union[str, List[str]]]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šç›´æ¥ä»URLæå–ç¬”è®°æ•°æ®ï¼Œæ”¯æŒè®¾å¤‡é‡è¯•æœºåˆ¶
    
    Args:
        url (str): å°çº¢ä¹¦ç¬”è®°URL
        device_serial (str, optional): è®¾å¤‡åºåˆ—å·
        enable_time_logging (bool, optional): æ˜¯å¦å¯ç”¨è€—æ—¶æ‰“å°ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns:
        Optional[Dict[str, Union[str, List[str]]]]: ç¬”è®°æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰æˆåŠŸåˆ™è¿”å›None
    """
    start_time = time.time()
    logger.info(f"[extract_note_from_url] å¼€å§‹å¤„ç†URL: {url}")
    try:
        extractor = XHSNoteExtractor(device_serial=device_serial, enable_time_logging=enable_time_logging)
        result = extractor.extract_note_data(url=url)
        elapsed_time = time.time() - start_time
        logger.info(f"[extract_note_from_url] æ€»è€—æ—¶: {elapsed_time:.3f}ç§’")
        return result
    except Exception as e:
        logger.error(f"[extract_note_from_url] æå–å¤±è´¥: {e}")
        elapsed_time = time.time() - start_time
        logger.info(f"[extract_note_from_url] æ€»è€—æ—¶: {elapsed_time:.3f}ç§’")
        return None


def convert_url_format(url: str) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè½¬æ¢URLæ ¼å¼
    
    Args:
        url (str): è¾“å…¥URL
        
    Returns:
        str: è½¬æ¢åçš„xhsdiscoveråè®®æ ¼å¼URL
    """
    start_time = time.time()
    logger.info(f"[convert_url_format] å¼€å§‹è½¬æ¢URL: {url}")
    parsed_data = XHSNoteExtractor.parse_xhs_url(url)
    result = XHSNoteExtractor.convert_to_xhsdiscover_format(
        parsed_data["note_id"], 
        parsed_data["xsec_token"]
    )
    elapsed_time = time.time() - start_time
    logger.info(f"[convert_url_format] è€—æ—¶: {elapsed_time:.3f}ç§’ï¼Œç»“æœ: {result}")
    return result