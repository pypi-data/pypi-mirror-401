"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ç”¨ã®å·®åˆ†åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Good ã¨ Bad ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¯”è¼ƒã—ã¦ï¼š
- Winning Templates (æˆåŠŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šGood ã«å¤šãã‚ã‚Šã€Bad ã«ãªã„)
- Failure Patterns (å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šBad ã«å¤šãã‚ã‚Šã€Good ã«ãªã„)
- Common Baseline (å…±é€šãƒ™ãƒ¼ã‚¹ï¼šä¸¡æ–¹ã«ã‚ã‚‹)
ã‚’ N-gram ã‚¨ãƒ³ã‚¸ãƒ³ã§æŠ½å‡º
"""

from typing import Dict, Set, List, Tuple
from pathlib import Path
import json
import logging

from japhrase import PhraseExtracter


logger = logging.getLogger(__name__)


class ComparisonAnalyzer:
    """
    Good/Bad ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¯”è¼ƒåˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    
    PhraseExtracter ã® N-gram ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ã„ã€
    é•·ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã™ã‚‹
    """
    
    def __init__(
        self,
        min_count: int = 2,
        min_length: int = 10,
        max_length: int = 100,
        use_pmi: bool = True
    ):
        """
        Args:
            min_count: ãƒ•ãƒ¬ãƒ¼ã‚ºã®æœ€å°å‡ºç¾æ•°
            min_length: ãƒ•ãƒ¬ãƒ¼ã‚ºã®æœ€å°æ–‡å­—æ•°ï¼ˆçŸ­ã„ã‚´ãƒŸã‚’é™¤å¤–ï¼‰
            max_length: ãƒ•ãƒ¬ãƒ¼ã‚ºã®æœ€å¤§æ–‡å­—æ•°ï¼ˆé•·ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨±å®¹ï¼‰
            use_pmi: PMI ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæŠ½å‡ºã«ã¯ True æ¨å¥¨ï¼‰
        """
        self.extractor = PhraseExtracter(
            min_count=min_count,
            min_length=min_length,
            max_length=max_length,
            use_pmi=use_pmi
        )
        self.min_count = min_count
        self.min_length = min_length
        self.max_length = max_length
        self.use_pmi = use_pmi
        
        logger.info(f"ComparisonAnalyzer initialized:")
        logger.info(f"  min_count={min_count}, min_length={min_length}, max_length={max_length}, use_pmi={use_pmi}")
    
    def compare_corpora(
        self,
        good_texts: List[str],
        bad_texts: List[str]
    ) -> Dict:
        """
        Good ã¨ Bad ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¯”è¼ƒã—ã¦å·®åˆ†ã‚’æŠ½å‡º
        
        Good ã‚³ãƒ¼ãƒ‘ã‚¹ã¨ Bad ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ã€ãã‚Œãã‚Œãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡ºã—ã¦
        ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã€å‹åˆ©ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã™ã‚‹
        
        ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¾ã¾ï¼ˆã‚«ãƒ³ãƒã§åˆ†å‰²ã—ãªã„ï¼‰N-gram ã‚¨ãƒ³ã‚¸ãƒ³ã«é£Ÿã‚ã›ã‚‹ã“ã¨ã§ã€
        é•·ã„ã€Œå‘ªæ–‡ã®å¡Šã€ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰ã‚’æŠ½å‡ºã§ãã‚‹
        
        Args:
            good_texts: Good ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒªã‚¹ãƒˆï¼‰
            bad_texts: Bad ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒªã‚¹ãƒˆï¼‰
        
        Returns:
            {
                "winning_templates": [...],  # Good ã§é«˜ã‚¹ã‚³ã‚¢
                "failure_patterns": [...],   # Bad ã§é«˜ã‚¹ã‚³ã‚¢
                "common_baseline": [...],    # ä¸¡æ–¹ã§å‡ºç¾
                "analysis": {...}
            }
        """
        logger.info(f"Extracting phrases from {len(good_texts)} Good and {len(bad_texts)} Bad prompts...")
        
        # å„ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
        # ã“ã“ã§ split(',') ã¯è¡Œã‚ãªã„ã€‚ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¾ã¾é£Ÿã‚ã›ã‚‹
        # ãã†ã™ã‚‹ã“ã¨ã§ã€PMI/ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒã€Œä¸¦ã³é †ã€ã‚’è¦‹ã¦ã€é•·ã„å¡Šã‚’èªè­˜ã™ã‚‹
        good_df = self.extractor.extract(good_texts)
        bad_df = self.extractor.extract(bad_texts)
        
        logger.info(f"Extracted {len(good_df)} phrases from Good corpus")
        logger.info(f"Extracted {len(bad_df)} phrases from Bad corpus")
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºã‚»ãƒƒãƒˆã‚’ä½œæˆ
        good_phrases = set(good_df['phrase'].values) if len(good_df) > 0 else set()
        bad_phrases = set(bad_df['phrase'].values) if len(bad_df) > 0 else set()
        
        # ã‚¹ã‚³ã‚¢è¾æ›¸ã‚’ä½œæˆï¼ˆãƒ•ãƒ¬ãƒ¼ã‚º -> ã‚¹ã‚³ã‚¢ï¼‰
        good_scores = dict(zip(good_df['phrase'].values, good_df['score'].values)) if len(good_df) > 0 else {}
        bad_scores = dict(zip(bad_df['phrase'].values, bad_df['score'].values)) if len(bad_df) > 0 else {}
        
        # é›†åˆæ¼”ç®—
        only_in_good = good_phrases - bad_phrases  # Good ã ã‘
        only_in_bad = bad_phrases - good_phrases   # Bad ã ã‘
        in_both = good_phrases & bad_phrases       # ä¸¡æ–¹
        
        logger.info(f"Only in Good: {len(only_in_good)}")
        logger.info(f"Only in Bad: {len(only_in_bad)}")
        logger.info(f"In both: {len(in_both)}")
        
        # å‹åˆ©ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡ºï¼ˆGood ã§å‡ºç¾ã—ã€Bad ã«ã¯ãªã„ï¼‰
        winning_templates = [
            (phrase, good_scores[phrase])
            for phrase in only_in_good
            if phrase in good_scores
        ]
        winning_templates.sort(key=lambda x: x[1], reverse=True)
        
        # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆBad ã§å‡ºç¾ã—ã€Good ã«ã¯ãªã„ï¼‰
        failure_patterns = [
            (phrase, bad_scores[phrase])
            for phrase in only_in_bad
            if phrase in bad_scores
        ]
        failure_patterns.sort(key=lambda x: x[1], reverse=True)
        
        # å…±é€šãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æŠ½å‡º
        common_baseline = [
            (phrase, good_scores.get(phrase, bad_scores.get(phrase, 0)))
            for phrase in in_both
        ]
        common_baseline.sort(key=lambda x: x[1], reverse=True)
        
        result = {
            "winning_templates": winning_templates,  # [(phrase, score), ...]
            "failure_patterns": failure_patterns,    # [(phrase, score), ...]
            "common_baseline": common_baseline,      # [(phrase, score), ...]
            "analysis": {
                "good_count": len(good_texts),
                "bad_count": len(bad_texts),
                "good_phrases": len(good_phrases),
                "bad_phrases": len(bad_phrases),
                "only_in_good": len(only_in_good),
                "only_in_bad": len(only_in_bad),
                "common_phrases": len(in_both),
                "min_count": self.min_count,
                "min_length": self.min_length,
                "max_length": self.max_length,
                "use_pmi": self.use_pmi
            }
        }
        
        return result
    
    def compare_from_files(
        self,
        good_file: Path,
        bad_file: Path
    ) -> Dict:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ Good/Bad ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§æ¯”è¼ƒ
        
        ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼š1è¡Œ = 1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Args:
            good_file: Good ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            bad_file: Bad ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
        Returns:
            æ¯”è¼ƒçµæœ
        """
        logger.info(f"Loading from files: {good_file}, {bad_file}")
        
        with open(good_file, "r", encoding="utf-8") as f:
            good_texts = [line.strip() for line in f if line.strip()]
        
        with open(bad_file, "r", encoding="utf-8") as f:
            bad_texts = [line.strip() for line in f if line.strip()]
        
        logger.info(f"Loaded {len(good_texts)} Good and {len(bad_texts)} Bad prompts")
        
        return self.compare_corpora(good_texts, bad_texts)
    
    def generate_report(self, comparison_result: Dict) -> str:
        """
        æ¯”è¼ƒçµæœã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            comparison_result: compare_corpora() ã®æˆ»ã‚Šå€¤
        
        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        report_lines = []
        report_lines.append("=" * 70)
        report_lines.append("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report_lines.append("=" * 70)
        report_lines.append("")
        
        # åˆ†æã‚µãƒãƒªãƒ¼
        analysis = comparison_result["analysis"]
        report_lines.append("ğŸ“Š åˆ†æã‚µãƒãƒªãƒ¼")
        report_lines.append("-" * 70)
        report_lines.append(f"Good ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {analysis['good_count']}")
        report_lines.append(f"Bad ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {analysis['bad_count']}")
        report_lines.append(f"Good ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°: {analysis['good_phrases']}")
        report_lines.append(f"Bad ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°: {analysis['bad_phrases']}")
        report_lines.append(f"Good ã ã‘ã«å‡ºç¾: {analysis['only_in_good']}")
        report_lines.append(f"Bad ã ã‘ã«å‡ºç¾: {analysis['only_in_bad']}")
        report_lines.append(f"ä¸¡æ–¹ã«å‡ºç¾: {analysis['common_phrases']}")
        report_lines.append("")
        
        # åˆ†æè¨­å®š
        report_lines.append("âš™ï¸  åˆ†æè¨­å®š")
        report_lines.append("-" * 70)
        report_lines.append(f"æœ€å°å‡ºç¾æ•°: {analysis['min_count']}")
        report_lines.append(f"æœ€å°æ–‡å­—æ•°: {analysis['min_length']}")
        report_lines.append(f"æœ€å¤§æ–‡å­—æ•°: {analysis['max_length']}")
        report_lines.append(f"PMI ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: {'Yes' if analysis['use_pmi'] else 'No'}")
        report_lines.append("")
        
        # å‹åˆ©ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        report_lines.append("ğŸ† å‹åˆ©ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (Good ã«å«ã¾ã‚Œã‚‹é•·ã„å¡Š)")
        report_lines.append("-" * 70)
        
        winning = comparison_result.get("winning_templates", [])
        if winning:
            for i, (phrase, score) in enumerate(winning[:15], 1):
                report_lines.append(f"  {i:2d}. [{score:6.2f}] {phrase}")
            if len(winning) > 15:
                report_lines.append(f"  ... and {len(winning) - 15} more templates")
        else:
            report_lines.append("  (è©²å½“ãªã—)")
        report_lines.append("")
        
        # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³
        report_lines.append("ğŸ’€ å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ (Bad ã«å«ã¾ã‚Œã‚‹é•·ã„å¡Š)")
        report_lines.append("-" * 70)
        
        failures = comparison_result.get("failure_patterns", [])
        if failures:
            for i, (phrase, score) in enumerate(failures[:15], 1):
                report_lines.append(f"  {i:2d}. [{score:6.2f}] {phrase}")
            if len(failures) > 15:
                report_lines.append(f"  ... and {len(failures) - 15} more patterns")
        else:
            report_lines.append("  (è©²å½“ãªã—)")
        report_lines.append("")
        
        # å…±é€šãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        report_lines.append("ğŸ”— å…±é€šãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (Good ã¨ Bad ã®ä¸¡æ–¹ã«å‡ºç¾)")
        report_lines.append("-" * 70)
        
        common = comparison_result.get("common_baseline", [])
        if common:
            for i, (phrase, score) in enumerate(common[:10], 1):
                report_lines.append(f"  {i:2d}. [{score:6.2f}] {phrase}")
            if len(common) > 10:
                report_lines.append(f"  ... and {len(common) - 10} more items")
        else:
            report_lines.append("  (è©²å½“ãªã—)")
        report_lines.append("")
        
        # æ¨å¥¨äº‹é …
        report_lines.append("ğŸ’¡ æ¨å¥¨äº‹é …")
        report_lines.append("-" * 70)
        
        if winning:
            top_winning = winning[0][0]
            report_lines.append(f"âœ… ã“ã®çµ„ã¿åˆã‚ã›ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:")
            report_lines.append(f"   '{top_winning}'")
        
        if failures:
            top_failure = failures[0][0]
            report_lines.append(f"âŒ ã“ã®çµ„ã¿åˆã‚ã›ã‚’é¿ã‘ã¦ãã ã•ã„:")
            report_lines.append(f"   '{top_failure}'")
        
        report_lines.append("")
        report_lines.append("=" * 70)
        
        return "\n".join(report_lines)
    
    def save_results(
        self,
        comparison_result: Dict,
        output_file: Path,
        include_report: bool = True
    ):
        """
        çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            comparison_result: æ¯”è¼ƒçµæœ
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            include_report: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚‚è¡Œã†ã‹
        """
        # JSON ä¿å­˜
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(comparison_result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to {output_file}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if include_report:
            report_file = output_file.with_stem(output_file.stem + "_report").with_suffix(".txt")
            report = self.generate_report(comparison_result)
            
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            logger.info(f"Report saved to {report_file}")
            
            return output_file, report_file
        
        return output_file, None


if __name__ == "__main__":
    import sys
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format="%(levelname)s: %(message)s"
    )
    
    # Windows ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
    if sys.platform == "win32":
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    # max_length ã‚’ 100 ã«è¨­å®šã—ã¦ã€é•·ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º
    analyzer = ComparisonAnalyzer(
        min_count=2,
        min_length=10,
        max_length=100,
        use_pmi=True
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    dataset_dir = Path(__file__).parent.parent / "data" / "toy_dataset"
    
    if dataset_dir.exists():
        good_file = dataset_dir / "good_positive.txt"
        bad_file = dataset_dir / "bad_positive.txt"
        
        if good_file.exists() and bad_file.exists():
            print("ğŸ” Comparing Good vs Bad prompts...\n")
            result = analyzer.compare_from_files(good_file, bad_file)
            
            print(analyzer.generate_report(result))
            
            # çµæœä¿å­˜
            output_file = dataset_dir / "comparison_results.json"
            json_file, report_file = analyzer.save_results(result, output_file)
            print(f"\nâœ… Results saved to {json_file}")
            if report_file:
                print(f"âœ… Report saved to {report_file}")
        else:
            print("âš ï¸  Dataset files not found. Run generate_comfy_toy_dataset.py first.")
    else:
        print("âš ï¸  Dataset directory not found. Run generate_comfy_toy_dataset.py first.")
