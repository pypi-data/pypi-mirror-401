"""
åŸ·ç­†è£œåŠ©ãƒ„ãƒ¼ãƒ« - ã‚¨ãƒ‡ã‚£ã‚¿é€£æºã¨ã‚»ãƒ«ãƒ•ãƒ»ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½

ã‚¨ãƒ‡ã‚£ã‚¿ç”¨ã®è¨­å®šè‡ªå‹•ç”Ÿæˆã‚„ã‚³ãƒ¼ãƒ‘ã‚¹å†…æ¤œç´¢ãªã©ã®ã€ã‚ˆã‚Šé«˜åº¦ãªåŸ·ç­†æ”¯æ´æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

__author__ = "Takeshi SHIMIZU"
__copyright__ = "Copyright 2023"

import json
import pandas as pd
from typing import List, Dict, Optional, Union
from pathlib import Path
import logging

from .extracter import PhraseExtracter
from .similarity import SimilarityAnalyzer

logger = logging.getLogger(__name__)


class EditorConfigGenerator:
    """
    ã‚¨ãƒ‡ã‚£ã‚¿ç”¨ã€Œè¡¨è¨˜ã‚†ã‚Œä¿®æ­£è¾æ›¸ã€ã®è‡ªå‹•ç”Ÿæˆ

    SimilarityAnalyzerã§æ¤œå‡ºã—ãŸé¡ä¼¼ãƒ•ãƒ¬ãƒ¼ã‚ºãƒšã‚¢ã‚’ã€
    VS Code ãªã©ã®ã‚¨ãƒ‡ã‚£ã‚¿è¨­å®šã«å¤‰æ›ã—ã¾ã™ã€‚

    ä½¿ç”¨ä¾‹:
        >>> extractor = PhraseExtracter()
        >>> phrases_df = extractor.get_dfphrase(texts)
        >>> generator = EditorConfigGenerator(phrases_df)
        >>> generator.export_vscode_config("settings.json")
    """

    def __init__(self, phrases_df: pd.DataFrame, similarity_threshold: float = 0.75):
        """
        Parameters:
            phrases_df: PhraseExtracterã®å‡ºåŠ›ï¼ˆseqcharã¨freqã‚’å«ã‚€DataFrameï¼‰
            similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆé«˜ã„ã»ã©å³å¯†ï¼‰
        """
        self.phrases_df = phrases_df
        self.similarity_threshold = similarity_threshold
        self.analyzer = SimilarityAnalyzer(method='levenshtein')

        self._detect_spelling_variations()

    def _detect_spelling_variations(self):
        """é¡ä¼¼ãƒ•ãƒ¬ãƒ¼ã‚ºãƒšã‚¢ã‚’æ¤œå‡º"""
        if len(self.phrases_df) == 0:
            self.variations = []
            return

        phrase_list = self.phrases_df['seqchar'].tolist()
        self.variations = []

        # å…¨ãƒšã‚¢ã«ã¤ã„ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—
        for i, phrase1 in enumerate(phrase_list):
            for j in range(i + 1, len(phrase_list)):
                phrase2 = phrase_list[j]

                similarity = self.analyzer.similarity_levenshtein(phrase1, phrase2)

                # é¡ä¼¼åº¦ãŒé«˜ãã€å®Œå…¨ä¸€è‡´ã§ãªã„ãƒšã‚¢ã‚’è¨˜éŒ²
                if (self.similarity_threshold <= similarity < 1.0 and
                    phrase1 != phrase2):
                    # å„ªå…ˆåº¦ï¼šé »åº¦ã®é«˜ã„æ–¹ã‚’æ¨™æº–å½¢ã¨ã™ã‚‹
                    freq1 = self.phrases_df[
                        self.phrases_df['seqchar'] == phrase1
                    ]['freq'].values[0]
                    freq2 = self.phrases_df[
                        self.phrases_df['seqchar'] == phrase2
                    ]['freq'].values[0]

                    if freq1 >= freq2:
                        standard = phrase1
                        variant = phrase2
                    else:
                        standard = phrase2
                        variant = phrase1

                    self.variations.append({
                        'standard': standard,
                        'variant': variant,
                        'similarity': float(similarity)
                    })

    def get_variations_df(self) -> pd.DataFrame:
        """
        æ¤œå‡ºã•ã‚ŒãŸè¡¨è¨˜ã‚†ã‚Œã‚’ DataFrame ã§å–å¾—

        Returns:
            pd.DataFrame: æ¨™æº–å½¢ã¨ç•°å½¢ã®å¯¾å¿œè¡¨
        """
        if not self.variations:
            return pd.DataFrame(columns=['standard', 'variant', 'similarity'])

        return pd.DataFrame(self.variations).drop_duplicates().sort_values(
            'similarity', ascending=False
        )

    def export_vscode_config(self, filepath: str, config_key: str = "cSpell.words"):
        """
        VS Code ã® settings.json ç”¨ã«è¨­å®šã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

        Parameters:
            filepath: å‡ºåŠ›å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            config_key: è¨­å®šã‚­ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cSpell.wordsï¼‰
        """
        if not self.variations:
            logger.warning("è¡¨è¨˜ã‚†ã‚ŒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return

        variations_df = self.get_variations_df()

        # VS Code è¨­å®šã¨ã—ã¦å‡ºåŠ›
        vscode_config = {
            config_key: list(variations_df['standard'].unique()),
            "cSpell.ignoreWords": list(variations_df['variant'].unique())
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(vscode_config, f, indent=2, ensure_ascii=False)

        logger.info(f"VS Codeè¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

    def export_replacement_rules(self, filepath: str):
        """
        ç½®æ›ãƒ«ãƒ¼ãƒ«ï¼ˆJSONå½¢å¼ï¼‰ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

        Parameters:
            filepath: å‡ºåŠ›å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if not self.variations:
            logger.warning("è¡¨è¨˜ã‚†ã‚ŒãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return

        variations_df = self.get_variations_df()

        # ç½®æ›ãƒ«ãƒ¼ãƒ«: ç•°å½¢ -> æ¨™æº–å½¢
        replacement_rules = {
            row['variant']: row['standard']
            for _, row in variations_df.iterrows()
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(replacement_rules, f, indent=2, ensure_ascii=False)

        logger.info(f"ç½®æ›ãƒ«ãƒ¼ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

    def generate_report(self) -> str:
        """
        è¡¨è¨˜ã‚†ã‚Œæ¤œå‡ºãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Returns:
            str: ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        variations_df = self.get_variations_df()

        report = []
        report.append("=" * 70)
        report.append("è¡¨è¨˜ã‚†ã‚Œä¿®æ­£è¾æ›¸ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 70)
        report.append("")

        if len(variations_df) == 0:
            report.append("è¡¨è¨˜ã‚†ã‚Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            report.append(f"æ¤œå‡ºã•ã‚ŒãŸè¡¨è¨˜ã‚†ã‚Œ: {len(variations_df)}ä»¶")
            report.append("")
            report.append(variations_df.to_string(index=False))

        report.append("")

        return "\n".join(report)


class SelfRecommender:
    """
    éå»åŸç¨¿ã‹ã‚‰ã®ã€Œã‚»ãƒ«ãƒ•ãƒ»ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€

    ç¾åœ¨ã®åŸç¨¿ã‹ã‚‰æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ã€éå»ã®åŸç¨¿ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢ã—ã€
    é‡è¤‡ã‚„å†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚

    ä½¿ç”¨ä¾‹:
        >>> recommender = SelfRecommender("past_articles_dir")
        >>> matches = recommender.find_related_articles(current_text)
    """

    def __init__(self, corpus_dir: Union[str, Path], min_count: int = 2):
        """
        Parameters:
            corpus_dir: éå»åŸç¨¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            min_count: ãƒ•ãƒ¬ãƒ¼ã‚ºã®æœ€å°å‡ºç¾å›æ•°
        """
        self.corpus_dir = Path(corpus_dir)
        self.extractor = PhraseExtracter(min_count=min_count, verbose=0)
        self.analyzer = SimilarityAnalyzer(method='jaccard')

        self._build_corpus_index()

    def _build_corpus_index(self):
        """éå»åŸç¨¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        self.corpus_texts = {}
        self.corpus_phrases = {}

        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­
        supported_ext = ['.txt', '.md']

        for filepath in self.corpus_dir.glob('**/*'):
            if filepath.is_file() and filepath.suffix in supported_ext:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        text = f.read()
                        self.corpus_texts[str(filepath)] = text

                        # ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
                        phrases = self.extractor.get_dfphrase([text])
                        self.corpus_phrases[str(filepath)] = phrases

                except Exception as e:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {filepath} - {e}")

        logger.info(f"ã‚³ãƒ¼ãƒ‘ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Œæˆ: {len(self.corpus_texts)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²")

    def find_related_articles(
        self,
        current_text: Union[str, List[str]],
        top_n: int = 5,
        similarity_threshold: float = 0.3
    ) -> pd.DataFrame:
        """
        ç¾åœ¨ã®åŸç¨¿ã¨é–¢é€£åº¦ãŒé«˜ã„éå»ã®è¨˜äº‹ã‚’æ¤œç´¢

        Parameters:
            current_text: ç¾åœ¨ã®åŸç¨¿ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒªã‚¹ãƒˆï¼‰
            top_n: è¿”ã™çµæœã®ä¸Šé™
            similarity_threshold: é¡ä¼¼åº¦ã®æœ€å°é–¾å€¤

        Returns:
            pd.DataFrame: ãƒãƒƒãƒã—ãŸè¨˜äº‹ã®è©³ç´°
        """
        # ç¾åœ¨ã®åŸç¨¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«çµ±ä¸€
        if isinstance(current_text, list):
            current_text_str = '\n'.join(current_text)
        else:
            current_text_str = current_text

        matches = []

        for filepath, corpus_text in self.corpus_texts.items():
            similarity = self.analyzer.calculate_similarity(
                current_text_str, corpus_text, method='jaccard'
            )

            if similarity >= similarity_threshold:
                matches.append({
                    'filepath': filepath,
                    'similarity': float(similarity),
                    'file_size': len(corpus_text)
                })

        if not matches:
            return pd.DataFrame(columns=['filepath', 'similarity', 'file_size'])

        result = pd.DataFrame(matches).sort_values('similarity', ascending=False)
        return result.head(top_n)

    def find_overlapping_phrases(
        self,
        current_text: Union[str, List[str]],
        top_n: int = 10
    ) -> Dict[str, pd.DataFrame]:
        """
        ç¾åœ¨ã®åŸç¨¿ã¨éå»ã®åŸç¨¿ã§å…±é€šã™ã‚‹ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ¤œå‡º

        Parameters:
            current_text: ç¾åœ¨ã®åŸç¨¿
            top_n: è¿”ã™ãƒ•ãƒ¬ãƒ¼ã‚ºã®ä¸Šé™

        Returns:
            Dict: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ -> å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºã®å¯¾å¿œè¡¨
        """
        # ç¾åœ¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
        if isinstance(current_text, list):
            text_list = current_text
        else:
            text_list = current_text.split('\n')

        current_phrases = self.extractor.get_dfphrase(text_list)

        if len(current_phrases) == 0:
            return {}

        current_phrase_set = set(current_phrases['seqchar'].values)

        overlaps = {}

        for filepath, corpus_phrases in self.corpus_phrases.items():
            if len(corpus_phrases) == 0:
                continue

            corpus_phrase_set = set(corpus_phrases['seqchar'].values)

            # å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ¤œå‡º
            common = current_phrase_set & corpus_phrase_set

            if common:
                # å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºã®è©³ç´°ã‚’å–å¾—
                common_details = []
                for phrase in common:
                    curr_freq = float(
                        current_phrases[current_phrases['seqchar'] == phrase]['freq'].values[0]
                    )
                    corp_freq = float(
                        corpus_phrases[corpus_phrases['seqchar'] == phrase]['freq'].values[0]
                    )

                    common_details.append({
                        'phrase': phrase,
                        'freq_current': curr_freq,
                        'freq_corpus': corp_freq,
                        'overlap_score': min(curr_freq, corp_freq)
                    })

                overlaps[filepath] = pd.DataFrame(common_details).sort_values(
                    'overlap_score', ascending=False
                ).head(top_n)

        return overlaps

    def generate_recommendation_report(
        self,
        current_text: Union[str, List[str]],
        top_n: int = 5
    ) -> str:
        """
        ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Parameters:
            current_text: ç¾åœ¨ã®åŸç¨¿
            top_n: ãƒ¬ãƒãƒ¼ãƒˆã«å«ã‚ã‚‹ä¸Šé™

        Returns:
            str: ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        report = []
        report.append("=" * 70)
        report.append("ã‚»ãƒ«ãƒ•ãƒ»ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 70)
        report.append("")

        # é–¢é€£è¨˜äº‹ã‚’æ¤œç´¢
        related = self.find_related_articles(current_text, top_n=top_n)

        if len(related) > 0:
            report.append(f"é–¢é€£è¨˜äº‹ï¼ˆä¸Šä½{min(top_n, len(related))}ä»¶ï¼‰:")
            report.append(related.to_string(index=False))
            report.append("")
        else:
            report.append("é–¢é€£è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            report.append("")

        # å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ¤œå‡º
        overlaps = self.find_overlapping_phrases(current_text, top_n=top_n)

        if overlaps:
            report.append(f"å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸Šä½{len(overlaps)}ä»¶ï¼‰:")
            report.append("")

            for filepath, phrases_df in overlaps.items():
                report.append(f"ğŸ“„ {Path(filepath).name}")
                report.append(f"  å…±é€šãƒ•ãƒ¬ãƒ¼ã‚º: {len(phrases_df)}å€‹")
                if len(phrases_df) > 0:
                    report.append(f"  {phrases_df.head(3).to_string(index=False)}")
                report.append("")
        else:
            report.append("å…±é€šãƒ•ãƒ¬ãƒ¼ã‚ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            report.append("")

        return "\n".join(report)
