import traceback
from typing import *
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from pathlib import Path
from loguru import logger
import asyncio
import time
import heapq
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from threading import Lock

# Rich ç›¸å…³å¯¼å…¥
from rich.console import Console
from rich.live import Live
from rich.layout import Layout
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.text import Text
from rich.columns import Columns
from rich.rule import Rule
from rich.console import Group

from xlin.file_util import ls, rm
from xlin.jsonlist_util import append_to_json_list, load_json, load_json_list, save_to_cache


@dataclass
class TaskReporter:
    """ä»»åŠ¡è¿›åº¦æŠ¥å‘Šå™¨"""
    worker_id: str
    _current_state: str = ""
    _progress: float = 0.0
    _lock: Lock = field(default_factory=Lock)
    _manager: Optional['TaskManager'] = None

    def set_current_state(self, state: str):
        """è®¾ç½®å½“å‰çŠ¶æ€"""
        with self._lock:
            self._current_state = state
            if self._manager:
                self._manager._update_worker_state(self.worker_id, state, self._progress)

    def set_progress(self, progress: float):
        """è®¾ç½®è¿›åº¦ (0.0-1.0)"""
        with self._lock:
            self._progress = max(0.0, min(1.0, progress))
            if self._manager:
                self._manager._update_worker_state(self.worker_id, self._current_state, self._progress)

    def get_state(self):
        """è·å–å½“å‰çŠ¶æ€"""
        with self._lock:
            return self._current_state, self._progress


@dataclass
class WorkerInfo:
    """å·¥ä½œçº¿ç¨‹ä¿¡æ¯"""
    worker_id: str
    current_task_id: str = ""
    current_state: str = "ç©ºé—²"
    progress: float = 0.0
    start_time: Optional[datetime] = None
    completed_tasks: int = 0
    current_batch_size: int = 0  # å½“å‰æ­£åœ¨å¤„ç†çš„æ‰¹æ¬¡å¤§å°
    is_batch_processing: bool = False  # æ˜¯å¦åœ¨è¿›è¡Œæ‰¹æ¬¡å¤„ç†


@dataclass
class TaskInfo:
    """ä»»åŠ¡ä¿¡æ¯"""
    task_id: str
    status: str = "ç­‰å¾…ä¸­"  # ç­‰å¾…ä¸­, å¤„ç†ä¸­, å·²å®Œæˆ, å¤±è´¥
    worker_id: str = ""
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_msg: str = ""


class TaskManager:
    """ä»»åŠ¡ç®¡ç†å™¨"""
    def __init__(self, max_workers: int, desc: str = "Processing", output_path: Optional[str] = None):
        self.max_workers = max_workers
        self.desc = desc
        self.output_path = output_path
        if self.output_path and not isinstance(self.output_path, Path):
            self.output_path = Path(output_path)
        self.workers: Dict[str, WorkerInfo] = {}
        self.tasks: Dict[str, TaskInfo] = {}
        self.console = Console()
        self.lock = Lock()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_tasks = 0
        self.completed_tasks = 0
        self.failed_tasks = 0
        self.cached_tasks = 0
        self.start_time = datetime.now()

        # ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
        self.cache_file_size = 0
        self.cache_bytes_written = 0

        # åˆå§‹åŒ–å·¥ä½œçº¿ç¨‹ä¿¡æ¯
        for i in range(max_workers):
            worker_id = f"Worker-{i+1:03d}"
            self.workers[worker_id] = WorkerInfo(worker_id=worker_id)

    def _update_worker_state(self, worker_id: str, state: str, progress: float):
        """æ›´æ–°å·¥ä½œçº¿ç¨‹çŠ¶æ€"""
        with self.lock:
            if worker_id in self.workers:
                worker = self.workers[worker_id]
                worker.current_state = state
                worker.progress = progress

    def assign_task(self, task_id: str, worker_id: str, batch_size: int = 1):
        """åˆ†é…ä»»åŠ¡ç»™å·¥ä½œçº¿ç¨‹"""
        with self.lock:
            if task_id not in self.tasks:
                self.tasks[task_id] = TaskInfo(task_id=task_id)

            task = self.tasks[task_id]
            worker = self.workers[worker_id]

            task.status = "å¤„ç†ä¸­"
            task.worker_id = worker_id
            task.start_time = datetime.now()

            worker.current_task_id = task_id
            worker.current_state = "å¼€å§‹å¤„ç†"
            worker.progress = 0.0
            worker.start_time = datetime.now()
            worker.current_batch_size = batch_size
            worker.is_batch_processing = batch_size > 1

    def complete_task(self, task_id: str, success: bool = True, error_msg: str = "", completed_items: int = 1):
        """å®Œæˆä»»åŠ¡"""
        with self.lock:
            if task_id in self.tasks:
                task = self.tasks[task_id]
                worker = self.workers.get(task.worker_id)

                task.status = "å·²å®Œæˆ" if success else "å¤±è´¥"
                task.end_time = datetime.now()
                task.error_msg = error_msg

                if worker:
                    worker.current_task_id = ""
                    worker.current_state = "ç©ºé—²"
                    worker.progress = 0.0
                    worker.completed_tasks += 1
                    worker.current_batch_size = 0
                    worker.is_batch_processing = False

                if success:
                    self.completed_tasks += completed_items  # æŒ‰å®é™…å¤„ç†çš„æ•°æ®é¡¹æ•°å¢åŠ 
                else:
                    self.failed_tasks += completed_items

    def update_cache_progress(self, current_file_size: int = None):
        """æ›´æ–°ç¼“å­˜è¿›åº¦ - æ ¹æ®ä»»åŠ¡å®Œæˆè¿›åº¦ä¼°ç®—æœ€ç»ˆæ–‡ä»¶å¤§å°"""
        with self.lock:
            # è·å–å½“å‰æ–‡ä»¶å¤§å°
            if current_file_size is not None:
                self.cache_bytes_written = current_file_size

            # è®¡ç®—ä»»åŠ¡å®Œæˆè¿›åº¦
            total_processed = self.completed_tasks
            if self.total_tasks > 0 and total_processed > 0:
                task_progress = total_processed / self.total_tasks

                # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ–‡ä»¶å¤§å°
                if task_progress >= 0.999:  # åŸºæœ¬å®Œæˆ
                    self.cache_file_size = max(self.cache_bytes_written, self.cache_file_size)
                elif task_progress > 0.05:  # æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œä¼°ç®—
                    estimated_final_size = self.cache_bytes_written / task_progress
                    # ç¡®ä¿ä¼°ç®—å€¼ä¸å°äºå½“å‰æ–‡ä»¶å¤§å°
                    estimated_final_size = max(estimated_final_size, self.cache_bytes_written)

                    # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥å¹³æ»‘ä¼°ç®—å€¼ï¼Œé¿å…å‰§çƒˆæ³¢åŠ¨
                    if self.cache_file_size == 0:
                        self.cache_file_size = estimated_final_size
                    else:
                        # å¦‚æœæ–°ä¼°ç®—å€¼æ›´å¤§ï¼Œç»™å®ƒæ›´é«˜çš„æƒé‡
                        if estimated_final_size > self.cache_file_size:
                            self.cache_file_size = self.cache_file_size * 0.6 + estimated_final_size * 0.4
                        else:
                            self.cache_file_size = self.cache_file_size * 0.8 + estimated_final_size * 0.2
                else:
                    # å¦‚æœè¿›åº¦å¤ªå°ï¼Œä½¿ç”¨ä¿å®ˆä¼°ç®—
                    if self.cache_file_size == 0:
                        self.cache_file_size = max(self.cache_bytes_written * 20, 1024)  # ä¼°ç®—æœ€ç»ˆå¤§å°ä¸ºå½“å‰çš„20å€

    def create_reporter(self, worker_id: str) -> TaskReporter:
        """åˆ›å»ºä»»åŠ¡æŠ¥å‘Šå™¨"""
        reporter = TaskReporter(worker_id=worker_id, _manager=self)
        return reporter

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            elapsed = datetime.now() - self.start_time

            # è®¡ç®—æ´»è·ƒå·¥ä½œçº¿ç¨‹æ•°
            active_workers = sum(1 for w in self.workers.values() if w.current_task_id != "")

            # è®¡ç®—å¤„ç†é€Ÿåº¦
            total_processed = self.completed_tasks + self.failed_tasks
            speed = total_processed / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0

            return {
                "total_tasks": self.total_tasks,
                "completed_tasks": self.completed_tasks,
                "failed_tasks": self.failed_tasks,
                "cached_tasks": self.cached_tasks,
                "pending_tasks": self.total_tasks - self.completed_tasks - self.failed_tasks,
                "active_workers": active_workers,
                "idle_workers": self.max_workers - active_workers,
                "elapsed_time": elapsed,
                "processing_speed": speed,
                "eta": timedelta(seconds=(self.total_tasks - total_processed) / speed) if speed > 0 else None
            }

    def create_display(self) -> Layout:
        """åˆ›å»ºæ˜¾ç¤ºå¸ƒå±€"""
        layout = Layout()
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="body", ratio=1),
            Layout(name="footer", size=5)  # å‡å°footeré«˜åº¦
        )

        return layout

    def update_display(self, layout: Layout):
        """æ›´æ–°æ˜¾ç¤ºå†…å®¹"""
        stats = self.get_stats()

        # å¤´éƒ¨ - æ€»ä½“è¿›åº¦
        total_progress = (stats["completed_tasks"] + stats["failed_tasks"]) / max(stats["total_tasks"], 1)
        header_text = Text()
        header_text.append(f"ğŸš€ {self.desc} ", style="bold blue")
        header_text.append(f"[{stats['completed_tasks']}/{stats['total_tasks']}] ", style="green")
        header_text.append(f"{total_progress:.1%}", style="yellow")

        if stats["eta"]:
            header_text.append(f" | ETA: {str(stats['eta']).split('.')[0]}", style="cyan")

        layout["header"].update(Panel(header_text, title="Task Manager", border_style="blue"))

        # ä¸»ä½“ - Workers æ¨ªå‘æ’åˆ— (ç±»ä¼¼ nvitop)
        worker_panels = []
        with self.lock:
            for worker_id, worker in self.workers.items():
                # åˆ›å»ºå›ºå®šæ ¼å¼çš„å•è¡Œworkerä¿¡æ¯
                worker_line = Text()

                # Worker ID (å›ºå®šå®½åº¦12å­—ç¬¦)
                worker_line.append(f"{worker_id:<12}", style="bold white")
                worker_line.append(" â”‚ ", style="dim white")

                # ä»»åŠ¡ä¿¡æ¯ (å›ºå®šå®½åº¦30å­—ç¬¦) + æ‰¹æ¬¡ä¿¡æ¯
                if worker.current_task_id:
                    task_display = worker.current_task_id[:25] + "..." if len(worker.current_task_id) > 25 else worker.current_task_id

                    # å¦‚æœæ˜¯æ‰¹æ¬¡å¤„ç†ï¼Œæ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
                    if worker.is_batch_processing:
                        batch_info = f"({worker.current_batch_size} items)"
                        # è°ƒæ•´task_displayé•¿åº¦ä»¥å®¹çº³æ‰¹æ¬¡ä¿¡æ¯
                        max_task_len = 28 - len(batch_info) - 1  # å‡å»æ‰¹æ¬¡ä¿¡æ¯å’Œç©ºæ ¼
                        if len(task_display) > max_task_len:
                            task_display = task_display[:max_task_len-3] + "..."
                        worker_line.append(f"ğŸ“‹ {task_display} {batch_info:<28}"[:32], style="cyan")
                    else:
                        worker_line.append(f"ğŸ“‹ {task_display:<28}", style="cyan")
                else:
                    worker_line.append(f"ğŸ’¤ {'Idle':<28}", style="dim white")

                worker_line.append(" â”‚ ", style="dim white")

                # çŠ¶æ€ä¿¡æ¯ (å›ºå®šå®½åº¦20å­—ç¬¦)
                state_display = worker.current_state[:18] if worker.current_state else "Waiting for tasks"
                if len(state_display) > 18:
                    state_display = state_display[:15] + "..."

                state_style = "yellow" if worker.current_task_id else "dim yellow"
                worker_line.append(f"ğŸ”„ {state_display:<18}", style=state_style)
                worker_line.append(" â”‚ ", style="dim white")

                # è¿›åº¦æ¡ (å›ºå®šå®½åº¦15å­—ç¬¦)
                progress_bar = "â–ˆ" * int(worker.progress * 10) + "â–‘" * (10 - int(worker.progress * 10))
                progress_text = f"[{progress_bar}] {worker.progress:>3.0%}"
                progress_style = "green" if worker.current_task_id else "dim green"
                worker_line.append(f"{progress_text:<15}", style=progress_style)
                worker_line.append(" â”‚ ", style="dim white")

                # è¿è¡Œæ—¶é—´ (å›ºå®šå®½åº¦10å­—ç¬¦)
                if worker.start_time and worker.current_task_id:
                    elapsed = datetime.now() - worker.start_time
                    time_str = str(elapsed).split('.')[0]
                    if len(time_str) > 8:
                        time_str = time_str[-8:]  # å–å8ä½
                else:
                    time_str = "--:--:--"

                time_style = "magenta" if worker.current_task_id else "dim magenta"
                worker_line.append(f"â±ï¸{time_str:>8}", style=time_style)
                worker_line.append(" â”‚ ", style="dim white")

                # å®Œæˆä»»åŠ¡æ•° (å›ºå®šå®½åº¦8å­—ç¬¦)
                worker_line.append(f"âœ… {worker.completed_tasks:>3}", style="bright_green")

                # æ ¹æ®workerçŠ¶æ€è®¾ç½®è¾¹æ¡†é¢œè‰²
                border_style = "green" if worker.current_task_id else "dim white"

                worker_panels.append(Panel(
                    worker_line,
                    title=f"Worker {worker_id.split('-')[1]}",
                    border_style=border_style,
                    height=3,  # å›ºå®šé«˜åº¦
                    padding=(0, 1)
                ))

        # å°†æ‰€æœ‰ worker é¢æ¿å‚ç›´æ’åˆ—
        layout["body"].update(Group(*worker_panels))

        # åº•éƒ¨ - ç»Ÿè®¡ä¿¡æ¯å’Œç¼“å­˜è¿›åº¦
        footer_content = Text()

        # ç¬¬ä¸€è¡Œï¼šå®æ—¶ç»Ÿè®¡ - ç´§å‡‘æ ¼å¼
        footer_content.append("ğŸ“Š ", style="bold blue")
        footer_content.append(f"Total: {stats['total_tasks']} | ", style="white")
        footer_content.append(f"âœ… {stats['completed_tasks']} | ", style="green")
        if stats['failed_tasks'] > 0:
            footer_content.append(f"âŒ {stats['failed_tasks']} | ", style="red")
        if stats['cached_tasks'] > 0:
            footer_content.append(f"ğŸ“ {stats['cached_tasks']} | ", style="yellow")

        # ç¬¬äºŒè¡Œï¼šå·¥ä½œçº¿ç¨‹çŠ¶æ€ + é€Ÿåº¦ + æ—¶é—´
        footer_content.append("ğŸ‘¥ ", style="bold blue")
        footer_content.append(f"Workers: {stats['active_workers']}ğŸŸ¢/{stats['idle_workers']}âšª ", style="white")
        footer_content.append("â”‚ ", style="dim white")
        footer_content.append(f"âš¡ {stats['processing_speed']:.2f} items/sec ", style="yellow")
        footer_content.append("â”‚ ", style="dim white")
        footer_content.append(f"â° {str(stats['elapsed_time']).split('.')[0]}", style="magenta")

        # ç¬¬ä¸‰è¡Œï¼šç¼“å­˜è¿›åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.cache_file_size > 0 and self.cache_bytes_written > 0:
            # è®¡ç®—ç¼“å­˜è¿›åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡100%
            cache_progress = min(self.cache_bytes_written / max(self.cache_file_size, self.cache_bytes_written), 1.0)
            cache_bar = "â–ˆ" * int(cache_progress * 20) + "â–‘" * (20 - int(cache_progress * 20))

            # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
            def format_size(bytes_size):
                for unit in ['B', 'KB', 'MB', 'GB']:
                    if bytes_size < 1024:
                        return f"{bytes_size:.1f}{unit}"
                    bytes_size /= 1024
                return f"{bytes_size:.1f}TB"

            footer_content.append(f"\nğŸ’¾ ", style="bold blue")

            # å¤„ç†æ–‡ä»¶è·¯å¾„æ˜¾ç¤ºï¼Œåªæ˜¾ç¤ºæ–‡ä»¶åæˆ–ç¼©çŸ­çš„è·¯å¾„
            display_path = self.output_path
            if display_path:
                # å¦‚æœè·¯å¾„å¤ªé•¿ï¼Œåªæ˜¾ç¤ºæ–‡ä»¶å
                if len(str(display_path)) > 50:  # å¦‚æœè·¯å¾„å¤ªé•¿
                    display_path = self.output_path.name
                else:
                    display_path = str(self.output_path)

            footer_content.append(f"{display_path} ", style="cyan")
            footer_content.append(f"[{cache_bar}] {cache_progress:.0%} ", style="green")
            footer_content.append(f"({format_size(self.cache_bytes_written)}/{format_size(self.cache_file_size)})", style="white")

            # æ·»åŠ ä¼°ç®—å‡†ç¡®æ€§æŒ‡ç¤ºå™¨
            total_processed = self.completed_tasks
            if total_processed > 0 and self.total_tasks > 0:
                task_progress = total_processed / self.total_tasks
                if task_progress < 0.1:
                    footer_content.append(" ğŸ“Š", style="dim yellow")  # ä¼°ç®—ä¸å¤Ÿå‡†ç¡®
                elif task_progress < 0.5:
                    footer_content.append(" ğŸ“ˆ", style="yellow")  # ä¼°ç®—ä¸­ç­‰å‡†ç¡®
                else:
                    footer_content.append(" âœ“", style="green")  # ä¼°ç®—è¾ƒå‡†ç¡®

        layout["footer"].update(Panel(footer_content, title="System Status", border_style="cyan"))


async def xmap_async(
    jsonlist: list[Any],
    work_func: Union[
      Callable[[Any, TaskReporter], dict],
      Callable[[list[Any], TaskReporter], list[dict]],
      Awaitable[Callable[[Any, TaskReporter], dict]],
      Awaitable[Callable[[list[Any], TaskReporter], list[dict]]],
    ],
    output_path: Optional[Union[str, Path]] = None,
    *,
    desc: str = "Processing",
    max_workers=8,  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    use_process_pool=True,  # CPUå¯†é›†å‹ä»»åŠ¡æ—¶è®¾ä¸ºTrue
    preserve_order=True,  # æ˜¯å¦ä¿æŒç»“æœé¡ºåº
    retry_count=0,  # å¤±è´¥é‡è¯•æ¬¡æ•°
    force_overwrite=False,  # æ˜¯å¦å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶
    is_batch_work_func=False,  # æ˜¯å¦æ‰¹é‡å¤„ç†å‡½æ•°
    batch_size=32,  # æ‰¹é‡å¤„ç†å¤§å°
    is_async_work_func=False,  # æ˜¯å¦å¼‚æ­¥å‡½æ•°
    verbose=False,  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    cache_id: str = "uuid",  # ç”¨äºå”¯ä¸€æ ‡è¯†å¤„ç†ç»“æœçš„é”®ï¼Œç”¨äºç¼“å­˜
):
    """é«˜æ€§èƒ½å¼‚æ­¥æ•°æ®å¤„ç†å‡½æ•°ï¼Œæ”¯æŒå¯è§†åŒ–è¿›åº¦ç›‘æ§å’Œæ‰¹æ¬¡å¤„ç†ã€‚

    xmap_async æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¼‚æ­¥æ•°æ®å¤„ç†æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
    - ğŸš€ é«˜å¹¶å‘å¼‚æ­¥å¤„ç†ï¼Œæ”¯æŒåŒæ­¥/å¼‚æ­¥å·¥ä½œå‡½æ•°
    - ğŸ“Š å®æ—¶å¯è§†åŒ–è¿›åº¦ç›‘æ§ï¼Œç±»ä¼¼ nvitop çš„ç•Œé¢é£æ ¼
    - ğŸ“¦ æ”¯æŒæ‰¹æ¬¡å¤„ç†å’Œå•é¡¹å¤„ç†ä¸¤ç§æ¨¡å¼
    - ğŸ’¾ è‡ªåŠ¨ç¼“å­˜å¤„ç†ç»“æœåˆ° JSONL æ–‡ä»¶
    - ğŸ”„ æ”¯æŒä»»åŠ¡é‡è¯•å’Œé”™è¯¯å¤„ç†
    - âš¡ è‡ªé€‚åº”å·¥ä½œçº¿ç¨‹ç®¡ç†å’Œè´Ÿè½½å‡è¡¡

    ## work_func æ ·ä¾‹

    ### å•é¡¹å¤„ç†å‡½æ•°ï¼š
    ```python
    def work_func(item: Any, reporter: TaskReporter) -> Any:
        # å¤„ç†å•ä¸ªæ•°æ®é¡¹
        reporter.set_current_state("å¤„ç†ä¸­...")
        reporter.set_progress(0.5)
        # å¤„ç†é€»è¾‘
        return processed_item
    ```

    ### æ‰¹æ¬¡å¤„ç†å‡½æ•°ï¼š
    ```python
    def batch_work_func(batch: List[Any], reporter: TaskReporter) -> List[Any]:
        # å¤„ç†ä¸€æ‰¹æ•°æ®é¡¹
        batch_size = len(batch)
        results = []
        for i, item in enumerate(batch):
            # å¤„ç†å•ä¸ªé¡¹ç›®
            results.append(processed_item)
            reporter.set_progress((i + 1) / batch_size)
        return results
    ```

    Args:
        jsonlist (List[Any]): è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨ï¼Œæ”¯æŒä»»æ„æ•°æ®ç±»å‹

        work_func (Callable): å·¥ä½œå‡½æ•°ï¼Œå¿…é¡»æ¥å—ä¸¤ä¸ªå‚æ•°ï¼š
            - item/batch: å•ä¸ªæ•°æ®é¡¹æˆ–æ•°æ®æ‰¹æ¬¡
            - reporter: TaskReporter å®ä¾‹ï¼Œç”¨äºä¸ŠæŠ¥è¿›åº¦
            æ”¯æŒå››ç§å‡½æ•°ç±»å‹ï¼š
            - åŒæ­¥å•é¡¹: (item, reporter) -> Any
            - åŒæ­¥æ‰¹æ¬¡: (List[item], reporter) -> List[Any]
            - å¼‚æ­¥å•é¡¹: async (item, reporter) -> Any
            - å¼‚æ­¥æ‰¹æ¬¡: async (List[item], reporter) -> List[Any]

        output_path (Optional[Union[str, Path]], optional):
            è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ JSONL æ ¼å¼ã€‚å¦‚æœä¸º None åˆ™ä¸ä¿å­˜æ–‡ä»¶ã€‚
            é»˜è®¤ä¸º Noneã€‚

        desc (str, optional):
            ä»»åŠ¡æè¿°ï¼Œæ˜¾ç¤ºåœ¨è¿›åº¦æ¡æ ‡é¢˜ä¸­ã€‚é»˜è®¤ä¸º "Processing"ã€‚

        max_workers (int, optional):
            æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ã€‚å¯¹äº I/O å¯†é›†å‹ä»»åŠ¡å»ºè®® 8-32ï¼Œ
            å¯¹äº CPU å¯†é›†å‹ä»»åŠ¡å»ºè®®è®¾ä¸º CPU æ ¸å¿ƒæ•°ã€‚é»˜è®¤ä¸º 8ã€‚

        use_process_pool (bool, optional):
            æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ã€‚True é€‚ç”¨äº CPU å¯†é›†å‹ä»»åŠ¡ï¼Œ
            False é€‚ç”¨äº I/O å¯†é›†å‹ä»»åŠ¡ã€‚é»˜è®¤ä¸º Trueã€‚

        preserve_order (bool, optional):
            æ˜¯å¦ä¿æŒè¾“å‡ºç»“æœçš„é¡ºåºä¸è¾“å…¥ä¸€è‡´ã€‚True ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ï¼Œ
            ä½†ä¿è¯é¡ºåºï¼›False æ€§èƒ½æ›´å¥½ä½†é¡ºåºå¯èƒ½å˜åŒ–ã€‚é»˜è®¤ä¸º Trueã€‚

        retry_count (int, optional):
            ä»»åŠ¡å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°ã€‚0 è¡¨ç¤ºä¸é‡è¯•ã€‚é»˜è®¤ä¸º 0ã€‚

        force_overwrite (bool, optional):
            æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ã€‚False ä¼šåœ¨æ–‡ä»¶å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚
            é»˜è®¤ä¸º Falseã€‚

        is_batch_work_func (bool, optional):
            å·¥ä½œå‡½æ•°æ˜¯å¦ä¸ºæ‰¹æ¬¡å¤„ç†å‡½æ•°ã€‚True æ—¶ä¼šå°†æ•°æ®æŒ‰ batch_size
            åˆ†ç»„åä¼ é€’ç»™å·¥ä½œå‡½æ•°ã€‚é»˜è®¤ä¸º Falseã€‚

        batch_size (int, optional):
            æ‰¹æ¬¡å¤„ç†æ—¶æ¯æ‰¹çš„æ•°æ®é‡ã€‚ä»…åœ¨ is_batch_work_func=True æ—¶ç”Ÿæ•ˆã€‚
            é»˜è®¤ä¸º 32ã€‚

        is_async_work_func (bool, optional):
            å·¥ä½œå‡½æ•°æ˜¯å¦ä¸ºå¼‚æ­¥å‡½æ•°ã€‚True æ—¶ä¼šä½¿ç”¨ await è°ƒç”¨å·¥ä½œå‡½æ•°ã€‚
            é»˜è®¤ä¸º Falseã€‚

        verbose (bool, optional):
            æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬é”™è¯¯å’Œé‡è¯•ä¿¡æ¯ã€‚é»˜è®¤ä¸º Falseã€‚

        cache_id (str, optional):
            ç”¨äºå”¯ä¸€æ ‡è¯†å¤„ç†ç»“æœçš„é”®ï¼Œç”¨äºç¼“å­˜ã€‚é»˜è®¤ä¸º "uuid"ã€‚

    Returns:
        List[Any]: å¤„ç†åçš„ç»“æœåˆ—è¡¨ã€‚ç»“æœé¡ºåºå–å†³äº preserve_order å‚æ•°ã€‚
                   å¯¹äºæ‰¹æ¬¡å¤„ç†ï¼Œè¿”å›çš„æ˜¯å±•å¼€åçš„å•é¡¹ç»“æœåˆ—è¡¨ã€‚

    Raises:
        FileExistsError: å½“ output_path æ–‡ä»¶å·²å­˜åœ¨ä¸” force_overwrite=False æ—¶
        Exception: å·¥ä½œå‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§å¼‚å¸¸

    ## ä½¿ç”¨ç¤ºä¾‹

    ### 1. åŒæ­¥å•ä¸ªå¤„ç†å‡½æ•°
    ```python
    def process_item(item, reporter: TaskReporter):
        # å¤„ç†å•ä¸ªé¡¹ç›®
        reporter.set_current_state("å¤„ç†ä¸­...")
        result = {"id": item["id"], "value": item["value"] * 2}
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(jsonlist, process_item)
    ```

    ### 2. åŒæ­¥æ‰¹é‡å¤„ç†å‡½æ•°
    ```python
    def process_batch(items, reporter: TaskReporter):
        # å¤„ç†æ‰¹é‡é¡¹ç›®
        reporter.set_current_state(f"æ‰¹é‡å¤„ç† {len(items)} é¡¹")
        results = []
        for i, item in enumerate(items):
            results.append({"id": item["id"], "value": item["value"] * 2})
            reporter.set_progress((i + 1) / len(items))
        return results

    results = await xmap_async(jsonlist, process_batch, is_batch_work_func=True)
    ```

    ### 3. å¼‚æ­¥å•ä¸ªå¤„ç†å‡½æ•°
    ```python
    async def async_process_item(item, reporter: TaskReporter):
        # å¼‚æ­¥å¤„ç†å•ä¸ªé¡¹ç›®
        reporter.set_current_state("å¼‚æ­¥å¤„ç†ä¸­...")
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        result = {"id": item["id"], "value": item["value"] * 2}
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(jsonlist, async_process_item, is_async_work_func=True)
    ```

    ### 4. å¼‚æ­¥æ‰¹é‡å¤„ç†å‡½æ•°
    ```python
    async def async_process_batch(items, reporter: TaskReporter):
        # å¼‚æ­¥å¤„ç†æ‰¹é‡é¡¹ç›®
        reporter.set_current_state(f"å¼‚æ­¥æ‰¹é‡å¤„ç† {len(items)} é¡¹")

        async def process_single_item(item):
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
            return {"id": item["id"], "value": item["value"] * 2}

        results = await asyncio.gather(*[process_single_item(item) for item in items])
        reporter.set_progress(1.0)
        return results

    results = await xmap_async(jsonlist, async_process_batch, is_async_work_func=True, is_batch_work_func=True)
    ```

    ### 5. ç®€å•å¼‚æ­¥å¤„ç†
    ```python
    import asyncio
    from rich_xmap import xmap_async, TaskReporter

    async def fetch_data(item, reporter: TaskReporter):
        reporter.set_current_state("æ­£åœ¨è·å–æ•°æ®...")
        # æ¨¡æ‹Ÿå¼‚æ­¥ç½‘ç»œè¯·æ±‚
        await asyncio.sleep(0.1)
        reporter.set_progress(1.0)
        return {"id": item["id"], "data": "fetched"}

    data = [{"id": f"item_{i}"} for i in range(100)]
    results = await xmap_async(
        data,
        fetch_data,
        desc="è·å–æ•°æ®",
        max_workers=10,
        is_async_work_func=True
    )
    ```

    ### 6. æ‰¹æ¬¡å¤„ç†ç¤ºä¾‹
    ```python
    def process_batch(batch, reporter: TaskReporter):
        reporter.set_current_state(f"å¤„ç†æ‰¹æ¬¡({len(batch)}é¡¹)")
        results = []
        for i, item in enumerate(batch):
            # æ‰¹é‡å¤„ç†é€»è¾‘
            results.append({"processed": item["value"] * 2})
            reporter.set_progress((i + 1) / len(batch))
        return results

    data = [{"value": i} for i in range(1000)]
    results = await xmap_async(
        data,
        process_batch,
        desc="æ‰¹æ¬¡å¤„ç†",
        is_batch_work_func=True,
        batch_size=50,
        output_path="results.jsonl"
    )
    ```

    ### 7. CPU å¯†é›†å‹ä»»åŠ¡
    ```python
    def cpu_intensive_task(item, reporter: TaskReporter):
        reporter.set_current_state("è®¡ç®—ä¸­...")
        # CPU å¯†é›†å‹è®¡ç®—
        result = complex_calculation(item)
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(
        data,
        cpu_intensive_task,
        desc="CPUè®¡ç®—",
        use_process_pool=True,  # ä½¿ç”¨è¿›ç¨‹æ± 
        max_workers=4  # CPU æ ¸å¿ƒæ•°
    )
    ```

    ## è¿›åº¦ç›‘æ§ç•Œé¢

    å‡½æ•°è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºç±»ä¼¼ nvitop çš„å®æ—¶ç›‘æ§ç•Œé¢ï¼š

    ```
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ ğŸš€ æ•°æ®å¤„ç† [150/200] 75.0% | ETA: 0:00:30              â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker 01 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ Worker-01 â”‚ ğŸ“‹ Task-0023... â”‚ ğŸ”„ å¤„ç†ä¸­... â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80% â”‚ â±ï¸ 0:00:15 â”‚ âœ… 45 â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker 02 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ Worker-02 â”‚ ğŸ’¤ Idle        â”‚ ğŸ”„ ç­‰å¾…ä»»åŠ¡... â”‚ [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0%  â”‚ â±ï¸ --:--:-- â”‚ âœ… 38 â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ System Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ ğŸ“Š Total: 200 | âœ… 150 | ğŸ“ 150 | ğŸ‘¥ Workers: 1ğŸŸ¢/7âšª â”‚ âš¡ 12.5/s â”‚ â° 0:02:15 â”‚
    â”‚ ğŸ’¾ results.jsonl [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 75% (1.2MB/1.6MB)                      â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    ```

    ## æ€§èƒ½å»ºè®®

    - **I/O å¯†é›†å‹ä»»åŠ¡**ï¼šè®¾ç½® use_process_pool=Falseï¼Œmax_workers=8-32
    - **CPU å¯†é›†å‹ä»»åŠ¡**ï¼šè®¾ç½® use_process_pool=Trueï¼Œmax_workers=CPUæ ¸å¿ƒæ•°
    - **å¤§æ•°æ®é‡**ï¼šä½¿ç”¨æ‰¹æ¬¡å¤„ç†ï¼Œbatch_size=32-128
    - **å®æ—¶æ€§è¦æ±‚é«˜**ï¼šè®¾ç½® preserve_order=False
    - **ç½‘ç»œè¯·æ±‚**ï¼šè®¾ç½®åˆé€‚çš„ retry_countï¼Œé€šå¸¸ 2-3 æ¬¡
    """
    jsonlist = list(jsonlist)
    # åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨
    task_manager = TaskManager(max_workers, desc, output_path=output_path)
    task_manager.total_tasks = len(jsonlist)

    need_caching = output_path is not None
    output_list: list[dict] = []
    start_idx = 0

    # å¤„ç†ç¼“å­˜
    if need_caching:
        if not preserve_order:
            # ä¸ä¿åºæ—¶ï¼Œç¼“å­˜ä¾èµ–äº cache_id æ¥è·Ÿè¸ªç¼“å­˜è¿›åº¦ï¼Œå¿…é¡»ä¿è¯æ¯ä¸ª item çš„ cache_id å”¯ä¸€
            assert cache_id is not None, "ç¼“å­˜æ—¶å¿…é¡»æä¾›å”¯ä¸€æ ‡è¯†ç¬¦æ¥è·Ÿè¸ªç¼“å­˜è¿›åº¦"
            assert all(item.get(cache_id) is not None for item in jsonlist), "æ‰€æœ‰é¡¹éƒ½å¿…é¡»åŒ…å«å”¯ä¸€æ ‡è¯†ç¬¦"
            assert len(set(item.get(cache_id) for item in jsonlist)) == len(jsonlist), "æ‰€æœ‰é¡¹çš„å”¯ä¸€æ ‡è¯†ç¬¦å¿…é¡»å”¯ä¸€ï¼Œé¿å…å†²çª"
        output_path = Path(output_path)
        if output_path.exists():
            if force_overwrite:
                if output_path.is_file():
                    if verbose:
                        logger.warning(f"å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶: {output_path}")
                    output_path.unlink()
                else:
                    if verbose:
                        logger.warning(f"å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•: {output_path}")
                    rm(output_path)
            else:
                if output_path.is_file():
                    output_list = load_json_list(output_path)
                    start_idx = len(output_list)
                    if not preserve_order:
                        # å¦‚æœä¸éœ€è¦ä¿åºè¾“å‡ºï¼Œåˆ™æŒ‰ output_list å°†å·²ç»å¤„ç†çš„é¡¹ä» jsonlist ä¸­ç§»åŠ¨åˆ°å‰é¢ï¼Œç¡®ä¿ start_idx ä¹‹åçš„é¡¹ä¸ºæœªå¤„ç†é¡¹
                        processed_ids = {item.get(cache_id) for item in output_list}
                        jsonlist_with_new_order = []
                        for item in jsonlist:
                            item_id = item.get(cache_id)
                            if item_id in processed_ids:
                                # å·²å¤„ç†çš„é¡¹æ”¾åˆ°å‰é¢
                                jsonlist_with_new_order.insert(0, item)
                            else:
                                # æœªå¤„ç†çš„é¡¹æ”¾åˆ°åé¢
                                jsonlist_with_new_order.append(item)
                        jsonlist = jsonlist_with_new_order
                else:
                    files = ls(output_path, filter=lambda f: f.name.endswith(".json"))
                    id2path = {f.name[:-5]: f for f in files}
                    jsonlist_with_new_order = []
                    for item in jsonlist:
                        item_id = item.get(cache_id)
                        if not preserve_order:
                            if item_id in processed_ids:
                                # å·²å¤„ç†çš„é¡¹æ”¾åˆ°å‰é¢
                                jsonlist_with_new_order.insert(0, item)
                            else:
                                # æœªå¤„ç†çš„é¡¹æ”¾åˆ°åé¢
                                jsonlist_with_new_order.append(item)
                        if item_id in id2path:
                            item_cache_path = id2path[item_id]
                            output_list.append(load_json(item_cache_path))
                            start_idx += 1
                        else:
                            if preserve_order:
                                # å¦‚æœéœ€è¦ä¿åºè¾“å‡ºï¼Œä½†ç¼“å­˜ä¸­æ²¡æœ‰è¯¥é¡¹ï¼Œåˆ™è·³è¿‡
                                break
                            # å¦‚æœä¸éœ€è¦ä¿åºè¾“å‡ºï¼Œåˆ™å¯ä»¥ç»§ç»­å¤„ç†
                            output_list.append(item)
                            start_idx += 1
                    if not preserve_order:
                        jsonlist = jsonlist_with_new_order

                if start_idx >= len(jsonlist):
                    return output_list
                if verbose:
                    logger.info(f"ç»§ç»­å¤„ç†: å·²æœ‰{start_idx}æ¡è®°å½•ï¼Œå…±{len(jsonlist)}æ¡")
        else:
            if output_path.name.endswith(".json") or output_path.name.endswith(".jsonl"):
                output_path.parent.mkdir(parents=True, exist_ok=True)
            else:
                output_path.mkdir(parents=True, exist_ok=True)

    # å‡†å¤‡è¦å¤„ç†çš„æ•°æ®
    remaining = jsonlist[start_idx:]
    if is_batch_work_func:
        remaining = [remaining[i:i + batch_size] for i in range(0, len(remaining), batch_size)]

    if not is_async_work_func:
        loop = asyncio.get_event_loop()
        if use_process_pool:
            executor = ProcessPoolExecutor(max_workers=max_workers)
        else:
            executor = ThreadPoolExecutor(max_workers=max_workers)

    async def working_at_task(index: int, item: Union[Any, list[Any]], worker_id: str):
        # åˆ›å»ºä»»åŠ¡æŠ¥å‘Šå™¨
        reporter = task_manager.create_reporter(worker_id)
        task_id = f"Task-{index:04d}"

        # ç¡®å®šæ‰¹æ¬¡å¤§å°
        batch_size_actual = len(item) if is_batch_work_func and isinstance(item, list) else 1

        # åˆ†é…ä»»åŠ¡ - ä¼ é€’æ‰¹æ¬¡å¤§å°
        task_manager.assign_task(task_id, worker_id, batch_size_actual)

        try:
            if is_async_work_func:
                result = await work_func(item, reporter)
            else:
                result = await loop.run_in_executor(executor, work_func, item, reporter)

            # å®Œæˆä»»åŠ¡ - ä¼ é€’å®é™…å¤„ç†çš„é¡¹ç›®æ•°
            task_manager.complete_task(task_id, success=True, completed_items=batch_size_actual)
            return index, result
        except Exception as e:
            # å¤±è´¥ä»»åŠ¡ - ä¼ é€’å®é™…å¤„ç†çš„é¡¹ç›®æ•°
            task_manager.complete_task(task_id, success=False, error_msg=str(e), completed_items=batch_size_actual)
            raise

    # å¼‚æ­¥è°ƒåº¦
    results: list[dict] = []
    pq = []

    sem = asyncio.Semaphore(max_workers)
    result_queue = asyncio.Queue()

    # åˆ›å»ºå¯è§†åŒ–ç•Œé¢
    layout = task_manager.create_display()

    async def task_fn(index: int, item: Any | list[Any]):
        worker_id = f"Worker-{(index % max_workers) + 1:03d}"

        # å®ç°é‡è¯•é€»è¾‘
        for retry_step_idx in range(retry_count + 1):
            async with sem:
                try:
                    result = await working_at_task(index, item, worker_id)
                    await result_queue.put(result)
                    break
                except Exception as e:
                    if retry_step_idx < retry_count:
                        if verbose:
                            logger.error(f"å¤„ç†å¤±è´¥ï¼Œç´¢å¼• {index} é‡è¯•ä¸­ ({retry_step_idx + 1}/{retry_count}): {e}")
                    else:
                        if verbose:
                            logger.error(f"æœ€ç»ˆå¤±è´¥ï¼Œæ— æ³•å¤„ç†ç´¢å¼• {index} çš„é¡¹ç›®: {e}\n{traceback.format_exc()}")
                        fallback_result = {"index": index, "error": f"{e}\n{traceback.format_exc()}"}
                        if is_batch_work_func:
                            fallback_result = [{"index": idx, "error": f"{e}\n{traceback.format_exc()}"} for idx in range(index, index + batch_size)]
                        # å°†é”™è¯¯ç»“æœæ”¾å…¥é˜Ÿåˆ—
                        await result_queue.put((index, fallback_result))
                        break

    async def producer():
        tasks = []
        for i, item in enumerate(remaining):
            index = i + start_idx
            task = asyncio.create_task(task_fn(index, item))
            tasks.append(task)
        await asyncio.gather(*tasks, return_exceptions=True)

    async def consumer():
        next_expect = start_idx
        nonlocal results

        while len(results) < len(jsonlist) - start_idx:
            try:
                idx, res = await asyncio.wait_for(result_queue.get(), timeout=1.0)
            except asyncio.TimeoutError:
                await asyncio.sleep(0.1)  # ç­‰å¾…ç»“æœ
                continue  # ç»§ç»­ç­‰å¾…ç»“æœ
            if preserve_order:
                heapq.heappush(pq, (idx, res))
                # ä¿åºè¾“å‡º
                output_buffer = []
                while pq and pq[0][0] == next_expect:
                    _, r = heapq.heappop(pq)
                    if is_batch_work_func:
                        output_buffer.extend(r)
                    else:
                        output_buffer.append(r)
                    next_expect += 1  # æ‰¹æ¬¡æ¨¡å¼ä¸‹ä¹Ÿæ˜¯æŒ‰ç´¢å¼•é€’å¢

                results.extend(output_buffer)
                if need_caching and output_buffer:
                    save_to_cache(output_buffer, output_path, cache_id, verbose)
                    task_manager.cached_tasks += len(output_buffer)
                    # æ›´æ–°ç¼“å­˜è¿›åº¦
                    if output_path and Path(output_path).exists():
                        file_size = Path(output_path).stat().st_size
                        task_manager.update_cache_progress(file_size)
            else:
                # éä¿åºè¾“å‡º
                if is_batch_work_func:
                    results.extend(res)
                    if need_caching:
                        save_to_cache(res, output_path, cache_id, verbose)
                        task_manager.cached_tasks += len(res)
                        # æ›´æ–°ç¼“å­˜è¿›åº¦
                        if output_path and Path(output_path).exists():
                            file_size = Path(output_path).stat().st_size
                            task_manager.update_cache_progress(file_size)
                else:
                    results.append(res)
                    if need_caching:
                        save_to_cache([res], output_path, cache_id, verbose)
                        task_manager.cached_tasks += 1
                        # æ›´æ–°ç¼“å­˜è¿›åº¦
                        if output_path and Path(output_path).exists():
                            file_size = Path(output_path).stat().st_size
                            task_manager.update_cache_progress(file_size)


    # å¯åŠ¨ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
    with Live(layout, console=task_manager.console, refresh_per_second=4) as live:
        async def update_display():
            # ä¿®æ­£æ›´æ–°æ¡ä»¶ï¼šä½¿ç”¨completed_tasks + failed_tasksæ¥åˆ¤æ–­æ˜¯å¦å®Œæˆ
            while True:
                stats = task_manager.get_stats()
                if stats["completed_tasks"] + stats["failed_tasks"] >= len(jsonlist):
                    break

                task_manager.update_display(layout)
                live.update(layout)
                await asyncio.sleep(0.25)

        # å¹¶è¡Œè¿è¡Œç”Ÿäº§è€…ã€æ¶ˆè´¹è€…å’Œæ˜¾ç¤ºæ›´æ–°
        await asyncio.gather(
            producer(),
            consumer(),
            update_display(),
        )

        # æœ€åæ›´æ–°ä¸€æ¬¡æ˜¾ç¤º
        task_manager.update_display(layout)
        live.update(layout)

    if not is_async_work_func:
        executor.shutdown(wait=True)

    return jsonlist[:start_idx] + results



def xmap(
    jsonlist: list[Any],
    work_func: Union[
      Callable[[Any, TaskReporter], dict],
      Callable[[list[Any], TaskReporter], list[dict]],
      Awaitable[Callable[[Any, TaskReporter], dict]],
      Awaitable[Callable[[list[Any], TaskReporter], list[dict]]],
    ],
    output_path: Optional[Union[str, Path]] = None,
    *,
    desc: str = "Processing",
    max_workers=8,  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    use_process_pool=True,  # CPUå¯†é›†å‹ä»»åŠ¡æ—¶è®¾ä¸ºTrue
    preserve_order=True,  # æ˜¯å¦ä¿æŒç»“æœé¡ºåº
    retry_count=0,  # å¤±è´¥é‡è¯•æ¬¡æ•°
    force_overwrite=False,  # æ˜¯å¦å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶
    is_batch_work_func=False,  # æ˜¯å¦æ‰¹é‡å¤„ç†å‡½æ•°
    batch_size=32,  # æ‰¹é‡å¤„ç†å¤§å°
    is_async_work_func=False,  # æ˜¯å¦å¼‚æ­¥å‡½æ•°
    verbose=False,  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    cache_id: str = "uuid",  # ç”¨äºå”¯ä¸€æ ‡è¯†å¤„ç†ç»“æœçš„é”®ï¼Œç”¨äºç¼“å­˜
):
    """é«˜æ€§èƒ½å¼‚æ­¥æ•°æ®å¤„ç†å‡½æ•°ï¼Œæ”¯æŒå¯è§†åŒ–è¿›åº¦ç›‘æ§å’Œæ‰¹æ¬¡å¤„ç†ã€‚

    xmap_async æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¼‚æ­¥æ•°æ®å¤„ç†æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
    - ğŸš€ é«˜å¹¶å‘å¼‚æ­¥å¤„ç†ï¼Œæ”¯æŒåŒæ­¥/å¼‚æ­¥å·¥ä½œå‡½æ•°
    - ğŸ“Š å®æ—¶å¯è§†åŒ–è¿›åº¦ç›‘æ§ï¼Œç±»ä¼¼ nvitop çš„ç•Œé¢é£æ ¼
    - ğŸ“¦ æ”¯æŒæ‰¹æ¬¡å¤„ç†å’Œå•é¡¹å¤„ç†ä¸¤ç§æ¨¡å¼
    - ğŸ’¾ è‡ªåŠ¨ç¼“å­˜å¤„ç†ç»“æœåˆ° JSONL æ–‡ä»¶
    - ğŸ”„ æ”¯æŒä»»åŠ¡é‡è¯•å’Œé”™è¯¯å¤„ç†
    - âš¡ è‡ªé€‚åº”å·¥ä½œçº¿ç¨‹ç®¡ç†å’Œè´Ÿè½½å‡è¡¡

    ## work_func æ ·ä¾‹

    ### å•é¡¹å¤„ç†å‡½æ•°ï¼š
    ```python
    def work_func(item: Any, reporter: TaskReporter) -> Any:
        # å¤„ç†å•ä¸ªæ•°æ®é¡¹
        reporter.set_current_state("å¤„ç†ä¸­...")
        reporter.set_progress(0.5)
        # å¤„ç†é€»è¾‘
        return processed_item
    ```

    ### æ‰¹æ¬¡å¤„ç†å‡½æ•°ï¼š
    ```python
    def batch_work_func(batch: List[Any], reporter: TaskReporter) -> List[Any]:
        # å¤„ç†ä¸€æ‰¹æ•°æ®é¡¹
        batch_size = len(batch)
        results = []
        for i, item in enumerate(batch):
            # å¤„ç†å•ä¸ªé¡¹ç›®
            results.append(processed_item)
            reporter.set_progress((i + 1) / batch_size)
        return results
    ```

    Args:
        jsonlist (List[Any]): è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨ï¼Œæ”¯æŒä»»æ„æ•°æ®ç±»å‹

        work_func (Callable): å·¥ä½œå‡½æ•°ï¼Œå¿…é¡»æ¥å—ä¸¤ä¸ªå‚æ•°ï¼š
            - item/batch: å•ä¸ªæ•°æ®é¡¹æˆ–æ•°æ®æ‰¹æ¬¡
            - reporter: TaskReporter å®ä¾‹ï¼Œç”¨äºä¸ŠæŠ¥è¿›åº¦
            æ”¯æŒå››ç§å‡½æ•°ç±»å‹ï¼š
            - åŒæ­¥å•é¡¹: (item, reporter) -> Any
            - åŒæ­¥æ‰¹æ¬¡: (List[item], reporter) -> List[Any]
            - å¼‚æ­¥å•é¡¹: async (item, reporter) -> Any
            - å¼‚æ­¥æ‰¹æ¬¡: async (List[item], reporter) -> List[Any]

        output_path (Optional[Union[str, Path]], optional):
            è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ JSONL æ ¼å¼ã€‚å¦‚æœä¸º None åˆ™ä¸ä¿å­˜æ–‡ä»¶ã€‚
            é»˜è®¤ä¸º Noneã€‚

        desc (str, optional):
            ä»»åŠ¡æè¿°ï¼Œæ˜¾ç¤ºåœ¨è¿›åº¦æ¡æ ‡é¢˜ä¸­ã€‚é»˜è®¤ä¸º "Processing"ã€‚

        max_workers (int, optional):
            æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ã€‚å¯¹äº I/O å¯†é›†å‹ä»»åŠ¡å»ºè®® 8-32ï¼Œ
            å¯¹äº CPU å¯†é›†å‹ä»»åŠ¡å»ºè®®è®¾ä¸º CPU æ ¸å¿ƒæ•°ã€‚é»˜è®¤ä¸º 8ã€‚

        use_process_pool (bool, optional):
            æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ã€‚True é€‚ç”¨äº CPU å¯†é›†å‹ä»»åŠ¡ï¼Œ
            False é€‚ç”¨äº I/O å¯†é›†å‹ä»»åŠ¡ã€‚é»˜è®¤ä¸º Trueã€‚

        preserve_order (bool, optional):
            æ˜¯å¦ä¿æŒè¾“å‡ºç»“æœçš„é¡ºåºä¸è¾“å…¥ä¸€è‡´ã€‚True ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ï¼Œ
            ä½†ä¿è¯é¡ºåºï¼›False æ€§èƒ½æ›´å¥½ä½†é¡ºåºå¯èƒ½å˜åŒ–ã€‚é»˜è®¤ä¸º Trueã€‚

        retry_count (int, optional):
            ä»»åŠ¡å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°ã€‚0 è¡¨ç¤ºä¸é‡è¯•ã€‚é»˜è®¤ä¸º 0ã€‚

        force_overwrite (bool, optional):
            æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ã€‚False ä¼šåœ¨æ–‡ä»¶å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚
            é»˜è®¤ä¸º Falseã€‚

        is_batch_work_func (bool, optional):
            å·¥ä½œå‡½æ•°æ˜¯å¦ä¸ºæ‰¹æ¬¡å¤„ç†å‡½æ•°ã€‚True æ—¶ä¼šå°†æ•°æ®æŒ‰ batch_size
            åˆ†ç»„åä¼ é€’ç»™å·¥ä½œå‡½æ•°ã€‚é»˜è®¤ä¸º Falseã€‚

        batch_size (int, optional):
            æ‰¹æ¬¡å¤„ç†æ—¶æ¯æ‰¹çš„æ•°æ®é‡ã€‚ä»…åœ¨ is_batch_work_func=True æ—¶ç”Ÿæ•ˆã€‚
            é»˜è®¤ä¸º 32ã€‚

        is_async_work_func (bool, optional):
            å·¥ä½œå‡½æ•°æ˜¯å¦ä¸ºå¼‚æ­¥å‡½æ•°ã€‚True æ—¶ä¼šä½¿ç”¨ await è°ƒç”¨å·¥ä½œå‡½æ•°ã€‚
            é»˜è®¤ä¸º Falseã€‚

        verbose (bool, optional):
            æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬é”™è¯¯å’Œé‡è¯•ä¿¡æ¯ã€‚é»˜è®¤ä¸º Falseã€‚

        cache_id (str, optional):
            ç”¨äºå”¯ä¸€æ ‡è¯†å¤„ç†ç»“æœçš„é”®ï¼Œç”¨äºç¼“å­˜ã€‚é»˜è®¤ä¸º "uuid"ã€‚

    Returns:
        List[Any]: å¤„ç†åçš„ç»“æœåˆ—è¡¨ã€‚ç»“æœé¡ºåºå–å†³äº preserve_order å‚æ•°ã€‚
                   å¯¹äºæ‰¹æ¬¡å¤„ç†ï¼Œè¿”å›çš„æ˜¯å±•å¼€åçš„å•é¡¹ç»“æœåˆ—è¡¨ã€‚

    Raises:
        FileExistsError: å½“ output_path æ–‡ä»¶å·²å­˜åœ¨ä¸” force_overwrite=False æ—¶
        Exception: å·¥ä½œå‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§å¼‚å¸¸

    ## ä½¿ç”¨ç¤ºä¾‹

    ### 1. åŒæ­¥å•ä¸ªå¤„ç†å‡½æ•°
    ```python
    def process_item(item, reporter: TaskReporter):
        # å¤„ç†å•ä¸ªé¡¹ç›®
        reporter.set_current_state("å¤„ç†ä¸­...")
        result = {"id": item["id"], "value": item["value"] * 2}
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(jsonlist, process_item)
    ```

    ### 2. åŒæ­¥æ‰¹é‡å¤„ç†å‡½æ•°
    ```python
    def process_batch(items, reporter: TaskReporter):
        # å¤„ç†æ‰¹é‡é¡¹ç›®
        reporter.set_current_state(f"æ‰¹é‡å¤„ç† {len(items)} é¡¹")
        results = []
        for i, item in enumerate(items):
            results.append({"id": item["id"], "value": item["value"] * 2})
            reporter.set_progress((i + 1) / len(items))
        return results

    results = await xmap_async(jsonlist, process_batch, is_batch_work_func=True)
    ```

    ### 3. å¼‚æ­¥å•ä¸ªå¤„ç†å‡½æ•°
    ```python
    async def async_process_item(item, reporter: TaskReporter):
        # å¼‚æ­¥å¤„ç†å•ä¸ªé¡¹ç›®
        reporter.set_current_state("å¼‚æ­¥å¤„ç†ä¸­...")
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        result = {"id": item["id"], "value": item["value"] * 2}
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(jsonlist, async_process_item, is_async_work_func=True)
    ```

    ### 4. å¼‚æ­¥æ‰¹é‡å¤„ç†å‡½æ•°
    ```python
    async def async_process_batch(items, reporter: TaskReporter):
        # å¼‚æ­¥å¤„ç†æ‰¹é‡é¡¹ç›®
        reporter.set_current_state(f"å¼‚æ­¥æ‰¹é‡å¤„ç† {len(items)} é¡¹")

        async def process_single_item(item):
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
            return {"id": item["id"], "value": item["value"] * 2}

        results = await asyncio.gather(*[process_single_item(item) for item in items])
        reporter.set_progress(1.0)
        return results

    results = await xmap_async(jsonlist, async_process_batch, is_async_work_func=True, is_batch_work_func=True)
    ```

    ### 5. ç®€å•å¼‚æ­¥å¤„ç†
    ```python
    import asyncio
    from rich_xmap import xmap_async, TaskReporter

    async def fetch_data(item, reporter: TaskReporter):
        reporter.set_current_state("æ­£åœ¨è·å–æ•°æ®...")
        # æ¨¡æ‹Ÿå¼‚æ­¥ç½‘ç»œè¯·æ±‚
        await asyncio.sleep(0.1)
        reporter.set_progress(1.0)
        return {"id": item["id"], "data": "fetched"}

    data = [{"id": f"item_{i}"} for i in range(100)]
    results = await xmap_async(
        data,
        fetch_data,
        desc="è·å–æ•°æ®",
        max_workers=10,
        is_async_work_func=True
    )
    ```

    ### 6. æ‰¹æ¬¡å¤„ç†ç¤ºä¾‹
    ```python
    def process_batch(batch, reporter: TaskReporter):
        reporter.set_current_state(f"å¤„ç†æ‰¹æ¬¡({len(batch)}é¡¹)")
        results = []
        for i, item in enumerate(batch):
            # æ‰¹é‡å¤„ç†é€»è¾‘
            results.append({"processed": item["value"] * 2})
            reporter.set_progress((i + 1) / len(batch))
        return results

    data = [{"value": i} for i in range(1000)]
    results = await xmap_async(
        data,
        process_batch,
        desc="æ‰¹æ¬¡å¤„ç†",
        is_batch_work_func=True,
        batch_size=50,
        output_path="results.jsonl"
    )
    ```

    ### 7. CPU å¯†é›†å‹ä»»åŠ¡
    ```python
    def cpu_intensive_task(item, reporter: TaskReporter):
        reporter.set_current_state("è®¡ç®—ä¸­...")
        # CPU å¯†é›†å‹è®¡ç®—
        result = complex_calculation(item)
        reporter.set_progress(1.0)
        return result

    results = await xmap_async(
        data,
        cpu_intensive_task,
        desc="CPUè®¡ç®—",
        use_process_pool=True,  # ä½¿ç”¨è¿›ç¨‹æ± 
        max_workers=4  # CPU æ ¸å¿ƒæ•°
    )
    ```

    ## è¿›åº¦ç›‘æ§ç•Œé¢

    å‡½æ•°è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºç±»ä¼¼ nvitop çš„å®æ—¶ç›‘æ§ç•Œé¢ï¼š

    ```
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ ğŸš€ æ•°æ®å¤„ç† [150/200] 75.0% | ETA: 0:00:30              â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker 01 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ Worker-01 â”‚ ğŸ“‹ Task-0023... â”‚ ğŸ”„ å¤„ç†ä¸­... â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80% â”‚ â±ï¸ 0:00:15 â”‚ âœ… 45 â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker 02 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ Worker-02 â”‚ ğŸ’¤ Idle        â”‚ ğŸ”„ ç­‰å¾…ä»»åŠ¡... â”‚ [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0%  â”‚ â±ï¸ --:--:-- â”‚ âœ… 38 â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ System Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ ğŸ“Š Total: 200 | âœ… 150 | ğŸ“ 150 | ğŸ‘¥ Workers: 1ğŸŸ¢/7âšª â”‚ âš¡ 12.5/s â”‚ â° 0:02:15 â”‚
    â”‚ ğŸ’¾ results.jsonl [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 75% (1.2MB/1.6MB)                      â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    ```

    ## æ€§èƒ½å»ºè®®

    - **I/O å¯†é›†å‹ä»»åŠ¡**ï¼šè®¾ç½® use_process_pool=Falseï¼Œmax_workers=8-32
    - **CPU å¯†é›†å‹ä»»åŠ¡**ï¼šè®¾ç½® use_process_pool=Trueï¼Œmax_workers=CPUæ ¸å¿ƒæ•°
    - **å¤§æ•°æ®é‡**ï¼šä½¿ç”¨æ‰¹æ¬¡å¤„ç†ï¼Œbatch_size=32-128
    - **å®æ—¶æ€§è¦æ±‚é«˜**ï¼šè®¾ç½® preserve_order=False
    - **ç½‘ç»œè¯·æ±‚**ï¼šè®¾ç½®åˆé€‚çš„ retry_countï¼Œé€šå¸¸ 2-3 æ¬¡
    """
    return asyncio.run(
        xmap_async(
            jsonlist=jsonlist,
            work_func=work_func,
            output_path=output_path,
            desc=desc,
            max_workers=max_workers,
            use_process_pool=use_process_pool,
            preserve_order=preserve_order,
            retry_count=retry_count,
            force_overwrite=force_overwrite,
            is_batch_work_func=is_batch_work_func,
            batch_size=batch_size,
            is_async_work_func=is_async_work_func,
            verbose=verbose,
            cache_id=cache_id,
        )
    )


import random

def fast_work_func(item, reporter: TaskReporter):
    if reporter:
        reporter.set_current_state("è½¬æ¢ä¸ºå¤§å†™")
        reporter.set_progress(0.0)

    time.sleep(0.1)  # æ¨¡æ‹Ÿå·¥ä½œ
    if reporter:
        reporter.set_progress(0.5)

    item["value"] = item["value"].upper()

    if reporter:
        reporter.set_progress(1.0)
        reporter.set_current_state("å®Œæˆ")
    return item

def slow_work_func(item, reporter: TaskReporter):
    if reporter:
        reporter.set_current_state(f"å¼€å§‹å¤„ç† {item['id']}")
        reporter.set_progress(0.0)

    # æ¨¡æ‹Ÿå¤šæ­¥éª¤å¤„ç†
    steps = ["é¢„å¤„ç†", "æ•°æ®è½¬æ¢", "éªŒè¯", "åå¤„ç†", "å®Œæˆ"]
    delay = random.randint(2, 10) / 5

    for i, step in enumerate(steps):
        if reporter:
            reporter.set_current_state(step)
            reporter.set_progress(i / len(steps))
        time.sleep(delay / len(steps))

    item = fast_work_func(item, reporter)
    if reporter:
        reporter.set_current_state("å®Œæˆ")
        reporter.set_progress(1.0)
    return item

def batch_work_func(items, reporter: TaskReporter):
    reporter.set_current_state("æ‰¹é‡å¤„ç†å¼€å§‹")
    reporter.set_progress(0.0)

    results = []
    total = len(items)

    for i, item in enumerate(items):
        reporter.set_current_state(f"å¤„ç†é¡¹ç›® {i+1}/{total}")
        reporter.set_progress(i / total)

        # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºä¸´æ—¶reporterï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        temp_reporter = TaskReporter(worker_id=reporter.worker_id)
        result = slow_work_func(item, temp_reporter)
        results.append(result)

    reporter.set_progress(1.0)
    reporter.set_current_state("æ‰¹é‡å¤„ç†å®Œæˆ")
    return results

async def async_work_func(item, reporter: TaskReporter):
    reporter.set_current_state(f"å¼‚æ­¥å¼€å§‹å¤„ç† {item['id']}")
    reporter.set_progress(0.0)

    # æ¨¡æ‹Ÿå¼‚æ­¥å¤šæ­¥éª¤å¤„ç†
    steps = ["å¼‚æ­¥é¢„å¤„ç†", "å¼‚æ­¥æ•°æ®è½¬æ¢", "å¼‚æ­¥éªŒè¯", "å¼‚æ­¥åå¤„ç†"]
    delay = random.randint(2, 10) / 5

    for i, step in enumerate(steps):
        reporter.set_current_state(step)
        reporter.set_progress(i / len(steps))
        await asyncio.sleep(delay / len(steps))

    item["value"] = item["value"].upper()

    reporter.set_progress(1.0)
    reporter.set_current_state("å¼‚æ­¥å¤„ç†å®Œæˆ")
    return item

async def async_batch_work_func(items, reporter: TaskReporter):
    reporter.set_current_state("å¼‚æ­¥æ‰¹é‡å¤„ç†å¼€å§‹")
    reporter.set_progress(0.0)

    # å¹¶è¡Œå¤„ç†æ‰¹é‡é¡¹ç›®
    async def process_single(item, idx):
        temp_reporter = TaskReporter(worker_id=f"{reporter.worker_id}-{idx}")
        return await async_work_func(item, temp_reporter)

    tasks = [process_single(item, i) for i, item in enumerate(items)]

    # ç›‘æ§è¿›åº¦
    completed = 0
    results = []
    for task in asyncio.as_completed(tasks):
        result = await task
        results.append(result)
        completed += 1
        reporter.set_progress(completed / len(tasks))
        reporter.set_current_state(f"å¼‚æ­¥æ‰¹é‡å¤„ç† {completed}/{len(tasks)}")

    reporter.set_progress(1.0)
    reporter.set_current_state("å¼‚æ­¥æ‰¹é‡å¤„ç†å®Œæˆ")
    return results


# æ€§èƒ½æµ‹è¯•ç”¨ä¾‹
async def test_xmap_benchmark():
    """
    xmapå‡½æ•°çš„æ€§èƒ½åŸºå‡†æµ‹è¯•

    æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
    1. æ™®é€šforå¾ªç¯ vs xmapæ€§èƒ½å¯¹æ¯”
    2. å•ä¸ªå¤„ç†æ¨¡å¼ vs æ‰¹é‡å¤„ç†æ¨¡å¼å¯¹æ¯”
    3. ä¿åºåŠŸèƒ½æ­£ç¡®æ€§éªŒè¯
    4. å„ç§é…ç½®çš„æ€§èƒ½è¡¨ç°

    æµ‹è¯•æ•°æ®ï¼š10000ä¸ªç®€å•çš„æ–‡æœ¬å¤„ç†ä»»åŠ¡

    è¾“å‡ºï¼š
    - å„ç§æ–¹æ³•çš„è€—æ—¶å¯¹æ¯”
    - æ€§èƒ½åŠ é€Ÿæ¯”
    - ä¿åºåŠŸèƒ½éªŒè¯ç»“æœ
    """
    from tqdm import tqdm
    skip_for = False
    skip_batch_async = False
    skip_single_async = False
    skip_batch_sync = True
    skip_single_sync = True
    skip_ordered = True

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    jsonlist = [{"id": i, "value": "Hello World"} for i in range(100)]

    # ä¸´æ—¶è¾“å‡ºè·¯å¾„
    output_path = Path("test_output.jsonl")
    max_workers = 16
    batch_size = 4

    if skip_for:
        for_time = 10
        for_result = [fast_work_func(item, None) for item in tqdm(jsonlist)]
    else:
        # æµ‹è¯•æ™®é€šforå¾ªç¯
        print("æµ‹è¯•æ™®é€šforå¾ªç¯...")
        start_time = time.time()
        # èŠ‚çº¦æ—¶é—´
        for_result = []
        for item in tqdm(jsonlist):
            processed = fast_work_func(item, None)
            # processed = slow_work_func(item, None)
            for_result.append(processed)
        for_time = time.time() - start_time
        # for_result = [{"id": i, "text": "Hello World".upper()} for i in range(100)]
        # for_time = 352.3638
        print(f"æ™®é€šforå¾ªç¯è€—æ—¶: {for_time:.4f}ç§’")

    # æµ‹è¯•xmapå‡½æ•° - éæ‰¹é‡æ¨¡å¼
    if skip_single_sync:
        xmap_time = 10  # æ¨¡æ‹Ÿè€—æ—¶
        xmap_result = [fast_work_func(item.copy(), None) for item in tqdm(jsonlist)]
        print(f"è·³è¿‡xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼) æµ‹è¯•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè€—æ—¶: {xmap_time:.4f}ç§’")
    else:
        print("\næµ‹è¯•xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼)...")
        start_time = time.time()
        xmap_result = await xmap_async(
            jsonlist=jsonlist,
            work_func=slow_work_func,
            output_path=output_path,
            max_workers=max_workers,
            desc="Processing items",
            use_process_pool=False,
            preserve_order=True,
            retry_count=0,
            force_overwrite=True,
            is_batch_work_func=False,
            is_async_work_func=False,
            verbose=False,
        )
        xmap_time = time.time() - start_time
        print(f"xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼) è€—æ—¶: {xmap_time:.4f}ç§’")

    # æµ‹è¯•xmapå‡½æ•° - æ‰¹é‡æ¨¡å¼
    if skip_batch_sync:
        xmap_batch_time = 8  # æ¨¡æ‹Ÿè€—æ—¶
        xmap_batch_result = [fast_work_func(item.copy(), None) for item in tqdm(jsonlist)]
        print(f"è·³è¿‡xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼) æµ‹è¯•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè€—æ—¶: {xmap_batch_time:.4f}ç§’")
    else:
        print("\næµ‹è¯•xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼)...")
        # æ¸…ç†ä¹‹å‰çš„è¾“å‡ºæ–‡ä»¶
        output_path = Path("test_output_batch.jsonl")
        start_time = time.time()

        xmap_batch_result = await xmap_async(
            jsonlist=jsonlist,
            work_func=batch_work_func,
            output_path=output_path,
            max_workers=max_workers,
            desc="Processing items in batches",
            use_process_pool=False,
            preserve_order=True,
            retry_count=0,
            force_overwrite=True,
            is_batch_work_func=True,
            batch_size=batch_size,
            is_async_work_func=False,
        )
        xmap_batch_time = time.time() - start_time
        print(f"xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼) è€—æ—¶: {xmap_batch_time:.4f}ç§’")

    if not skip_ordered:
        # æµ‹è¯•ä¿åºåŠŸèƒ½
        print("\næµ‹è¯•xmapå‡½æ•°ä¿åºåŠŸèƒ½...")
        start_time = time.time()

        def slow_work_func2(item, reporter: TaskReporter):
            # æ·»åŠ éšæœºå»¶è¿Ÿæ¨¡æ‹Ÿä¸åŒå¤„ç†æ—¶é—´ï¼Œå»¶è¿Ÿä¸IDæˆåæ¯”ï¼Œè®©åé¢çš„å…ƒç´ å…ˆå®Œæˆ
            import random
            delay = 0.001 * (1000 - item["id"]) / 1000.0  # åé¢çš„IDå¤„ç†æ›´å¿«
            time.sleep(delay + random.uniform(0, 0.001))
            item["value"] = item["value"].upper()
            item["processed_order"] = item["id"]
            return item

        test_data = jsonlist[:100]  # ä½¿ç”¨è¾ƒå°‘æ•°æ®è¿›è¡Œæµ‹è¯•
        xmap_ordered_result = await xmap_async(
            jsonlist=test_data,
            work_func=slow_work_func2,
            preserve_order=True,
            max_workers=max_workers,
            use_process_pool=False,
            is_batch_work_func=False,
            verbose=False,
        )
        xmap_unordered_result = await xmap_async(
            jsonlist=test_data,
            work_func=slow_work_func2,
            preserve_order=False,
            max_workers=max_workers,
            use_process_pool=False,
            is_batch_work_func=False,
            verbose=False,
        )

        ordered_time = time.time() - start_time
        print(f"ä¿åºæµ‹è¯•è€—æ—¶: {ordered_time:.4f}ç§’")

        # éªŒè¯ä¿åºç»“æœ
        ordered_ids = [item["processed_order"] for item in xmap_ordered_result]
        expected_ids = list(range(100))

        print(f"ä¿åºç»“æœæ­£ç¡®æ€§: {'âœ“' if ordered_ids == expected_ids else 'âœ—'}")
        print(f"ä¿åºå‰10ä¸ªID: {ordered_ids[:10]}")

        unordered_ids = [item["processed_order"] for item in xmap_unordered_result]
        print(f"éä¿åºå‰10ä¸ªID: {unordered_ids[:10]}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é¡ºåºå·®å¼‚
        order_difference = sum(1 for i, (a, b) in enumerate(zip(ordered_ids, unordered_ids)) if a != b)
        print(f"é¡ºåºå·®å¼‚æ•°é‡: {order_difference}/100")
        print(f"ä¿åºåŠŸèƒ½æµ‹è¯•: {'âœ“' if order_difference < len(ordered_ids) else 'éœ€è¦æ›´å¼ºçš„å¹¶å‘æµ‹è¯•'}")

        # éªŒè¯ç»“æœ
        if not skip_single_sync and not skip_batch_sync:
            for i, (for_item, xmap_item, xmap_batch_item) in enumerate(zip(for_result, xmap_result, xmap_batch_result)):
                assert for_item["id"] == xmap_item["id"], f"IDä¸åŒ¹é…: forå¾ªç¯ {for_item['id']} vs xmap {xmap_item['id']}"
                assert for_item["id"] == xmap_batch_item["id"], f"IDä¸åŒ¹é…: forå¾ªç¯ {for_item['id']} vs xmapæ‰¹é‡ {xmap_batch_item['id']}"
                assert for_item["value"] == xmap_item["value"], f"å€¼ä¸åŒ¹é…: forå¾ªç¯ {for_item['value']} vs xmap {xmap_item['value']}"
                assert for_item["value"] == xmap_batch_item["value"], f"å€¼ä¸åŒ¹é…: forå¾ªç¯ {for_item['value']} vs xmapæ‰¹é‡ {xmap_batch_item['value']}"
        else:
            print("è·³è¿‡åŒæ­¥æµ‹è¯•ç»“æœéªŒè¯ï¼ˆå› ä¸ºè·³è¿‡äº†æŸäº›åŒæ­¥æµ‹è¯•ï¼‰")

    print("\næµ‹è¯• async xmap å‡½æ•°æ€§èƒ½å¯¹æ¯”...")
    # æµ‹è¯•å¼‚æ­¥xmapå‡½æ•° - éæ‰¹é‡æ¨¡å¼
    if skip_single_async:
        async_xmap_time = 6  # æ¨¡æ‹Ÿè€—æ—¶
        async_xmap_result = [fast_work_func(item.copy(), None) for item in tqdm(jsonlist)]
        print(f"è·³è¿‡å¼‚æ­¥xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼) æµ‹è¯•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè€—æ—¶: {async_xmap_time:.4f}ç§’")
    else:
        print("\næµ‹è¯•å¼‚æ­¥xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼)...")
        start_time = time.time()
        async_xmap_result = await xmap_async(
            jsonlist=jsonlist,
            work_func=async_work_func,
            output_path=output_path,
            max_workers=max_workers,
            desc="Processing items asynchronously",
            use_process_pool=False,
            preserve_order=True,
            retry_count=0,
            force_overwrite=True,
            is_batch_work_func=False,
            is_async_work_func=True,
            verbose=False,
        )
        async_xmap_time = time.time() - start_time
        print(f"å¼‚æ­¥xmapå‡½æ•° (éæ‰¹é‡æ¨¡å¼) è€—æ—¶: {async_xmap_time:.4f}ç§’")

    # æµ‹è¯•å¼‚æ­¥xmapå‡½æ•° - æ‰¹é‡æ¨¡å¼
    if skip_batch_async:
        async_xmap_batch_time = 4  # æ¨¡æ‹Ÿè€—æ—¶
        async_xmap_batch_result = [fast_work_func(item.copy(), None) for item in tqdm(jsonlist)]
        print(f"è·³è¿‡å¼‚æ­¥xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼) æµ‹è¯•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè€—æ—¶: {async_xmap_batch_time:.4f}ç§’")
    else:
        print("\næµ‹è¯•å¼‚æ­¥xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼)...")
        # æ¸…ç†ä¹‹å‰çš„è¾“å‡ºæ–‡ä»¶
        output_path = Path("test_output_async_batch.jsonl")
        start_time = time.time()
        async_xmap_batch_result = await xmap_async(
            jsonlist=jsonlist,
            work_func=async_batch_work_func,
            output_path=output_path,
            max_workers=max_workers,
            desc="Processing items in batches asynchronously",
            use_process_pool=False,
            preserve_order=True,
            retry_count=0,
            force_overwrite=True,
            is_batch_work_func=True,
            batch_size=batch_size,
            is_async_work_func=True,
        )
        async_xmap_batch_time = time.time() - start_time
        print(f"å¼‚æ­¥xmapå‡½æ•° (æ‰¹é‡æ¨¡å¼) è€—æ—¶: {async_xmap_batch_time:.4f}ç§’")

    # è¾“å‡ºæ€§èƒ½å¯¹æ¯”
    print("\n===== æ€§èƒ½å¯¹æ¯”åˆ†æ =====")
    print(f"{'æ–¹æ³•åç§°':<20} {'è€—æ—¶(ç§’)':<12} {'åŠ é€Ÿæ¯”'}")

    # æ˜¾ç¤ºforå¾ªç¯ç»“æœ
    if skip_for:
        print(f"{'æ™®é€šforå¾ªç¯':<20} {for_time:.4f} (è·³è¿‡)")
    else:
        print(f"{'æ™®é€šforå¾ªç¯':<20} {for_time:.4f}")

    # æ˜¾ç¤ºxmapå•ä¸ªæ¨¡å¼ç»“æœ
    if skip_single_sync:
        print(f"{'xmap(éæ‰¹é‡)':<20} {xmap_time:.4f} (è·³è¿‡) {for_time/xmap_time:.2f}x")
    else:
        print(f"{'xmap(éæ‰¹é‡)':<20} {xmap_time:.4f} {for_time/xmap_time:.2f}x")

    # æ˜¾ç¤ºxmapæ‰¹é‡æ¨¡å¼ç»“æœ
    if skip_batch_sync:
        print(f"{'xmap(æ‰¹é‡)':<20} {xmap_batch_time:.4f} (è·³è¿‡) {for_time/xmap_batch_time:.2f}x")
    else:
        print(f"{'xmap(æ‰¹é‡)':<20} {xmap_batch_time:.4f} {for_time/xmap_batch_time:.2f}x")

    if not skip_ordered:
        print(f"{'xmap(ä¿åºæµ‹è¯•)':<20} {ordered_time:.4f}")

    # æ˜¾ç¤ºå¼‚æ­¥xmapå•ä¸ªæ¨¡å¼ç»“æœ
    if skip_single_async:
        print(f"{'å¼‚æ­¥xmap(éæ‰¹é‡)':<20} {async_xmap_time:.4f} (è·³è¿‡) {for_time/async_xmap_time:.2f}x")
    else:
        print(f"{'å¼‚æ­¥xmap(éæ‰¹é‡)':<20} {async_xmap_time:.4f} {for_time/async_xmap_time:.2f}x")

    # æ˜¾ç¤ºå¼‚æ­¥xmapæ‰¹é‡æ¨¡å¼ç»“æœ
    if skip_batch_async:
        print(f"{'å¼‚æ­¥xmap(æ‰¹é‡)':<20} {async_xmap_batch_time:.4f} (è·³è¿‡) {for_time/async_xmap_batch_time:.2f}x")
    else:
        print(f"{'å¼‚æ­¥xmap(æ‰¹é‡)':<20} {async_xmap_batch_time:.4f} {for_time/async_xmap_batch_time:.2f}x")

# ç¤ºä¾‹ç”¨æ³•
async def main():
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = [
        {"id": f"item-{i:03d}", "value": f"test-value-{i}"}
        for i in range(50)
    ]

    print("ğŸš€ å¼€å§‹å¼‚æ­¥å¤„ç†ä»»åŠ¡æ¼”ç¤º...")

    # ä½¿ç”¨å¼‚æ­¥å·¥ä½œå‡½æ•°
    results = await xmap_async(
        test_data,
        async_work_func,
        output_path=Path(__file__).parent.parent / "output/async_results.jsonl",
        desc="å¼‚æ­¥ä»»åŠ¡å¤„ç†æ¼”ç¤º",
        max_workers=8,
        is_async_work_func=True,
        use_process_pool=False,  # å¼‚æ­¥å‡½æ•°ä½¿ç”¨çº¿ç¨‹æ± 
        verbose=True,
    )

    print(f"\nâœ… å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªé¡¹ç›®")

    await test_xmap_benchmark()

if __name__ == "__main__":
    asyncio.run(main())

