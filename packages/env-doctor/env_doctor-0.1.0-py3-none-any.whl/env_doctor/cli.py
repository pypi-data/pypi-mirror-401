"""
Modernized CLI using the detector architecture.

This version demonstrates how to migrate from procedural checks
to the new detector-based system.
"""
import sys
import os
import argparse
import platform
import json
from datetime import datetime
from typing import Dict, Any, Optional
from .core.registry import DetectorRegistry
from .core.detector import Status
from .db import get_max_cuda_for_driver, get_install_command, DB_DATA

# Legacy imports for functions not yet refactored
from .checks import (
    scan_imports_in_folder,
    check_broken_imports
)

# Import detectors to register them!
# This triggers all @register decorators
from .detectors.nvidia_driver import NvidiaDriverDetector
from .detectors.cuda_toolkit import CudaToolkitDetector
from .detectors.python_libraries import PythonLibraryDetector
from .detectors.wsl2 import WSL2Detector
from .detectors.cudnn import CudnnDetector

# Import model checker for model compatibility
from .utilities import ModelChecker

def check_compilation_health(cuda_result, torch_result):
    """
    Check if system CUDA matches PyTorch CUDA for compilation compatibility.
    
    Args:
        cuda_result: DetectionResult from CudaToolkitDetector
        torch_result: DetectionResult from PythonLibraryDetector (torch)
    """
    print("\nüè≠  COMPILATION HEALTH (For Flash-Attention/AutoGPTQ)")
    
    if not cuda_result.detected:
        print("‚ö†Ô∏è   System CUDA (nvcc) NOT found.")
        print("    -> You cannot install 'flash-attention' or 'auto-gptq' from source.")
        for rec in cuda_result.recommendations:
            print(f"    ‚Üí {rec}")
        return
    
    if not torch_result.detected:
        print("‚ùì  PyTorch not installed. Skipping check.")
        return
    
    torch_cuda = torch_result.metadata.get("cuda_version", "Unknown")
    if torch_cuda == "Unknown":
        print("‚ùì  Torch CUDA version unknown. Skipping check.")
        return

    sys_cuda = cuda_result.version
    sys_mm = ".".join(sys_cuda.split(".")[:2])
    torch_mm = ".".join(torch_cuda.split(".")[:2])

    if sys_mm == torch_mm:
        print(f"‚úÖ  PERFECT SYMMETRY: System ({sys_cuda}) == Torch ({torch_cuda})")
    else:
        print(f"‚ùå  ASYMMETRY DETECTED: System ({sys_cuda}) != Torch ({torch_cuda})")
        print("    -> pip install flash-attention will likely FAIL.")
        print(f"    ‚Üí Consider installing CUDA Toolkit {torch_mm}")


def check_system_path():
    """Check LD_LIBRARY_PATH for CUDA linking (TensorFlow/JAX)."""
    print("\nüîó  SYSTEM LINKING (For TensorFlow/JAX)")
    ld_path = os.environ.get("LD_LIBRARY_PATH", "")
    if not ld_path:
        print("‚ö†Ô∏è   LD_LIBRARY_PATH is unset.")
        print("    ‚Üí For TensorFlow/JAX, you may need to set this.")
        return
    print(f"    LD_LIBRARY_PATH: {ld_path}")
    if "cuda" not in ld_path.lower():
        print("‚ö†Ô∏è   Warning: LD_LIBRARY_PATH is set but does not seem to point to any CUDA folders.")


def print_detection_result(result, emoji="üì¶"):
    """
    Pretty-print a DetectionResult.
    
    Args:
        result: DetectionResult object
        emoji: Emoji prefix for the component
    """
    component_name = result.component.replace("_", " ").title()
    
    if result.status == Status.SUCCESS:
        print(f"‚úÖ  {component_name}: {result.version}")
        if result.path:
            print(f"    Path: {result.path}")
        
        # Print metadata
        for key, value in result.metadata.items():
            if key not in ["detection_method"]:  # Skip internal keys
                display_key = key.replace("_", " ").title()
                print(f"    ‚Üí {display_key}: {value}")
    
    elif result.status == Status.NOT_FOUND:
        print(f"‚ùå  {component_name}: Not Found")
    
    elif result.status == Status.WARNING:
        print(f"‚ö†Ô∏è   {component_name}: {result.version or 'Warning'}")
    
    elif result.status == Status.ERROR:
        print(f"‚ùå  {component_name}: Error")
    
    # Print issues
    for issue in result.issues:
        print(f"    ‚ö†Ô∏è  {issue}")
    
    # Print recommendations
    for rec in result.recommendations:
        print(f"    ‚Üí {rec}")


def check_library_compatibility(lib_result, max_cuda):
    """
    Check if a library's CUDA version is compatible with the driver.

    Args:
        lib_result: DetectionResult from PythonLibraryDetector
        max_cuda: Maximum CUDA version supported by driver (string)
    """
    if not lib_result.detected or not max_cuda:
        return

    lib_cuda = lib_result.metadata.get("cuda_version", "Unknown")
    if lib_cuda == "Unknown" or "CPU" in lib_cuda:
        return

    try:
        # Extract numeric CUDA version
        cuda_num = lib_cuda.split(" ")[0].replace("x", "0")
        if float(cuda_num) > float(max_cuda):
            lib_name = lib_result.component.replace("python_library_", "")
            print(f"    ‚ùå CRITICAL CONFLICT: {lib_name} uses CUDA {lib_cuda}, Driver supports up to {max_cuda}!")
            print(f"    ‚Üí Run 'env-doctor install {lib_name}' to fix.")
        else:
            print(f"    ‚úÖ Compatible with Driver (CUDA {max_cuda})")
    except (ValueError, IndexError):
        pass


def determine_overall_status(results: Dict[str, Any]) -> str:
    """
    Determine overall status from detection results.

    Args:
        results: Dictionary of detection results

    Returns:
        str: "pass", "warning", or "fail"
    """
    has_errors = False
    has_warnings = False

    # Check each result
    for key, result in results.items():
        if result is None:
            continue

        if isinstance(result, dict):
            # Handle libraries dict
            for lib_result in result.values():
                if lib_result.status == Status.ERROR:
                    has_errors = True
                elif lib_result.status in [Status.WARNING, Status.NOT_FOUND]:
                    has_warnings = True
        else:
            # Handle single result
            if result.status == Status.ERROR:
                has_errors = True
            elif result.status in [Status.WARNING, Status.NOT_FOUND]:
                has_warnings = True

    if has_errors:
        return "fail"
    elif has_warnings:
        return "warning"
    else:
        return "pass"


def count_issues(results: Dict[str, Any]) -> int:
    """
    Count total issues across all detection results.

    Args:
        results: Dictionary of detection results

    Returns:
        int: Total number of issues
    """
    count = 0

    for key, result in results.items():
        if result is None:
            continue

        if isinstance(result, dict):
            # Handle libraries dict
            for lib_result in result.values():
                count += len(lib_result.issues)
        else:
            # Handle single result
            count += len(result.issues)

    return count


def determine_exit_code(results: Dict[str, Any]) -> int:
    """
    Determine exit code based on detection results.

    Args:
        results: Dictionary of detection results

    Returns:
        int: Exit code (0 = pass, 1 = warnings/failures, 2 = errors)
    """
    has_errors = False
    has_warnings = False

    for key, result in results.items():
        if result is None:
            continue

        if isinstance(result, dict):
            # Handle libraries dict
            for lib_result in result.values():
                if lib_result.status == Status.ERROR:
                    has_errors = True
                elif lib_result.status in [Status.WARNING, Status.NOT_FOUND]:
                    has_warnings = True
        else:
            # Handle single result
            if result.status == Status.ERROR:
                has_errors = True
            elif result.status in [Status.WARNING, Status.NOT_FOUND]:
                has_warnings = True

    if has_errors:
        return 2
    elif has_warnings:
        return 1
    else:
        return 0


def check_command(output_json: bool = False, ci: bool = False):
    """
    Main diagnostic command using detector architecture.

    This is the MODERNIZED version that uses DetectorRegistry
    instead of direct function calls.

    Args:
        output_json: Output as JSON (machine-readable)
        ci: CI-friendly mode (implies JSON + proper exit codes)
    """
    # === Collect all detection results ===
    # STEP 1: Environment Detection
    wsl2_detector = DetectorRegistry.get("wsl2")
    wsl2_result = wsl2_detector.detect() if wsl2_detector.can_run() else None

    # STEP 2: Hardware Detection
    driver_detector = DetectorRegistry.get("nvidia_driver")
    driver_result = driver_detector.detect()
    max_cuda = driver_result.metadata.get("max_cuda_version", None) if driver_result.detected else None

    # STEP 3: System CUDA Detection
    cuda_detector = DetectorRegistry.get("cuda_toolkit")
    cuda_result = cuda_detector.detect()

    # STEP 4: cuDNN Detection
    cudnn_detector = DetectorRegistry.get("cudnn")
    cudnn_result = cudnn_detector.detect() if cudnn_detector.can_run() else None

    # STEP 5: Python Libraries Detection
    libs = ["torch", "tensorflow", "jax"]
    torch_result = None
    lib_results = {}

    from .detectors.python_libraries import PythonLibraryDetector

    for lib in libs:
        lib_detector = PythonLibraryDetector(lib)
        lib_result = lib_detector.detect()
        lib_results[lib] = lib_result

        if lib == "torch":
            torch_result = lib_result

    # Organize results for JSON output
    results = {
        "wsl2": wsl2_result,
        "driver": driver_result,
        "cuda": cuda_result,
        "cudnn": cudnn_result,
        "libraries": lib_results
    }

    # === Choose output format ===
    if ci or output_json:
        # JSON output
        output = {
            "status": determine_overall_status(results),
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "driver": "found" if driver_result.detected else "not_found",
                "cuda": "found" if cuda_result.detected else "not_found",
                "cudnn": "found" if (cudnn_result and cudnn_result.detected) else "not_found",
                "issues_count": count_issues(results)
            },
            "checks": {
                "wsl2": wsl2_result.to_dict() if wsl2_result else None,
                "driver": driver_result.to_dict(),
                "cuda": cuda_result.to_dict(),
                "cudnn": cudnn_result.to_dict() if cudnn_result else None,
                "libraries": {
                    lib: result.to_dict()
                    for lib, result in lib_results.items()
                }
            }
        }
        print(json.dumps(output, indent=2))
        sys.exit(determine_exit_code(results))
    else:
        # Human output (existing code)
        print("\nü©∫  ENV-DOCTOR DIAGNOSIS  ü©∫")
        print("==============================")

        # --- Show DB Status ---
        meta = DB_DATA.get("_metadata", {})
        if meta:
            print(f"üõ°Ô∏è  DB Verified: {meta.get('last_verified', 'Unknown')}")
            print(f"    Method: {meta.get('method', 'Unknown')}")
            print("------------------------------")

        # === STEP 1: Environment Detection ===
        if wsl2_result:
            print_detection_result(wsl2_result, "üêß")
            print("------------------------------")

        # === STEP 2: Hardware Detection ===
        if driver_result.detected:
            print(f"‚úÖ  GPU Driver Found: {driver_result.version}")
            print(f"    ‚Üí Max Supported CUDA: {max_cuda}")
            print(f"    ‚Üí Detection Method: {driver_result.metadata.get('detection_method', 'unknown')}")
        else:
            print("‚ö†Ô∏è   No NVIDIA Driver detected.")
            for rec in driver_result.recommendations:
                print(f"    ‚Üí {rec}")

        # === STEP 3: System CUDA Detection ===
        if cuda_result.detected:
            print(f"‚úÖ  System CUDA (nvcc): {cuda_result.version}")
            if cuda_result.path:
                print(f"    Path: {cuda_result.path}")

            # Show quick status
            install_count = cuda_result.metadata.get("installation_count", 1)
            if install_count > 1:
                print(f"    ‚ö†Ô∏è  {install_count} CUDA installations detected")

            if cuda_result.status == Status.WARNING:
                print(f"    ‚ö†Ô∏è  Configuration issues detected (run 'doctor debug' for details)")
            elif cuda_result.status == Status.ERROR:
                print(f"    ‚ùå Critical issues detected (run 'doctor debug' for details)")
        else:
            print("‚ÑπÔ∏è   System CUDA (nvcc) not found.")
            if cuda_result.recommendations:
                print(f"    ‚Üí {cuda_result.recommendations[0]}")

        print("------------------------------")

        # cuDNN Detection
        if cudnn_result and cudnn_result.detected:
            print(f"‚úÖ  cuDNN: v{cudnn_result.version}")

        # === STEP 4: Python Libraries Detection ===
        for lib, lib_result in lib_results.items():
            if lib_result.detected:
                print(f"üì¶  Found {lib}: v{lib_result.version}")

                # Show bundled CUDA info
                cuda_ver = lib_result.metadata.get("cuda_version", "Unknown")
                if cuda_ver != "Unknown":
                    print(f"    ‚Üí Bundled CUDA: {cuda_ver}")

                    # Check compatibility with driver
                    if max_cuda:
                        check_library_compatibility(lib_result, max_cuda)
                else:
                    print(f"    ‚Üí Bundled CUDA: Not Detected")
            else:
                print(f"‚ùå  {lib} is NOT installed.")

        # === STEP 5: Compilation Health Check ===
        if torch_result and torch_result.detected:
            check_compilation_health(cuda_result, torch_result)

        # === STEP 6: System Path Check ===
        check_system_path()

        # === STEP 7: Code Migration Check ===
        # (Not yet refactored - still using legacy function)
        check_broken_imports()

        # === STEP 8: Offer detailed analysis ===
        if cuda_result.detected and (cuda_result.issues or cuda_result.metadata.get("installation_count", 1) > 1):
            print("\nüí°  TIP: Run 'env-doctor cuda-info' for detailed CUDA analysis")



def install_command(package_name):
    """
    Provide installation prescription for a package.
    
    Uses NvidiaDriverDetector to determine compatible CUDA version.
    """
    print(f"\nü©∫  PRESCRIPTION FOR: {package_name}")
    
    # Use detector instead of direct function call
    driver_detector = DetectorRegistry.get("nvidia_driver")
    driver_result = driver_detector.detect()
    
    if not driver_result.detected:
        print("‚ö†Ô∏è  No NVIDIA Driver found. Assuming CPU-only.")
        print(f"   pip install {package_name}")
        return

    max_cuda = driver_result.metadata.get("max_cuda_version", "Unknown")
    print(f"Detected Driver: {driver_result.version} (Supports up to CUDA {max_cuda})")
    
    command = get_install_command(package_name, max_cuda)
    print("\n‚¨áÔ∏è   Run this command:")
    print("---------------------------------------------------")
    print(command)
    print("---------------------------------------------------")


def scan_command(output_json: bool = False):
    """
    Scan local directory for AI library imports.

    Args:
        output_json: Output as JSON (machine-readable)

    Note: This still uses legacy function as it's not environment detection.
    """
    libs = scan_imports_in_folder()

    if output_json:
        # JSON output
        output = {
            "status": "pass" if len(libs) > 0 else "fail",
            "timestamp": datetime.now().isoformat(),
            "dependencies": libs,
            "issues": [] if len(libs) > 0 else ["No common AI imports found"],
            "recommendations": [
                f"env-doctor install {lib}" for lib in libs if lib in ["torch", "tensorflow", "jax"]
            ]
        }
        print(json.dumps(output, indent=2))
        sys.exit(0)
    else:
        # Human output
        print("\nüîç  SCANNING CURRENT DIRECTORY...")
        if libs:
            print(f"Found imports for: {', '.join(libs)}")
            print("\nTo get safe install commands for these, run:")
            for lib in libs:
                if lib in ["torch", "tensorflow", "jax"]:
                    print(f"  env-doctor install {lib}")
        else:
            print("No common AI imports found.")


def debug_command():
    """
    NEW COMMAND: Debug mode that shows all detector results in detail.
    
    This is useful for troubleshooting and seeing raw detector output.
    """
    print("\nüîç  DEBUG MODE - Detailed Detector Information")
    print("=" * 60)
    
    # Get all registered detectors
    detector_names = DetectorRegistry.get_names()
    print(f"Registered Detectors: {', '.join(detector_names)}\n")
    
    # Run each detector and show results
    for name in detector_names:
        if name == "python_library":
            # Special case: python_library needs a library name
            continue
        
        print(f"\n--- {name.upper().replace('_', ' ')} ---")
        try:
            detector = DetectorRegistry.get(name)
            # CHECK if detector can run on this platform
            if not detector.can_run():
                print(f"Status: skipped (not applicable on {platform.system()})")
                continue

            result = detector.detect()
            
            print(f"Status: {result.status.value}")
            print(f"Component: {result.component}")
            if result.version:
                print(f"Version: {result.version}")
            if result.path:
                print(f"Path: {result.path}")
            if result.metadata:
                print(f"Metadata: {result.metadata}")
            if result.issues:
                print(f"Issues: {result.issues}")
            if result.recommendations:
                print(f"Recommendations: {result.recommendations}")
        except Exception as e:
            print(f"ERROR: {e}")
    
    # Test python libraries separately
    print(f"\n--- PYTHON LIBRARIES ---")
    from .detectors.python_libraries import PythonLibraryDetector
    for lib in ["torch", "tensorflow", "jax"]:
        print(f"\n{lib}:")
        detector = PythonLibraryDetector(lib)
        result = detector.detect()
        print(f"  Status: {result.status.value}")
        if result.version:
            print(f"  Version: {result.version}")
        if result.metadata:
            print(f"  Metadata: {result.metadata}")




def print_cuda_detailed_info(cuda_result):
    """
    Print detailed CUDA toolkit information from comprehensive detection.
    
    Args:
        cuda_result: DetectionResult from CudaToolkitDetector
    """
    print("\n" + "="*60)
    print("üîß  DETAILED CUDA TOOLKIT ANALYSIS")
    print("="*60)
    
    if not cuda_result.detected:
        print("‚ùå  No CUDA Toolkit detected")
        for rec in cuda_result.recommendations:
            print(f"    ‚Üí {rec}")
        return
    
    # 1. Main version info
    print(f"\nüìå  Primary CUDA Version: {cuda_result.version}")
    if cuda_result.path:
        print(f"    nvcc location: {cuda_result.path}")
    
    # 2. Installation count
    install_count = cuda_result.metadata.get("installation_count", 0)
    if install_count > 1:
        print(f"\n‚ö†Ô∏è   Multiple Installations Detected: {install_count}")
        for i, inst in enumerate(cuda_result.metadata.get("installations", []), 1):
            print(f"    {i}. Version {inst['version']}: {inst['path']}")
    
    # 3. Environment Variables
    print("\nüîê  Environment Variables:")
    
    cuda_home = cuda_result.metadata.get("cuda_home", {})
    if cuda_home.get("status") == "set":
        print(f"    ‚úÖ CUDA_HOME: {cuda_home['value']}")
    elif cuda_home.get("status") == "missing":
        print(f"    ‚ùå CUDA_HOME: Not set")
    elif cuda_home.get("status") == "invalid":
        print(f"    ‚ùå CUDA_HOME: {cuda_home['value']} (path doesn't exist)")
    
    # 4. PATH Configuration
    path_config = cuda_result.metadata.get("path_config", {})
    if path_config.get("correct"):
        print(f"    ‚úÖ PATH: CUDA bin directory found")
    else:
        print(f"    ‚ùå PATH: CUDA bin directory missing")
        print(f"       {path_config.get('reason', 'Unknown issue')}")
    
    # 5. LD_LIBRARY_PATH (Linux only)
    if "ld_library_path" in cuda_result.metadata:
        ld_info = cuda_result.metadata["ld_library_path"]
        if ld_info.get("correct"):
            print(f"    ‚úÖ LD_LIBRARY_PATH: CUDA lib directory found")
        else:
            print(f"    ‚ùå LD_LIBRARY_PATH: {ld_info.get('reason', 'Not configured')}")
    
    # 6. Runtime Library
    print("\nüìö  Runtime Library:")
    libcudart = cuda_result.metadata.get("libcudart", {})
    if libcudart.get("found"):
        version = libcudart.get("version", "Unknown")
        print(f"    ‚úÖ libcudart: Found (v{version})")
        print(f"       Location: {libcudart.get('path', 'Unknown')}")
    else:
        print(f"    ‚ùå libcudart: Not found")
    
    # 7. Driver Compatibility
    print("\nüñ•Ô∏è   Driver Compatibility:")
    driver_compat = cuda_result.metadata.get("driver_compatibility", {})
    if driver_compat.get("compatible"):
        print(f"    ‚úÖ {driver_compat.get('message', 'Compatible')}")
        if "driver_version" in driver_compat:
            print(f"       Driver: {driver_compat['driver_version']}")
            print(f"       Max CUDA: {driver_compat['max_cuda']}")
    else:
        print(f"    ‚ùå {driver_compat.get('message', 'Incompatible')}")
    
    # 8. Issues & Recommendations
    if cuda_result.issues:
        print("\n‚ö†Ô∏è   Issues Detected:")
        for issue in cuda_result.issues:
            print(f"    ‚Ä¢ {issue}")
    
    if cuda_result.recommendations:
        print("\nüí°  Recommendations:")
        for rec in cuda_result.recommendations:
            print(f"    ‚Üí {rec}")
    
    print("\n" + "="*60)



# New command: cuda-info
def cuda_info_command(output_json: bool = False):
    """
    Display comprehensive CUDA toolkit information.

    Args:
        output_json: Output as JSON (machine-readable)
    """
    cuda_detector = DetectorRegistry.get("cuda_toolkit")
    cuda_result = cuda_detector.detect()

    if output_json:
        print(json.dumps(cuda_result.to_dict(), indent=2))
        sys.exit(0 if cuda_result.status in [Status.SUCCESS, Status.WARNING] else 1)
    else:
        print_cuda_detailed_info(cuda_result)


def print_cudnn_detailed_info(cudnn_result):
    """
    Print detailed cuDNN information from detection.

    Args:
        cudnn_result: DetectionResult from CudnnDetector
    """
    print("\n" + "="*60)
    print("üß†  DETAILED CUDNN ANALYSIS")
    print("="*60)

    if not cudnn_result.detected:
        print("‚ùå  cuDNN library not found")
        for rec in cudnn_result.recommendations:
            print(f"    ‚Üí {rec}")
        return

    # 1. Main version info
    print(f"\nüìå  cuDNN Version: {cudnn_result.version}")
    if cudnn_result.path:
        print(f"    Primary Library: {cudnn_result.path}")

    # 2. Library count
    lib_count = cudnn_result.metadata.get("library_count", 1)
    if lib_count > 1:
        print(f"\nüìö  Multiple cuDNN Libraries Found: {lib_count}")
        libraries = cudnn_result.metadata.get("libraries", [])
        for lib in libraries:
            print(f"    ‚Ä¢ {lib['path']}")

    # 3. Platform info
    platform_info = cudnn_result.metadata.get("platform", "Unknown")
    print(f"\nüîß  Platform: {platform_info}")

    # 4. Symlink/PATH status
    if platform_info == "Linux":
        symlink_status = cudnn_result.metadata.get("symlink_status", {})
        if symlink_status:
            print("\nüîó  Symlink Status:")
            if symlink_status.get("valid"):
                for symlink in symlink_status["valid"]:
                    print(f"    ‚úÖ {symlink}")
            if symlink_status.get("missing"):
                for symlink in symlink_status["missing"]:
                    print(f"    ‚ùå Missing: {symlink}")
            if symlink_status.get("broken"):
                for symlink in symlink_status["broken"]:
                    print(f"    ‚ö†Ô∏è  Broken: {symlink}")
    else:
        path_status = cudnn_result.metadata.get("path_status", {})
        if path_status:
            print("\nüîó  PATH Configuration:")
            if path_status.get("in_path"):
                print(f"    ‚úÖ cuDNN DLL in PATH: {path_status.get('directory')}")
            else:
                print(f"    ‚ùå cuDNN DLL not in PATH")
                if path_status.get("suggested_path"):
                    print(f"       Suggested: {path_status.get('suggested_path')}")

    # 5. Multiple versions check
    multiple_versions = cudnn_result.metadata.get("multiple_versions")
    if multiple_versions and len(multiple_versions) > 1:
        print(f"\n‚ö†Ô∏è   Multiple Versions Detected: {', '.join(multiple_versions)}")
        print("    Consider removing old versions to avoid conflicts")

    # 6. CUDA compatibility
    cuda_compat = cudnn_result.metadata.get("cuda_compatibility", {})
    if cuda_compat:
        print("\nüîó  CUDA Compatibility:")
        if cuda_compat.get("compatible"):
            print(f"    ‚úÖ {cuda_compat.get('message', 'Compatible')}")
        else:
            print(f"    ‚ùå {cuda_compat.get('message', 'Incompatibility detected')}")

    # 7. Issues & Recommendations
    if cudnn_result.issues:
        print("\n‚ö†Ô∏è   Issues Detected:")
        for issue in cudnn_result.issues:
            print(f"    ‚Ä¢ {issue}")

    if cudnn_result.recommendations:
        print("\nüí°  Recommendations:")
        for rec in cudnn_result.recommendations:
            print(f"    ‚Üí {rec}")

    print("\n" + "="*60)


def cudnn_info_command(output_json: bool = False):
    """
    Display comprehensive cuDNN library information.

    Args:
        output_json: Output as JSON (machine-readable)
    """
    cudnn_detector = DetectorRegistry.get("cudnn")
    if not cudnn_detector.can_run():
        if output_json:
            print(json.dumps({"error": "cuDNN detector not supported on this platform"}))
        else:
            print("‚ùå  cuDNN detector not supported on this platform")
        sys.exit(1)
        return

    cudnn_result = cudnn_detector.detect()

    if output_json:
        print(json.dumps(cudnn_result.to_dict(), indent=2))
        sys.exit(0 if cudnn_result.status in [Status.SUCCESS, Status.WARNING] else 1)
    else:
        print_cudnn_detailed_info(cudnn_result)


def dockerfile_command(dockerfile_path: str = "Dockerfile"):
    """
    Validate a Dockerfile for GPU/CUDA configuration issues.

    Args:
        dockerfile_path: Path to Dockerfile (default: ./Dockerfile)
    """
    from .validators.dockerfile_validator import DockerfileValidator

    print(f"\nüê≥  DOCKERFILE VALIDATION: {dockerfile_path}")
    print("="*60)

    validator = DockerfileValidator(dockerfile_path)
    result = validator.validate()

    _print_validation_result(result)

    # Exit with error code if errors found
    sys.exit(1 if result.error_count > 0 else 0)


def docker_compose_command(compose_path: str = "docker-compose.yml"):
    """
    Validate a docker-compose.yml for GPU configuration issues.

    Args:
        compose_path: Path to docker-compose.yml (default: ./docker-compose.yml)
    """
    from .validators.compose_validator import ComposeValidator

    print(f"\nüê≥  DOCKER COMPOSE VALIDATION: {compose_path}")
    print("="*60)

    validator = ComposeValidator(compose_path)
    result = validator.validate()

    _print_validation_result(result)

    # Exit with error code if errors found
    sys.exit(1 if result.error_count > 0 else 0)


def _print_validation_result(result):
    """
    Print a validation result with colorized, grouped output.

    Args:
        result: ValidationResult object
    """
    from .validators.models import Severity

    if not result.issues:
        print("\n‚úÖ  No issues found! Configuration looks good.")
        return

    # Print issues grouped by severity
    for severity in [Severity.ERROR, Severity.WARNING, Severity.INFO]:
        issues = result.get_issues_by_severity(severity)
        if not issues:
            continue

        # Severity header with emoji
        if severity == Severity.ERROR:
            print(f"\n‚ùå  ERRORS ({len(issues)}):")
        elif severity == Severity.WARNING:
            print(f"\n‚ö†Ô∏è   WARNINGS ({len(issues)}):")
        else:
            print(f"\n‚ÑπÔ∏è   INFO ({len(issues)}):")

        print("-" * 60)

        # Print each issue
        for issue in issues:
            if issue.line_number > 0:
                print(f"\nLine {issue.line_number}:")
            else:
                print()

            print(f"  Issue: {issue.issue}")
            print(f"  Fix:   {issue.recommendation}")

            if issue.corrected_command:
                print(f"\n  Suggested fix:")
                for line in issue.corrected_command.split('\n'):
                    print(f"    {line}")

    # Print summary
    print("\n" + "="*60)
    print("SUMMARY:")
    print(f"  ‚ùå Errors:   {result.error_count}")
    print(f"  ‚ö†Ô∏è  Warnings: {result.warning_count}")
    print(f"  ‚ÑπÔ∏è  Info:     {result.info_count}")

    if result.error_count > 0:
        print("\n‚ùå  Validation FAILED. Fix errors before deploying.")
    elif result.warning_count > 0:
        print("\n‚ö†Ô∏è   Validation passed with warnings. Review before deploying.")
    else:
        print("\n‚úÖ  All checks passed!")
def model_command(model_name: str, precision: str = None):
    """
    Check if a model can run on available hardware.

    Args:
        model_name: Name of the model to check
        precision: Optional specific precision to check
    """
    checker = ModelChecker()
    result = checker.check_compatibility(model_name, precision)

    if not result["success"]:
        print(f"\n‚ùå  {result['error']}")
        if result.get("suggestions"):
            print(f"\nüí°  Did you mean:")
            for sugg in result["suggestions"]:
                print(f"    ‚Ä¢ {sugg}")
            print(f"\n    Run 'env-doctor model --list' to see all available models")
        print()
        return

    print_model_compatibility(result)


def print_model_compatibility(result: dict):
    """
    Pretty-print model compatibility analysis.

    Args:
        result: Compatibility check result from ModelChecker
    """
    model_info = result["model_info"]
    gpu_info = result["gpu_info"]
    vram_reqs = result["vram_requirements"]
    compat = result["compatibility"]
    recs = result["recommendations"]

    # Header
    print(f"\nü§ñ  Checking: {result['model_name'].upper()}")
    print(f"    Parameters: {model_info['params_b']}B")
    if model_info.get("hf_id"):
        print(f"    HuggingFace: {model_info['hf_id']}")

    # GPU Info
    print(f"\nüñ•Ô∏è   Your Hardware:")
    if gpu_info["available"]:
        if gpu_info["gpu_count"] == 1:
            print(
                f"    {gpu_info['primary_gpu_name']} "
                f"({gpu_info['primary_gpu_vram_mb'] // 1024}GB VRAM)"
            )
        else:
            print(f"    {gpu_info['gpu_count']}x {gpu_info['primary_gpu_name']}")
            print(f"    Total VRAM: {gpu_info['total_vram_mb'] // 1024}GB")
    else:
        print(f"    ‚ùå No NVIDIA GPU detected")

    # Compatibility Table
    print(f"\n" + "=" * 60)
    print(f"üíæ  VRAM Requirements & Compatibility")
    print("=" * 60)

    if compat.get("no_gpu_available"):
        print("\n‚ùå  No GPU available - cannot run this model locally\n")
    else:
        fits_any = False

        for precision in ["fp32", "fp16", "bf16", "int8", "int4", "fp8"]:
            if precision not in vram_reqs:
                continue

            req_info = vram_reqs[precision]
            fit_info = compat["fits_on_single_gpu"][precision]

            required_mb = req_info["vram_mb"]
            required_gb = required_mb / 1024
            source = req_info["source"]
            source_indicator = "" if source == "measured" else "~"

            if fit_info["fits"]:
                free_gb = fit_info["free_vram_mb"] / 1024
                print(
                    f"  ‚úÖ  {precision.upper():5s}: "
                    f"{source_indicator}{required_gb:6.1f}GB ({source:9s}) - "
                    f"{free_gb:5.1f}GB free"
                )
                fits_any = True
            else:
                shortage_gb = fit_info["shortage_mb"] / 1024
                print(
                    f"  ‚ùå  {precision.upper():5s}: "
                    f"{source_indicator}{required_gb:6.1f}GB ({source:9s}) - "
                    f"Need {shortage_gb:5.1f}GB more"
                )

        # Overall status
        print("\n" + "=" * 60)
        if fits_any:
            print("‚úÖ  This model WILL FIT on your GPU!\n")
        else:
            print("‚ùå  This model WON'T FIT on your GPU\n")

    # Recommendations
    if recs:
        print("üí°  Recommendations:")
        print("=" * 60)
        for i, rec in enumerate(recs, 1):
            print(f"{i}. {rec}")
        print()

    # Reference
    if model_info.get("hf_id"):
        print("=" * 60)
        print("üìö  Reference:")
        print(f"    https://huggingface.co/{model_info['hf_id']}")
        print("=" * 60)


def list_models_command():
    """
    List all available models in database.
    """
    from .utilities import VRAMCalculator

    calc = VRAMCalculator()
    models_by_category = calc.list_all_models()

    print("\nüìã  Available Models in Database")
    print("=" * 60)

    category_names = {
        "llm": "üß†  Large Language Models (LLMs)",
        "diffusion": "üé®  Diffusion Models (Image Generation)",
        "audio": "üîä  Audio Models (Speech Recognition)",
        "language": "üìù  Language Models (Text Processing)",
    }

    for category in ["llm", "diffusion", "audio", "language"]:
        if category not in models_by_category:
            continue

        print(f"\n{category_names.get(category, category.upper())}")
        print("-" * 60)

        for model in sorted(
            models_by_category[category], key=lambda x: x["params_b"]
        ):
            name = model["name"]
            params = model["params_b"]
            print(f"  ‚Ä¢ {name:30s} ({params:6.1f}B params)")

    print("\n" + "=" * 60)
    print("üí°  Usage:")
    print("    env-doctor model <model-name>")
    print("    env-doctor model <model-name> --precision int4")
    print("\nüìñ  To add models:")
    print("    See docs/ADDING_MODELS.md")
    print("=" * 60 + "\n")


# Update main() to add new command
def main():
    """Main entry point with argument parsing."""
    # Enable UTF-8 output on Windows
    import sys
    if sys.platform == "win32":
        # Configure stdout to use UTF-8
        import io
        if sys.stdout.encoding != "utf-8":
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

    parser = argparse.ArgumentParser(
        description="env-doctor: The AI Environment Fixer",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  env-doctor check              # Diagnose your environment
  env-doctor cuda-info          # Detailed CUDA toolkit analysis
  env-doctor cudnn-info         # Detailed cuDNN library analysis
  env-doctor dockerfile         # Validate Dockerfile for GPU issues
  env-doctor docker-compose     # Validate docker-compose.yml for GPU issues
  env-doctor model llama-3-8b   # Check if model fits on your GPU
  env-doctor model --list       # List all available models
  env-doctor install torch      # Get safe install command for PyTorch
  env-doctor scan               # Scan project for AI library imports
  env-doctor debug              # Show detailed detector information
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Check command
    check_parser = subparsers.add_parser(
        "check",
        help="Diagnose environment compatibility"
    )
    check_parser.add_argument(
        '--json',
        action='store_true',
        help='Output as JSON (machine-readable)'
    )
    check_parser.add_argument(
        '--ci',
        action='store_true',
        help='CI-friendly mode (implies --json with proper exit codes)'
    )

    # CUDA Info command (NEW)
    cuda_info_parser = subparsers.add_parser(
        "cuda-info",
        help="Detailed CUDA toolkit analysis"
    )
    cuda_info_parser.add_argument(
        '--json',
        action='store_true',
        help='Output as JSON (machine-readable)'
    )

    # cuDNN Info command (NEW)
    cudnn_info_parser = subparsers.add_parser(
        "cudnn-info",
        help="Detailed cuDNN library analysis"
    )
    cudnn_info_parser.add_argument(
        '--json',
        action='store_true',
        help='Output as JSON (machine-readable)'
    )

    # Dockerfile validation command (NEW)
    dockerfile_p = subparsers.add_parser(
        "dockerfile",
        help="Validate Dockerfile for GPU/CUDA configuration issues"
    )
    dockerfile_p.add_argument(
        "path",
        nargs="?",
        default="Dockerfile",
        help="Path to Dockerfile (default: ./Dockerfile)"
    )

    # Docker Compose validation command (NEW)
    compose_p = subparsers.add_parser(
        "docker-compose",
        help="Validate docker-compose.yml for GPU configuration issues"
    )
    compose_p.add_argument(
        "path",
        nargs="?",
        default="docker-compose.yml",
        help="Path to docker-compose.yml (default: ./docker-compose.yml)"
    )

    # Install command
    install_p = subparsers.add_parser(
        "install",
        help="Get safe installation command for a library"
    )
    install_p.add_argument(
        "library",
        help="Library name (e.g., torch, tensorflow, jax)"
    )

    # Scan command
    scan_parser = subparsers.add_parser(
        "scan",
        help="Scan local files for AI library imports"
    )
    scan_parser.add_argument(
        '--json',
        action='store_true',
        help='Output as JSON (machine-readable)'
    )
    
    # Debug command
    subparsers.add_parser(
        "debug",
        help="Show detailed detector information (for troubleshooting)"
    )

    # Model command
    model_p = subparsers.add_parser(
        "model",
        help="Check if AI model fits on your GPU"
    )
    model_p.add_argument(
        "model_name",
        nargs="?",
        help="Model name (e.g., llama-3-8b, stable-diffusion-xl)"
    )
    model_p.add_argument(
        "--precision",
        choices=["fp32", "fp16", "bf16", "int8", "int4", "fp8"],
        help="Check specific precision (default: show all)"
    )
    model_p.add_argument(
        "--list",
        action="store_true",
        help="List all available models"
    )

    args = parser.parse_args()

    # Route to appropriate command
    if args.command == "check":
        check_command(
            output_json=getattr(args, 'json', False),
            ci=getattr(args, 'ci', False)
        )
    elif args.command == "cuda-info":
        cuda_info_command(
            output_json=getattr(args, 'json', False)
        )
    elif args.command == "cudnn-info":
        cudnn_info_command(
            output_json=getattr(args, 'json', False)
        )
    elif args.command == "dockerfile":
        dockerfile_command(args.path)
    elif args.command == "docker-compose":
        docker_compose_command(args.path)
    elif args.command == "model":
        if args.list:
            list_models_command()
        elif args.model_name:
            model_command(args.model_name, args.precision)
        else:
            model_p.print_help()
    elif args.command == "install":
        install_command(args.library)
    elif args.command == "scan":
        scan_command(
            output_json=getattr(args, 'json', False)
        )
    elif args.command == "debug":
        debug_command()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()