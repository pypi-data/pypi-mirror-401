Metadata-Version: 2.4
Name: chunkrank
Version: 0.2.3
Summary: Model-aware text chunking and answer re-ranking for LLM pipelines. Automatically adapts chunk size to tokenizer and context window, then consolidates and ranks answers across chunks.
License: MIT
License-File: LICENCE
Author: Your Name
Author-email: you@example.com
Requires-Python: >=3.14,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.14
Requires-Dist: deptry (>=0.24.0,<0.25.0)
Requires-Dist: mypy (>=1.19.1,<2.0.0)
Requires-Dist: numpy (>=1.26)
Requires-Dist: rank-bm25 (>=0.2.2)
Requires-Dist: regex (>=2023.10.3)
Requires-Dist: ruff (>=0.14.10,<0.15.0)
Requires-Dist: scikit-learn (>=1.5)
Project-URL: Documentation, https://gitlab.com/gaafa/smart-chunks/-/blob/main/README.md
Project-URL: Issue Tracker, https://gitlab.com/gaafa/smart-chunks/-/issues
Project-URL: Source Code, https://gitlab.com/gaafa/smart-chunks
Description-Content-Type: text/markdown

# ChunkRank: Model-Aware Chunking + Answer Ranking
```
Used internally for long-document QA and evaluation pipelines handling 1,000+ PDFs.
```
```
ChunkRank is a lightweight Python library that automatically chunks 
text based on an LLMâ€™s tokenizer and context window, then consolidates
and ranks answers across chunks. In short ChunkRank is a model-aware text 
chunking and answer re-ranking library for LLM pipelines.
```

ðŸ”— PyPI : https://pypi.org/project/chunkrank/

---

## Why ChunkRank?

When working with LLMs, long documents must be split into chunks, but:
- Every model has **different tokenizers and context limits**
- Chunk sizes are usually **hard-coded and error-prone**
- Answer quality drops when responses come from **multiple chunks**
- Existing RAG frameworks are **heavy** when you only need chunking + ranking

**ChunkRank solves this gap.**

---

## What It Does

âœ…**Model-aware chunking**  
- Pass a model name (`gpt-4o-mini`, `claude-3.5-sonnet`, `Llama-3.1-8B` etc.)   
- ChunkRank automatically:
  - Selects the correct tokenizer
  - Applies the correct context window
  - Reserves token space for prompts and responses

No manual token math. No trial-and-error.
  
âœ…**Answer consolidation & ranking**  
- Query runs across multiple chunks
- Multiple candidate answers are produced
- ChunkRank **re-ranks** them to return the best answer
Works standalone â€” no full RAG stack required.

---

## Installation

```bash
pip install chunkrank
```
or for development:
```bash
poetry install

```
## Quick Example

``` python
from chunkrank import ChunkRankPipeline

text = open("document.txt").read()

pipe = ChunkRankPipeline(model="gpt-4o-mini")

answer = pipe.process(
    question="What is the main topic of this document?",
    text=text
)

print(answer)
```
---


## Core API

``` python
chunks = chunkrank.split(text, model="gpt-4o-mini")

answers = chunkrank.answer(question, chunks)

best_answer = chunkrank.rank(answers)
```
---


## Supported Capabilities

- Automatic model â†’ tokenizer â†’ context resolution
- Token, sentence, and paragraph chunking strategies
- Cross-encoder based answer re-ranking
- Works with OpenAI, Anthropic, HF, Llama-based models
- Drop-in utility for QA, summarization, extraction

---

## How It Fits

| Tool | What it does |
|------|-------------|
| LangChain / LlamaIndex | Full RAG pipelines |
| Haystack | End-to-end retrieval frameworks |
| **ChunkRank** | Focused, model-aware chunking + answer ranking |

**ChunkRank complements RAG frameworks â€” it doesnâ€™t replace them.**

---
## Roadmap
1. Build the **model registry** (model â†’ context window + tokenizer).  
2. Implement **chunking strategies** (tokens, sentences, paragraphs).  
3. Integrate a **re-ranking engine** (start with Hugging Face cross-encoder).  
4. Package and release to PyPI with a simple API.  
---

## Community

- [Contributors](CONTRIBUTORS.md)
- [Maintainers](MAINTAINERS.md)

---

