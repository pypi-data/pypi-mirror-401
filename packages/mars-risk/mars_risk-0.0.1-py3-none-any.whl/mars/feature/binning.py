from joblib import Parallel, delayed
from typing import List, Dict, Optional, Union, Any, Literal, Tuple

import numpy as np
import polars as pl
from sklearn.tree import DecisionTreeClassifier

from mars.core.base import MarsTransformer
from mars.utils.logger import logger

class MarsNativeBinner(MarsTransformer):
    """
    [æé€Ÿåˆ†ç®±å¼•æ“] MarsNativeBinner
    
    å®Œå…¨åŸºäº Polars å’Œ Sklearn åŸç”Ÿå®ç°çš„é«˜æ€§èƒ½åˆ†ç®±å™¨ã€‚
    é’ˆå¯¹å¤§è§„æ¨¡å®½è¡¨ (å¦‚ 2000+ ç‰¹å¾, 20ä¸‡+ æ ·æœ¬) è¿›è¡Œäº†å†…å­˜å’Œé€Ÿåº¦çš„æè‡´ä¼˜åŒ–ã€‚
    
    æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ (Performance Strategies)
    -------------------------------------
    1. **Quantile/Uniform**: 
       åˆ©ç”¨çº¯ Polars è¡¨è¾¾å¼è¿›è¡Œæ ‡é‡èšåˆè®¡ç®—ï¼Œé¿å…äº† Python å¾ªç¯å’Œæ•°æ®å¤åˆ¶ï¼ŒFit é€Ÿåº¦æå‡ 100xã€‚
    2. **Decision Tree (DT)**: 
       ä½¿ç”¨ `joblib` è¿›è¡Œå¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒï¼Œé€šè¿‡ç”Ÿæˆå™¨æƒ°æ€§ä¼ è¾“æ•°æ®ï¼Œå¤§å¹…é™ä½å†…å­˜å³°å€¼ï¼ŒFit é€Ÿåº¦æå‡ N_Cores å€ã€‚
    3. **Transform**: 
       ä½¿ç”¨ Polars çš„ `cut` å’Œ `when-then` è¡¨è¾¾å¼è¿›è¡Œæ˜ å°„ï¼Œè½¬æ¢é˜¶æ®µå®ç°æ¯«ç§’çº§å“åº”ã€‚

    Attributes
    ----------
    bin_cuts_ : Dict[str, List[float]]
        è®­ç»ƒåå­˜å‚¨çš„åˆ‡ç‚¹å­—å…¸ã€‚æ ¼å¼: ``{col_name: [-inf, split1, split2, ..., inf]}``ã€‚
    """

    def __init__(
        self,
        features: Optional[List[str]] = None,
        method: Literal["cart", "quantile", "uniform"] = "quantile",
        n_bins: int = 5,
        special_values: Optional[List[Union[int, float, str]]] = None,
        missing_values: Optional[List[Union[int, float, str]]] = None,
        min_samples: float = 0.05,
        n_jobs: int = -1  
    ) -> None:
        """
        åˆå§‹åŒ–åˆ†ç®±å™¨ã€‚

        Parameters
        ----------
        features : List[str], optional
            éœ€è¦åˆ†ç®±çš„ç‰¹å¾åç§°åˆ—è¡¨ã€‚å¦‚æœä¸ä¼ ï¼Œfit æ—¶ä¼šè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ•°å€¼å‹åˆ—ã€‚
        method : Literal["cart", "quantile", "uniform"], default="quantile"
            åˆ†ç®±æ–¹æ³•ï¼š
            - 'cart': å†³ç­–æ ‘åˆ†ç®± (Decision Tree)ï¼Œæœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šï¼Œä¾èµ– targetã€‚
            - 'quantile': ç­‰é¢‘åˆ†ç®± (Quantile)ï¼Œæ¯ä¸ªç®±å­æ ·æœ¬æ•°å¤§è‡´ç›¸ç­‰ã€‚
            - 'uniform': ç­‰å®½åˆ†ç®± (Uniform)ï¼Œæ¯ä¸ªç®±å­åŒºé—´è·¨åº¦ç›¸ç­‰ã€‚
        n_bins : int, default=5
            æœŸæœ›çš„åˆ†ç®±æ•°é‡ (ä¸åŒ…å«ç‰¹æ®Šå€¼å’Œç¼ºå¤±å€¼ç®±)ã€‚
        special_values : List[Union[int, float, str]], optional
            ç‰¹æ®Šå€¼åˆ—è¡¨ (å¦‚ -999, -998)ã€‚è¿™äº›å€¼å°†ä¸å‚ä¸æ•°å€¼è®¡ç®—ï¼Œå¹¶è¢«å•ç‹¬åˆ†ä¸ºç‹¬ç«‹ç®±ã€‚
        missing_values : List[Union[int, float, str]], optional
            ç¼ºå¤±å€¼åˆ—è¡¨ (å¦‚ -1, None)ã€‚è¿™äº›å€¼å°†è¢«å½’ç±»ä¸º "Missing"ã€‚
        min_samples : float, default=0.05
            ä»…å¯¹ method='cart' æœ‰æ•ˆã€‚å†³ç­–æ ‘å¶å­èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ¯”ä¾‹ï¼Œç”¨äºæ§åˆ¶è¿‡æ‹Ÿåˆã€‚
        n_jobs : int, default=-1
            ä»…å¯¹ method='cart' æœ‰æ•ˆã€‚å¹¶è¡Œä½œä¸šçš„æ ¸å¿ƒæ•°ï¼Œ-1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ¸å¿ƒã€‚
        """
        super().__init__()
        self.features: Optional[List[str]] = features
        self.method: str = method
        self.n_bins: int = n_bins
        self.special_values: List[Any] = special_values if special_values is not None else []
        self.missing_values: List[Any] = missing_values if missing_values is not None else []
        self.min_samples: float = min_samples
        self.n_jobs: int = n_jobs
        
        # å­˜å‚¨è®­ç»ƒå¥½çš„åˆ‡ç‚¹: {col: [-inf, split1, split2, ..., inf]}
        self.bin_cuts_: Dict[str, List[float]] = {}

    def _fit_impl(self, X: pl.DataFrame, y: Optional[Any] = None, **kwargs) -> None:
        """
        è®­ç»ƒå®ç°çš„å…¥å£å‡½æ•°ã€‚
        """
        # 1. ç¡®å®šç›®æ ‡åˆ— (ä»…ç­›é€‰æ•°å€¼åˆ—)
        target_cols = self.features if self.features else X.columns
        target_cols = [c for c in target_cols if c in X.columns and self._is_numeric(X[c])]

        if not target_cols:
            logger.warning("No numeric columns found for binning.")
            return

        # ========================================================
        # [ä¼˜åŒ–] æé€Ÿé¢„è¿‡æ»¤ 
        # ========================================================
        valid_cols = []
        n_rows = X.height
        
        # 1. æ„å»ºæ‰¹é‡è¡¨è¾¾å¼ (ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç»Ÿè®¡é‡)
        stats_exprs = []
        for c in target_cols:
            stats_exprs.append(pl.col(c).null_count().alias(f"{c}_null"))
            stats_exprs.append(pl.col(c).min().alias(f"{c}_min"))
            stats_exprs.append(pl.col(c).max().alias(f"{c}_max"))
            
        # 2. è§¦å‘å¹¶è¡Œè®¡ç®— (One-Shot)
        stats_row = X.select(stats_exprs).row(0)
        
        # 3. è§£æç»“æœ
        for i, c in enumerate(target_cols):
            base_idx = i * 3
            null_cnt = stats_row[base_idx]
            min_val = stats_row[base_idx + 1]
            max_val = stats_row[base_idx + 2]
            
            # Case A: å…¨ç©ºåˆ— -> è·³è¿‡
            #   è¿™ç§åˆ—æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œåˆ†ç®±æ²¡æœ‰æ„ä¹‰
            if null_cnt == n_rows:
                logger.warning(f"Feature '{c}' is all null. Skipped.")
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue
            
            # Case B: å•ä¸€å€¼æ£€æŸ¥ (Constant Value)
            #   åªæœ‰å½“ min == max ä¸” æ²¡æœ‰ç©ºå€¼ æ—¶ï¼Œæ‰æ˜¯çœŸæ­£çš„"å•ä¸€å€¼"ã€‚
            #   å¦‚æœ min == max ä½†æœ‰ç©ºå€¼ (å¦‚ [1, 1, null])ï¼Œå®ƒå®é™…ä¸Šæ˜¯äºŒå€¼ç‰¹å¾ (1 vs Missing)ï¼Œå¿…é¡»ä¿ç•™ï¼
            if min_val == max_val and null_cnt == 0:
                logger.warning(f"Feature '{c}' has constant value ({min_val}) and no nulls. Skipped.")
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue

            # å…¶ä»–æƒ…å†µ (åŒ…æ‹¬ [1, 1, null]) å…¨éƒ¨ä¿ç•™
            valid_cols.append(c)

        if not valid_cols:
            logger.warning("No valid features remain after null check.")
            return

        # 2. æ£€æŸ¥ä¾èµ–å…³ç³»
        if y is None and self.method == "cart":
            raise ValueError("Decision Tree Binning ('cart') requires target 'y'.")

        logger.info(f"âš™ï¸ Fitting bins for {len(valid_cols)} features (Native Mode: {self.method})...")

        # 3. ç­–ç•¥åˆ†å‘ (åªä¼ å…¥æœ‰æ•ˆåˆ—)
        if self.method == "quantile":
            self._fit_quantile(X, valid_cols)
        elif self.method == "uniform":
            self._fit_uniform(X, valid_cols)
        elif self.method == "cart":
            self._fit_cart_parallel(X, y, valid_cols)
        else:
            raise ValueError(f"Unknown method: {self.method}")

    def _fit_quantile(self, X: pl.DataFrame, cols: List[str]) -> None:
        """
        æ‰§è¡Œæé€Ÿç­‰é¢‘åˆ†ç®± (Quantile Binning)ã€‚
        
        æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ Polars è¡¨è¾¾å¼ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åˆ—çš„åˆ†ä½æ•°ï¼Œé¿å… Python å¾ªç¯ã€‚
        æ³¨æ„ï¼šä¸ºäº†é¿å… Polars å¤„ç† `List(Float)` ç±»å‹æ—¶çš„å¼€é”€ï¼Œé‡‡ç”¨äº†ç”Ÿæˆå¤šä¸ªæ ‡é‡è¡¨è¾¾å¼çš„æ–¹æ³•ã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è¾“å…¥æ•°æ®ã€‚
        cols : List[str]
            éœ€è¦è®¡ç®—çš„æ•°å€¼åˆ—åˆ—è¡¨ã€‚
        """
        # 1. æ„å»ºåˆ†ä½ç‚¹ (ä¸åŒ…å« 0% å’Œ 100%)
        if self.n_bins <= 1:
            quantiles = [0.5]
        else:
            quantiles = np.linspace(0, 1, self.n_bins + 1)[1:-1].tolist()
        
        # 2. å‡†å¤‡è¿‡æ»¤ç‰¹æ®Šå€¼çš„é€»è¾‘ (None åœ¨ Polars ä¸­è‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€åŒ…å«)
        exclude_vals = self.special_values + self.missing_values
        exclude_vals_clean = [v for v in exclude_vals if v is not None]

        # 3. æ„å»ºè¡¨è¾¾å¼åˆ—è¡¨ (Flattened)
        # å°† "Nåˆ— x Mä¸ªåˆ†ä½æ•°" æ‹†è§£æˆ N*M ä¸ªç‹¬ç«‹çš„æ ‡é‡è¡¨è¾¾å¼
        # ä¾‹å¦‚: feature_a:::0 (20%), feature_a:::1 (40%)...
        q_exprs = []

        for c in cols:
            target_col = pl.col(c)
            # å¦‚æœå­˜åœ¨ç‰¹æ®Šå€¼ï¼Œä½¿ç”¨ when-then å°†å…¶è§†ä¸º Null (Polars quantile ä¼šè‡ªåŠ¨å¿½ç•¥ Null)
            if exclude_vals_clean:
                target_col = pl.when(pl.col(c).is_in(exclude_vals_clean)).then(None).otherwise(pl.col(c))
            
            # ä¸ºæ¯ä¸ªåˆ†ä½ç‚¹ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„è¡¨è¾¾å¼
            for i, q in enumerate(quantiles):
                alias_name = f"{c}:::{i}"
                q_exprs.append(target_col.quantile(q).alias(alias_name))
        
        # 4. è§¦å‘è®¡ç®— (One-Shot Query)
        # Polars å¼•æ“ä¼šå¹¶è¡Œä¼˜åŒ–è¿™äº›æ ‡é‡èšåˆè®¡ç®—
        stats = X.select(q_exprs)
        row = stats.row(0)
        
        # 5. è§£æç»“æœ
        # å°†æ‰å¹³çš„ç»“æœé‡ç»„å› {col: [cuts]} ç»“æ„
        temp_cuts: Dict[str, List[float]] = {c: [] for c in cols}
        
        # row æ˜¯ tupleï¼Œstats.columns æ˜¯åˆ—ååˆ—è¡¨
        for val, name in zip(row, stats.columns):
            c_name, _ = name.split(":::")
            # [Fix] å¢åŠ å¯¹ NaN çš„è¿‡æ»¤
            # val is not None: è¿‡æ»¤ Polars çš„ null
            # not np.isnan(val): è¿‡æ»¤ numpy çš„ nan
            if val is not None and not np.isnan(val):
                temp_cuts[c_name].append(val)

        # 6. æœ€ç»ˆå°è£…
        for c in cols:
            # å»é‡å¹¶æ’åºï¼Œæ·»åŠ  -inf å’Œ inf
            cuts = sorted(list(set(temp_cuts[c]))) 
            self.bin_cuts_[c] = [float('-inf')] + cuts + [float('inf')]

    def _fit_uniform(self, X: pl.DataFrame, cols: List[str]) -> None:
        """
        æ‰§è¡Œæé€Ÿç­‰å®½åˆ†ç®± (Uniform/Step Binning)ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. **ä½åŸºæ•°æ£€æŸ¥**ï¼šè‹¥å”¯ä¸€å€¼æ•°é‡ <= n_binsï¼Œç›´æ¥æŒ‰å”¯ä¸€å€¼åˆ‡åˆ†ï¼Œé¿å…ç©ºç®±ã€‚
        2. **ç©ºç®±åˆå¹¶**ï¼šè®¡ç®—å‡ºåˆ‡ç‚¹åï¼Œç«‹å³æ ¡éªŒå„ç®±æ ·æœ¬æ•°ï¼Œè‡ªåŠ¨å‰”é™¤ Count=0 çš„åŒºé—´ã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è¾“å…¥æ•°æ®ã€‚
        cols : List[str]
            éœ€è¦è®¡ç®—çš„æ•°å€¼åˆ—åˆ—è¡¨ã€‚
        """
        exclude_vals = [v for v in (self.special_values + self.missing_values) if v is not None]
        
        # 1. æ„å»ºèšåˆè¡¨è¾¾å¼ (è®¡ç®—æ¯åˆ—çš„ min, max ä»¥åŠ approx_n_unique)
        exprs = []
        for c in cols:
            target_col = pl.col(c)
            if exclude_vals:
                target_col = target_col.filter(~pl.col(c).is_in(exclude_vals))
            
            exprs.append(target_col.min().alias(f"{c}_min"))
            exprs.append(target_col.max().alias(f"{c}_max"))
            # ä½¿ç”¨ approx_n_unique å¿«é€Ÿä¼°ç®—åŸºæ•° (æ€§èƒ½è¿œé«˜äº n_unique)
            exprs.append(target_col.n_unique().alias(f"{c}_n_unique"))

        # 2. è§¦å‘è®¡ç®— (One-Shot)
        stats = X.select(exprs)
        row = stats.row(0)
        
        # 3. è§£æç»“æœå¹¶ç”Ÿæˆåˆ‡ç‚¹
        for i, c in enumerate(cols):
            # stats ç»“æ„: [c1_min, c1_max, c1_nu, c2_min, c2_max, c2_nu, ...]
            base_idx = i * 3
            min_val = row[base_idx]
            max_val = row[base_idx + 1]
            n_unique = row[base_idx + 2]
            
            # å¼‚å¸¸å¤„ç†ï¼šå…¨ç©º
            if min_val is None or max_val is None:
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue
            
            # --- ä¼˜åŒ–1: ä½åŸºæ•°æ£€æŸ¥ ---
            # å¦‚æœå”¯ä¸€å€¼å¾ˆå°‘ï¼Œç›´æ¥æŸ¥è¯¢å‡ºæ‰€æœ‰å”¯ä¸€å€¼ä½œä¸ºåˆ‡åˆ†ä¾æ®
            if n_unique <= self.n_bins:
                # è¿™é‡Œéœ€è¦é¢å¤–æŸ¥ä¸€æ¬¡è¯¥åˆ—çš„å…·ä½“å”¯ä¸€å€¼ (å› ä¸ºå‰é¢åªæŸ¥äº†æ•°é‡)
                # è¿™ç§æ“ä½œä»…åœ¨ä½åŸºæ•°æ—¶è§¦å‘ï¼Œå¼€é”€æå°
                unique_vals = X.select(pl.col(c).unique().sort()).to_series().to_list()
                # è¿‡æ»¤ç‰¹æ®Šå€¼
                clean_vals = [v for v in unique_vals if v not in exclude_vals and v is not None]
                
                if len(clean_vals) <= 1:
                    self.bin_cuts_[c] = [float('-inf'), float('inf')]
                else:
                    # å–ç›¸é‚»å€¼çš„ä¸­é—´ç‚¹: (1, 2, 3) -> (1.5, 2.5)
                    mid_points = [(clean_vals[k] + clean_vals[k+1])/2 for k in range(len(clean_vals)-1)]
                    self.bin_cuts_[c] = [float('-inf')] + mid_points + [float('inf')]
                continue

            # --- å¸¸è§„é€»è¾‘: ç­‰å®½åˆ‡åˆ† ---
            if min_val == max_val:
                self.bin_cuts_[c] = [float('-inf'), float('inf')]
                continue

            step = (max_val - min_val) / self.n_bins
            # ç”Ÿæˆåˆå§‹åˆ‡ç‚¹
            # raw_cuts = [min_val + step * k for k in range(1, self.n_bins)]
            raw_cuts = np.linspace(min_val, max_val, self.n_bins + 1)[1:-1].tolist()
            
            # --- ä¼˜åŒ–2: ç©ºç®±åˆå¹¶ (Post-Optimization) ---
            # å³ä½¿æ˜¯è¿ç»­å˜é‡ï¼Œç­‰å®½ä¹Ÿå¯èƒ½åˆ‡å‡ºç©ºç®±ã€‚æˆ‘ä»¬éœ€è¦æ ¹æ®æ•°æ®åˆ†å¸ƒä¿®æ­£åˆ‡ç‚¹ã€‚
            full_cuts = [float('-inf')] + raw_cuts + [float('inf')]
            optimized_cuts = self._remove_empty_bins(X, c, full_cuts, exclude_vals)
            
            self.bin_cuts_[c] = optimized_cuts

    def _remove_empty_bins(self, X: pl.DataFrame, col: str, cuts: List[float], exclude_vals: List[Any]) -> List[float]:
        """
        [å†…éƒ¨æ–¹æ³•] ç§»é™¤æ ·æœ¬æ•°ä¸º 0 çš„ç©ºç®±å­ (Empty Bin Pruning)ã€‚
        
        åº”ç”¨åœºæ™¯ï¼š
        é€šå¸¸ç”¨äºç­‰å®½åˆ†ç®± (Uniform Binning) åå¤„ç†ã€‚ç”±äºæ•°æ®åˆ†å¸ƒä¸å‡ï¼Œç­‰å®½åˆ‡åˆ†ææ˜“äº§ç”Ÿ
        ä¸­é—´æ²¡æœ‰æ ·æœ¬çš„"ç©ºæ¡£"ã€‚è¯¥æ–¹æ³•ä¼šè¯†åˆ«å¹¶åˆå¹¶è¿™äº›ç©ºæ¡£ã€‚

        åˆå¹¶ç­–ç•¥ï¼š
        ----------------
        å¦‚æœå‘ç°æŸåŒºé—´ `(cuts[i], cuts[i+1]]` çš„ count ä¸º 0 (ç©ºç®±)ï¼š
        1. æˆ‘ä»¬é€‰æ‹©**ç§»é™¤è¯¥åŒºé—´çš„å³è¾¹ç•Œ** `cuts[i+1]`ã€‚
        2. è§†è§‰æ•ˆæœä¸Šï¼Œè¿™ç›¸å½“äºå½“å‰ç©ºç®±è¢«"å‘å³åˆå¹¶"åˆ°äº†ä¸‹ä¸€ä¸ªç®±å­ä¸­ï¼Œæˆ–è€…è¯´å½“å‰åŒºé—´çš„
           åˆ†å‰²çº¿å¤±æ•ˆäº†ï¼Œä¸¤ä¸ªåŒºé—´è¿é€šäº†ã€‚
        3. è¿™ç§ç­–ç•¥èƒ½æœ€å¤§ç¨‹åº¦ä¿æŒåˆ‡ç‚¹çš„è¿ç»­æ€§ï¼Œä¸”å®ç°é€»è¾‘ç®€å•é«˜æ•ˆã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è¾“å…¥æ•°æ®è¡¨ã€‚
        col : str
            ç›®æ ‡åˆ—åã€‚
        cuts : List[float]
            åŸå§‹åˆ‡ç‚¹åˆ—è¡¨ (åŒ…å« -inf å’Œ inf)ã€‚
        exclude_vals : List[Any]
            ä¸å‚ä¸åˆ†ç®±ç»Ÿè®¡çš„ç‰¹æ®Šå€¼åˆ—è¡¨ã€‚

        Returns
        -------
        List[float]
            ä¼˜åŒ–åçš„ã€å»é™¤äº†ç©ºç®±è¾¹ç•Œçš„åˆ‡ç‚¹åˆ—è¡¨ã€‚
        """
        # 1. å‡†å¤‡ä¸­é—´åˆ‡ç‚¹ (breaks)
        # cuts åŒ…å«äº† -inf å’Œ infï¼Œä½† Polars çš„ cut/hist å‡½æ•°åªéœ€è¦ä¸­é—´çš„åˆ†å‰²ç‚¹
        breaks = cuts[1:-1]
        
        # å¦‚æœæ²¡æœ‰ä¸­é—´åˆ‡ç‚¹ (å³åªæœ‰ [-inf, inf])ï¼Œè¯´æ˜åªæœ‰1ç®±ï¼Œç›´æ¥è¿”å›
        if not breaks:
            return cuts

        # 2. æ„å»ºç›®æ ‡åˆ—çš„è¿‡æ»¤è¡¨è¾¾å¼
        # æˆ‘ä»¬åªç»Ÿè®¡"æœ‰æ•ˆå€¼"çš„åˆ†å¸ƒï¼Œå¿½ç•¥ç‰¹æ®Šå€¼
        target_col = pl.col(col)
        if exclude_vals:
            target_col = target_col.filter(~pl.col(col).is_in(exclude_vals))

        # 3. æé€Ÿç›´æ–¹å›¾ç»Ÿè®¡ (Histogram Calculation)
        # ç›¸æ¯” Python for å¾ªç¯ï¼Œåˆ©ç”¨ Polars è¡¨è¾¾å¼å¼•æ“è®¡ç®—åˆ†å¸ƒå¿« 100 å€ä»¥ä¸Šã€‚
        # 
        # é€»è¾‘åˆ†è§£ï¼š
        # a. cut(breaks): å°†æ•°æ®æ˜ å°„åˆ°å¯¹åº”çš„åŒºé—´ç´¢å¼• (0, 1, 2...)
        # b. group_by("bin_idx"): æŒ‰åŒºé—´åˆ†ç»„
        # c. len(): ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°
        bin_counts = (
            X.select(
                target_col.cut(breaks, labels=[str(i) for i in range(len(breaks)+1)], left_closed=True)
                .alias("bin_idx")
            )
            .group_by("bin_idx")
            .len()
            .sort("bin_idx")
        )
        
        # bin_counts ç»“æœç¤ºä¾‹ (å¯èƒ½åŒ…å« null):
        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ bin_idx â”† len   â”‚
        # â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡
        # â”‚ "0"     â”† 150   â”‚
        # â”‚ "1"     â”† 0     â”‚ <-- ç©ºç®±
        # â”‚ null    â”† 10    â”‚ <-- NaN æˆ– è„æ•°æ®å¯¼è‡´çš„ null (è¿™å°±æ˜¯æŠ¥é”™æ ¹æºï¼)
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
        
        # 4. æå–æœ‰æ•ˆç®±ç´¢å¼• (Valid Indices Extraction)
        # [Critical Fix]: å¿…é¡»å¢åŠ  is_not_null() è¿‡æ»¤ã€‚
        # åŸå› ï¼šå½“æ•°æ®åŒ…å« NaN æ—¶ï¼Œcut ç®—å­ä¼šå°†å…¶æ˜ å°„ä¸º nullã€‚å¦‚æœä¸è¿‡æ»¤ï¼Œ
        # åœ¨åç»­ int(idx) è½¬æ¢æ—¶ä¼šæŠ›å‡º "TypeError: int() argument must be... not 'NoneType'"ã€‚
        
        valid_indices = set(
            int(idx) for idx in bin_counts.filter(
                (pl.col("len") > 0) & 
                (pl.col("bin_idx").is_not_null())  # <--- æ ¸å¿ƒä¿®å¤ç‚¹
            )["bin_idx"].to_list()
        )
        
        # 5. é‡æ„åˆ‡ç‚¹ (Reconstruct Cuts)
        # åŸå§‹ cuts: [-inf, c1, c2, c3, inf]
        # å¯¹åº”çš„ç®±:    Bin0, Bin1, Bin2, Bin3
        # 
        # é€»è¾‘ï¼š
        # æˆ‘ä»¬éå†æ‰€æœ‰å¯èƒ½çš„ç®±ç´¢å¼• iã€‚å¦‚æœ Bin(i) æ˜¯æœ‰æ•ˆçš„ (åœ¨ valid_indices é‡Œ)ï¼Œ
        # æˆ‘ä»¬å°±ä¿ç•™å®ƒçš„**å³è¾¹ç•Œ** (cuts[i+1])ã€‚
        # å¦‚æœ Bin(i) æ˜¯ç©ºçš„ï¼Œæˆ‘ä»¬å°±è·³è¿‡å®ƒçš„å³è¾¹ç•Œï¼Œä»è€Œå®ç°åˆå¹¶ã€‚
        
        new_cuts = [cuts[0]] # å§‹ç»ˆä¿ç•™ -inf
        
        for i in range(len(breaks) + 1):
            # i ä»£è¡¨ç®±å­ç´¢å¼• (0 åˆ° N-1)
            if i in valid_indices:
                # åªæœ‰å½“ç®±å­ä¸ä¸ºç©ºæ—¶ï¼Œæ‰ä¿ç•™è¿™ä¸ªç®±å­çš„ç»“æŸè¾¹ç•Œ
                new_cuts.append(cuts[i+1])
            else:
                # ç®±å­ä¸ºç©º (count=0)ï¼Œè·³è¿‡æ·»åŠ  cuts[i+1]ã€‚
                # æ•ˆæœï¼šå½“å‰ç®±å­çš„ç©ºé—´åˆå¹¶åˆ°äº†ä¸‹ä¸€ä¸ªç®±å­ä¸­ã€‚
                pass
                
        # 6. å…œåº•å¤„ç† (Finalize)
        # ç¡®ä¿ inf æ€»æ˜¯å­˜åœ¨ã€‚å¦‚æœæœ€åä¸€ä¸ªç®±å­ä¹Ÿæ˜¯ç©ºçš„ï¼Œä¸Šé¢çš„å¾ªç¯é€»è¾‘å¯èƒ½ä¼šæ¼æ‰ infã€‚
        if new_cuts[-1] != float('inf'):
            new_cuts.append(float('inf'))
            
        # å»é‡å¹¶æ’åºï¼Œç¡®ä¿åˆ‡ç‚¹ä¸¥æ ¼å•è°ƒé€’å¢
        return sorted(list(set(new_cuts)))

    def _fit_cart_parallel(self, X: pl.DataFrame, y: Any, cols: List[str]) -> None:
        """
        æ‰§è¡Œå¹¶è¡Œçš„å†³ç­–æ ‘åˆ†ç®± (Decision Tree Binning)ã€‚

        åŸç†ï¼š
        1. å°†æ¯ä¸€åˆ—æ•°æ®å’Œ Target è½¬æ¢ä¸º Numpy æ•°ç»„ã€‚
        2. ä½¿ç”¨ `joblib` å°†ä»»åŠ¡åˆ†å‘åˆ°å¤šä¸ª CPU æ ¸å¿ƒã€‚
        3. æ¯ä¸ªå­è¿›ç¨‹ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªå•ç‰¹å¾çš„ `DecisionTreeClassifier`ã€‚
        4. æå–æ ‘çš„é˜ˆå€¼ä½œä¸ºåˆ‡ç‚¹ã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è¾“å…¥æ•°æ®ã€‚
        y : Any
            ç›®æ ‡å˜é‡ (Label)ã€‚
        cols : List[str]
            éœ€è¦è®¡ç®—çš„æ•°å€¼åˆ—åˆ—è¡¨ã€‚
        """
        # 1. å‡†å¤‡ Y (åªéœ€è½¬æ¢ä¸€æ¬¡ï¼Œå‡å°‘å†…å­˜å¤åˆ¶)
        y_np = np.array(y)
        if len(y_np) != X.height:
             raise ValueError("Target 'y' length mismatch.")

        # 2. å®šä¹‰ Worker å‡½æ•° (å¿…é¡»æ˜¯æ— å‰¯ä½œç”¨çš„çº¯å‡½æ•°æˆ–é—­åŒ…)
        # è¯¥å‡½æ•°å°†åœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­è¿è¡Œ
        def worker(col_name: str, col_data_np: np.ndarray) -> tuple:
            try:
                # A. è¿‡æ»¤ Mask (å»é™¤ Special/Missing/NaN)
                # æ³¨æ„ï¼šnp.isin å¯¹ None çš„å¤„ç†æ¯”è¾ƒæ£˜æ‰‹ï¼Œå»ºè®®å…ˆå¤„ç† NaN
                mask_nan = np.isnan(col_data_np)
                
                # æ ‡è®° Special/Missing (ä¸å« Noneï¼Œå› ä¸º numpy float array é‡Œ None ä¹Ÿæ˜¯ NaN)
                ignore_vals = [v for v in (self.special_values + self.missing_values) if v is not None]
                
                if ignore_vals:
                    mask_ignore = np.isin(col_data_np, ignore_vals)
                    mask_valid = ~(mask_nan | mask_ignore)
                else:
                    mask_valid = ~mask_nan
                
                clean_X = col_data_np[mask_valid].reshape(-1, 1)
                clean_y = y_np[mask_valid]
                
                # B. è¾¹ç•Œæ£€æŸ¥ï¼šæœ‰æ•ˆæ•°æ®å¤ªå°‘åˆ™ä¸åˆ†ç®±
                if len(clean_X) < 100:
                    return col_name, [float('-inf'), float('inf')]
                
                # C. è®­ç»ƒ Sklearn DT
                cart = DecisionTreeClassifier(
                    max_leaf_nodes=self.n_bins,
                    min_samples_leaf=self.min_samples,
                    random_state=42
                )
                cart.fit(clean_X, clean_y)
                
                # D. æå–é˜ˆå€¼
                # threshold ä¸­ -2 è¡¨ç¤ºå¶å­èŠ‚ç‚¹ï¼Œéœ€è¦è¿‡æ»¤
                cuts = cart.tree_.threshold[cart.tree_.threshold != -2]
                cuts = np.sort(np.unique(cuts))
                
                full_cuts = [float('-inf')] + list(cuts) + [float('inf')]
                return col_name, full_cuts
            
            except Exception as e:
                # å®¹é”™ï¼šå•ä¸ªç‰¹å¾å¤±è´¥ä¸å½±å“æ•´ä½“
                return col_name, [float('-inf'), float('inf')]

        # 3. å‡†å¤‡æ•°æ®ç”Ÿæˆå™¨
        # å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨ generator æƒ°æ€§è·å–æ•°æ®ã€‚
        # é¿å…è°ƒç”¨ `X.to_numpy()` ä¸€æ¬¡æ€§æŠŠæ•´ä¸ªå¤§å®½è¡¨è½¬å…¥å†…å­˜ï¼Œè€Œæ˜¯æ¯æ¬¡åªå–ä¸€åˆ—ã€‚
        task_gen = (
            (c, X.select(c).to_series().to_numpy()) 
            for c in cols
        )
        
        logger.info(f"ğŸš€ Starting parallel DT fitting with n_jobs={self.n_jobs}...")
        
        # 4. å¹¶è¡Œæ‰§è¡Œ
        # backend="loky" æ˜¯ joblib çš„é»˜è®¤åç«¯ï¼Œå¯¹å¤§æ•°æ®ä¼ è¾“æœ‰ä¼˜åŒ– (memmap)
        results = Parallel(n_jobs=self.n_jobs, backend="loky")(
            delayed(worker)(c, data) for c, data in task_gen
        )
        
        # 5. æ”¶é›†ç»“æœ
        for c, cuts in results:
            self.bin_cuts_[c] = cuts

    def _transform_impl(self, X: pl.DataFrame) -> pl.DataFrame:
        """
        é¢„æµ‹é˜¶æ®µï¼šåº”ç”¨åˆ†ç®±è§„åˆ™ã€‚
        
        é€»è¾‘é¡ºåº (Waterfall)ï¼š
        1. Missing Layer: åŒ¹é… null/nan åŠç”¨æˆ·å®šä¹‰çš„ç¼ºå¤±å€¼ã€‚
        2. Special Layer: åŒ¹é…ç”¨æˆ·å®šä¹‰çš„ç‰¹æ®Šå€¼ (å¦‚ -999)ã€‚
        3. Normal Layer: å¯¹å‰©ä½™æ•°å€¼è¿›è¡ŒåŒºé—´åˆ‡åˆ†ã€‚

        Returns
        -------
        pl.DataFrame
            åŒ…å«åŸå§‹åˆ—å’Œæ–°ç”Ÿæˆçš„åˆ†ç®±åˆ— (`{col}_bin`) çš„ DataFrameã€‚
        """
        exprs = []
        
        for col, cuts in self.bin_cuts_.items():
            if col not in X.columns: continue
            
            # --- Layer 1: Missing Bin (ä¼˜å…ˆçº§æœ€é«˜) ---
            missing_condition = pl.col(col).is_null() | pl.col(col).is_nan()
            
            # è¿½åŠ ç”¨æˆ·å®šä¹‰çš„ç¼ºå¤±å€¼ (å¦‚ -1)
            for val in self.missing_values:
                if val is None: continue
                missing_condition = missing_condition | (pl.col(col) == val)
            
            layer_missing = pl.when(missing_condition).then(pl.lit("Missing"))
            
            # --- Layer 2: Special Bin (ä¼˜å…ˆçº§ç¬¬äºŒ) ---
            layer_special = pl.when(False).then(pl.lit("None")) # åˆå§‹åŒ–ç©ºåˆ†æ”¯
            
            for val in self.special_values:
                if val is None: continue
                label = f"Special_{val}"
                layer_special = layer_special.when(pl.col(col) == val).then(pl.lit(label))
            
            # --- Layer 3: Normal Bin (ä¼˜å…ˆçº§æœ€ä½) ---
            breaks = cuts[1:-1]
            if not breaks:
                layer_normal = pl.lit("00_[-inf, inf)")
            else:
                # ç”Ÿæˆå¯è¯»æ€§å¼ºçš„æ ‡ç­¾: 00_[l, r), 01_[l, r)...
                labels = []
                for i in range(len(cuts) - 1):
                    low, high = cuts[i], cuts[i+1]
                    
                    # æ ¼å¼åŒ–æ•°å€¼ (å»é™¤å¤šä½™çš„.000)
                    low_str = "-inf" if low == float('-inf') else f"{low:.3f}".rstrip('0').rstrip('.')
                    high_str = "inf" if high == float('inf') else f"{high:.3f}".rstrip('0').rstrip('.')
                    
                    # è¡¥é½å°æ•°ç‚¹æœ«å°¾å¯èƒ½è¢«åˆ æ‰çš„æƒ…å†µ (å¦‚ 25. -> 25)
                    if low_str.endswith('.'): low_str = low_str[:-1]
                    if high_str.endswith('.'): high_str = high_str[:-1]

                    labels.append(f"{i:02d}_[{low_str}, {high_str})")
                
                # ä½¿ç”¨ Polars çš„ cut ç®—å­è¿›è¡Œå¿«é€ŸäºŒåˆ†æŸ¥æ‰¾æ˜ å°„
                layer_normal = pl.col(col).cut(breaks, labels=labels, left_closed=True).cast(pl.Utf8)
            
            # --- ç»„è£…ç€‘å¸ƒæµ ---
            final_expr = (
                layer_missing
                .otherwise(
                    layer_special.otherwise(layer_normal)
                )
                .alias(f"{col}_bin")
            )
            
            exprs.append(final_expr)

        return X.with_columns(exprs)

    def _is_numeric(self, series: pl.Series) -> bool:
        """
        åˆ¤æ–­ Polars Series æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹ã€‚
        
        Returns
        -------
        bool
            å¦‚æœæ˜¯æ•´å‹æˆ–æµ®ç‚¹å‹è¿”å› Trueï¼Œå¦åˆ™ Falseã€‚
        """
        return series.dtype in [
            pl.Int8, pl.Int16, pl.Int32, pl.Int64, 
            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, 
            pl.Float32, pl.Float64
        ]
        
        
class MarsOptimalBinner(MarsNativeBinner):
    """
    [æ··åˆåŠ¨åŠ›åˆ†ç®±å¼•æ“] MarsOptimalBinner

    è¯¥ç±»å®ç°äº†åŸºäºæ··åˆåŠ¨åŠ›æ¶æ„ (Hybrid Engine) çš„æœ€ä¼˜åˆ†ç®±ç®—æ³•ã€‚
    
    è®¾è®¡ç›®æ ‡ï¼š
    è§£å†³ä¼ ç»Ÿ OptBinning åœ¨å¤§è§„æ¨¡æ•°æ®ï¼ˆå¦‚ 20ä¸‡è¡Œ x 2000åˆ—ï¼‰ä¸Šç›´æ¥æ±‚è§£ MIP (æ··åˆæ•´æ•°è§„åˆ’) 
    å¯¼è‡´çš„è®¡ç®—æ€§èƒ½ç“¶é¢ˆï¼ŒåŒæ—¶ä¿ç•™å…¶æ•°å­¦è§„åˆ’å¸¦æ¥çš„æœ€ä¼˜æ€§å’Œå•è°ƒæ€§çº¦æŸèƒ½åŠ›ã€‚

    æ ¸å¿ƒæ¶æ„ (Architecture):
    -----------------------
    1. **Numeric Pipeline (æ•°å€¼å‹ç‰¹å¾)**: "ä¸¤é˜¶æ®µç«ç®­" æ¨¡å¼
       - **Stage 1 (Pre-binning)**: åˆ©ç”¨ Polars è¿›è¡Œæé€Ÿåˆ†ä½æ•°/ç­‰å®½é¢„åˆ†ç®± (O(N))ã€‚
         å°†åŸå§‹æ•°æ®ç¦»æ•£åŒ–ä¸ºç»†ç²’åº¦ (å¦‚ 50 ç®±) çš„å€™é€‰åŒºé—´ã€‚
       - **Stage 2 (Optimization)**: å°†é¢„åˆ†ç®±åˆ‡ç‚¹æ³¨å…¥ OptBinning (MIP Solver)ã€‚
         åˆ©ç”¨çº¦æŸç¼–ç¨‹ (CP) æ±‚è§£æ»¡è¶³å•è°ƒæ€§çº¦æŸçš„æœ€ä¼˜åˆå¹¶æ–¹æ¡ˆ (O(1))ã€‚
    
    2. **Categorical Pipeline (ç±»åˆ«å‹ç‰¹å¾)**:
       - **Pre-filtering**: å¯¹é«˜åŸºæ•°ç‰¹å¾è¿›è¡Œ Top-K è¿‡æ»¤ï¼Œå°†é•¿å°¾ç±»åˆ«å½’å¹¶ä¸º "Other_Pre"ã€‚
       - **Optimization**: è°ƒç”¨ OptBinning å¤„ç†ç±»åˆ«åˆå¹¶ã€‚

    Attributes
    ----------
    bin_cuts_ : Dict[str, List[float]]
        æ•°å€¼å‹ç‰¹å¾çš„æœ€ä¼˜åˆ‡ç‚¹å­—å…¸ã€‚
        æ ¼å¼: ``{col: [-inf, c1, c2, ..., inf]}``
    
    cat_cuts_ : Dict[str, List[List[Any]]]
        ç±»åˆ«å‹ç‰¹å¾çš„åˆ†ç®±è§„åˆ™å­—å…¸ã€‚
        æ ¼å¼: ``{col: [['A', 'B'], ['C'], ['D']]}``ï¼Œè¡¨ç¤º Aå’ŒB å½’ä¸ºç®±0ï¼ŒC å½’ä¸ºç®±1...
    """

    def __init__(
        self,
        features: Optional[List[str]] = None,
        cat_features: Optional[List[str]] = None,
        n_bins: int = 5,
        n_prebins: int = 50,
        prebinning_method: Literal["quantile", "uniform", "cart"] = "quantile",
        monotonic_trend: str = "auto_asc_desc",
        solver: str = "cp",
        time_limit: int = 10,
        special_values: Optional[List[Union[int, float, str]]] = None,
        missing_values: Optional[List[Union[int, float, str]]] = None,
        cat_cutoff: int = 100,
        n_jobs: int = -1  
    ) -> None:
        """
        åˆå§‹åŒ–æ··åˆåŠ¨åŠ›åˆ†ç®±å™¨ã€‚

        Parameters
        ----------
        features : List[str], optional
            æ‰€æœ‰éœ€è¦åˆ†ç®±çš„ç‰¹å¾åç§°åˆ—è¡¨ã€‚
        
        cat_features : List[str], optional
            æ˜¾å¼æŒ‡å®šå“ªäº›ç‰¹å¾æ˜¯ç±»åˆ«å‹ (Categorical)ã€‚
            æœªåœ¨æ­¤åˆ—è¡¨ä¸­çš„ç‰¹å¾å°†è¢«è‡ªåŠ¨è¯†åˆ«å¹¶è§†ä¸ºæ•°å€¼å‹ã€‚
        
        n_bins : int, default=5
            æœ€ç»ˆæœŸæœ›çš„æœ€ä¼˜ç®±æ•° (Max bins)ã€‚
        
        n_prebins : int, default=50
            [æ€§èƒ½å…³é”®] é¢„åˆ†ç®±çš„ç»†ç²’åº¦ã€‚
            æ•°å€¼è¶Šå¤§ï¼ŒSolver çš„æœç´¢ç©ºé—´è¶Šå¤§ï¼Œç»“æœè¶Šæ¥è¿‘ç†è®ºæœ€ä¼˜ï¼Œä½†è€—æ—¶å¢åŠ ã€‚å»ºè®® 20-50ã€‚
        
        prebinning_method : Literal["quantile", "uniform"], default="quantile"
            é¢„åˆ†ç®±æ–¹æ³•ã€‚'quantile' é€‚åˆé•¿å°¾åˆ†å¸ƒï¼Œ'uniform' é€‚åˆå‡åŒ€åˆ†å¸ƒã€‚
        
        monotonic_trend : str, default="auto_asc_desc"
            å•è°ƒæ€§çº¦æŸç±»å‹ (auto, ascending, descending, convex, concave)ã€‚
        
        solver : str, default="cp"
            æ•°å­¦è§„åˆ’æ±‚è§£å™¨ã€‚'cp' (Constraint Programming) é€šå¸¸æ¯” 'mip' æ›´å¿«ã€‚
        
        time_limit : int, default=10
            å•ä¸ªç‰¹å¾çš„æ±‚è§£è¶…æ—¶æ—¶é—´ (ç§’)ã€‚**è¶…æ—¶åå°†è‡ªåŠ¨å›é€€åˆ°é¢„åˆ†ç®±ç»“æœ**ã€‚
            
        special_values : List, optional
            ç‰¹æ®Šå€¼åˆ—è¡¨ (å¦‚ -999)ã€‚ç‹¬ç«‹æˆç®±ï¼Œä¸å‚ä¸æ•°å­¦è§„åˆ’ã€‚
            
        missing_values : List, optional
            ç¼ºå¤±å€¼åˆ—è¡¨ (å¦‚ -1, None)ã€‚ç‹¬ç«‹æˆç®±ã€‚
            
        cat_cutoff : int, default=100
            ç±»åˆ«ç‰¹å¾é¢„å¤„ç†é˜ˆå€¼ã€‚è‹¥åŸºæ•°è¶…è¿‡æ­¤å€¼ï¼Œä»…ä¿ç•™ Top-K é«˜é¢‘ç±»åˆ«ï¼Œå…¶ä½™å½’ä¸º Otherã€‚
            
        n_jobs : int, default=-1
            å¹¶è¡Œè®¡ç®—çš„æ ¸å¿ƒæ•°ã€‚
        """
        # åˆå§‹åŒ–çˆ¶ç±» MarsNativeBinner (è´Ÿè´£ Stage 1)
        super().__init__(
            features=features,
            method=prebinning_method,
            n_bins=n_bins,
            special_values=special_values,
            missing_values=missing_values,
            n_jobs=n_jobs
        )
        self.cat_features: List[str] = cat_features if cat_features is not None else []
        self.n_prebins: int = n_prebins
        self.monotonic_trend: str = monotonic_trend
        self.solver: str = solver
        self.time_limit: int = time_limit
        self.cat_cutoff: int = cat_cutoff
        
        # ä¸“é—¨å­˜å‚¨ç±»åˆ«ç‰¹å¾çš„åˆ†ç®±è§„åˆ™
        # ç»“æ„: {col_name: [['A', 'B'], ['C'], ['D']]}
        self.cat_cuts_: Dict[str, List[List[Any]]] = {}

        # æ£€æŸ¥ä¾èµ–
        try:
            import optbinning
        except ImportError:
            logger.warning("âš ï¸ 'optbinning' not installed. Optimal binning will fallback to pre-binning.")

    def _fit_impl(self, X: pl.DataFrame, y: Optional[Any] = None, **kwargs) -> None:
        """
        è®­ç»ƒå…¥å£ï¼šå®ç°æ•°å€¼ä¸ç±»åˆ«ç‰¹å¾çš„åˆ†æµå¤„ç†ã€‚

        Parameters
        ----------
        X : pl.DataFrame
            è¾“å…¥ç‰¹å¾æ•°æ®ã€‚
        y : Any
            ç›®æ ‡å˜é‡ (å¿…é¡»æä¾›ä»¥è®¡ç®— IV/WOE)ã€‚
        """
        if y is None:
            raise ValueError("Optimal Binning requires target 'y' to calculate IV/WOE.")

        y_np = np.array(y)
        
        # 1. ç‰¹å¾åˆ†ç±» (è‡ªåŠ¨æ¨æ–­ + ç”¨æˆ·æŒ‡å®š)
        all_target_cols = self.features if self.features else X.columns
        cat_set = set(self.cat_features)
        
        # æ•°å€¼åˆ—: (åœ¨ç›®æ ‡ä¸­) & (ä¸åœ¨ç±»åˆ«ç™½åå•ä¸­) & (ç‰©ç†ç±»å‹æ˜¯æ•°å­—)
        num_cols = [
            c for c in all_target_cols 
            if c not in cat_set and c in X.columns and self._is_numeric(X[c])
        ]
        
        # ç±»åˆ«åˆ—: (åœ¨ç›®æ ‡ä¸­) & (åœ¨ç±»åˆ«ç™½åå•ä¸­)
        cat_cols = [
            c for c in all_target_cols 
            if c in cat_set and c in X.columns
        ]

        if not num_cols and not cat_cols:
            logger.warning("No valid numeric or categorical columns found.")
            return

        # 2. å¹¶è¡Œæµæ°´çº¿æ‰§è¡Œ
        if num_cols:
            self._fit_numerical_pipeline(X, y_np, num_cols)

        if cat_cols:
            self._fit_categorical_pipeline(X, y_np, cat_cols)

    def _fit_numerical_pipeline(self, X: pl.DataFrame, y_np: np.ndarray, num_cols: List[str]) -> None:
        """
        [Pipeline] æ•°å€¼å‹ç‰¹å¾æ··åˆåŠ¨åŠ›å¤„ç†æµæ°´çº¿ã€‚
        
        Process:
            1. Polars Pre-binning -> 2. OptBinning Solver -> 3. Fallback Check
        """
        logger.info(f"ğŸš€ [Numeric] Starting Hybrid Pipeline for {len(num_cols)} features...")
        
        # --- Stage 1: æé€Ÿé¢„åˆ†ç®± (Pre-binning) ---
        # åˆ©ç”¨çˆ¶ç±»èƒ½åŠ›å¿«é€Ÿç”Ÿæˆ n_prebins ä¸ªåˆ‡ç‚¹
        pre_binner = MarsNativeBinner(
            features=num_cols,
            method=self.method, 
            n_bins=self.n_prebins, # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨é¢„åˆ†ç®±ç²’åº¦
            special_values=self.special_values,
            missing_values=self.missing_values,
            n_jobs=self.n_jobs
        )
        pre_binner.fit(X, y_np)
        pre_cuts_map = pre_binner.bin_cuts_

        # ç­›é€‰å‡ºæœ‰æ„ä¹‰çš„åˆ— (åˆ‡ç‚¹æ•° > 2 è¡¨ç¤ºä¸ä»…ä»…æ˜¯ inf)
        active_cols = []
        for col, cuts in pre_cuts_map.items():
            if len(cuts) > 2: 
                active_cols.append(col)
            else:
                # é¢„åˆ†ç®±éƒ½åˆ†ä¸å‡º(å¦‚å•ä¸€å€¼)ï¼Œç›´æ¥ä¿ç•™ç»“æœï¼Œä¸é€å…¥ Solver
                self.bin_cuts_[col] = cuts 

        if not active_cols:
            return

        # --- Stage 2: å¹¶è¡Œä¼˜åŒ– (Optimization) ---
        logger.info(f"ğŸ§  [Numeric] Optimizing {len(active_cols)} features with Solver...")
        
        # å®šä¹‰ Worker
        def num_worker(col: str, pre_cuts: List[float], col_data: np.ndarray) -> Tuple[str, List[float]]:
            # é»˜è®¤å›é€€æ–¹æ¡ˆ
            fallback_res = (col, pre_cuts)
            
            try:
                from optbinning import OptimalBinning
                
                # 1. åŸºç¡€æ–¹å·®æ£€æŸ¥: å¦‚æœæ–¹å·®æå°ï¼ŒSolver å¯èƒ½ä¼šæŠ¥é”™
                valid_mask = ~np.isnan(col_data)
                valid_data = col_data[valid_mask]
                if len(valid_data) < 10 or np.var(valid_data) < 1e-8:
                    return fallback_res

                # 2. æ³¨å…¥ Stage 1 åˆ‡ç‚¹ (User Splits)
                user_splits = np.array(pre_cuts[1:-1]) 
                
                opt = OptimalBinning(
                    name=col, dtype="numerical", solver=self.solver,
                    monotonic_trend=self.monotonic_trend,
                    user_splits=user_splits,  # <--- æ ¸å¿ƒï¼šæ³¨å…¥é¢„åˆ†ç®±
                    max_n_bins=self.n_bins,   # æœ€ç»ˆç›®æ ‡ç®±æ•°
                    time_limit=self.time_limit, 
                    min_bin_size=0.0,         # é¢„åˆ†ç®±å·²æ§åˆ¶ç²’åº¦ï¼Œæ­¤å¤„æ”¾å®½
                    verbose=False
                )
                
                opt.fit(valid_data, y_np[valid_mask])
                
                # 3. çŠ¶æ€æ£€æŸ¥
                if opt.status in ["OPTIMAL", "FEASIBLE"]:
                    return col, [float('-inf')] + list(opt.splits) + [float('inf')]
                
                return fallback_res # çŠ¶æ€å¼‚å¸¸ï¼Œå›é€€

            except Exception:
                # ä»»ä½• Python å¼‚å¸¸éƒ½è§¦å‘å›é€€ï¼Œä¿è¯é²æ£’æ€§
                return fallback_res

        # æ•°æ®ç”Ÿæˆå™¨ (æƒ°æ€§åŠ è½½)
        task_gen = (
            (c, pre_cuts_map[c], X.select(c).to_series().to_numpy()) 
            for c in active_cols
        )
        
        results = Parallel(n_jobs=self.n_jobs, backend="loky")(
            delayed(num_worker)(c, cuts, data) for c, cuts, data in task_gen
        )
        
        for col, cuts in results:
            self.bin_cuts_[col] = cuts

    def _fit_categorical_pipeline(self, X: pl.DataFrame, y_np: np.ndarray, cat_cols: List[str]) -> None:
        """
        [Pipeline] ç±»åˆ«å‹ç‰¹å¾å¤„ç†æµæ°´çº¿ (å¸¦ Top-K ä¼˜åŒ–)ã€‚
        """
        logger.info(f"ğŸ§  [Categorical] Optimizing {len(cat_cols)} features...")

        def cat_worker(col: str, col_data_raw: np.ndarray) -> Tuple[str, Optional[List[List[Any]]]]:
            try:
                from optbinning import OptimalBinning
                col_data = col_data_raw.astype(str)
                
                # --- ğŸš€ Optimization: Top-K Pre-filtering ---
                # å¦‚æœåŸºæ•°è¿‡å¤§ï¼Œå…ˆä¿ç•™ Top-Kï¼Œå…¶ä½™ç½®ä¸º "Other_Pre"
                # é¿å… OptBinning åœ¨é¢„å¤„ç†é˜¶æ®µå¡æ­»
                unique_vals, counts = np.unique(col_data, return_counts=True)
                if len(unique_vals) > self.cat_cutoff:
                    # è·å– Top K çš„ç´¢å¼•
                    top_indices = np.argsort(-counts)[:self.cat_cutoff]
                    top_vals = set(unique_vals[top_indices])
                    
                    # ä½¿ç”¨ Numpy å‘é‡åŒ–æ“ä½œè¿›è¡Œæ›¿æ¢ (ä¸åœ¨ TopK çš„å˜ä¸º "Other_Pre")
                    # è¿™ä¸€æ­¥æ¯”ä¼ ç»™ OptBinning å‡ ä¸‡ä¸ªç±»åˆ«è¦å¿«å¾—å¤š
                    mask_keep = np.isin(col_data, list(top_vals))
                    col_data = np.where(mask_keep, col_data, "Other_Pre")

                # --- Optimization End ---

                opt = OptimalBinning(
                    name=col, dtype="categorical", solver=self.solver,
                    max_n_bins=self.n_bins, 
                    time_limit=self.time_limit,
                    cat_cutoff=0.05, # è¾…åŠ©: è¿›ä¸€æ­¥å½’ç±»ä½é¢‘ (<5%)
                    verbose=False
                )
                opt.fit(col_data, y_np)
                
                if opt.status in ["OPTIMAL", "FEASIBLE"]:
                    return col, opt.splits
                
                return col, None # å¤±è´¥è¿”å› None
            except Exception:
                return col, None

        task_gen = (
            (c, X.select(c).to_series().to_numpy()) 
            for c in cat_cols
        )
        
        results = Parallel(n_jobs=self.n_jobs, backend="loky")(
            delayed(cat_worker)(c, data) for c, data in task_gen
        )
        
        for col, splits in results:
            if splits is not None:
                self.cat_cuts_[col] = splits
            # æ³¨æ„ï¼šè‹¥ splits ä¸º Noneï¼Œè¯¥åˆ—å°†ä¸äº§ç”Ÿ _bin ç»“æœ (Soft Fail)
            # ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œå®ç°ç®€å•çš„ top-n fallback é€»è¾‘

    def _transform_impl(self, X: pl.DataFrame) -> pl.DataFrame:
        """
        [Transform] æé€Ÿé¢„æµ‹å®ç°ã€‚
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. ä½¿ç”¨ Polars `cut` å¤„ç†æ•°å€¼ã€‚
        2. ä½¿ç”¨ `replace` å¤„ç†ç±»åˆ«ï¼Œå¹¶å…¼å®¹ Polars æ–°æ—§ç‰ˆæœ¬ã€‚
        3. ä¸¥æ ¼çš„ç©ºå€¼/ç‰¹æ®Šå€¼åˆ†å±‚é€»è¾‘ (Waterfall Logic)ã€‚

        Returns
        -------
        pl.DataFrame
            åŒ…å«åŸå§‹åˆ—å’Œæ–°ç”Ÿæˆ `{col}_bin` åˆ—çš„æ•°æ®è¡¨ã€‚
        """
        exprs = []
        
        # =====================================================
        # Part A: æ•°å€¼å‹ç‰¹å¾ (Numeric)
        # =====================================================
        for col, cuts in self.bin_cuts_.items():
            if col not in X.columns: continue
            
            # 1. Missing Layer (Priority 1)
            missing_condition = pl.col(col).is_null() | pl.col(col).is_nan()
            for val in self.missing_values:
                if val is not None:
                    missing_condition |= (pl.col(col) == val)
            
            layer_missing = pl.when(missing_condition).then(pl.lit("Missing"))
            
            # 2. Special Layer (Priority 2)
            layer_special = pl.when(False).then(pl.lit("None"))
            for val in self.special_values:
                if val is not None:
                    layer_special = layer_special.when(pl.col(col) == val).then(pl.lit(f"Special_{val}"))
            
            # 3. Normal Layer (Priority 3)
            breaks = cuts[1:-1]
            if not breaks:
                layer_normal = pl.lit("00_[-inf, inf)")
            else:
                # ç”Ÿæˆ Labels: 00_[2.5, 10.0)
                labels = []
                for i in range(len(cuts) - 1):
                    low, high = cuts[i], cuts[i+1]
                    # æ ¼å¼åŒ–ä¼˜åŒ–
                    l_s = "-inf" if low == float('-inf') else f"{low:.3g}"
                    h_s = "inf" if high == float('inf') else f"{high:.3g}"
                    labels.append(f"{i:02d}_[{l_s}, {h_s})")
                
                layer_normal = pl.col(col).cut(breaks, labels=labels, left_closed=True).cast(pl.Utf8)
            
            # ç»„è£…
            exprs.append(
                layer_missing.otherwise(layer_special.otherwise(layer_normal)).alias(f"{col}_bin")
            )

        # =====================================================
        # Part B: ç±»åˆ«å‹ç‰¹å¾ (Categorical)
        # =====================================================
        for col, splits in self.cat_cuts_.items():
            if col not in X.columns: continue

            # 1. æ„å»ºæ˜ å°„å­—å…¸ (Value -> Label)
            mapping_dict = {}
            for i, group in enumerate(splits):
                # ç”Ÿæˆå¯è¯»æ ‡ç­¾: "00_[A,B...]"
                disp_grp = group[:3] if len(group) > 3 else group
                suffix = ",..." if len(group) > 3 else ""
                grp_str = ",".join(str(g) for g in disp_grp) + suffix
                label = f"{i:02d}_[{grp_str}]"
                
                for val in group:
                    mapping_dict[str(val)] = label
            
            target_col = pl.col(col).cast(pl.Utf8)
            
            # 2. Missing Layer
            missing_condition = target_col.is_null()
            for val in self.missing_values:
                if val is not None:
                    missing_condition |= (target_col == str(val))
            layer_missing = pl.when(missing_condition).then(pl.lit("Missing"))
            
            # 3. Special Layer
            layer_special = pl.when(False).then(pl.lit("None"))
            for val in self.special_values:
                if val is not None:
                    layer_special = layer_special.when(target_col == str(val)).then(pl.lit(f"Special_{val}"))
            
            # 4. Normal Layer (Map)
            # ä½¿ç”¨ replace æ˜ å°„ã€‚å¯¹äºæœªè§è¿‡çš„ç±»åˆ«ï¼Œè¿™ä¸€æ­¥ä¿ç•™åŸå€¼ã€‚
            # éšåæˆ‘ä»¬ç”¨ "Other" å¡«å……é‚£äº›æœªè¢«æ˜ å°„çš„å€¼ï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦ä»¥ "bin_prefix" å¼€å¤´æˆ–ç›´æ¥å¡«å……ï¼‰
            
            # æ–¹æ¡ˆï¼šåˆ©ç”¨ replace çš„ return_dtype è¡Œä¸º
            # æ˜ å°„è¡¨ä¸­å­˜åœ¨çš„ -> å˜æˆ Label
            # æ˜ å°„è¡¨ä¸­ä¸å­˜åœ¨çš„ -> ä¿æŒåŸ String
            # æœ€åï¼šå¦‚æœå€¼ä¸åœ¨ mapping_dict.values() ä¸­ï¼Œåˆ™è§†ä¸º Otherã€‚
            # ä½†æ›´ç®€å•çš„åšæ³•æ˜¯ï¼š
            layer_normal = target_col.replace(mapping_dict)
            
            # æ£€æŸ¥æ˜¯å¦æ˜ å°„æˆåŠŸ (Label æ ¼å¼é€šå¸¸æ˜¯ "00_...")
            # ä»»ä½•æ²¡æœ‰å˜æˆ Label çš„ï¼Œéƒ½æ˜¯æœªè§è¿‡çš„ç±»åˆ« -> "Other"
            # è¿™é‡Œçš„é€»è¾‘å‡è®¾åŸå§‹æ•°æ®ä¸åŒ…å«ä¸ Label ç›¸åŒçš„æ ¼å¼
            known_labels = list(set(mapping_dict.values()))
            layer_normal = pl.when(layer_normal.is_in(known_labels)).then(layer_normal).otherwise(pl.lit("Other"))

            exprs.append(
                layer_missing.otherwise(layer_special.otherwise(layer_normal)).alias(f"{col}_bin")
            )

        return X.with_columns(exprs)