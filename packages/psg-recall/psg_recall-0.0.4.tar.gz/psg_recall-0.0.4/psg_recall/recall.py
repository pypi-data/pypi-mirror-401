"""A flexible library for semantic memory retrieval from any knowledge source."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_recall.ipynb.

# %% auto 0
__all__ = ['KnowledgeSource', 'Embedder', 'SentenceTransformerEmbedder', 'OpenAIEmbedder', 'cosine_similarity',
           'SemanticTextSource']

# %% ../nbs/00_recall.ipynb 2
# The minimal interface for any knowledge source is just `search`.
# Read-only sources (like external APIs) don't need load/update methods.

class KnowledgeSource:
    def search(self, query: str) -> list[str]:
        raise NotImplementedError

# %% ../nbs/00_recall.ipynb 3
# Embedders convert text into vectors (arrays of floats) that capture semantic meaning.
# Different providers can be swapped in without changing the rest of the code.

class Embedder:
    def embed(self, text: str) -> list[float]:
        raise NotImplementedError

# %% ../nbs/00_recall.ipynb 4
# Two example embedder implementations:
# - SentenceTransformerEmbedder: runs locally, free, good quality
# - OpenAIEmbedder: API-based, excellent quality, small cost per request

class SentenceTransformerEmbedder(Embedder):
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name)
    
    def embed(self, text: str) -> list[float]:
        return self.model.encode(text).tolist()


class OpenAIEmbedder(Embedder):
    def __init__(self, model: str = 'text-embedding-3-small'):
        from openai import OpenAI
        self.client = OpenAI()
        self.model = model
    
    def embed(self, text: str) -> list[float]:
        response = self.client.embeddings.create(input=text, model=self.model)
        return response.data[0].embedding

# %% ../nbs/00_recall.ipynb 5
# SemanticTextSource: A knowledge source that searches text files using embeddings.
# 
# How it works:
# 1. load() reads all .txt files and converts them to vectors
# 2. search() embeds the query, then finds documents with the highest cosine similarity
#
# Cosine similarity measures how "aligned" two vectors are (1.0 = identical, 0 = unrelated)

import numpy as np
from pathlib import Path

def cosine_similarity(a: list[float], b: list[float]) -> float:
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

class SemanticTextSource(KnowledgeSource):
    def __init__(self, folder: str, embedder: Embedder):
        self.folder = Path(folder)
        self.embedder = embedder
        self.docs = []        # original text
        self.vectors = []     # embeddings
    
    def load(self):
        """Load and embed all documents."""
        for filepath in self.folder.glob('**/*.txt'):
            content = filepath.read_text()
            self.docs.append(content)
            self.vectors.append(self.embedder.embed(content))
        return self  # allow chaining
    
    def search(self, query: str, top_k: int = 3) -> list[str]:
        """Find most similar documents."""
        if not self.vectors:
            raise ValueError("No documents loaded. Call load() first.")
        query_vec = self.embedder.embed(query)
        scores = [cosine_similarity(query_vec, v) for v in self.vectors]
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        return [self.docs[i] for i in top_indices]
